
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="一直使用hadoop但都没有用过hadoop-plugins插件，倒不是看不上这个插件的意思。只是个人感觉使用SecureCRT太好用了。上传一个jar直接一拉进去使用lrzsz（z-moden）就直接搞定了。 但，身边搞hadoop的大部分都使用，今天略有兴致的弄了一弄，把环境hadoop- &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://www.winseliu.com/posts/11">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->

  <!--
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-43198550-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


  -->
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:www.winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/03/27/hadoop-eclipse-plugin-use/">使用hadoop-eclipse-plugin插件</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-27T01:16:00+08:00" pubdate data-updated="true">Wed 2013-03-27 01:16</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>一直使用hadoop但都没有用过hadoop-plugins插件，倒不是看不上这个插件的意思。只是个人感觉使用SecureCRT太好用了。上传一个jar直接一拉进去使用lrzsz（z-moden）就直接搞定了。</p>

<p>但，身边搞hadoop的大部分都使用，今天略有兴致的弄了一弄，把环境hadoop-eclipse-plugins整起来了。在公司本来就使用插件来开发的，同时也看看Run-on-Hadoop的大致的实现。</p>

<h2>环境准备：</h2>

<ul>
<li>eclipse-jee-3.7</li>
<li>jdk7</li>
<li>hadoop-1.0.0</li>
</ul>


<h2>插件编译-导出-安装</h2>

<p>打开eclipse，把hadoop-1.0.0\src\contrib\eclipse-plugin整个工程导入（本来就是一个eclipse的project）。工程的lib目录下面已经包括了hadoop-core-1.0.0.jar包，但是hadoop-core-1.0.0.jar需要其他的依赖的jar包。我把这些依赖的jar从hadoop-1.0.0\lib下面复制到eclipse的/MapReduceTools/lib目录下。</p>

<p>然后，打开eclipse的/MapReduceTools/META-INF/MANIFEST.MF文件，切换到Runtime标签页，在classpath区域点击“Add&hellip;”添加lib下面的所有jar。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2577/d8eb44b7-008d-3bbb-9695-d4d200432100.png" alt="" /></p>

<p>然后，再切换到Overview标签页，导出插件。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2579/d631e742-2d17-3fe8-8fdc-04617b2ff7f6.png" alt="" /></p>

<p>导出以后，把导出的插件拷贝都<strong>eclipse/dropins</strong>目录下。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2568/bfa52d58-6556-3419-8d90-1a129d637b7d.png" alt="" /></p>

<p>然后重启。</p>

<h2>配置环境：</h2>

<ol>
<li><p>设置HADOOP_HOME的位置。</p>

<p> 设置Preferences中的Hadoop Map/Reduce的<strong>Hadoop installation directory</strong>为你hadoop的主目录。我这里是C:\cygwin\home\Winseliu\hadoop-1.0.0（主要是用来把hadoop/lib下的包加入到MapRed Project的classpath）。</p></li>
<li><p>切换到Mapreduce透视图（Window | Open Perspective | other | Map/Reduce），然后新建一个Hadoop Location。操作如图：</p>

<p> <img src="http://dl.iteye.com/upload/attachment/0082/2570/39e0d163-8a83-36ae-9896-50a0fb5e087b.png" alt="" /></p></li>
<li><p>然后，如果集群启动了，就可以看到下图的效果了。</p>

<p> <img src="http://dl.iteye.com/upload/attachment/0082/2572/370005b7-3ccf-30c0-832f-8f987cb8f426.png" alt="" /></p>

<p> mapred插件提供的MapReduce Driver类还是不错的，把从写JobMain繁琐的工作中稍稍解放了一点。</p>

<p> <img src="http://dl.iteye.com/upload/attachment/0082/2581/662d38e3-2196-3daf-814a-1a217d4e9892.png" alt="" /></p></li>
</ol>


<h2>Run-on-Hadoop的实现</h2>

<p>一般我们在设置Job（JobConf）.setJarByClass(WordCount.class)都是设置Main对应的主类。</p>

<p>查看源码，就可以得知。其实最终，设置了conf的<strong>mapred.jar</strong>属性！在提交的job时刻，会把该属性对应的jar拷贝到HDFS，最后发布到每台运行的机器上。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2599/f1c7a584-a703-3850-aaf0-6189e81897fa.png" alt="" /></p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2594/7af83776-ebcd-3783-8101-4b3ec748e6cc.png" alt="" /></p>

<p>所以，也就是说，只要把需要的<strong>class导出为jar</strong>，然后把该<strong>jar对应的路径给mapred.jar的属性</strong>即可。</p>

<p>理着这思路，在源码中有org.apache.hadoop.eclipse.server.JarModule类调用JDT的JarPackageData来处理jar的导出工作。</p>

<p>JarModule类在<code>org.apache.hadoop.eclipse.servers.RunOnHadoopWizard.performFinish()</code>方法中被调用。也就是点击Run-on-Hadoop菜单后弹出的对话框的<strong>Finish按钮</strong>被点击时。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2605/cccce4cc-1056-3315-bcdc-1c9453fdb23a.png" alt="" /></p>

<p>然后，会把该jar的路径写入到core-site.xml的配置文件中。该core-site.xml文件的路径会被加入到classpath，也就说Main方法的Configuration会加载这个配置文件！把conf的目录加入到classpath:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  classPath =
</span><span class='line'>          iConf.getAttribute(
</span><span class='line'>              IJavaLaunchConfigurationConstants.ATTR_CLASSPATH,
</span><span class='line'>              new ArrayList());
</span><span class='line'>  IPath confIPath = new Path(confDir.getAbsolutePath());
</span><span class='line'>      IRuntimeClasspathEntry cpEntry =
</span><span class='line'>          JavaRuntime.newArchiveRuntimeClasspathEntry(confIPath);
</span><span class='line'>      classPath.add(0, cpEntry.getMemento());</span></code></pre></td></tr></table></div></figure>


<p>后面的步骤就和一个普通的java类被调用一样的效果咯！</p>

<p>发布到集群运行的包涉及影响的文件：</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2638/d113ef06-1bbb-387f-9ebe-e7e4580c023b.png" alt="" /></p>

<h2>参考资源：</h2>

<ul>
<li><a href="http://my.oschina.net/zhujinbao/blog/56236">http://my.oschina.net/zhujinbao/blog/56236</a></li>
</ul>


<hr />

<p><a href="http://winseclone.iteye.com/blog/1837035">【原文地址】</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/03/24/pseudo-distributed-hadoop-in-windows/">Windows配置hadoop伪分布式环境(续)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-24T00:00:00+08:00" pubdate data-updated="true">Sun 2013-03-24 00:00</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在前一篇文章中，介绍了一写常见问题的解决方法。</p>

<p>但是，当我重装系统，再次按照<a href="/blog/2012/11/25/windows-install-pseudo-distributed-hadoop1/">前面一篇文章</a>  <strong>安装cygwin和hadoop-1</strong>时，发现伪分布式环境使用mapred时，总是报错。（忘了，但是好像当时没有遇到过这种情况。就当是安装win8送给自己的礼物吧！）。</p>

<p>怀疑了很多东西，配置有问题，重新自定hadoop.tmp.dir，把hadoop-1.1.0换成hadoop-1.0.0等等。</p>

<p>错误日志如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ hhadoop fs -rmr /test/output ; hhadoop jar hadoop-examples-1.0.0.jar wordcount /test/input /test/output
</span><span class='line'>Deleted hdfs://WINSE:9000/test/output
</span><span class='line'>13/03/23 22:46:07 INFO input.FileInputFormat: Total input paths to process : 1
</span><span class='line'>13/03/23 22:46:08 INFO mapred.JobClient: Running job: job_201303232144_0002
</span><span class='line'>13/03/23 22:46:09 INFO mapred.JobClient:  map 0% reduce 0%
</span><span class='line'>13/03/23 22:46:16 INFO mapred.JobClient: Task Id : attempt_201303232144_0002_m_000002_0, Status : FAILED
</span><span class='line'>java.lang.Throwable: Child Error
</span><span class='line'>        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:272)
</span><span class='line'>Caused by: java.io.IOException: Task process exit with nonzero status of -1.
</span><span class='line'>        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:259)
</span><span class='line'>
</span><span class='line'>13/03/23 22:46:16 WARN mapred.JobClient: Error reading task outputhttp://WINSE:50060/tasklog?plaintext=true&attemptid=attempt_201303232144_0002_m_000002_0&filter=stdout
</span><span class='line'>13/03/23 22:46:16 WARN mapred.JobClient: Error reading task outputhttp://WINSE:50060/tasklog?plaintext=true&attemptid=attempt_201303232144_0002_m_000002_0&filter=stderr
</span><span class='line'>13/03/23 22:46:22 INFO mapred.JobClient: Task Id : attempt_201303232144_0002_m_000002_1, Status : FAILED</span></code></pre></td></tr></table></div></figure>


<p>经过不断的修改源码，加入sysout打印，算是最终找出程序出现错误的地方！</p>

<p>org.apache.hadoop.mapred.DefaultTaskController.java #launchTask
org.apache.hadoop.mapred.JvmManager.java #runChild
org.apache.hadoop.mapred.TaskRunner.java #launchJvmAndWait</p>

<p>org.apache.hadoop.fs.FileUtil.java #checkReturnValue
org.apache.hadoop.fs.RawLocalFileSystem.java  #setPermission  #mkdirs</p>

<p>发现在org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(Path)方法中，建立文件的路径方法检查attempt_201303232144_0002_m_000001_0<strong>是否为文件夹</strong>时会失败！</p>

<p>而在cygwin中查看文件属性：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Winseliu@WINSE ~/hadoop/logs/userlogs/job_201303232144_0002
</span><span class='line'>$ ll
</span><span class='line'>总用量 9
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000001_0 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000001_0
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000001_1 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000001_1
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000001_2 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000001_2
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000001_3 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000001_3
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000002_0 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000002_0
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000002_1 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000002_1
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000002_2 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000002_2
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000002_3 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000002_3
</span><span class='line'>-rwxr-xr-x 1 Winseliu None 404 3月  23 22:46 job-acls.xml</span></code></pre></td></tr></table></div></figure>


<p>对于linux来说，这些就是引用到另一个文件夹，它本身应该也是文件夹！但是window的jdk不认识这些东西！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public boolean mkdirs(Path f) throws IOException {
</span><span class='line'>    Path parent = f.getParent();
</span><span class='line'>    File p2f = pathToFile(f);
</span><span class='line'>    return (parent == null || mkdirs(parent)) &&
</span><span class='line'>      (p2f.mkdir() || p2f.isDirectory());
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>所以在判断p2f.isDirectory()返回false，然后会抛出IOException，最终以-1的状态退出Map Child的程序！</p>

<p>其使用org.apache.hadoop.mapred.TaskRunner.prepareLogFiles(TaskAttemptID, boolean)方法来指定输出日志的位置。在最终执行的会在shell命令中把sysout和syserr输出到日志文件中（ $COMMAND  1>>$stdout  2>>$stderr ，本文最后有贴运行时的cmd）。</p>

<p>而userlogs的父目录是使用hadoop.log.dir系统属性来进行配置的！</p>

<p>mapred.DefaultTaskController.launchTask()</p>

<p>|&ndash;mapred.TaskLog.buildCommandLine()</p>

<h2>临时解决方法：</h2>

<p>把hadoop.log.dir定位到真正mapred日志的目录( mapred.local.dir : ${hadoop.tmp.dir}/mapred/local )！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export HADOOP_LOG_DIR=/cluster/mapred/local</span></code></pre></td></tr></table></div></figure>


<p>最终的效果，运行的程序会把日志输出到attempt目录下的stdout,stderr文件中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Winseliu@WINSE /cluster/mapred/local/userlogs/job_201303240035_0001/attempt_201303240035_0001_m_000000_0
</span><span class='line'>$ ll
</span><span class='line'>总用量 6
</span><span class='line'>lrwxrwxrwx  1 Winseliu None   89 3月  24 00:36 attempt_201303240035_0001_m_000000_0 -&gt; /cluster/mapred/local/userlogs/job_201303240035_0001/attempt_201303240035_0001_m_000000_0
</span><span class='line'>----------+ 1 Winseliu None  136 3月  24 00:36 log.index
</span><span class='line'>-rw-r--r--+ 1 Winseliu None    0 3月  24 00:36 stderr
</span><span class='line'>-rw-r--r--+ 1 Winseliu None    0 3月  24 00:36 stdout
</span><span class='line'>----------+ 1 Winseliu None 1238 3月  24 00:36 syslog</span></code></pre></td></tr></table></div></figure>


<p>上面的软链接是调用org.apache.hadoop.mapred.TaskLog.createTaskAttemptLogDir()生成的，本文最后有贴运行时ln命令及参数。</p>

<p>ln当linkname是一个已经存在文件夹时，会在linkname的文件夹下建立一个以targetname作为名称的链接。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Winseliu@WINSE ~/tt
</span><span class='line'>$ mkdir f1
</span><span class='line'>
</span><span class='line'>Winseliu@WINSE ~/tt
</span><span class='line'>$ ln -s f1 f1
</span><span class='line'>
</span><span class='line'>Winseliu@WINSE ~/tt
</span><span class='line'>$ ls -Rl
</span><span class='line'>.:
</span><span class='line'>总用量 0
</span><span class='line'>drwxr-xr-x+ 1 Winseliu None 0 3月  24 13:56 f1
</span><span class='line'>
</span><span class='line'>./f1:
</span><span class='line'>总用量 1
</span><span class='line'>lrwxrwxrwx 1 Winseliu None 2 3月  24 13:56 f1 -&gt; f1
</span></code></pre></td></tr></table></div></figure>


<p>把windows的/cluster映射到cygwin(linux)的/cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Winseliu@WINSE ~/hadoop
</span><span class='line'>$ ll /cygdrive/c | grep cluster
</span><span class='line'>drwxr-xr-x+ 1 Winseliu         None                      0 3月  24 00:08 cluster
</span><span class='line'>
</span><span class='line'>Winseliu@WINSE ~/hadoop
</span><span class='line'>$ ll / | grep cluster
</span><span class='line'>lrwxrwxrwx   1 Winseliu None     19 3月  23 09:39 cluster -&gt; /cygdrive/c/cluster</span></code></pre></td></tr></table></div></figure>


<p>但是，运行wordcount的例子时，还是不正常！查看tasktracker的日志时，发现有String转成Integer的NumberFormatException异常!</p>

<p>修改org.apache.hadoop.mapred.JvmManager.JvmManagerForType.<strong>JvmRunner</strong>.kill()方法。添加pidStr为空字符串的检查！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>String pidStr = jvmIdToPid.get(jvmId);
</span><span class='line'>if (pidStr != null && !pidStr.isEmpty()) {</span></code></pre></td></tr></table></div></figure>


<p>然后，终于看到Finish咯！在/test/output/part-r-00000中也看到了结果。</p>

<h2>其他一些简化处理，即配置文件：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>alias startCluster="~/hadoop/bin/start-all.sh"
</span><span class='line'>alias stopCluster="~/hadoop/bin/stop-all.sh; ~/hadoop/bin/stop-all.sh"
</span><span class='line'>
</span><span class='line'>alias hhadoop="~/hadoop/bin/hadoop"
</span><span class='line'>
</span><span class='line'>Winseliu@WINSE ~
</span><span class='line'>$ ll | grep hadoop
</span><span class='line'>lrwxrwxrwx  1 Winseliu None    12 3月  23 10:44 hadoop -&gt; hadoop-1.0.0
</span><span class='line'>drwx------+ 1 Winseliu None     0 3月  24 00:06 hadoop-1.0.0</span></code></pre></td></tr></table></div></figure>


<p>配置文件：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;!-- core-site.xml --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;fs.default.name&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hdfs://WINSE:9000&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span><span class='line'>&lt;value&gt;/cluster&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- hdfs-site.xml --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.replication&lt;/name&gt;
</span><span class='line'>&lt;value&gt;1&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.permissions&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;None&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.safemode.extension&lt;/name&gt;
</span><span class='line'>&lt;value&gt;1000&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- mapred-site.xml --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;mapred.job.tracker&lt;/name&gt;
</span><span class='line'>&lt;value&gt;WINSE:9001&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;</span></code></pre></td></tr></table></div></figure>


<p>关于查看启动的进程，看可以通过任务管理器来查看：</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/1102/e0e310ee-731c-37c3-8e74-30e425d374f5.png" alt="" /></p>

<p>还可以看看pid的修改时间，来确认服务的启动：
<img src="http://dl.iteye.com/upload/attachment/0082/2365/d6277aa6-8821-354a-831a-b9b518272b10.png" alt="" /></p>

<p>我是经常通过50070和50030来查看的~~ 看到50030的Nodes为1时，就说明集群启动正常了。</p>

<p> 执行时的日志：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cmd : ln -s /cluster/mapred/local\userlogs\job_201303241340_0001\attempt_201303241340_0001_m_000000_0 C:\cluster\mapred\local\userlogs\job_201303241340_0001\attempt_201303241340_0001_m_000000_0
</span><span class='line'>
</span><span class='line'>cmdLine : export HADOOP_CLIENT_OPTS="-Dhadoop.tasklog.taskid=attempt_201303241340_0001_m_000000_0 -Dhadoop.tasklog.iscleanup=false -Dhadoop.tasklog.totalLogFileSize=0"
</span><span class='line'>export SHELL="/bin/bash"
</span><span class='line'>export HADOOP_WORK_DIR="\cluster\mapred\local\taskTracker\Winseliu\jobcache\job_201303241340_0001\attempt_201303241340_0001_m_000000_0\work"
</span><span class='line'>export HOME="/homes/"
</span><span class='line'>export LOGNAME="Winseliu"
</span><span class='line'>export HADOOP_TOKEN_FILE_LOCATION="/cluster/mapred/local/taskTracker/Winseliu/jobcache/job_201303241340_0001/jobToken"
</span><span class='line'>export HADOOP_ROOT_LOGGER="INFO,TLA"
</span><span class='line'>export LD_LIBRARY_PATH="\cluster\mapred\local\taskTracker\Winseliu\jobcache\job_201303241340_0001\attempt_201303241340_0001_m_000000_0\work"
</span><span class='line'>export USER="Winseliu"
</span><span class='line'>
</span><span class='line'>exec '/cygdrive/c/Java/jdk1.7.0_02/jre/bin/java' '-Djava.library.path=/home/Winseliu/hadoop-1.0.0/lib/native/Windows_NT_unknown-x86-32;\cluster\mapred\local\
</span><span class='line'>taskTracker\Winseliu\jobcache\job_201303241340_0001\attempt_201303241340_0001_m_000000_0\work' '-Xmx200m' '-Djava.net.preferIPv4Stack=true' '-Dhadoop.metrics
</span><span class='line'>.log.level=WARN' '-Djava.io.tmpdir=/cluster/mapred/local/taskTracker/Winseliu/jobcache/job_201303241340_0001/attempt_201303241340_0001_m_000000_0/work/tmp' '
</span><span class='line'>-classpath' 'C:\cygwin\home\Winseliu\conf;C:\Java\jdk1.7.0_02\lib\tools.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\;C:\cygwin\home\Winseliu\hadoop-1.0.0\hadoop
</span><span class='line'>-core-1.0.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\asm-3.2.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\aspectjrt-1.6.5.jar;C:\cygwin\home\Winseliu\had
</span><span class='line'>oop-1.0.0\lib\aspectjtools-1.6.5.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-beanutils-1.7.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-be
</span><span class='line'>anutils-core-1.8.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-cli-1.2.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-codec-1.4.jar;C:\cygwin\
</span><span class='line'>home\Winseliu\hadoop-1.0.0\lib\commons-collections-3.2.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-configuration-1.6.jar;C:\cygwin\home\Winseliu\h
</span><span class='line'>adoop-1.0.0\lib\commons-daemon-1.0.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-digester-1.8.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-e
</span><span class='line'>l-1.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-httpclient-3.0.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-lang-2.4.jar;C:\cygwin\home\
</span><span class='line'>Winseliu\hadoop-1.0.0\lib\commons-logging-1.1.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-logging-api-1.0.4.jar;C:\cygwin\home\Winseliu\hadoop-1.0
</span><span class='line'>.0\lib\commons-math-2.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-net-1.4.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\core-3.1.1.jar;C:\cygwin\
</span><span class='line'>home\Winseliu\hadoop-1.0.0\lib\hadoop-capacity-scheduler-1.0.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\hadoop-fairscheduler-1.0.0.jar;C:\cygwin\home\Win
</span><span class='line'>seliu\hadoop-1.0.0\lib\hadoop-thriftfs-1.0.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\hsqldb-1.8.0.10.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jackso
</span><span class='line'>n-core-asl-1.0.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jackson-mapper-asl-1.0.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jasper-compiler-5.5.12.ja
</span><span class='line'>r;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jasper-runtime-5.5.12.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jdeb-0.8.jar;C:\cygwin\home\Winseliu\hadoop-1.0
</span><span class='line'>.0\lib\jersey-core-1.8.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jersey-json-1.8.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jersey-server-1.8.jar;C:\cyg
</span><span class='line'>win\home\Winseliu\hadoop-1.0.0\lib\jets3t-0.6.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jetty-6.1.26.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jetty-
</span><span class='line'>util-6.1.26.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jsch-0.1.42.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\junit-4.5.jar;C:\cygwin\home\Winseliu\hadoo
</span><span class='line'>p-1.0.0\lib\kfs-0.2.2.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\log4j-1.2.15.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\mockito-all-1.8.5.jar;C:\cygwin\
</span><span class='line'>home\Winseliu\hadoop-1.0.0\lib\mysql-connector-java-5.1.10-bin.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\ojdbc6.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\l
</span><span class='line'>ib\oro-2.0.8.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\servlet-api-2.5-20081211.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\slf4j-api-1.4.3.jar;C:\cygwin
</span><span class='line'>\home\Winseliu\hadoop-1.0.0\lib\slf4j-log4j12-1.4.3.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\sqoop-1.4.1-incubating.jar;C:\cygwin\home\Winseliu\hadoop-1.
</span><span class='line'>0.0\lib\xmlenc-0.52.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jsp-2.1\jsp-2.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jsp-2.1\jsp-api-2.1.jar;\cluste
</span><span class='line'>r\mapred\local\taskTracker\Winseliu\jobcache\job_201303241340_0001\jars\classes;\cluster\mapred\local\taskTracker\Winseliu\jobcache\job_201303241340_0001\jar
</span><span class='line'>s;\cluster\mapred\local\taskTracker\Winseliu\jobcache\job_201303241340_0001\attempt_201303241340_0001_m_000000_0\work' '-Dhadoop.log.dir=C:\cluster\mapred\lo
</span><span class='line'>cal' '-Dhadoop.root.logger=INFO,TLA' '-Dhadoop.tasklog.taskid=attempt_201303241340_0001_m_000000_0' '-Dhadoop.tasklog.iscleanup=false' '-Dhadoop.tasklog.tota
</span><span class='line'>lLogFileSize=0' 'org.apache.hadoop.mapred.Child' '127.0.0.1' '49203' 'attempt_201303241340_0001_m_000000_0' 'C:\cluster\mapred\local\userlogs\job_20130324134
</span><span class='line'>0_0001\attempt_201303241340_0001_m_000000_0' '-1682417583'  &lt; /dev/null  1&gt;&gt; /cygdrive/c/cluster/mapred/local/userlogs/job_201303241340_0001/attempt_20130324
</span><span class='line'>1340_0001_m_000000_0/stdout 2&gt;&gt; /cygdrive/c/cluster/mapred/local/userlogs/job_201303241340_0001/attempt_201303241340_0001_m_000000_0/stderr</span></code></pre></td></tr></table></div></figure>


<p>linux的版本的日志目录结构：</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/1222/e1eeb8b1-cc16-3439-bc0f-2758e283012a.png" alt="" /></p>

<hr />

<p><a href="http://winseclone.iteye.com/blog/1835591">【原文地址】</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/03/15/compile-hadoop-source-and-modify-jsp/">编译hadoop的jsp源码</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-15T23:21:00+08:00" pubdate data-updated="true">Fri 2013-03-15 23:21</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>从apache下载的tar.gz的hadoop-1.1.0包中本来就包括了src的源码。可以方便我们查看源码调试。</p>

<p><del>题外话： 从github上下载了最新的hadoop-common的源码，发现hadoop-2.0已经是使用maven管理代码了。</del></p>

<p>在eclipse中新建java project，去掉<strong>Use Default location</strong>的复选框的勾，项目目录为hadoop-1.1.0程序所在的位置。然后点击finish即可。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7334/643c83b4-e123-362e-bc40-d801805584f4.png" alt="" /></p>

<p>完成后，项目下面的lib包，以及Source Folder源码包都已经正确的配置好了。如下图。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7344/64575adf-3ee5-3a83-a184-bcff4f9df4d8.png" alt="" /></p>

<p>编译hadoop的源码，需要用到sed，sh的linux shell命令（根据网上的资料）。安装好了cygwin，把c:\cygwin\bin加入到PATH环境变量。然后直接使用eclipse ant（eclipse自带）编译。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Winseliu@WINSE ~
</span><span class='line'>$ cygcheck -c cygwin
</span><span class='line'>Cygwin Package Information
</span><span class='line'>Package              Version        Status
</span><span class='line'>cygwin               1.7.17-1       OK
</span></code></pre></td></tr></table></div></figure>


<p><img src="http://dl.iteye.com/upload/attachment/0081/7351/e1b2d853-fcaf-3ccc-8663-8f579c67755f.png" alt="" /></p>

<p>由于linux和windows的换行符的不同（同事周帅哥在导数据也遇到这样的问题），直接编译会失败。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7353/29b31fa1-7f02-3979-b72d-fc3019f355dd.png" alt="" /></p>

<p>需要对src/saveVersion.sh的shell文件进行修改：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-  user=`whoami`
</span><span class='line'>+  user=`whoami | tr -d '\r'` </span></code></pre></td></tr></table></div></figure>


<p>然后再编译一次就ok了！</p>

<hr />

<p>经过上面步骤已经可以正确的编译hadoop-core的源码了。</p>

<p>在监控集群的时刻，我们一般都在自己常用的windows系统上面通过50030和50070来了解集群的情况。但是如果没有域名服务器，那，我们就不得不修改hosts文件。在出现访问失败的情况下，我们可以使用ip地址替换URL中对应的hostname来访问，但是比较麻烦。</p>

<p>如果在服务器响应请求的时刻，解析生成html的时刻就已经是ip地址那就最好不过了！
其实，直接看看jsp的源码，修改起来不算太难。把jsp里面的hostname转换为IP地址即可。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7361/951ae5f2-9dc3-31cd-aef3-657608a93e00.png" alt="" /></p>

<p>把上图的hostname通过InetAddress获取转换为IpAddress地址。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-    String namenodeHost = jspHelper.nameNodeAddr.getHostName();
</span><span class='line'>+    String namenodeHost = jspHelper.nameNodeAddr.getAddress().getHostAddress();
</span><span class='line'>
</span><span class='line'>-              InetAddress.getByName(namenodeHost).getCanonicalHostName() + ":" +
</span><span class='line'>+              InetAddress.getByName(namenodeHost).getHostAddress() + ":" +
</span></code></pre></td></tr></table></div></figure>


<p>全部修改完成后，再次运行hadoop-1.1.0 build.xml的ant命令，会调用自定义的jsp-compile把jsp转换成java类保存到build/src目录下面。然后javac再编译build/src目录下的源码。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7370/421dc6d3-4a8a-340a-8bca-705369c0a057.png" alt="" /></p>

<p>如果你只想编译这些jsp，把javac中的srcdir的目录只保留build.src应该就可以咯。</p>

<p>我是直接把build/src作为Source Folder，然后把这个Source Folder下的编译文件放置的特定的目录，然后覆盖原来jar里面的class即可！</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7396/7b3ccd33-936a-30ce-8082-d82f34d768bf.png" alt="" /></p>

<h2>参考：</h2>

<ul>
<li><a href="http://wenku.baidu.com/view/c1ad44323968011ca3009199.html">Hadoop源代码eclipse编译教程</a></li>
</ul>


<hr />

<p><a href="http://winse.iteye.com/blog/1830311">【原文地址】</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/03/02/quickly-build-a-second-hadoop-cluster/">快速搭建第二个hadoop分布式集群环境</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-02T00:06:00+08:00" pubdate data-updated="true">Sat 2013-03-02 00:06</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>万事开头难，第一次搭建集群环境确实是比较难，比较苦恼。但，也不是说第二次搭建集群环境就会容易。</p>

<p>一般，第一次操作我们都会在测试环境中进行，当我们要搭建正式的环境时，是否还要像第一次那样搭建环境呢？
这里提供一种稍稍便捷一点的配置方式来搭建集群，<strong>所有的操作</strong>都在<strong>namenode</strong>上面进行！</p>

<p>192.168.3.100 h100为测试环境的namenode。</p>

<p>将要搭建的环境包括3台机器，已经全部安装好redhat的操作系统：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>192.168.80.81 h81 #namenode
</span><span class='line'>192.168.80.82 h82 #datanode1
</span><span class='line'>192.168.80.83 h83 #datanode2</span></code></pre></td></tr></table></div></figure>


<p>使用SecureCRT工具，root用户登录到新环境namenode。步骤参考，有些步骤需要输入密码(<strong>可以通过expect来模拟</strong>)，不能一次性全部执行。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>## 生成root用户的密钥对
</span><span class='line'>ssh-keygen 
</span><span class='line'>
</span><span class='line'>## 建立到100机器的无密钥登录
</span><span class='line'>ssh-copy-id -i .ssh/id_rsa.pub hadoop@192.168.3.100
</span><span class='line'>
</span><span class='line'>## 拷贝JDK，将加入hadoop用户的环境变量
</span><span class='line'>mkdir -p /opt/java
</span><span class='line'>scp -r hadoop@192.168.3.100:/opt/java/jdk1.6.0_29 /opt/java
</span><span class='line'>
</span><span class='line'>## 把集群的IP和机器名对应加入hosts文件
</span><span class='line'>vi /etc/hosts
</span><span class='line'>
</span><span class='line'># 192.168.80.81 h81
</span><span class='line'># 192.168.80.82 h82
</span><span class='line'># 192.168.80.83 h83
</span><span class='line'>
</span><span class='line'>## 定义常量
</span><span class='line'>namenode='h81'
</span><span class='line'>hosts=`cat /etc/hosts | grep 192.168 | awk '{print $2}'`
</span><span class='line'>
</span><span class='line'>## 修改时间，~~可以日期和时间一起修改的~~
</span><span class='line'>DATE='2013-03-01'
</span><span class='line'>TIME='10:30:00'
</span><span class='line'>
</span><span class='line'>for host in $hosts
</span><span class='line'>do
</span><span class='line'>  ssh $host date -s $DATE
</span><span class='line'>  ssh $host date -s $TIME
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>## 建立namenode到datanodes的无密钥访问，这里需要输入对应datanode的root用户的密码
</span><span class='line'>for host in $hosts
</span><span class='line'>do
</span><span class='line'>  ssh-copy-id -i .ssh/id_rsa.pub $host
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>#### 
</span><span class='line'>for host in $hosts
</span><span class='line'>do
</span><span class='line'>  ## 在集群所有节点上创建hadoop用户，会提示很多次输入密码。可以通过修改/etc/shadow替换密码输入的步骤
</span><span class='line'>  ssh $host useradd hadoop
</span><span class='line'>  ssh $host passwd hadoop
</span><span class='line'>
</span><span class='line'>  ## 拷贝jdk到datanodes
</span><span class='line'>  if [ $host != $namenode ]
</span><span class='line'>  then
</span><span class='line'>      scp /etc/hosts $host:/etc/hosts
</span><span class='line'>      ssh $host mkdir -p /opt/java
</span><span class='line'>      scp -r /opt/java/jdk1.6.0_29 $host:/opt/java 2&gt;&1 &gt; jdk.scp.$host.log & 
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>  ## 修改集群的hostname主机名称
</span><span class='line'>  ssh $host hostname $host
</span><span class='line'>  ssh $host cat /etc/sysconfig/network | sed s/localhost.localdomain/$host/g &gt; /tmp/network && cat /tmp/network &gt; /etc/sysconfig/network
</span><span class='line'>
</span><span class='line'>  ## 创建数据目录，并把权限分配给hadoop
</span><span class='line'>  ssh $host mkdir /opt/cloud
</span><span class='line'>  ssh $host chown hadoop /opt/cloud
</span><span class='line'>
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>## ！切换到hadoop用户
</span><span class='line'>su - hadoop
</span><span class='line'>
</span><span class='line'>## 生成hadoop用户的密钥对
</span><span class='line'>ssh-keygen
</span><span class='line'>
</span><span class='line'>## 在hadoop用户的终端定义变量（root的终端变量获取不到的）
</span><span class='line'>namenode='h81'
</span><span class='line'>hosts=`cat /etc/hosts | grep 192.168 | awk '{print $2}'`
</span><span class='line'>
</span><span class='line'>## 使namenode的hadoop用户无密钥登录到集群各个机器
</span><span class='line'>for host in $hosts
</span><span class='line'>do
</span><span class='line'>  ssh-copy-id -i .ssh/id_rsa.pub $host
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>## 更新hadoop用户的环境变量，
</span><span class='line'>vi .bashrc
</span><span class='line'>
</span><span class='line'># export JAVA_HOME=/opt/java/jdk1.6.0_29
</span><span class='line'># PATH=$JAVA_HOME/bin:/usr/sbin:$PATH
</span><span class='line'># export PATH
</span><span class='line'>
</span><span class='line'>## 修改datanodes的环境环境变量，同时为集群创建必要的目录
</span><span class='line'>for host in $hosts
</span><span class='line'>do
</span><span class='line'>  if [ $host != $namenode ]
</span><span class='line'>  then
</span><span class='line'>      scp .bashrc $host:~/.bashrc
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>  ssh $host source .bashrc
</span><span class='line'>  ssh $host mkdir -p /home/hadoop/cloud/zookeeper
</span><span class='line'>  ssh $host mkdir -p /home/hadoop/pids/katta/pids
</span><span class='line'>  ssh $host mkdir -p /home/hadoop/pids/hadoop/pids 
</span><span class='line'>
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>## 建立namenode下的hadoop用户到192.168.3.100的无密钥访问
</span><span class='line'>ssh-copy-id -i .ssh/id_rsa.pub 192.168.3.100
</span><span class='line'>
</span><span class='line'>## 从100上拷贝集群程序
</span><span class='line'>rsync -vaz --delete  --exclude=logs --exclude=log  192.168.3.100:~/lucene ~/
</span><span class='line'>rsync -vaz --delete  --exclude=logs --exclude=log  192.168.3.100:~/sqoop-1.4.1 ~/
</span><span class='line'>rsync -vaz --delete  --exclude=logs --exclude=log  192.168.3.100:~/zookeeper-3.3.5 ~/
</span><span class='line'>
</span><span class='line'>rsync -vaz --delete  --exclude=logs --exclude=log  192.168.3.100:~/hadoop-1.0.0 ~/
</span><span class='line'>rsync -vaz --delete  --exclude=logs --exclude=log  192.168.3.100:~/hbase-0.92.1 ~/
</span><span class='line'>rsync -vaz --delete  --exclude=logs --exclude=log  192.168.3.100:~/katta-core-0.6.4 ~/
</span><span class='line'>rsync -vaz --delete  --exclude=logs --exclude=log  192.168.3.100:~/lucene-shared-lib ~/
</span><span class='line'>
</span><span class='line'>## 查找配置文件中与测试环境有关的信息
</span><span class='line'>[hadoop@h81 ~]$ find */conf | while read conf; do if grep -E 'h100|192.168.3.100' $conf &gt; /dev/null; then echo $conf;fi;done
</span><span class='line'>hadoop-1.0.0/conf/mapred-site.xml
</span><span class='line'>hadoop-1.0.0/conf/core-site.xml
</span><span class='line'>hadoop-1.0.0/conf/masters
</span><span class='line'>hbase-0.92.1/conf/hbase-site.xml
</span><span class='line'>katta-core-0.6.4/conf/katta.zk.properties
</span><span class='line'>katta-core-0.6.4/conf/masters
</span><span class='line'>lucene/conf/config-env.sh
</span><span class='line'>[hadoop@h81 ~]$ 
</span><span class='line'>
</span><span class='line'>## 替换为新的nameode的hostname
</span><span class='line'>find */conf | while read conf; do if grep -E 'h100|192.168.3.100' $conf &gt; /dev/null; then  cat $conf | sed s/h100/h81/g &gt; /tmp/conf && cat /tmp/conf &gt; $conf ;fi;done
</span><span class='line'>
</span><span class='line'>## 修改其他配置
</span><span class='line'>vi hadoop-1.0.0/conf/slaves
</span><span class='line'>vi hbase-0.92.1/conf/regionservers
</span><span class='line'>vi katta-core-0.6.4/conf/nodes
</span><span class='line'>
</span><span class='line'>## 确认是否还有原有集群的余孽！
</span><span class='line'>find */conf | while read conf; do if grep -E 'h10' $conf &gt; /dev/null; then echo $conf;fi;done
</span><span class='line'>
</span><span class='line'>## 拷贝集群程序到datanodes
</span><span class='line'>for host in $hosts
</span><span class='line'>do
</span><span class='line'>  if [ $host != $namenode ]
</span><span class='line'>  then
</span><span class='line'>      rsync -vaz --delete  --exclude=logs --exclude=log  ~/hadoop-1.0.0 $host:~/ &
</span><span class='line'>      rsync -vaz --delete  --exclude=logs --exclude=log  ~/hbase-0.92.1 $host:~/ &
</span><span class='line'>      rsync -vaz --delete  --exclude=logs --exclude=log  ~/katta-core-0.6.4 $host:~/ &
</span><span class='line'>      rsync -vaz --delete  --exclude=logs --exclude=log  ~/lucene-shared-lib $host:~/ &
</span><span class='line'>  fi
</span><span class='line'>done</span></code></pre></td></tr></table></div></figure>


<p><strong>如果你觉得sed修改麻烦，要备份，在写回！其实有<code>sed -i</code>（&ndash;in-place）参数提供了直接写入的功能。</strong></p>

<p>在批处理文件内容替换时，使用到了临时文件，当然也可以先把文件备份后，再写入新文件中，如下面的方式。
但，先备份再写入新文件 有个缺陷就是原始文件的权限会丢失！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh $host \
</span><span class='line'>mv /etc/sysconfig/network /etc/sysconfig/network.back && \ 
</span><span class='line'>cat /etc/sysconfig/network.back | sed s/localhost.localdomain/$host/g &gt; /etc/sysconfig/network</span></code></pre></td></tr></table></div></figure>


<p>最后你懂的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> hadoop-1.0.0/bin/hadoop namenode -format
</span><span class='line'> hadoop-1.0.0/bin/start-all.sh</span></code></pre></td></tr></table></div></figure>


<p>通过for，scp， ssh， sed， awk，rsync，vi， find，ssh-copy-id， mkdir等命令仅在namenode上完成集群的部署工作。</p>

<p>仅新增节点，又不想修改namenode配置文件！可以用下面的方法启动新节点：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@h101 ~]$ hadoop-1.0.0/bin/hadoop-daemon.sh start datanode
</span><span class='line'>
</span><span class='line'>starting datanode, logging to /home/hadoop/hadoop-1.0.0/libexec/../logs/hadoop-hadoop-datanode-h101.out
</span><span class='line'>
</span><span class='line'>[hadoop@h101 ~]$ hadoop-1.0.0/bin/hadoop-daemon.sh start tasktracker
</span><span class='line'>
</span><span class='line'>starting tasktracker, logging to /home/hadoop/hadoop-1.0.0/libexec/../logs/hadoop-hadoop-tasktracker-h101.out</span></code></pre></td></tr></table></div></figure>


<hr />

<p><a href="http://winse.iteye.com/blog/1820209">【原文地址】</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/02/26/linux-top-command-mannual/">Top命令使用</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-02-26T00:00:00+08:00" pubdate data-updated="true"></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>使用过linux的系统的人，应该都用过top命令。
top集成了系统的许多功能，可以查看时间，查看系统的负载，查看cpu和mem的使用情况，查看系统运行的程序等。</p>

<p>top命令显示界面可以分成3部分：</p>

<ul>
<li>系统总体性能（Summary_Area）</li>
<li>命令输入光标（Message/Prompt line）</li>
<li>任务显示区（Columns Header，Task Area）</li>
</ul>


<h2>常用的命令</h2>

<ul>
<li>[ q ] ，或 [ ctrl + c ]      退出top命令。</li>
<li>[ h ] ，或 [ ? ]             查看帮助，然后可以按ESC回到top界面。</li>
</ul>


<h2>在Top输出界面显示CPU内核数量 - [ 1 ]（数字1）</h2>

<p>top命令默认在一行中显示所有CPUs。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0080/8394/c2b927b9-9f56-39c7-82a7-9842d7096443.png" alt="" /></p>

<p>可以在该交互界面输入 [ 1 ] （数字1），显示当前系统的cpu数量，以及cpus的使用情况。如下图所示。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0080/8396/1544352d-9368-3444-a89b-fa9752417fbc.png" alt="" /></p>

<h2>刷新Top命令界面</h2>

<p>手动刷新可以通过 [ space ] 和 [ enter ] 键来执行。</p>

<p>如果需要修改刷新频率，可以通过命令 [ d ] 或 [ s ] ，然后再输入数字（新的时间），最后键入 [ enter ] 使设置生效。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0080/8401/69556fcc-3ba9-3ea6-8ca5-525dc139e0a3.png" alt="" /></p>

<h2>高亮运行中的进程 - [ b / x / y ]</h2>

<p>输入命令 [ b ] 能开启高亮显示，这个是行列高亮的总开关。(在SSH远程登录时可能需要先输入命令 [ B ] 启动高亮才行)
高亮的行表示运行中的程序，高亮的列为当前数据排序列。</p>

<p>如果还需要对行或列进行控制，可以输入 [ y ] 或 [ x ] 来执行。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0080/8405/28d64d90-d2ea-3e4a-ae3b-dc02dc824bf1.png" alt="" /></p>

<p>还有 [ z ] 命令能改变颜色，但是在远程登录的情况下不起作用。</p>

<h2>显示详细命令和参数- [ c ]</h2>

<p>输入[c] 用来显示 命令路径和其传递的参数。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0080/8407/e0df5c72-a96d-38d7-97be-8fc09bc9b574.png" alt="" /></p>

<h2>修改排序字段</h2>

<p>通过命令 [ M ] 把Task_Area的<strong>排序列</strong>切换到%MEM列， [ N ] 切换为PID， [ P ] 切换到%CPU， [ T ] 切换到Time+。</p>

<p>如果你觉得这些不能满足你，那，你就的自定义。通过 [ F ] / [ O ] （大写的o字母）来选择需要排序的列。小写的 f 和 o 用来选择需要显示的列。
键入 [ F ] 后，会显示所有字段。输入需要排序列前面的字母标识，然后回车即可。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0080/8412/1d0f445e-b6ae-3f08-a3de-df1f76e73d7a.png" alt="" /></p>

<p>在交互界面，可以通过 [ R ] 命令来反转排序。在界面显示的列中还可以通过 [ &lt; ] / [ > ] 直接切换排序列。</p>

<h2>把Top输出切分成多个窗口- [ A ]</h2>

<p>按 [ A ] 后，会显示4个分屏的窗口，使用 [ a ] / [ w ] 可以切换4种状态作为当前状态，然后再按 [ A ] 可使当前状态全屏。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0080/8416/3a62f8c2-50d4-3305-89b1-42e009bb05d5.png" alt="" /></p>

<p>也可以通过输入 [ G ] 命令，再使用数字选择对应的状态即可。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0080/8418/bb906d0f-d4bd-319e-9b22-529114628caf.png" alt="" /></p>

<h2>显示Summary区域的信息</h2>

<p>本来写的是隐藏的，但是作为监控来说，为啥隐藏这些有用的信息呢？
但是，如果默认未显示，可以使用下列的命令显示。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0080/8410/89c2c42a-67fd-31da-ae65-860958a0ec3d.png" alt="" /></p>

<ul>
<li>键入命令 [ l ]（字母L的小写） - 显示/隐藏 系统负载，对应上图的第1行。</li>
<li>键入命令 [ t ] - 显示/隐藏 CPU的状态，对应第2，3行。</li>
<li>键入命令 [ m ] - 显示/隐藏 内存信息，对应第4，5行。</li>
</ul>


<h2>其他不常用的命令</h2>

<ul>
<li>一般使用top都是一起交互方式使用，使用命令行参数 [ -b ] ，可以以类似日志方式（追加）来保存当前系统的运行状态。</li>
<li>如果希望把配置保存起来，作为下次的默认配置，可以使用 [ W ]</li>
<li>使用 [ -u ] / [ u ] / [ p ]来控制监控特定的进程/用户。</li>
<li>使用 [ r ] 来修改程序的优先级别（nice值）</li>
<li>使用 [ k ] 关闭特定pid的程序。</li>
</ul>


<h2>参考资料：</h2>

<ul>
<li><a href="http://www.thegeekstuff.com/2010/01/15-practical-unix-linux-top-command-examples/">Can You Top This? 15 Practical Linux Top Command Examples</a></li>
<li><a href="http://os.51cto.com/art/201108/285581.htm">top具体参数说明</a></li>
<li><a href="http://bbs.linuxtone.org/thread-1684-1-1.html">linux命令详解</a>（很棒）</li>
<li><a href="http://tolywang.itpub.net/post/48/130884">http://tolywang.itpub.net/post/48/130884</a></li>
<li><a href="http://blog.csdn.net/aten_xie/article/details/6564599">http://blog.csdn.net/aten_xie/article/details/6564599</a></li>
</ul>


<hr />

<p><a href="http://winse.iteye.com/blog/1814999">【原文地址】</a></p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/12">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/10">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2014/09/30/docker-ssh-on-centos/">配置ssh登录docker-centos</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/">在windows开发测试mapreduce几种方式</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/10/18/modify-hosts-build-hadoop-cluster-on-docker/">Dnsmasq解决docker集群节点互通问题</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/16/build-and-configuration-spark/">编译配置Spark</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/12/read-spark1-source-starter/">[读码] Spark1.1.0前篇</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/07/thinking/">思考</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/30/docker-ssh-on-centos/">配置ssh登录docker-centos</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/27/docker-start-guide-on-centos/">Docker入门</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/">在windows开发测试mapreduce几种方式</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/12/scala-wordcount-on-hadoop/">Scala Wordcount on Hadoop2</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (25) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (11) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (66)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.2.0</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.98.3</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-0.13.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.4.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'winseliu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
