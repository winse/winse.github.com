
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="弄hadoop总是需要折腾不少机器，单单执行 rsync 就挺折腾人的，有时还要排除部分机器来查看一堆机器使用内存情况，等等。以前都使用 expect 结合 for in 来实现，总归简单用着也觉得还行。 但是最近，升级hadoop、tez、安装ganglia被折腾的不行。复制 for 语句到累， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/23">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/01/25/pdsh-simple-usage/">Pdsh</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-01-25T19:50:35+08:00" pubdate data-updated="true">Mon 2016-01-25 19:50</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>弄hadoop总是需要折腾不少机器，单单执行 <code>rsync</code> 就挺折腾人的，有时还要排除部分机器来查看一堆机器使用内存情况，等等。以前都使用 <code>expect</code> 结合 <code>for in</code> 来实现，总归简单用着也觉得还行。</p>

<p>但是最近，升级hadoop、tez、安装ganglia被折腾的不行。复制 <code>for</code> 语句到累，原来看过 <code>pdsh</code> 的介绍，不过原来就部署4-5台机器，最近查找Ganglia安装问题的博文里面再次 <code>pdsh</code> ，觉得非常亲切和简洁。再次安装使用也就有了本文。</p>

<h2>安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@bigdatamgr1 pdsh-2.29]# umask 0022
</span><span class='line'>[root@bigdatamgr1 pdsh-2.29]# ./configure -h
</span><span class='line'>[root@bigdatamgr1 pdsh-2.29]# ./configure --with-dshgroups  --with-exec --with-ssh 
</span><span class='line'>[root@bigdatamgr1 pdsh-2.29]# make && make install
</span></code></pre></td></tr></table></div></figure>


<p>挺多选项的，用 <code>disgroups</code> 加上 <code>ssh</code> 差不多够用了，以后不够用的时刻再慢慢研究这些选项。</p>

<p>当然更简单的安装方式是使用yum： <code>yum install pdsh -y</code></p>

<h2>简单使用</h2>

<p>使用pdsh管理机器的前提是已经建立了到目标机器的SSH无密钥登录，而建立这N台机器的无秘钥登录还是少不了 <code>expect</code> (当然你愿意一个个输入yes和密码也是OK的)！</p>

<ul>
<li>加载的模块</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 查看，安装的ssh/exec
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -L
</span><span class='line'>
</span><span class='line'># 设置默认使用的模块
</span><span class='line'>[eshore@bigdatamgr1 ~]$ export PDSH_RCMD_TYPE=exec
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -w bigdata[1-2] ssh %h hostname
</span><span class='line'>bigdata2: bigdata2
</span><span class='line'>bigdata1: bigdata1
</span><span class='line'>
</span><span class='line'># 命令行指定模块
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -R ssh -w bigdata1,bigdata2 hostname
</span><span class='line'>bigdata2: bigdata2
</span><span class='line'>bigdata1: bigdata1
</span><span class='line'>
</span><span class='line'># 一个个的指定
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -w ssh:bigdata1,ssh:bigdata2 hostname
</span><span class='line'>bigdata2: bigdata2
</span><span class='line'>bigdata1: bigdata1
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -w ssh:bigdata[1,2] hostname
</span><span class='line'>bigdata2: bigdata2
</span><span class='line'>bigdata1: bigdata1
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>主机加载</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -w bigdata[1-2,5,6-8] -X nodes hostname
</span><span class='line'>bigdata5: bigdata5
</span><span class='line'>bigdata6: bigdata6
</span><span class='line'>bigdata2: bigdata2
</span><span class='line'>bigdata8: bigdata8
</span><span class='line'>bigdata7: bigdata7
</span></code></pre></td></tr></table></div></figure>


<p>pdsh除了使用 <code>-w</code> 来指定主机列表，还可以通过文件来指定，如编译时的 <code>--with-machines</code> ，同时可以通过读取默认的位置的文件来获取。在编译pdsh时可以通过 <code>--with-dshgroups</code> 参数来激活此选项，默认可以将一组主机列表写入一个文件中并放到本地主机的 <code>~/.dsh/group</code> 或 <code>/etc/dsh/group</code> 目录下，这样就可以通过 <code>-g</code> 参数调用了。同时 <code>-X groupname</code> 可以用来排除主机列表中属于groupname组的主机（下面会提到group分组）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ export PDSH_RCMD_TYPE=ssh
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ mkdir -p .dsh/group
</span><span class='line'>[eshore@bigdatamgr1 ~]$ cd .dsh/group/
</span><span class='line'>[eshore@bigdatamgr1 group]$ vi nodes
</span><span class='line'>bigdata1
</span><span class='line'>bigdata3
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -g nodes hostname
</span><span class='line'>bigdata3: bigdata3
</span><span class='line'>bigdata1: bigdata1
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -w bigdata[1-8] -X nodes hostname
</span><span class='line'>bigdata2: bigdata2
</span><span class='line'>bigdata8: bigdata8
</span><span class='line'>bigdata5: bigdata5
</span><span class='line'>bigdata6: bigdata6
</span><span class='line'>bigdata4: bigdata4
</span><span class='line'>bigdata7: bigdata7</span></code></pre></td></tr></table></div></figure>


<p><code>-w</code> 参数也可以用来读取特定文件中的主机列表，同时结合其他规则和进行过滤（具体查看man帮助）。<code>-x</code> 在主机列表基础上进行过滤（提供多一种的方式来实现过滤）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ cat slaves | head -2
</span><span class='line'>bigdata1
</span><span class='line'>bigdata2
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -w ^slaves hostname | head -5
</span><span class='line'>bigdata8: bigdata8
</span><span class='line'>bigdata6: bigdata6
</span><span class='line'>bigdata5: bigdata5
</span><span class='line'>bigdata2: bigdata2
</span><span class='line'>bigdata3: bigdata3
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -w ^slaves,-bigdata[2-8]
</span><span class='line'>pdsh&gt; hostname
</span><span class='line'>bigdata1: bigdata1
</span><span class='line'>pdsh&gt; 
</span><span class='line'>pdsh&gt; exit
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -w ^slaves,-/bigdata.?/
</span><span class='line'>pdsh@bigdatamgr1: no remote hosts specified
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -w ^slaves -x bigdata[1-7] hostname
</span><span class='line'>bigdata8: bigdata8
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>输出格式化</li>
</ul>


<p>当一台主机的输出多余一行时，pdsh输出的内容看起来并不和谐。使用dshbak格式化</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ pdsh -w bigdata[1-2] free -m  | dshbak -c
</span><span class='line'>----------------
</span><span class='line'>bigdata1
</span><span class='line'>----------------
</span><span class='line'>             total       used       free     shared    buffers     cached
</span><span class='line'>Mem:         64405      59207       5198          0        429      31356
</span><span class='line'>-/+ buffers/cache:      27420      36985
</span><span class='line'>Swap:        65535         57      65478
</span><span class='line'>----------------
</span><span class='line'>bigdata2
</span><span class='line'>----------------
</span><span class='line'>             total       used       free     shared    buffers     cached
</span><span class='line'>Mem:         64405      58192       6213          0        505      29847
</span><span class='line'>-/+ buffers/cache:      27838      36566
</span><span class='line'>Swap:        65535         58      65477
</span></code></pre></td></tr></table></div></figure>


<h2>批量SSH无密钥登录</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master4 ~]$ cat ssh-copy-id.expect 
</span><span class='line'>#!/usr/bin/expect  
</span><span class='line'>
</span><span class='line'>## Usage $0 [user@]host password
</span><span class='line'>
</span><span class='line'>set host [lrange $argv 0 0];
</span><span class='line'>set password [lrange $argv 1 1] ;
</span><span class='line'>
</span><span class='line'>set timeout 30;
</span><span class='line'>
</span><span class='line'>spawn ssh-copy-id $host ;
</span><span class='line'>
</span><span class='line'>expect {
</span><span class='line'>  "(yes/no)?" { send yes\n; exp_continue; }
</span><span class='line'>  "password:" { send $password\n; exp_continue; }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>exec sleep 1;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master4 ~]$ pdsh -w ^slaves ./ssh-copy-id.expect %h 'PASSWD'
</span><span class='line'>
</span><span class='line'># 验证是否全部成功
</span><span class='line'>[hadoop@hadoop-master4 ~]# pdsh -w ^slaves -x hadoop-slaver[1-16] -R ssh hostname
</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>pdsh -w ssh:user00[1-10] "date"
</span><span class='line'>此命令用于在user001到user0010上执行date命令。
</span><span class='line'>pdsh -w ssh:user0[10-31],/1$/ "uptime"
</span><span class='line'>此命令在选择远程主机时使用了正则表达式，表示在user010到user031中选择以1结尾的主机名，即在user011、user021、user031上执行uptime命令
</span><span class='line'>
</span><span class='line'>-l    指定在远程主机上使用的用户名称。例如：
</span><span class='line'>pdsh -R ssh -l opsuser -w user00[1-9] "date"
</span><span class='line'>
</span><span class='line'>对于-g组，把对应的主机写入到/etc/dsh/group/或~/.dsh/group/目录下的文件中即可
</span><span class='line'>
</span><span class='line'>[root@dispatch1 ~]# pdsh -w dispatch1,search1,horizon1 -l bigendian jps 
</span><span class='line'>[root@dispatch1 ~]# vi servers
</span><span class='line'>dispatch1
</span><span class='line'>search1
</span><span class='line'>horizon1
</span><span class='line'>[root@dispatch1 ~]# pdsh -w ^servers -l bigendian hostname 
</span><span class='line'>dispatch1: dispatch1
</span><span class='line'>horizon1: horizon1
</span><span class='line'>search1: search1
</span><span class='line'>
</span><span class='line'>-f    设置同时连接到远程主机的个数
</span><span class='line'>
</span><span class='line'>dshbak格式化输出
</span><span class='line'>
</span><span class='line'>pdcp -R ssh -g userhosts /home/opsuser/mysqldb.tar.gz /home/opsuser #复制文件  
</span></code></pre></td></tr></table></div></figure>


<ul>
<li><a href="http://ixdba.blog.51cto.com/2895551/1550184">并行分布式运维工具pdsh</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Some quick tips on how to get started using pdsh:
</span><span class='line'>Set up your environment:
</span><span class='line'>export PDSH_SSH_ARGS_APPEND=”-o ConnectTimeout=5 -o CheckHostIP=no -o StrictHostKeyChecking=no” (Add this to your .bashrc to save time.)</span></code></pre></td></tr></table></div></figure>


<ul>
<li><a href="https://radfest.wordpress.com/2012/05/24/parallel-remote-shelling-via-pdsh/">Parallel remote &ldquo;shelling&rdquo; via pdsh</a></li>
<li><a href="http://kumu-linux.github.io/blog/2013/06/19/pdsh/">Pdsh使用方法</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-01-23T17:47:28+08:00" pubdate data-updated="true">Sat 2016-01-23 17:47</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>前一篇介绍了全部手工安装Ganglia的文章，当时安装测试的环境比较简单。按照网上的步骤安装好，看到图了以为就懂了。Ganglia的基本多播/单播的概念都没弄懂。</p>

<p>这次有机会把Ganglia安装到正式环境，由于网络复杂一些，遇到新的问题。也更进一步的了解了Ganglia。</p>

<p>后端Gmetad(ganglia meta daemon)和Gmond(ganglia monitoring daemon)是Ganglia的两个组件。</p>

<p>Gmetad负责收集各个cluster的数据，并更新到rrd数据库中；Gmond把本机的数据UDP广播（或者单播给某台机），同时收集集群节点的数据供Gmetad读取。Gmetad并不用于监控数据的汇总，是对已经采集好的全部数据处理并存储到rrdtool数据库。</p>

<h2>搭建yum环境</h2>

<p>由于正式环境没有提供外网环境，所以需要把安装光盘拷贝到机器，作为yum的本地源。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mount -t iso9660 -o loop rhel-server-6.4-x86_64-dvd\[ED2000.COM\].iso iso/
</span><span class='line'>ln -s iso rhel6.4
</span><span class='line'>
</span><span class='line'>vi /etc/yum.repos.d/rhel.repo 
</span><span class='line'>[os]
</span><span class='line'>name = Linux OS Packages
</span><span class='line'>baseurl = file:///opt/rhel6.4
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck = 0
</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>再极端点，yum程序都没有安装。到 Packages 目录用 rpm 安装 <code>yum*</code> 。</p>

<p>安装httpd后，把 rhel6.4 源建一个软链接到 <code>/var/www/html/rhel6.4</code> ，其他机器就可以使用该源来进行安装软件了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat /etc/yum.repos.d/rhel.repo
</span><span class='line'>[http]
</span><span class='line'>name=LOCAL YUM server
</span><span class='line'>baseurl = http://cu-omc1/rhel6.4
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0</span></code></pre></td></tr></table></div></figure>


<p>注意：如果用CentOS的ISO会有两个光盘，两个地址用逗号分隔全部加到baseurl（http方式也一样）：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[centos-local]
</span><span class='line'>name=Centos Local
</span><span class='line'>baseurl=file:///mnt/cdrom,file:///mnt/cdrom2 
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0</span></code></pre></td></tr></table></div></figure>


<h2>使用yum安装依赖</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install -y gcc gd httpd php php-devel php-mysql php-pear php-common php-gd php-mbstring php-cli 
</span><span class='line'>
</span><span class='line'>yum install -y rrdtool 
</span><span class='line'>
</span><span class='line'>yum install -y apr*
</span><span class='line'>
</span><span class='line'># 编译Ganglia时加 --with-libpcre=no 可以不安装pcre
</span><span class='line'>yum install -y pcre*
</span><span class='line'>
</span><span class='line'># yum install -y zlib-devel
</span></code></pre></td></tr></table></div></figure>


<h2><del>(仅)编译安装Ganglia</del> 官网有的不再推荐自己手动编译</h2>

<p>下载下面的软件(yum没有这些软件)：</p>

<ul>
<li><a href="http://rpm.pbone.net/index.php3/stat/4/idpl/15992683/dir/scientific_linux_6/com/rrdtool-devel-1.3.8-6.el6.x86_64.rpm.html">rrdtool-devel-1.3.8-6.el6.x86_64.rpm</a></li>
<li><a href="http://download.savannah.gnu.org/releases/confuse/">confuse-2.7.tar.gz</a></li>
<li><a href="http://sourceforge.net/projects/ganglia/files/ganglia%20monitoring%20core/">ganglia</a></li>
<li><a href="http://sourceforge.net/projects/ganglia/files/ganglia-web/">ganglia-web</a></li>
</ul>


<p>安装：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>umask 0022 # 临时修改下，不然后面会遇到权限问题
</span><span class='line'>
</span><span class='line'>rpm -ivh rrdtool-devel-1.3.8-6.el6.x86_64.rpm 
</span><span class='line'>
</span><span class='line'># 如果yum可以安装的话：yum install -y libconfuse*
</span><span class='line'>tar zxf confuse-2.7.tar.gz
</span><span class='line'>cd confuse-2.7
</span><span class='line'>./configure CFLAGS=-fPIC --disable-nls
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>tar zxf ganglia-3.7.2.tar.gz 
</span><span class='line'>cd ganglia-3.7.2
</span><span class='line'>./configure --with-gmetad --enable-gexec --enable-status --prefix=/usr/local/ganglia
</span><span class='line'># 可选项，用于指定默认配置位置 `-sysconfdir=/etc/ganglia`
</span><span class='line'>
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>cp gmetad/gmetad.init /etc/init.d/gmetad
</span><span class='line'>chkconfig gmetad on
</span><span class='line'># 查看gmetad的情况
</span><span class='line'>chkconfig --list | grep gm
</span><span class='line'>
</span><span class='line'>df -h # 把rrds目录放到最大的分区，再做个链接到data目录下
</span><span class='line'>mkdir -p /data/ganglia/rrds
</span><span class='line'>chown nobody:nobody /data/ganglia/rrds
</span><span class='line'>ln -s /usr/local/ganglia/sbin/gmetad /usr/sbin/gmetad
</span><span class='line'>
</span><span class='line'>gmetad -h # 查看默认的config位置。下面步骤AB 二选一 根据是否配置 sysconfdir 选项
</span><span class='line'># 步骤A
</span><span class='line'># cp gmetad/gmetad.conf /etc/ganglia/
</span><span class='line'># 步骤B
</span><span class='line'>vi /etc/init.d/gmetad 
</span><span class='line'>  /usr/local/ganglia/etc/gmetad.conf #修改原来的默认配置路径
</span><span class='line'> 
</span><span class='line'>cd ganglia-3.7.2/gmond/
</span><span class='line'>ln -s /usr/local/ganglia/sbin/gmond /usr/sbin/gmond
</span><span class='line'>cp gmond.init /etc/init.d/gmond
</span><span class='line'>chkconfig gmond on
</span><span class='line'>chkconfig --list gmond
</span><span class='line'>  
</span><span class='line'>gmond -h # 查看默认的config位置。
</span><span class='line'>./gmond -t &gt;/usr/local/ganglia/etc/gmond.conf
</span><span class='line'>vi /etc/init.d/gmond 
</span><span class='line'>  /usr/local/ganglia/etc/gmond.conf #修改原来的默认配置路径
</span></code></pre></td></tr></table></div></figure>


<h2>配置</h2>

<ul>
<li>Ganglia配置</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>vi /usr/local/ganglia/etc/gmetad.conf
</span><span class='line'>  datasource "HADOOP" hadoop-master1
</span><span class='line'>  datasource "CU" cu-ud1
</span><span class='line'>  rrd_rootdir "/data/ganglia/rrds"
</span><span class='line'>  gridname "bigdata"
</span><span class='line'>
</span><span class='line'>vi /usr/local/ganglia/etc/gmond.conf
</span><span class='line'>  cluster {
</span><span class='line'>   name = "CU"
</span><span class='line'>
</span><span class='line'>  udp_send_channel {
</span><span class='line'>   bind_hostname = yes
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://ixdba.blog.51cto.com/2895551/1149003">http://ixdba.blog.51cto.com/2895551/1149003</a></p>

<p>Ganglia的收集数据工作可以工作在单播（unicast)或多播(multicast)模式下，默认为多播模式。</p>

<ul>
<li>单播：发送自己 <strong>收集</strong> 到的监控数据到特定的一台或几台机器上，可以跨网段</li>
<li>多播：发送自己收集到的监控数据到同一网段内所有的机器上，同时收集同一网段内的所有机器发送过来的监控数据。因为是以广播包的形式发送，因此需要同一网段内。但同一网段内，又可以定义不同的发送通道。</li>
</ul>


<p>主机多网卡(多IP)情况下需要绑定到特定的IP，设置bind_hostname来设置要绑定的IP地址。单IP情况下可以不需要考虑。</p>

<p>多播情况下只能在单一网段进行，如果集群存在多个网段，可以分拆成多个子集群（data_source)，或者使用单播来进行配置。期望配置简单点的话，配置多个 data_source 。</p>

<ul>
<li><code>data_source "cluster-db" node1 node2</code>  定义集群名称，以及获取集群监控数据的节点。由于采用multicast模式，每台gmond节点都有本集群内节点服务器的所有监控数据，因此不必把所有节点都列出来。node1 node2是or的关系，如果node1无法下载，则才会尝试去node2下载，所以它们应该都是同一个集群的节点，保存着同样的数据。</li>
<li><code>cluster.name</code> 本节点属于哪个cluster，需要与data_source对应。</li>
<li><code>host.location</code> 类似于hostname的作用。</li>
<li><code>udp_send_channel.mcast_join/host</code> 多播地址，工作在239.2.11.71通道下。如果使用单播模式，则要写host=node1，单播模式下可以配置多个upd_send_channel</li>
<li><code>udp_recv_channel.mcast_join</code></li>
</ul>


<p><strong>参考思路</strong> (未具体实践)：多网段情况可以用单播解决，要是单网段要配置多个data_source(集群)那就换个多播的端口吧！</p>

<h2>启动以及测试</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>service httpd restart
</span><span class='line'>service gmetad start
</span><span class='line'>service gmond start
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 ganglia]# netstat -anp | grep gm
</span><span class='line'>tcp        0      0 0.0.0.0:8649                0.0.0.0:*                   LISTEN      916/gmond           
</span><span class='line'>tcp        0      0 0.0.0.0:8651                0.0.0.0:*                   LISTEN      12776/gmetad        
</span><span class='line'>tcp        0      0 0.0.0.0:8652                0.0.0.0:*                   LISTEN      12776/gmetad        
</span><span class='line'>udp        0      0 239.2.11.71:8649            0.0.0.0:*                               916/gmond           
</span><span class='line'>udp        0      0 192.168.31.11:60126         239.2.11.71:8649            ESTABLISHED 916/gmond           
</span><span class='line'>unix  2      [ ]         DGRAM                    1331526917 12776/gmetad        
</span><span class='line'>[root@cu-omc1 ganglia]# bin/gstat -a
</span><span class='line'>CLUSTER INFORMATION
</span><span class='line'>       Name: CU
</span><span class='line'>      Hosts: 0
</span><span class='line'>Gexec Hosts: 0
</span><span class='line'> Dead Hosts: 0
</span><span class='line'>  Localtime: Wed Jun 15 20:17:36 2016
</span><span class='line'>
</span><span class='line'>There are no hosts up at this time
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>netstat -anp | grep -E "gmond|gmetad"
</span><span class='line'>
</span><span class='line'># 启动如果有问题，使用调试模式启动查找问题
</span><span class='line'>/usr/sbin/gmetad -d 10
</span><span class='line'>
</span><span class='line'>/usr/local/ganglia/bin/gstat -a
</span><span class='line'>/usr/local/ganglia/bin/gstat -a -i hadoop-master1 查看master1上数据的情况
</span><span class='line'>
</span><span class='line'>telnet localhost 8649 - 能抓取数据的配置deaf=no才有绑定到本机IP
</span><span class='line'>telnet localhost 8651</span></code></pre></td></tr></table></div></figure>


<p>问题：多播地址绑定失败</p>

<p>如果telnet8649没有数据，查看下route是否有 [hostname对应的IP] 到 [239.2.11.71] 的路由！(多网卡多IP的时刻，可能default的路由并非主机名对应IP的地址)</p>

<blockquote><p><a href="http://llydmissile.blog.51cto.com/7784666/1411239">http://llydmissile.blog.51cto.com/7784666/1411239</a>
<a href="http://www.cnblogs.com/Cherise/p/4350581.html">http://www.cnblogs.com/Cherise/p/4350581.html</a></p>

<p>测试过程中可能会出现以下错误：Error creating multicast server mcast_join=239.2.11.71 port=8649 mcast_if=NULL family=&lsquo;inet4&rsquo;. Will try again&hellip;，系统不支持多播，需要将多播ip地址加入路由表，使用route add -host 239.2.11.71 dev eth0命令即可，将该命令加入/etc/rc.d/rc.local文件中，一劳永逸</p></blockquote>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master4 ~]# gmond -d 10
</span><span class='line'>loaded module: core_metrics
</span><span class='line'>loaded module: cpu_module
</span><span class='line'>loaded module: disk_module
</span><span class='line'>loaded module: load_module
</span><span class='line'>loaded module: mem_module
</span><span class='line'>loaded module: net_module
</span><span class='line'>loaded module: proc_module
</span><span class='line'>loaded module: sys_module
</span><span class='line'>udp_recv_channel mcast_join=239.2.11.71 mcast_if=NULL port=8649 bind=239.2.11.71 buffer=0
</span><span class='line'>Error creating multicast server mcast_join=239.2.11.71 port=8649 mcast_if=NULL family='inet4'.  Will try again...</span></code></pre></td></tr></table></div></figure>


<p>环境的default route被清理掉了(或者是由于网关和本机不在同一网段)。需要手动添加一条到网卡的route。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master4 ~]# route
</span><span class='line'>Kernel IP routing table
</span><span class='line'>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
</span><span class='line'>192.168.32.0    *               255.255.255.0   U     0      0        0 bond0
</span><span class='line'>192.168.31.0    192.168.32.254  255.255.255.0   UG    0      0        0 bond0
</span><span class='line'>link-local      *               255.255.0.0     U     1006   0        0 bond0
</span><span class='line'>[root@hadoop-master4 ~]# route add -host 239.2.11.71 dev bond0
</span><span class='line'>[root@hadoop-master4 ~]# route
</span><span class='line'>Kernel IP routing table
</span><span class='line'>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
</span><span class='line'>239.2.11.71     *               255.255.255.255 UH    0      0        0 bond0
</span><span class='line'>192.168.32.0    *               255.255.255.0   U     0      0        0 bond0
</span><span class='line'>192.168.31.0    192.168.32.254  255.255.255.0   UG    0      0        0 bond0
</span><span class='line'>link-local      *               255.255.0.0     U     1006   0        0 bond0</span></code></pre></td></tr></table></div></figure>


<p><strong>还有就是防火墙！！！！！！</strong></p>

<h2>安装GWeb</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd ~/ganglia-web-3.7.1
</span><span class='line'>vi Makefile # 一次性配置好，不再需要去修改conf_default.php
</span><span class='line'>  GDESTDIR = /var/www/html/ganglia
</span><span class='line'>  GCONFDIR = /usr/local/ganglia/etc/
</span><span class='line'>  GWEB_STATEDIR = /var/www/html/ganglia
</span><span class='line'>  # Gmetad rootdir (parent location of rrd folder)
</span><span class='line'>  GMETAD_ROOTDIR = /data/ganglia
</span><span class='line'>  APACHE_USER = apache
</span><span class='line'>make install
</span><span class='line'>
</span><span class='line'># 注意：内网还是需要改下 conf_default.php 一堆jquery的js。
</span><span class='line'># 如果Web不能访问，查看下防火墙以及SELinux
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>httpd登录密码配置</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>htpasswd -c /var/www/html/ganglia/etc/htpasswd.users gangliaadmin 
</span><span class='line'>
</span><span class='line'>vi /etc/httpd/conf/httpd.conf 
</span><span class='line'>
</span><span class='line'>  &lt;Directory "/var/www/html/ganglia"&gt;
</span><span class='line'>  #  SSLRequireSSL
</span><span class='line'>     Options None
</span><span class='line'>     AllowOverride None
</span><span class='line'>     &lt;IfVersion &gt;= 2.3&gt;
</span><span class='line'>        &lt;RequireAll&gt;
</span><span class='line'>           Require all granted
</span><span class='line'>  #        Require host 127.0.0.1
</span><span class='line'>
</span><span class='line'>           AuthName "Ganglia Access"
</span><span class='line'>           AuthType Basic
</span><span class='line'>           AuthUserFile /var/www/html/ganglia/etc/htpasswd.users
</span><span class='line'>           Require valid-user
</span><span class='line'>        &lt;/RequireAll&gt;
</span><span class='line'>     &lt;/IfVersion&gt;
</span><span class='line'>     &lt;IfVersion &lt; 2.3&gt;
</span><span class='line'>        Order allow,deny
</span><span class='line'>        Allow from all
</span><span class='line'>  #     Order deny,allow
</span><span class='line'>  #     Deny from all
</span><span class='line'>  #     Allow from 127.0.0.1
</span><span class='line'>
</span><span class='line'>        AuthName "Ganglia Access"
</span><span class='line'>        AuthType Basic
</span><span class='line'>        AuthUserFile /var/www/html/ganglia/etc/htpasswd.users
</span><span class='line'>        Require valid-user
</span><span class='line'>     &lt;/IfVersion&gt;
</span><span class='line'>  &lt;/Directory&gt;
</span><span class='line'>
</span><span class='line'>service httpd restart
</span></code></pre></td></tr></table></div></figure>


<p>如果图出不来，可以看看httpd的错误日志！！！！！！</p>

<p>如果在nginx做权限控制，一样很简单：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location /ganglia {
</span><span class='line'>      proxy_pass http://localhost/ganglia;
</span><span class='line'>      auth_basic "Ganglia Access";
</span><span class='line'>      auth_basic_user_file "/var/www/html/ganglia/etc/htpasswd.users";
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>集群配置</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /usr/local 
</span><span class='line'># for h in cu-ud{1,2} hadoop-master{1,2} ; do echo $h ; done
</span><span class='line'>for h in cu-ud1 cu-ud2 hadoop-master1 hadoop-master2 ; do 
</span><span class='line'>  cd /usr/local;
</span><span class='line'>  rsync -vaz  ganglia $h:/usr/local/ ;
</span><span class='line'>  ssh $h ln -s /usr/local/ganglia/sbin/gmond /usr/sbin/gmond ;
</span><span class='line'>  scp /etc/init.d/gmond $h:/etc/init.d/ ;
</span><span class='line'>  ssh $h "chkconfig gmond on" ;
</span><span class='line'>  ssh $h "yum install apr* -y" ; 
</span><span class='line'>  ssh $h "service gmond start" ; 
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'># 不同的集群，gmond.conf的cluster.name需要修改
</span><span class='line'>
</span><span class='line'>telnet hadoop-master1 8649
</span><span class='line'>netstat -anp | grep gm
</span></code></pre></td></tr></table></div></figure>


<p>要是集群有变动，添加还好，删除的话，会存在原来的旧数据，页面会提示机器down掉了。可以删除rrds目录下对应集群中节点的数据，然后重庆gmetad/httpd即可。</p>

<h2>参考</h2>

<h3>内容</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>防火墙规则设置
</span><span class='line'>iptables -I INPUT 3 -p tcp -m tcp --dport 80 -j ACCEPT
</span><span class='line'>iptables -I INPUT 3 -p udp -m udp --dport 8649 -j ACCEPT
</span><span class='line'>
</span><span class='line'>service iptables save
</span><span class='line'>service iptables restart
</span><span class='line'>
</span><span class='line'>关闭selinux
</span><span class='line'>vi /etc/selinux/config
</span><span class='line'>SELINUX=disabled
</span><span class='line'>setenforce 0
</span></code></pre></td></tr></table></div></figure>


<p>实际应用中，需要监控的机器往往在不同的网段内，这个时候，就不能用gmond默认的多播方式（用于同一个网段内）来传送数据，必须使用单播的方法。</p>

<p>gmond可以配置成为一个cluster，这些gmond节点之间相互发送各自的监控数据。所以每个gmond节点上实际上都会有 cluster内的所有节点的监控数据。gmetad只需要去某一个节点获取数据就可以了。</p>

<p>web front-end 一个基于web的监控界面，通常和Gmetad安装在同一个节点上(还需确认是否可以不在一个节点上，因为php的配置文件中ms可配置gmetad的地址及端口)，它从Gmetad取数据，并且读取rrd数据库，生成图片，显示出来。</p>

<p>gmetad周期性的去gmond节点或者gmetad节点poll数据。一个gmetad可以设置多个datasource，每个datasource可以有多个备份，一个失败还可以去其他host取数据。Gmetad只有tcp通道，一方面他向datasource发送请求，另一方面会使用一个tcp端口，发 布自身收集的xml文件，默认使用8651端口。所以gmetad即可以从gmond也可以从其他的gmetad得到xml数据。</p>

<p>对于IO来说，Gmetad默认15秒向gmond取一次xml数据，如果gmond和gmetad都是在同一个节点，这样就相当于本地io请求。同时gmetad请求完xml文件后，还需要对其解析，也就是说按默认设置每15秒需要解析一个10m级别的xml文件，这样cpu的压力就会很大。同时它还有写入RRD数据库，还要处理来自web客户端的解析请求，也会读RRD数据库。这样本身的IO CPU 网络压力就很大，因此这个节点至少应该是个空闲的而且能力比较强的节点。</p>

<ul>
<li>多播模式配置
这个是默认的方式，基本上不需要修改配置文件，且所有节点的配置是一样的。这种模式的好处是所有的节点上的 gmond 都有完备的数据，gmetad 连接其中任意一个就可以获取整个集群的所有监控数据，很方便。
其中可能要修改的是 mcast_if 这个参数，用于指定多播的网络接口。如果有多个网卡，要填写对应的内网接口。</li>
<li>单播模式配置
监控机上的接收 Channel 配置。我们使用 UDP 单播模式，非常简单。我们的集群有部分机器在另一个机房，所以监听了 0.0.0.0，如果整个集群都在一个内网中，建议只 bind 内网地址。如果有防火墙，要打开相关的端口。</li>
<li>最重要的配置项是 data_source: <code>data_source "my-cluster" localhost:8648</code> 如果使用的是默认的 8649 端口，则端口部分可以省略。如果有多个集群，则可以指定多个 data_source，每行一个。</li>
<li>最后是 gridname 配置，用于给整个 Grid 命名</li>
<li><a href="https://github.com/ganglia/gmond_python_modules">https://github.com/ganglia/gmond_python_modules</a></li>
</ul>


<h3>网址</h3>

<ul>
<li><a href="http://yhz.me/blog/Install-Ganglia-On-CentOS.html">在 CentOS 6.5 上安装 Ganglia 3.6.0</a></li>
<li>*<a href="http://ixdba.blog.51cto.com/2895551/1149003">分布式监控系统ganglia配置文档</a></li>
<li><p>*<a href="http://www.3mu.me/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%80%E6%BA%90%E7%9B%91%E6%8E%A7%E8%BD%AF%E4%BB%B6ganglia-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/">企业级开源监控软件Ganglia 安装与配置</a></p></li>
<li><p>*<a href="http://jerrypeng.me/2014/07/04/server-side-java-monitoring-ganglia/">Java 服务端监控方案（二. Ganglia 篇）</a></p></li>
<li><a href="http://jerrypeng.me/2014/07/22/server-side-java-monitoring-nagios/">Java 服务端监控方案（三. Nagios 篇）</a></li>
<li><p><a href="https://github.com/ganglia/ganglia-web/wiki/Nagios-Integration">https://github.com/ganglia/ganglia-web/wiki/Nagios-Integration</a></p></li>
<li><p><a href="https://ganglia.wikimedia.org/latest/">维基百科Ganglia</a></p></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/01/19/hole/">坑</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2016-01-19T16:53:29+08:00" pubdate data-updated="true">Tue 2016-01-19 16:53</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>Set</h2>

<p><img src="/images/blogs/hole-set-add-diff.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Set&lt;BlockedInfo&gt; diffs = new HashSet&lt;&gt;();
</span><span class='line'>diffs.addAll(oldBlockedList);
</span><span class='line'>diffs.addAll(newBlockedList);
</span><span class='line'>Iterator&lt;BlockedInfo&gt; iterator = diffs.iterator();
</span><span class='line'>while (iterator.hasNext()) {
</span><span class='line'>  BlockedInfo i = iterator.next();
</span><span class='line'>  if (oldBlockedList.contains(i) && newBlockedList.contains(i)) {
</span><span class='line'>      iterator.remove();
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>第二段代码希望找出前后两个list的差别，即XOR的效果。但是。。。为什么呢？想一想。</p>

<p>用guava库一行代码搞定：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Sets.difference(Sets.union(oldBlockedList, newBlockedList), Sets.intersection(oldBlockedList, newBlockedList))</span></code></pre></td></tr></table></div></figure>


<h2>hive建表</h2>

<p>由于fs.defaultFS和hive.metastore.warehouse.dir对不上，建表后不能删除。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;fs.defaultFS&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;hdfs://zfcluster&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;hdfs://zfcluster:8020/hive/warehousedir&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>删除表报错：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hive&gt; drop table es_t_house_monitor2;
</span><span class='line'>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.IllegalArgumentException: Wrong FS: hdfs://zfcluster:8020/hive/warehousedir/es_t_house_monitor2, expected: hdfs://zfcluster)</span></code></pre></td></tr></table></div></figure>


<p>建错了，只能先改！然后把hive.metastore.warehouse.dir和fs.defaultFS调成一致。</p>

<p>修改hive持久化（数据库）的表sds，找到location是该路径的改掉。然后就可以drop表了。</p>

<h2>nginx rewrite中大括号</h2>

<p>由于大括号用于一段规则的结束开始，所以直接在rewrite写 {} 是报错的。需要把包括{}大括号的规则用双引号括起来。</p>

<p>注: 对花括号( { 和 } )来说, 他们既能用在重定向的正则表达式里，也是用在配置文件里分割代码块, 为了避免冲突, 正则表达式里带花括号的话，应该用双引号（或者单引号）包围。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/01/12/tez-ui-config-and-run/">配置TEZ-UI</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-01-12T20:28:35+08:00" pubdate data-updated="true">Tue 2016-01-12 20:28</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>tez-ui很早就出来了，荒废了很多时间。今天才把它配置出来，效果挺不错的，和spark-web差不多。</p>

<p>记录了在hive-1.2.1上配置tez-0.7.0过程，配置运行hadoop2.6.3-timeline以及为tez添加tez-ui特性的步骤。</p>

<h2>编译tez-0.7.0</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 apache-tez-0.7.0-src]$ mvn package -Dhadoop.version=2.6.3 -DskipTests
</span><span class='line'>
</span><span class='line'>[INFO] Reactor Summary:
</span><span class='line'>[INFO] 
</span><span class='line'>[INFO] tez ................................................ SUCCESS [  0.831 s]
</span><span class='line'>[INFO] tez-api ............................................ SUCCESS [  6.580 s]
</span><span class='line'>[INFO] tez-common ......................................... SUCCESS [  0.124 s]
</span><span class='line'>[INFO] tez-runtime-internals .............................. SUCCESS [  0.676 s]
</span><span class='line'>[INFO] tez-runtime-library ................................ SUCCESS [  1.378 s]
</span><span class='line'>[INFO] tez-mapreduce ...................................... SUCCESS [  0.989 s]
</span><span class='line'>[INFO] tez-examples ....................................... SUCCESS [  0.105 s]
</span><span class='line'>[INFO] tez-dag ............................................ SUCCESS [  2.391 s]
</span><span class='line'>[INFO] tez-tests .......................................... SUCCESS [  0.187 s]
</span><span class='line'>[INFO] tez-ui ............................................. SUCCESS [02:23 min]
</span><span class='line'>[INFO] tez-plugins ........................................ SUCCESS [  0.017 s]
</span><span class='line'>[INFO] tez-yarn-timeline-history .......................... SUCCESS [  0.595 s]
</span><span class='line'>[INFO] tez-yarn-timeline-history-with-acls ................ SUCCESS [  0.316 s]
</span><span class='line'>[INFO] tez-mbeans-resource-calculator ..................... SUCCESS [  0.189 s]
</span><span class='line'>[INFO] tez-tools .......................................... SUCCESS [  0.017 s]
</span><span class='line'>[INFO] tez-dist ........................................... SUCCESS [ 16.554 s]
</span><span class='line'>[INFO] Tez ................................................ SUCCESS [  0.015 s]
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] BUILD SUCCESS
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Total time: 02:55 min
</span><span class='line'>[INFO] Finished at: 2016-01-12T19:08:50+08:00
</span><span class='line'>[INFO] Final Memory: 63M/756M
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span></code></pre></td></tr></table></div></figure>


<h2>tez嵌入到hive</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// 上传tez程序到hdfs
</span><span class='line'>[hadoop@cu2 ~]$ cd sources/apache-tez-0.7.0-src/tez-dist/target/
</span><span class='line'>[hadoop@cu2 target]$ hadoop fs -mkdir -p /apps/tez-0.7.0
</span><span class='line'>[hadoop@cu2 target]$ hadoop fs -put tez-0.7.0.tar.gz /apps/tez-0.7.0/
</span><span class='line'>
</span><span class='line'>// TEZ_CONF_DIR = HADOOP_CONF_DIR
</span><span class='line'>[hadoop@cu2 ~]$ cd hadoop-2.6.3/etc/hadoop/
</span><span class='line'>[hadoop@cu2 hadoop]$ vi tez-site.xml
</span><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;tez.lib.uris&lt;/name&gt;
</span><span class='line'>&lt;value&gt;${fs.defaultFS}/apps/tez-0.7.0/tez-0.7.0.tar.gz&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>
</span><span class='line'>// 本地tez jars加入HADOOP_CLASSPATH
</span><span class='line'>[hadoop@cu2 apache-tez-0.7.0-src]$ cd tez-dist/target/
</span><span class='line'>archive-tmp/              maven-archiver/           tez-0.7.0/                tez-0.7.0-minimal.tar.gz  tez-0.7.0.tar.gz          tez-dist-0.7.0-tests.jar  
</span><span class='line'>[hadoop@cu2 apache-tez-0.7.0-src]$ cd tez-dist/target/
</span><span class='line'>[hadoop@cu2 target]$ mv tez-0.7.0 ~/
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 ~]$ vi apache-hive-1.2.1-bin/conf/hive-env.sh
</span><span class='line'>
</span><span class='line'>// 多个jline版本 http://stackoverflow.com/questions/28997441/hive-startup-error-terminal-initialization-failed-falling-back-to-unsupporte
</span><span class='line'>export HADOOP_USER_CLASSPATH_FIRST=true
</span><span class='line'>export TEZ_HOME=/home/hadoop/tez-0.7.0
</span><span class='line'>export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$TEZ_HOME/*:$TEZ_HOME/lib/*
</span><span class='line'>
</span><span class='line'>// http://stackoverflow.com/questions/26988388/hive-0-14-0-not-starting [/tmp/hive on HDFS should be writable. Current permissions are: rwxrwxr-x]
</span><span class='line'>// hive.metastore.warehouse.dir  hive.exec.scratchdir
</span><span class='line'>[hadoop@cu2 hive]$ rm -rf /tmp/hive
</span><span class='line'>[hadoop@cu2 hive]$ hadoop fs -rmr /tmp/hive
</span><span class='line'>// 或者修改权限 hadoop fs -chmod 777 /tmp/hive</span></code></pre></td></tr></table></div></figure>


<h2>启用/使用tez</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 hadoop]$ cat ~/hive/conf/hive-site.xml 
</span><span class='line'>...
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hive.execution.engine&lt;/name&gt;
</span><span class='line'>&lt;value&gt;tez&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hive]$ bin/hive
</span><span class='line'>...
</span><span class='line'>hive&gt; select count(*) from t_ods_access_log2;
</span><span class='line'>Query ID = hadoop_20160112200359_f8be3d1c-9adc-42c0-abb9-2643dfef2cc7
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Status: Running (Executing on YARN cluster with App id application_1452600034599_0001)
</span><span class='line'>
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>Map 1 ..........   SUCCEEDED      1          1        0        0       0       0
</span><span class='line'>Reducer 2 ......   SUCCEEDED      1          1        0        0       0       0
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>VERTICES: 02/02  [==========================&gt;&gt;] 100%  ELAPSED TIME: 20.83 s    
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>OK
</span><span class='line'>67
</span><span class='line'>Time taken: 27.823 seconds, Fetched: 1 row(s)
</span></code></pre></td></tr></table></div></figure>


<h2>部署/启动hadoop-timeline</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 hadoop]$ vi etc/hadoop/yarn-site.xml 
</span><span class='line'>...
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;yarn.timeline-service.enabled&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;yarn.timeline-service.hostname&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;hadoop-master2&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;yarn.timeline-service.http-cross-origin.enabled&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;yarn.resourcemanager.system-metrics-publisher.enabled&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop]$ for h in hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do rsync -vaz --exclude=logs hadoop-2.6.3 $h:~/ ; done
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop]$ sbin/yarn-daemon.sh start timelineserver
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop]$ sbin/stop-all.sh
</span><span class='line'>[hadoop@cu2 hadoop]$ sbin/start-all.sh
</span></code></pre></td></tr></table></div></figure>


<h2>部署tez-ui</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// 放置tez-ui
</span><span class='line'>[hadoop@cu2 target]$ cd ../../tez-ui/
</span><span class='line'>[hadoop@cu2 tez-ui]$ cd target/
</span><span class='line'>[hadoop@cu2 target]$ ll
</span><span class='line'>total 1476
</span><span class='line'>drwxrwxr-x 3 hadoop hadoop    4096 Jan 12 19:08 classes
</span><span class='line'>drwxrwxr-x 2 hadoop hadoop    4096 Jan 12 19:08 maven-archiver
</span><span class='line'>drwxrwxr-x 8 hadoop hadoop    4096 Jan 12 19:08 tez-ui-0.7.0
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop    3058 Jan 12 19:08 tez-ui-0.7.0-tests.jar
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop 1491321 Jan 12 19:08 tez-ui-0.7.0.war
</span><span class='line'>[hadoop@cu2 target]$ mv tez-ui-0.7.0 ~/
</span><span class='line'>
</span><span class='line'>// 部署tez-ui
</span><span class='line'>[hadoop@cu2 ~]$ cd apache-tomcat-7.0.67/conf/
</span><span class='line'>修改端口为9999
</span><span class='line'>[hadoop@cu2 apache-tomcat-7.0.67]$ vi conf/server.xml 
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 apache-tomcat-7.0.67]$ cd conf/Catalina/localhost/
</span><span class='line'>[hadoop@cu2 localhost]$ vi tez-ui.xml
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 apache-tomcat-7.0.67]$ bin/startup.sh 
</span><span class='line'>
</span><span class='line'>// tez添加tez-ui功能
</span><span class='line'>[hadoop@cu2 hive]$ vi ~/hadoop-2.6.3/etc/hadoop/tez-site.xml 
</span><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;tez.lib.uris&lt;/name&gt;
</span><span class='line'>&lt;value&gt;${fs.defaultFS}/apps/tez-0.7.0/tez-0.7.0.tar.gz&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;tez.history.logging.service.class&lt;/name&gt;
</span><span class='line'>&lt;value&gt;org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;tez.tez-ui.history-url.base&lt;/name&gt;
</span><span class='line'>&lt;value&gt;http://hadoop-master2:9999/tez-ui/&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span></code></pre></td></tr></table></div></figure>


<p>再运行一遍hive，查询一两个SQL。</p>

<p>最终效果：</p>

<p><img src="/images/blogs/tez-ui.png" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://tez.apache.org/install.html">http://tez.apache.org/install.html</a></li>
<li><a href="http://tez.apache.org/tez-ui.html">http://tez.apache.org/tez-ui.html</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/01/07/hadoop-install-and-upgrade-4-ha-upgrade/">Hadoop安装与升级-(4)HA升级</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-01-07T23:04:27+08:00" pubdate data-updated="true">Thu 2016-01-07 23:04</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>官网的文档[HDFSHighAvailabilityWithQJM.html]和[HdfsRollingUpgrade.html]（Note that rolling upgrade is supported only from Hadoop-2.4.0 onwards.）很详细，但是没有一个整体的案例。这里整理下操作记录下来。</p>

<ol>
<li>关闭所有的namenode，部署新版本的hadoop</li>
<li>启动所有的journalnode，是所有！！升级namenode的同时，也会升级所有journalnode！！</li>
<li>使用-upgrade选项启动一台namenode。启动的这台namenode会直接进入active状态，升级本地的元数据，同时会升级shared edit log（也就是journalnode的数据）</li>
<li>使用-bootstrapStandby启动其他namenode，同步更新。不能使用-upgrade选项！（我也没试，不知道试了是啥效果）</li>
</ol>


<h2>关闭集群，部署新版本的hadoop</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/stop-dfs.sh
</span><span class='line'>16/01/08 09:10:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Stopping namenodes on [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master2: stopping namenode
</span><span class='line'>hadoop-master1: stopping namenode
</span><span class='line'>hadoop-slaver1: stopping datanode
</span><span class='line'>hadoop-slaver2: stopping datanode
</span><span class='line'>hadoop-slaver3: stopping datanode
</span><span class='line'>Stopping journal nodes [hadoop-master1]
</span><span class='line'>hadoop-master1: stopping journalnode
</span><span class='line'>16/01/08 09:10:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Stopping ZK Failover Controllers on NN hosts [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master1: stopping zkfc
</span><span class='line'>hadoop-master2: stopping zkfc
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ 
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ cd ~/hadoop-2.6.3
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ ll
</span><span class='line'>total 52
</span><span class='line'>drwxr-xr-x 2 hadoop hadoop  4096 Dec 18 01:52 bin
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop    32 Jan  8 06:05 etc -&gt; /home/hadoop/hadoop-2.2.0/ha-etc
</span><span class='line'>drwxr-xr-x 2 hadoop hadoop  4096 Dec 18 01:52 include
</span><span class='line'>drwxr-xr-x 3 hadoop hadoop  4096 Dec 18 01:52 lib
</span><span class='line'>drwxr-xr-x 2 hadoop hadoop  4096 Dec 18 01:52 libexec
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop 15429 Dec 18 01:52 LICENSE.txt
</span><span class='line'>drwxrwxr-x 2 hadoop hadoop  4096 Jan  8 03:37 logs
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop   101 Dec 18 01:52 NOTICE.txt
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop  1366 Dec 18 01:52 README.txt
</span><span class='line'>drwxr-xr-x 2 hadoop hadoop  4096 Dec 18 01:52 sbin
</span><span class='line'>drwxr-xr-x 3 hadoop hadoop  4096 Jan  7 08:00 share
</span><span class='line'>
</span><span class='line'>#// 同步
</span><span class='line'>[hadoop@hadoop-master1 ~]$ for h in hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do rsync -vaz --delete --exclude=logs ~/hadoop-2.6.3 $h:~/ ; done</span></code></pre></td></tr></table></div></figure>


<h2>启动所有Journalnode</h2>

<p>2.6和2.2用的是一份配置！etc通过软链接到2.2的ha-etc配置。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ sbin/hadoop-daemons.sh --hostnames "hadoop-master1" --script /home/hadoop/hadoop-2.2.0/bin/hdfs start journalnode
</span><span class='line'>hadoop-master1: starting journalnode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-journalnode-hadoop-master1.out
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ jps
</span><span class='line'>31047 JournalNode
</span><span class='line'>244 QuorumPeerMain
</span><span class='line'>31097 Jps</span></code></pre></td></tr></table></div></figure>


<h2>升级一台namenode</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ bin/hdfs namenode -upgrade
</span><span class='line'>...
</span><span class='line'>16/01/08 09:13:54 INFO namenode.NameNode: createNameNode [-upgrade]
</span><span class='line'>...
</span><span class='line'>16/01/08 09:13:57 INFO namenode.FSImage: Starting upgrade of local storage directories.
</span><span class='line'>   old LV = -47; old CTime = 0.
</span><span class='line'>   new LV = -60; new CTime = 1452244437060
</span><span class='line'>16/01/08 09:13:57 INFO namenode.NNUpgradeUtil: Starting upgrade of storage directory /data/tmp/dfs/name
</span><span class='line'>16/01/08 09:13:57 INFO namenode.FSImageTransactionalStorageInspector: No version file in /data/tmp/dfs/name
</span><span class='line'>16/01/08 09:13:57 INFO namenode.NNUpgradeUtil: Performing upgrade of storage directory /data/tmp/dfs/name
</span><span class='line'>16/01/08 09:13:57 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>官网文档上说，除了升级了namenode的本地元数据外，sharededitlog也被升级了的。</p>

<p>查看journalnode的日志，确实journalnode也升级了：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ less logs/hadoop-hadoop-journalnode-hadoop-master1.log 
</span><span class='line'>...
</span><span class='line'>2016-01-08 09:13:57,070 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Starting upgrade of edits directory /data/journal/zfcluster
</span><span class='line'>2016-01-08 09:13:57,072 INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Starting upgrade of storage directory /data/journal/zfcluster
</span><span class='line'>2016-01-08 09:13:57,185 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Starting upgrade of edits directory: .
</span><span class='line'>   old LV = -47; old CTime = 0.
</span><span class='line'>   new LV = -60; new CTime = 1452244437060
</span><span class='line'>2016-01-08 09:13:57,185 INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Performing upgrade of storage directory /data/journal/zfcluster
</span><span class='line'>2016-01-08 09:13:57,222 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastWriterEpoch from 2 to 3 for client /172.17.0.1
</span><span class='line'>2016-01-08 09:16:57,731 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from 3 to 4 for client /172.17.0.1
</span><span class='line'>2016-01-08 09:16:57,735 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage FileJournalManager(root=/data/journal/zfcluster)
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>升级的namenode是前台运行的，不要关闭这个进程。接下来把另一台namenode同步一下。</p>

<h2>同步另一台namenode</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.3]$ bin/hdfs namenode -bootstrapStandby
</span><span class='line'>...
</span><span class='line'>=====================================================
</span><span class='line'>About to bootstrap Standby ID nn2 from:
</span><span class='line'>           Nameservice ID: zfcluster
</span><span class='line'>        Other Namenode ID: nn1
</span><span class='line'>  Other NN's HTTP address: http://hadoop-master1:50070
</span><span class='line'>  Other NN's IPC  address: hadoop-master1/172.17.0.1:8020
</span><span class='line'>             Namespace ID: 639021326
</span><span class='line'>            Block pool ID: BP-1695500896-172.17.0.1-1452152050513
</span><span class='line'>               Cluster ID: CID-7d5c31d8-5cd4-46c8-8e04-49151578e5bb
</span><span class='line'>           Layout version: -60
</span><span class='line'>       isUpgradeFinalized: false
</span><span class='line'>=====================================================
</span><span class='line'>16/01/08 09:15:19 INFO ha.BootstrapStandby: The active NameNode is in Upgrade. Prepare the upgrade for the standby NameNode as well.
</span><span class='line'>16/01/08 09:15:19 INFO common.Storage: Lock on /data/tmp/dfs/name/in_use.lock acquired by nodename 5008@hadoop-master2
</span><span class='line'>16/01/08 09:15:21 INFO namenode.TransferFsImage: Opening connection to http://hadoop-master1:50070/imagetransfer?getimage=1&txid=1126&storageInfo=-60:639021326:1452244437060:CID-7d5c31d8-5cd4-46c8-8e04-49151578e5bb
</span><span class='line'>16/01/08 09:15:21 INFO namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
</span><span class='line'>16/01/08 09:15:21 INFO namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
</span><span class='line'>16/01/08 09:15:21 INFO namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001126 size 977 bytes.
</span><span class='line'>16/01/08 09:15:21 INFO namenode.NNUpgradeUtil: Performing upgrade of storage directory /data/tmp/dfs/name
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<h2>重新启动集群</h2>

<p>ctrl+c关闭hadoop-master1 upgrade的namenode。启动整个集群。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ sbin/start-dfs.sh
</span><span class='line'>16/01/08 09:16:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Starting namenodes on [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master1: starting namenode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-namenode-hadoop-master1.out
</span><span class='line'>hadoop-master2: starting namenode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-namenode-hadoop-master2.out
</span><span class='line'>hadoop-slaver3: starting datanode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-datanode-hadoop-slaver3.out
</span><span class='line'>hadoop-slaver2: starting datanode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-datanode-hadoop-slaver2.out
</span><span class='line'>hadoop-slaver1: starting datanode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-datanode-hadoop-slaver1.out
</span><span class='line'>Starting journal nodes [hadoop-master1]
</span><span class='line'>hadoop-master1: journalnode running as process 31047. Stop it first.
</span><span class='line'>16/01/08 09:16:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Starting ZK Failover Controllers on NN hosts [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master2: starting zkfc, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-zkfc-hadoop-master2.out
</span><span class='line'>hadoop-master1: starting zkfc, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-zkfc-hadoop-master1.out
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ jps
</span><span class='line'>31047 JournalNode
</span><span class='line'>244 QuorumPeerMain
</span><span class='line'>31596 DFSZKFailoverController
</span><span class='line'>31655 Jps
</span><span class='line'>31294 NameNode
</span></code></pre></td></tr></table></div></figure>


<h2>后记：Journalnode重置</h2>

<p>在HA和non-HA环境来回的切换，最后启动HA时master起不来，执行bootstrapStandby也不行。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2016-01-08 06:15:36,746 WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: Unable to determine input streams from QJM to [172.17.0.1:8485]. Skipping.
</span><span class='line'>org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 1/1. 1 exceptions thrown:
</span><span class='line'>172.17.0.1:8485: Asked for firstTxId 1022 which is in the middle of file /data/journal/zfcluster/current/edits_0000000000000001021-0000000000000001022
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FileJournalManager.getRemoteEditLogs(FileJournalManager.java:198)
</span><span class='line'>        at org.apache.hadoop.hdfs.qjournal.server.Journal.getEditLogManifest(Journal.java:640)
</span><span class='line'>        at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.getEditLogManifest(JournalNodeRpcServer.java:181)
</span><span class='line'>        at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.getEditLogManifest(QJournalProtocolServerSideTranslatorPB.java:203)
</span><span class='line'>        at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:17453)
</span><span class='line'>        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>关闭集群，启动journalnode，跳转到没有问题的namenode机器，执行initializeSharedEdits命令。然后在有问题的namenode上重新初始化！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start journalnode
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.2.0]$ bin/hdfs namenode -initializeSharedEdits
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start namenode
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ bin/hdfs namenode -bootstrapStandby
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/start-dfs.sh</span></code></pre></td></tr></table></div></figure>


<p>后话： 其实上面HA升级的步骤，如果upgrade时没有启动journalnode，导致出了问题的话，把journalnode重置应该也是可以的（谨慎，没有试验过，猜想而已）。</p>

<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html#HDFS_UpgradeFinalizationRollback_with_HA_Enabled">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html#HDFS_UpgradeFinalizationRollback_with_HA_Enabled</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Upgrade_and_Rollback">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Upgrade_and_Rollback</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/24">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/22">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2019/04/10/try-k8s/">Try K8s</a>
			</li>
		
			<li class="post">
				<a href="/blog/2018/08/25/video-auto-translate/">视频自动翻译</a>
			</li>
		
			<li class="post">
				<a href="/blog/2018/06/09/reasonable-way-to-access-the-internet/">科学上网（续）</a>
			</li>
		
			<li class="post">
				<a href="/blog/2018/01/20/gitalk-on-octopress/">Gitalk on Octopress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/11/16/sphinx-generate-docs/">使用Sphinx生成/管理文档</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/10/30/windows-run-ubuntu/">Windows Run Ubuntu</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/07/30/kubeadm-install-kubenetes-on-centos7/">Kubeadm部署kubernetes</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/07/08/casperjs-crawler/">爬虫之CasperJS</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/05/23/spark-on-hive-speculation-shit-bug/">Hive on Spark预测性执行BUG一枚</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/01/27/vnc-server-on-centos7/">在Centos7上安装VNC Server</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/01/25/develop-environment-prepare/">[整理] 环境准备工具集</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/01/19/nginx-https/">Nginx配置https</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/09/19/163-open-movies-download/">批量下载163-open的视频</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/04/23/hadoop-guide-catalog/">[整理] Hadoop入门</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/04/04/rpm-build-your-package/">RPM打包</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/11/22/gfw-ladder/">搭梯笔记</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/02/23/quickly-open-program-in-windows/">[Windows运行]快速打开程序</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2021/09/15/k2-again/">斐讯K2刷机padavan</a>
      </li>
    
      <li class="post">
        <a href="/blog/2020/05/11/redmine-on-arm-pi/">在树莓派上部署redmine</a>
      </li>
    
      <li class="post">
        <a href="/blog/2020/04/12/appium-android-auto-test/">appium-Android自动化测试</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/07/26/android-linux-via-termux/">Android Linux via Termux</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/04/10/try-bk-dot-tencent-dot-com/">Try bk.tencent.com</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/04/10/try-k8s/">Try K8s</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/10/20/jcef-build-on-win64/">编译JCEF - Win64</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/08/25/video-auto-translate/">视频自动翻译</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/alluxio/'>alluxio</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/bigdata/'>bigdata</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/deprecated/'>deprecated</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/devops/'>devops</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (15) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/elasticsearch/'>elasticsearch</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/es/'>es</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/flume/'>flume</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (5) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (44) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (8) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (13) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (8) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jenkins/'>jenkins</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/k2/'>k2</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/k8s/'>k8s</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kubeadm/'>kubeadm</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/logstash/'>logstash</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/map/'>map</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/nginx/'>nginx</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/puppet/'>puppet</a> (11) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (27) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/staf/'>staf</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (69) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/vagrant/'>vagrant</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/wsl/'>wsl</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/zookeeper/'>zookeeper</a> (1) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (217)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
<!--
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
&#8211;>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2021 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

<script>

var time=location.pathname.substring(6).substring(0,11);
var eName=location.pathname.substring(17);
var gitalk = new Gitalk({
  clientID: 'c14f68eac6330d15d984',
  clientSecret: '73b7c1fffa98e299ff0cdd332821201933858e6e',
  repo: 'winse.github.com',
  owner: 'winse',
  admin: ['winse'],
  id: eName,
  labels: ['Gitalk', time],
  body: "http://winseliu.com" + location.pathname,
  createIssueManually: true,
  
  // facebook-like distraction free mode
  distractionFreeMode: false
})

gitalk.render('gitalk-container')

</script>



<script>
/*
$.ajax({
  type: "POST",
  url: "http://log.winseliu.com:20000",
  data: JSON.stringify({
    title: document.title,
    location: JSON.stringify(location),
    referrer: document.referrer,
    userAgent: navigator.userAgent
  }),
  contentType: "application/json; charset=utf-8",
  dataType: "json"
});
*/
</script>









</body>
</html>
