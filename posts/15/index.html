
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="升级hive的标准动作： 更新metadata，就是执行sql语句。更新前先备份原来的库！！
调整依赖，我这里是升级spark，编译参考spark-without-hive
修改参数(hive/spark/hadoop)来适应新版本
hiveserver2 ui：启动hiveserver2服务， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/15">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
  <li><a href="https://yunpan.cn/cuYhpFBPgQYgT" >Books[5aee]</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/13/hiveserver2-ui-and-upgrade-hive2-dot-0-0/">Hiveserver2 Ui and Upgrade hive2.0.0</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-13T04:03:43+00:00" pubdate data-updated="true">Wed 2016-04-13 04:03</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>升级hive的标准动作：</p>

<ul>
<li>更新metadata，就是执行sql语句。更新前先备份原来的库！！</li>
<li>调整依赖，我这里是升级spark，编译参考<a href="/blog/2016/03/28/hive-on-spark/">spark-without-hive</a></li>
<li>修改参数(hive/spark/hadoop)来适应新版本</li>
<li>hiveserver2 ui：启动hiveserver2服务，访问10002端口即可。<a href="https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2#SettingUpHiveServer2-WebUIforHiveServer2">UI配置</a></li>
</ul>


<p>环境说明：</p>

<ul>
<li>centos5</li>
<li>hadoop-2.6.3</li>
<li>spark-1.6.0-without-hive</li>
<li>hive-2.0.0</li>
</ul>


<h2>操作详情</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
<span class='line-number'>192</span>
<span class='line-number'>193</span>
<span class='line-number'>194</span>
<span class='line-number'>195</span>
<span class='line-number'>196</span>
<span class='line-number'>197</span>
<span class='line-number'>198</span>
<span class='line-number'>199</span>
<span class='line-number'>200</span>
<span class='line-number'>201</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 备份
</span><span class='line'>[hadoop@file1 tools]$ mysqldump -uroot -p hive &gt;hive1.2.1-20160413.backup.sql
</span><span class='line'>
</span><span class='line'># 准备好程序后的目录结构
</span><span class='line'>[hadoop@file1 ~]$ ll
</span><span class='line'>总计 20
</span><span class='line'>drwxrwxr-x 3 hadoop hadoop 4096 04-13 11:59 collect
</span><span class='line'>drwx------ 3 hadoop hadoop 4096 04-07 16:43 dfs
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop   18 04-11 10:09 hadoop -&gt; tools/hadoop-2.6.3
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop   40 04-13 10:26 hive -&gt; /home/hadoop/tools/apache-hive-2.0.0-bin
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop   42 04-13 10:52 spark -&gt; tools/spark-1.6.0-bin-hadoop2-without-hive
</span><span class='line'>drwxrwxr-x 6 hadoop hadoop 4096 04-13 12:10 tmp
</span><span class='line'>drwxrwxr-x 9 hadoop hadoop 4096 04-13 11:48 tools
</span><span class='line'>[hadoop@file1 tools]$ ll
</span><span class='line'>总计 84
</span><span class='line'>drwxrwxr-x  8 hadoop hadoop  4096 04-08 09:25 apache-hive-1.2.1-bin
</span><span class='line'>drwxrwxr-x  8 hadoop hadoop  4096 04-13 10:16 apache-hive-2.0.0-bin
</span><span class='line'>drwxr-xr-x 11 hadoop hadoop  4096 04-07 16:34 hadoop-2.6.3
</span><span class='line'>-rw-rw-r--  1 hadoop hadoop 46879 04-13 10:11 hive1.2.1-20160413.backup.sql
</span><span class='line'>drwxrwxr-x  2 hadoop hadoop  4096 03-31 15:28 mysql
</span><span class='line'>lrwxrwxrwx  1 hadoop hadoop    36 04-13 10:17 spark -&gt; spark-1.6.0-bin-hadoop2-without-hive
</span><span class='line'>drwxrwxr-x 11 hadoop hadoop  4096 04-07 18:23 spark-1.3.1-bin-hadoop2.6.3-without-hive
</span><span class='line'>drwxrwxr-x 11 hadoop hadoop  4096 03-28 11:15 spark-1.6.0-bin-hadoop2-without-hive
</span><span class='line'>drwxr-xr-x 11 hadoop hadoop  4096 03-31 16:14 zookeeper-3.4.6
</span><span class='line'>
</span><span class='line'># 环境变量我直接加载的是link软链接的，我这直接修改软链就行了。根据情况调整。
</span><span class='line'># apache-hive-2.0.0-bin同级目录建立spark软链接，或者再hive-env.sh中指定SPARK_HOME的位置
</span><span class='line'>
</span><span class='line'># hive-1.2.1并没有txn的表，所有要单独执行下hive-txn-schema-2.0.0.mysql.sql，
</span><span class='line'># 然后再更新（后面的Duplicate column的错没问题的）
</span><span class='line'>[hadoop@file1 tools]$ cd apache-hive-2.0.0-bin/scripts/metastore/upgrade/mysql/
</span><span class='line'>[hadoop@file1 mysql]$ mysql -uroot -p
</span><span class='line'>Enter password: 
</span><span class='line'>Welcome to the MySQL monitor.  Commands end with ; or \g.
</span><span class='line'>Your MySQL connection id is 10765
</span><span class='line'>Server version: 5.5.48 MySQL Community Server (GPL)
</span><span class='line'>
</span><span class='line'>Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.
</span><span class='line'>
</span><span class='line'>Oracle is a registered trademark of Oracle Corporation and/or its
</span><span class='line'>affiliates. Other names may be trademarks of their respective
</span><span class='line'>owners.
</span><span class='line'>
</span><span class='line'>Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
</span><span class='line'>
</span><span class='line'>mysql&gt; use hive;
</span><span class='line'>Reading table information for completion of table and column names
</span><span class='line'>You can turn off this feature to get a quicker startup with -A
</span><span class='line'>
</span><span class='line'>Database changed
</span><span class='line'>mysql&gt; source hive-txn-schema-2.0.0.mysql.sql
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.04 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.03 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.04 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.03 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>
</span><span class='line'>mysql&gt; source upgrade-1.2.0-to-2.0.0.mysql.sql
</span><span class='line'>+------------------------------------------------+
</span><span class='line'>|                                                |
</span><span class='line'>+------------------------------------------------+
</span><span class='line'>| Upgrading MetaStore schema from 1.2.0 to 2.0.0 |
</span><span class='line'>+------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>+---------------------------------------------------------------------------------------------------------------+
</span><span class='line'>|                                                                                                               |
</span><span class='line'>+---------------------------------------------------------------------------------------------------------------+
</span><span class='line'>| &lt; HIVE-7018 Remove Table and Partition tables column LINK_TARGET_ID from Mysql for other DBs do not have it &gt; |
</span><span class='line'>+---------------------------------------------------------------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected, 1 warning (0.03 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>+---------------------------------+
</span><span class='line'>| Completed remove LINK_TARGET_ID |
</span><span class='line'>+---------------------------------+
</span><span class='line'>| Completed remove LINK_TARGET_ID |
</span><span class='line'>+---------------------------------+
</span><span class='line'>1 row in set (0.02 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.02 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 31 rows affected (0.01 sec)
</span><span class='line'>Records: 31  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.05 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.02 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.03 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'CQ_HIGHEST_TXN_ID'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'CQ_META_INFO'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'CQ_HADOOP_JOB_ID'
</span><span class='line'>ERROR 1050 (42S01): Table 'COMPLETED_COMPACTIONS' already exists
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'TXN_AGENT_INFO'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'TXN_HEARTBEAT_COUNT'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'HL_HEARTBEAT_COUNT'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'TXN_META_INFO'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'HL_AGENT_INFO'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'HL_BLOCKEDBY_EXT_ID'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'HL_BLOCKEDBY_INT_ID'
</span><span class='line'>ERROR 1050 (42S01): Table 'AUX_TABLE' already exists
</span><span class='line'>Query OK, 1 row affected (0.01 sec)
</span><span class='line'>Rows matched: 1  Changed: 1  Warnings: 0
</span><span class='line'>
</span><span class='line'>+---------------------------------------------------------+
</span><span class='line'>|                                                         |
</span><span class='line'>+---------------------------------------------------------+
</span><span class='line'>| Finished upgrading MetaStore schema from 1.2.0 to 2.0.0 |
</span><span class='line'>+---------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'># 拷贝hive原来的配置和依赖jar
</span><span class='line'>
</span><span class='line'>[hadoop@file1 mysql]$ cd ~/tools/apache-hive-2.0.0-bin/conf/
</span><span class='line'>[hadoop@file1 conf]$ cp ~/tools/apache-hive-1.2.1-bin/conf/hive-site.xml ./
</span><span class='line'>[hadoop@file1 conf]$ cp ~/tools/apache-hive-1.2.1-bin/conf/spark-defaults.conf ./
</span><span class='line'>[hadoop@file1 conf]$ cp ~/tools/apache-hive-1.2.1-bin/conf/hive-env.sh ./
</span><span class='line'>
</span><span class='line'># 用到spark需要加大PermSize
</span><span class='line'>[hadoop@file1 hive]$ vi conf/hive-env.sh
</span><span class='line'>export HADOOP_USER_CLASSPATH_FIRST=true
</span><span class='line'>export HADOOP_OPTS="$HADOOP_OPTS -XX:MaxPermSize=256m"
</span><span class='line'>
</span><span class='line'>[hadoop@file1 conf]$ cd ../lib/
</span><span class='line'>[hadoop@file1 lib]$ cp ~/tools/apache-hive-1.2.1-bin/lib/mysql-connector-java-5.1.34.jar ./
</span><span class='line'>
</span><span class='line'># centos5需要删除下面两个jar，centos6没必要删
</span><span class='line'>[hadoop@file1 apache-hive-2.0.0-bin]$ rm lib/hive-jdbc-2.0.0-standalone.jar 
</span><span class='line'>[hadoop@file1 apache-hive-2.0.0-bin]$ rm lib/snappy-java-1.0.5.jar 
</span><span class='line'>
</span><span class='line'># spark-1.6.0更新
</span><span class='line'>
</span><span class='line'># http://spark.apache.org/docs/latest/hadoop-provided.html
</span><span class='line'># http://stackoverflow.com/questions/30906412/noclassdeffounderror-com-apache-hadoop-fs-fsdatainputstream-when-execute-spark-s
</span><span class='line'>[hadoop@file1 apache-hive-2.0.0-bin]$ cd ~/tools/spark-1.6.0-bin-hadoop2-without-hive/conf/
</span><span class='line'>[hadoop@file1 conf]$ cp spark-env.sh.template spark-env.sh
</span><span class='line'>[hadoop@file1 conf]$ vi spark-env.sh
</span><span class='line'>HADOOP_HOME=/home/hadoop/hadoop
</span><span class='line'>SPARK_DIST_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath`
</span><span class='line'>
</span><span class='line'>[hadoop@file1 ~]$ cp ~/tools/spark-1.6.0-bin-hadoop2-without-hive/lib/spark-1.6.0-yarn-shuffle.jar ~/tools/hadoop-2.6.3/share/hadoop/yarn/
</span><span class='line'>[hadoop@file1 ~]$ rm ~/tools/hadoop-2.6.3/share/hadoop/yarn/spark-1.3.1-yarn-shuffle.jar 
</span><span class='line'>
</span><span class='line'>[hadoop@file1 ~]$ rsync -vaz --delete ~/tools/hadoop-2.6.3/share file2:~/tools/hadoop-2.6.3/ 
</span><span class='line'>[hadoop@file1 ~]$ rsync -vaz --delete ~/tools/hadoop-2.6.3/share file3:~/tools/hadoop-2.6.3/ 
</span><span class='line'>
</span><span class='line'>[hadoop@file1 ~]$ hdfs dfs -put ~/tools/spark-1.6.0-bin-hadoop2-without-hive/lib/spark-assembly-1.6.0-hadoop2.6.3.jar /spark/
</span><span class='line'>
</span><span class='line'>[hadoop@file1 apache-hive-2.0.0-bin]$ vi conf/spark-defaults.conf 
</span><span class='line'>spark.yarn.jar    hdfs:///spark/spark-assembly-1.6.0-hadoop2.6.3.jar
</span><span class='line'>
</span><span class='line'># 重启yarn（如果你用hiveserver2，先往下看，后面还会修改配置重启的）
</span><span class='line'>
</span><span class='line'>[hadoop@file1 apache-hive-2.0.0-bin]$ cd ~/tools/hadoop-2.6.3/
</span><span class='line'>[hadoop@file1 hadoop-2.6.3]$ sbin/stop-yarn.sh 
</span><span class='line'>[hadoop@file1 hadoop-2.6.3]$ sbin/start-yarn.sh 
</span></code></pre></td></tr></table></div></figure>


<p>更新到这里，执行hive命令是ok了的。但是hiveserver还有问题。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 启动hiveserver2
</span><span class='line'>[hadoop@file1 hive]$ nohup bin/hiveserver2 &
</span><span class='line'>
</span><span class='line'># 启动spark historyserver
</span><span class='line'>[hadoop@file1 spark]$ cat start-historyserver.sh 
</span><span class='line'>source $HADOOP_HOME/libexec/hadoop-config.sh
</span><span class='line'>sbin/start-history-server.sh hdfs:///spark-eventlogs
</span><span class='line'>
</span><span class='line'>[hadoop@file1 hive]$ bin/beeline -u jdbc:hive2://file1:10000/ -n hadoop -p hadoop
</span><span class='line'>which: no hbase in (/home/hadoop/hadoop/bin:/home/hadoop/hive/bin:/opt/jdk1.7.0_60/bin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin:/home/hadoop/tools/hadoop-2.6.3/bin:/home/hadoop/tools/hadoop-2.6.3:/home/hadoop/tools/apache-hive-1.2.1-bin:/home/hadoop/bin)
</span><span class='line'>ls: /home/hadoop/hive/lib/hive-jdbc-*-standalone.jar: 没有那个文件或目录
</span><span class='line'>SLF4J: Class path contains multiple SLF4J bindings.
</span><span class='line'>SLF4J: Found binding in [jar:file:/home/hadoop/tools/apache-hive-2.0.0-bin/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
</span><span class='line'>SLF4J: Found binding in [jar:file:/home/hadoop/tools/hadoop-2.6.3/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
</span><span class='line'>SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
</span><span class='line'>SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
</span><span class='line'>Connecting to jdbc:hive2://file1:10000/
</span><span class='line'>Error: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: hadoop is not allowed to impersonate hadoop (state=,code=0)
</span><span class='line'>Beeline version 2.0.0 by Apache Hive
</span><span class='line'>beeline&gt; </span></code></pre></td></tr></table></div></figure>


<p>Beeline连接hiveserver2失败，模拟的hadoop用户授权失败。需要修改hadoop的参数。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># https://community.hortonworks.com/questions/4905/error-while-running-hive-queries-from-zeppelin.html
</span><span class='line'># http://stackoverflow.com/questions/25073792/error-e0902-exception-occured-user-root-is-not-allowed-to-impersonate-root
</span><span class='line'># core-site.xml添加，并重启集群hdfs & yarn
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@file1 hadoop-2.6.3]$ sbin/stop-all.sh
</span><span class='line'>[hadoop@file1 hadoop-2.6.3]$ sbin/start-all.sh 
</span><span class='line'>
</span><span class='line'>[hadoop@file1 hive]$ bin/beeline -u jdbc:hive2://file1:10000 -n hadoop -p hadoop
</span><span class='line'>...
</span><span class='line'>0: jdbc:hive2://file1:10000/&gt; set hive.execution.engine=spark;
</span><span class='line'>No rows affected (0.019 seconds)
</span><span class='line'>0: jdbc:hive2://file1:10000/&gt; select count(*) from t_info where edate=20160413;
</span><span class='line'>INFO  : Compiling command(queryId=hadoop_20160413114039_f930d3e7-af83-4b12-a536-404a4e20eeea): select count(*) from t_info where edate=20160413
</span><span class='line'>INFO  : Semantic Analysis Completed
</span><span class='line'>INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c0, type:bigint, comment:null)], properties:null)
</span><span class='line'>INFO  : Completed compiling command(queryId=hadoop_20160413114039_f930d3e7-af83-4b12-a536-404a4e20eeea); Time taken: 0.523 seconds
</span><span class='line'>INFO  : Executing command(queryId=hadoop_20160413114039_f930d3e7-af83-4b12-a536-404a4e20eeea): select count(*) from t_info where edate=20160413
</span><span class='line'>INFO  : Query ID = hadoop_20160413114039_f930d3e7-af83-4b12-a536-404a4e20eeea
</span><span class='line'>INFO  : Total jobs = 1
</span><span class='line'>INFO  : Launching Job 1 out of 1
</span><span class='line'>INFO  : Starting task [Stage-1:MAPRED] in serial mode
</span><span class='line'>
</span><span class='line'>INFO  : 
</span><span class='line'>Query Hive on Spark job[0] stages:
</span><span class='line'>INFO  : 0
</span><span class='line'>INFO  : 1
</span><span class='line'>INFO  : 
</span><span class='line'>Status: Running (Hive on Spark job[0])
</span><span class='line'>INFO  : Job Progress Format
</span><span class='line'>CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
</span><span class='line'>INFO  : 2016-04-13 11:41:20,519 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:23,577 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:26,817 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:29,858 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:32,903 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:35,942 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:37,969 Stage-0_0: 0(+9)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:38,981 Stage-0_0: 1(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:39,994 Stage-0_0: 3(+7)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:43,030 Stage-0_0: 3(+7)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:45,056 Stage-0_0: 5(+5)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:46,072 Stage-0_0: 6(+4)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:47,085 Stage-0_0: 8(+2)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:48,096 Stage-0_0: 9(+1)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:51,125 Stage-0_0: 9(+1)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:52,134 Stage-0_0: 10/10 Finished       Stage-1_0: 1/1 Finished
</span><span class='line'>INFO  : Status: Finished successfully in 64.78 seconds
</span><span class='line'>INFO  : Completed executing command(queryId=hadoop_20160413114039_f930d3e7-af83-4b12-a536-404a4e20eeea); Time taken: 71.767 seconds
</span><span class='line'>INFO  : OK
</span><span class='line'>+-----------+--+
</span><span class='line'>|    c0     |
</span><span class='line'>+-----------+--+
</span><span class='line'>| 89867722  |
</span><span class='line'>+-----------+--+
</span><span class='line'>1 row selected (72.45 seconds)
</span></code></pre></td></tr></table></div></figure>


<p>本来升级是想看看UI长什么样子，有点失望，功能太少了。只能看当前执行的SQL和session，历史记录不能查看。期待新版本UI更强大。</p>

<p>升级后beeline上下键切换历史的也不起作用了，hive-2.0.0没也啥吸引的功能（<strong>hive2准备淘汰mr了</strong>），觉得不爽可以直接替换 软链 退回hive1.2.1-spark1.3.1（实践后没问题，spark.yarn.jar记得改）</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/11/spark-on-yarn-memory-allocate/">Spark-on-yarn内存分配</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-11T11:44:51+00:00" pubdate data-updated="true">Mon 2016-04-11 11:44</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>上次写了一篇关于配置参数是如何影响mapreduce的实际调度的<a href="/blog/2016/03/17/hadoop-memory-opts-and-args/">参考</a>：</p>

<ul>
<li>opts（yarn.app.mapreduce.am.command-opts、mapreduce.map.java.opts、mapreduce.reduce.java.opts）是实际运行程序是内存参数。</li>
<li>memory（yarn.app.mapreduce.am.resource.mb、mapreduce.map.memory.mb、mapreduce.reduce.memory.mb）是用于ResourceManager计算集群资源使用和调度。</li>
</ul>


<p>了解参数区别，就没有再深究task内存的问题了。</p>

<h2>新问题-内存分配</h2>

<p>这次又遇到内存问题：spark使用yarn-client的方式运行时，spark有memoryOverhead的设置，但是加了额外的内存后，再经过集群调度内存浪费严重，对于本来就小内存的集群来说完全无法接受。</p>

<ul>
<li>am默认是512加上384 overhead，也就是896m。但是调度后am分配内存资源为1024。</li>
<li>executor默认是1024加上384，等于1408M。单调度后executor分配内存资源为2048。</li>
</ul>


<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-0.png" alt="" /></p>

<p>从appmaster的日志可以看出来请求的内存大小是1408：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-1.png" alt="" /></p>

<p><strong>一个executor就浪费了500M，本来可以跑4个executor的但现在只能执行3个！</strong></p>

<p>关于内存参数的具体含义查看官网： <a href="http://spark.apache.org/docs/latest/running-on-yarn.html">spark-on-yarn</a> 和 <a href="http://hadoop.apache.org/docs/r2.6.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a></p>

<table>
<thead>
<tr>
<th></th>
<th style="text-align:center;"> <em>参数</em>                                  </th>
<th></th>
<th style="text-align:left;"> <em>值</em></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.am.memory                    </td>
<td></td>
<td style="text-align:left;"> 512m</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.driver.memory                     </td>
<td></td>
<td style="text-align:left;"> 1g</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.executor.memoryOverhead      </td>
<td></td>
<td style="text-align:left;"> executorMemory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.driver.memoryOverhead        </td>
<td></td>
<td style="text-align:left;"> driverMemory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.am.memoryOverhead            </td>
<td></td>
<td style="text-align:left;"> AM memory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.nodemanager.resource.memory-mb     </td>
<td></td>
<td style="text-align:left;"> 8192</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.scheduler.minimum-allocation-mb    </td>
<td></td>
<td style="text-align:left;"> 1024</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.scheduler.maximum-allocation-mb    </td>
<td></td>
<td style="text-align:left;"> 8192</td>
</tr>
</tbody>
</table>


<p>分配的内存看着像是 <strong>最小分配内存</strong> 的整数倍。把 <code>yarn.scheduler.minimum-allocation-mb</code> 修改为512，重启yarn再运行，executor的分配的内存果真减少到1536(<strong>512*3</strong>)。</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-3.png" alt="" /></p>

<p>同时 <a href="http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html">http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html</a> 这篇文章也讲 <strong>在YARN中，Container申请的内存大小必须为yarn.scheduler.minimum-allocation-mb的整数倍</strong> 。我们不去猜，调试下调度代码，看看究竟是什么情况。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ sbin/yarn-daemon.sh stop resourcemanager 
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop]$ grep "minimum-allocation-mb" -1 yarn-site.xml 
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;&lt;value&gt;512&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ export YARN_RESOURCEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8000"
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ sbin/yarn-daemon.sh start resourcemanager </span></code></pre></td></tr></table></div></figure>


<p>本地eclipse在 <code>CapacityScheduler#allocate</code> 打断点，然后跑任务：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hive&gt; set hive.execution.engine=spark;
</span><span class='line'>hive&gt; select count(*) from t_ods_access_log2 where month=201512;</span></code></pre></td></tr></table></div></figure>


<p>AppMaster内存分配：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-appmaster.png" alt="" /></p>

<p>Executor内存分配：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-executor.png" alt="" /></p>

<p>request进到allocate后，最终调用 <code>DefaultResourceCalculator.normalize</code> 重新计算了一遍请求需要的资源，把内存调整了。默认的DefaultResourceCalculator可以通过 capacity-scheduler.xml 的 <code>yarn.scheduler.capacity.resource-calculator</code> 来修改。</p>

<p>具体代码调度过程如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public Allocation allocate(ApplicationAttemptId applicationAttemptId,
</span><span class='line'>      List&lt;ResourceRequest&gt; ask, List&lt;ContainerId&gt; release, 
</span><span class='line'>      List&lt;String&gt; blacklistAdditions, List&lt;String&gt; blacklistRemovals) {
</span><span class='line'>    ...
</span><span class='line'>    // Sanity check
</span><span class='line'>    SchedulerUtils.normalizeRequests(
</span><span class='line'>        ask, getResourceCalculator(), getClusterResource(),
</span><span class='line'>        getMinimumResourceCapability(), maximumAllocation);
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>  public static void normalizeRequest(
</span><span class='line'>      ResourceRequest ask, 
</span><span class='line'>      ResourceCalculator resourceCalculator, 
</span><span class='line'>      Resource clusterResource,
</span><span class='line'>      Resource minimumResource,
</span><span class='line'>      Resource maximumResource,
</span><span class='line'>      Resource incrementResource) {
</span><span class='line'>    Resource normalized = 
</span><span class='line'>        Resources.normalize(
</span><span class='line'>            resourceCalculator, ask.getCapability(), minimumResource,
</span><span class='line'>            maximumResource, incrementResource);
</span><span class='line'>    ask.setCapability(normalized);
</span><span class='line'>  }   
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>  public static Resource normalize(
</span><span class='line'>      ResourceCalculator calculator, Resource lhs, Resource min,
</span><span class='line'>      Resource max, Resource increment) {
</span><span class='line'>    return calculator.normalize(lhs, min, max, increment);
</span><span class='line'>  }
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>  public Resource normalize(Resource r, Resource minimumResource,
</span><span class='line'>      Resource maximumResource, Resource stepFactor) {
</span><span class='line'>    int normalizedMemory = Math.min(
</span><span class='line'>        roundUp(
</span><span class='line'>            Math.max(r.getMemory(), minimumResource.getMemory()),
</span><span class='line'>            stepFactor.getMemory()),
</span><span class='line'>            maximumResource.getMemory());
</span><span class='line'>    return Resources.createResource(normalizedMemory);
</span><span class='line'>  }
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>  public static int roundUp(int a, int b) {
</span><span class='line'>    return divideAndCeil(a, b) * b;
</span><span class='line'>  }
</span><span class='line'>  </span></code></pre></td></tr></table></div></figure>


<p></p>

<h2>小结</h2>

<p>今天又重新认识一个yarn参数 <code>yarn.scheduler.minimum-allocation-mb</code> ，不仅仅是最小分配的内存，同时分配的资源也是minimum-allocation-mb的整数倍，还告诉我们 <code>yarn.nodemanager.resource.memory-mb</code> 也最好是minimum-allocation-mb的整数倍。</p>

<p>间接的学习了新的参数，可以通过 <code>yarn.scheduler.capacity.resource-calculator</code> 参数 来修改 CapacityScheduler 调度器的资源计算类。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/08/snappy-centos5-on-hive-on-spark/">Hive-on-spark Snappy on Centos5</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-08T14:27:06+00:00" pubdate data-updated="true">Fri 2016-04-08 14:27</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>hive的assembly包就是一个坑货！既然是一个单独的可运行的jar放到lib包下面干嘛呢！！纯属记录工作过程总的经历，想找干货的飘过吧！！</p>

<p><br/></p>

<p>上周支撑部门其他项目的hadoop项目，由于 <strong>hive mr</strong> 比较慢，想用spark试一试看能不能优化。但是系统使用Centos5，我们项目使用的是Centos6。按部就班的编译呗，hive-on-saprk启用SNAPPY的必要条件：</p>

<ul>
<li>hadoop使用snappy需要native的支持，首先当然是Centos5上编译hadoop。(现在看来可以不必要，但每次hdfs命令都提示我native的错误就很不爽)</li>
<li>hive增加spark。</li>
</ul>


<p>各程序版本信息：</p>

<ul>
<li>hadoop-2.6.3</li>
<li>hive-1.2.1</li>
<li>spark-1.3.1</li>
<li>centos5.4</li>
</ul>


<h2>编译hadoop-snappy</h2>

<ul>
<li>centos5手动</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost snappy-1.1.3]# ./autogen.sh 
</span><span class='line'>Remember to add `AC_PROG_LIBTOOL' to `configure.ac'.
</span><span class='line'>You should update your `aclocal.m4' by running aclocal.
</span><span class='line'>libtoolize: `config.guess' exists: use `--force' to overwrite
</span><span class='line'>libtoolize: `config.sub' exists: use `--force' to overwrite
</span><span class='line'>libtoolize: `ltmain.sh' exists: use `--force' to overwrite
</span><span class='line'>Makefile.am:4: Libtool library used but `LIBTOOL' is undefined
</span><span class='line'>Makefile.am:4: 
</span><span class='line'>Makefile.am:4: The usual way to define `LIBTOOL' is to add `AC_PROG_LIBTOOL'
</span><span class='line'>Makefile.am:4: to `configure.ac' and run `aclocal' and `autoconf' again.
</span><span class='line'>Makefile.am:20: `dist_doc_DATA' is used but `docdir' is undefined</span></code></pre></td></tr></table></div></figure>


<p>在centos5上面手动编译搞不定，不是专业写C的，这些问题就是天书啊(查了很多资料，试了很多方法都没通)！！ <strong>Snappy可以在centos6上面编译，编译好以后再centos5上面也能用，编译hadoop-snappy也是ok的</strong> 。</p>

<ul>
<li>centos5-rpm</li>
</ul>


<p>这里直接用rpm安装snappy。觉得创建虚拟机麻烦的话，也可以用docker。docker不同版本的centos下载： <a href="https://github.com/CentOS/sig-cloud-instance-images/">https://github.com/CentOS/sig-cloud-instance-images/</a> 。然后docker共享host主机的文件： <code>docker run -ti -v /home/hadoop:/home/hadoop -v /opt:/opt -v /data:/data centos:centos5 /bin/bash</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@8fb11f6b3ced ~]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 5.11 (Final)
</span><span class='line'>
</span><span class='line'>https://www.rpmfind.net/linux/rpm2html/search.php?query=snappy
</span><span class='line'>https://www.rpmfind.net/linux/rpm2html/search.php?query=snappy-devel
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# rpm -ivh snappy-1.0.5-1.el5.x86_64.rpm 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# rpm -ivh snappy-devel-1.0.5-1.el5.x86_64.rpm                                                                                  
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# rpm -ql snappy-devel snappy
</span><span class='line'>/usr/include/snappy-c.h
</span><span class='line'>/usr/include/snappy-sinksource.h
</span><span class='line'>/usr/include/snappy-stubs-public.h
</span><span class='line'>/usr/include/snappy.h
</span><span class='line'>/usr/lib64/libsnappy.so
</span><span class='line'>/usr/share/doc/snappy-devel-1.0.5
</span><span class='line'>/usr/share/doc/snappy-devel-1.0.5/format_description.txt
</span><span class='line'>/usr/lib64/libsnappy.so.1
</span><span class='line'>/usr/lib64/libsnappy.so.1.1.3
</span><span class='line'>/usr/share/doc/snappy-1.0.5
</span><span class='line'>/usr/share/doc/snappy-1.0.5/AUTHORS
</span><span class='line'>/usr/share/doc/snappy-1.0.5/COPYING
</span><span class='line'>/usr/share/doc/snappy-1.0.5/ChangeLog
</span><span class='line'>/usr/share/doc/snappy-1.0.5/NEWS
</span><span class='line'>/usr/share/doc/snappy-1.0.5/README
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# export JAVA_HOME=/opt/jdk1.7.0_17
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# export MAVEN_HOME=/opt/apache-maven-3.3.9
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# export PATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PATH
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]#  
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# yum install which gcc gcc-c++ zlib-devel make -y
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# cd protobuf-2.5.0
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# ./configure 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# make && make install
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# which protoc
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# yum install cmake openssl openssl-devel -y
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# cd hadoop-2.6.3-src/
</span><span class='line'># bundle.snappy和snappy.lib一起使用，可以把系统的snappy.so文件拷贝到lib/native下面（方便拷贝）
</span><span class='line'># &lt;http://grepcode.com/file/repo1.maven.org/maven2/org.apache.hadoop/hadoop-project-dist/2.6.0/META-INF/maven/org.apache.hadoop/hadoop-project-dist/pom.xml&gt;
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# mvn clean package -Dmaven.javadoc.skip=true -DskipTests -Drequire.snappy=true -Dbundle.snappy=true -Dsnappy.lib=/usr/lib64 -Pdist,native
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# ll hadoop-dist/target/hadoop-2.6.3/lib/native/
</span><span class='line'>total 3808
</span><span class='line'>-rw-r--r-- 1 root root 1036552 Apr 12 09:35 libhadoop.a
</span><span class='line'>-rw-r--r-- 1 root root 1212600 Apr 12 09:36 libhadooppipes.a
</span><span class='line'>lrwxrwxrwx 1 root root      18 Apr 12 09:35 libhadoop.so -&gt; libhadoop.so.1.0.0
</span><span class='line'>-rwxr-xr-x 1 root root  613267 Apr 12 09:35 libhadoop.so.1.0.0
</span><span class='line'>-rw-r--r-- 1 root root  401836 Apr 12 09:36 libhadooputils.a
</span><span class='line'>-rw-r--r-- 1 root root  364026 Apr 12 09:35 libhdfs.a
</span><span class='line'>lrwxrwxrwx 1 root root      16 Apr 12 09:35 libhdfs.so -&gt; libhdfs.so.0.0.0
</span><span class='line'>-rwxr-xr-x 1 root root  229672 Apr 12 09:35 libhdfs.so.0.0.0
</span><span class='line'>lrwxrwxrwx 1 root root      18 Apr 12 09:35 libsnappy.so -&gt; libsnappy.so.1.1.3
</span><span class='line'>lrwxrwxrwx 1 root root      18 Apr 12 09:35 libsnappy.so.1 -&gt; libsnappy.so.1.1.3
</span><span class='line'>-rwxr-xr-x 1 root root   21568 Apr 12 09:35 libsnappy.so.1.1.3
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# cd hadoop-dist/target/hadoop-2.6.3/
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3]# bin/hadoop checknative -a
</span><span class='line'>16/04/12 09:38:29 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
</span><span class='line'>16/04/12 09:38:29 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
</span><span class='line'>Native library checking:
</span><span class='line'>hadoop:  true /data/bigdata/sources/hadoop-2.6.3-src/hadoop-dist/target/hadoop-2.6.3/lib/native/libhadoop.so.1.0.0
</span><span class='line'>zlib:    true /lib64/libz.so.1
</span><span class='line'>snappy:  true /data/bigdata/sources/hadoop-2.6.3-src/hadoop-dist/target/hadoop-2.6.3/lib/native/libsnappy.so.1
</span><span class='line'>lz4:     true revision:99
</span><span class='line'>bzip2:   false 
</span><span class='line'>openssl: false org.apache.hadoop.crypto.OpensslCipher.initIDs()V
</span><span class='line'>16/04/12 09:38:29 INFO util.ExitUtil: Exiting with status 1</span></code></pre></td></tr></table></div></figure>


<p>把native下面的打tar包，然后替换生产的。一切都是正常的。接下来坑爹的是spark-snappy，具体的说应该是hive-assmably坑！！</p>

<h2>hive-on-spark snappy</h2>

<p>spark官网也没讲使用snappy需要做什么额外的配置（默认spark.io.compression.codec默认为snappy）。部署后设置 <code>hive.execution.engine=spark</code> 执行spark查询，立马就报错了 <strong> Caused by: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.0.5-libsn
appyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9&#8217; not found (required by /tmp/snappy-1.0.5-libsnappyjava.so)</strong> 从错误堆栈看与hadoop-native-snappy没关系，而是一个snappy-java的包。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 ~]$ strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX
</span><span class='line'>GLIBCXX_3.4
</span><span class='line'>GLIBCXX_3.4.1
</span><span class='line'>GLIBCXX_3.4.2
</span><span class='line'>GLIBCXX_3.4.3
</span><span class='line'>GLIBCXX_3.4.4
</span><span class='line'>GLIBCXX_3.4.5
</span><span class='line'>GLIBCXX_3.4.6
</span><span class='line'>GLIBCXX_3.4.7
</span><span class='line'>GLIBCXX_3.4.8
</span><span class='line'>GLIBCXX_FORCE_NEW</span></code></pre></td></tr></table></div></figure>


<p>确实缺少GLIBCXX_3.4.9，最新版本的centos5.11也是一样输出的。</p>

<p>spark的配置为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark.yarn.jar    hdfs:///spark/spark-assembly-1.3.1-hadoop2.6.3.jar
</span><span class='line'>
</span><span class='line'>spark.master  yarn-client
</span><span class='line'>
</span><span class='line'>spark.dynamicAllocation.enabled    true
</span><span class='line'>spark.shuffle.service.enabled      true
</span><span class='line'>spark.dynamicAllocation.minExecutors    2 
</span><span class='line'>spark.dynamicAllocation.maxExecutors    18
</span><span class='line'>
</span><span class='line'>spark.driver.maxResultSize   0
</span><span class='line'>spark.master=yarn-client
</span><span class='line'>spark.driver.memory=5g
</span><span class='line'>spark.eventLog.enabled  true
</span><span class='line'>spark.eventLog.compress  true
</span><span class='line'>spark.eventLog.dir    hdfs:///spark-eventlogs
</span><span class='line'>spark.yarn.historyServer.address file1:18080
</span><span class='line'>
</span><span class='line'>spark.serializer        org.apache.spark.serializer.KryoSerializer
</span><span class='line'>spark.kryoserializer.buffer.max    512m</span></code></pre></td></tr></table></div></figure>


<p>报错的具体信息：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>- 16/04/12 20:20:08 INFO storage.BlockManagerMaster: Registered BlockManager
</span><span class='line'>- java.lang.reflect.InvocationTargetException
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>-        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>-        at java.lang.reflect.Method.invoke(Method.java:606)
</span><span class='line'>-        at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:322)
</span><span class='line'>-        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
</span><span class='line'>-        at org.xerial.snappy.Snappy.&lt;clinit&gt;(Snappy.java:48)
</span><span class='line'>-        at org.apache.spark.io.SnappyCompressionCodec.&lt;init&gt;(CompressionCodec.scala:150)
</span><span class='line'>-        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
</span><span class='line'>-        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
</span><span class='line'>-        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
</span><span class='line'>-        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
</span><span class='line'>-        at org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:68)
</span><span class='line'>-        at org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:60)
</span><span class='line'>-        at org.apache.spark.scheduler.EventLoggingListener.&lt;init&gt;(EventLoggingListener.scala:67)
</span><span class='line'>-        at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:400)
</span><span class='line'>-        at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:61)
</span><span class='line'>-        at org.apache.hive.spark.client.RemoteDriver.&lt;init&gt;(RemoteDriver.java:169)
</span><span class='line'>-        at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>-        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>-        at java.lang.reflect.Method.invoke(Method.java:606)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
</span><span class='line'>- Caused by: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.0.5-libsnappyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by /tmp/snappy-1.0.5-libs
</span><span class='line'>-        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
</span><span class='line'>-        at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1965)
</span><span class='line'>-        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1890)
</span><span class='line'>-        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1851)
</span><span class='line'>-        at java.lang.Runtime.load0(Runtime.java:795)
</span><span class='line'>-        at java.lang.System.load(System.java:1062)
</span><span class='line'>-        at org.xerial.snappy.SnappyNativeLoader.load(SnappyNativeLoader.java:39)
</span><span class='line'>-        ... 28 more</span></code></pre></td></tr></table></div></figure>


<p>spark用到了snappy-java来处理snappy的解压缩。用jinfo获取SparkSubmit进程的classpath，用这个classpath跑helloworld确实是报错的，但是单独用hadoop-common下面的 snappy-java-1.0.4.1.jar 是没问题的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 snappy-java-test]$ cat Hello.java 
</span><span class='line'>import org.xerial.snappy.Snappy;
</span><span class='line'>
</span><span class='line'>public class Hello { 
</span><span class='line'>public static void main(String[] args) throws Exception {
</span><span class='line'>String input = "Hello snappy-java!";
</span><span class='line'>
</span><span class='line'>byte[] compressed = Snappy.compress(input.getBytes("utf-8"));
</span><span class='line'>byte[] uncompressed = Snappy.uncompress(compressed);
</span><span class='line'>
</span><span class='line'>String result = new String(uncompressed, "utf-8");
</span><span class='line'>System.out.println(result);
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[hadoop@file1 snappy-java-test]$ java -cp .:/home/hadoop/tools/hadoop-2.6.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar Hello
</span><span class='line'>Hello snappy-java!</span></code></pre></td></tr></table></div></figure>


<p>而而而，classpath中就只有hadoop-common和hadoop-mapreduce下面有snappy-java包，并且都是1.0.4.1，那TMD的使用SparkSubmit-classpath加载Snappy是哪个jar里面的呢？</p>

<p>调整后的helloworld为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 snappy-java-test]$ cat Hello.java 
</span><span class='line'>import org.xerial.snappy.Snappy;
</span><span class='line'>
</span><span class='line'>public class Hello { 
</span><span class='line'>public static void main(String[] args) throws Exception {
</span><span class='line'>String input = "Hello snappy-java!";
</span><span class='line'>
</span><span class='line'>System.out.println(Snappy.class.getProtectionDomain());
</span><span class='line'>byte[] compressed = Snappy.compress(input.getBytes("utf-8"));
</span><span class='line'>byte[] uncompressed = Snappy.uncompress(compressed);
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>String result = new String(uncompressed, "utf-8");
</span><span class='line'>System.out.println(result);
</span><span class='line'>}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>添加getProtectionDomain查看加载类的jar。再编译跑一次，这次终于找到真凶了！！hive-assembly，assembly包还放在lib下面就tmd的是一个坑货！！hive-exec的guava已经坑了很多人了，这次换hive-jdbc了！！(我这里的环境是centos5，centos6是没有这个问题的！！)</p>

<p><img src="/images/blogs/hive-on-spark-centos5-snappy-hive-jdbc.png" alt="" /></p>

<p>如果指定使用hadoop编译依赖的snappy.so.1.1.3动态链接库会出现版本不兼容的问题。还是干掉hive-jdbc-standalone吧。。。囧</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 查看源码SnappyLoader#loadSnappySystemProperties，可以通过配置指定使用系统动态链接库
</span><span class='line'>[hadoop@file1 snappy-java-test]$ cat org-xerial-snappy.properties 
</span><span class='line'>org.xerial.snappy.use.systemlib=true
</span><span class='line'>[hadoop@file1 snappy-java-test]$ ln -s /home/hadoop/tools/hadoop-2.6.3/lib/native/libsnappy.so libsnappyjava.so
</span><span class='line'>[hadoop@file1 snappy-java-test]$ ll
</span><span class='line'>总计 1240
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop     854 04-08 10:11 Hello.class
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop     408 04-08 10:11 Hello.java
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop      55 04-12 19:37 libsnappyjava.so -&gt; /home/hadoop/tools/hadoop-2.6.3/lib/native/libsnappy.so
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop      37 04-12 19:15 org-xerial-snappy.properties
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop 1251514 2014-04-29 snappy-java-1.0.5.jar
</span><span class='line'>[hadoop@file1 snappy-java-test]$ java -cp .:snappy-java-1.0.5.jar -Djava.library.path=. Hello
</span><span class='line'>ProtectionDomain  (file:/home/hadoop/snappy-java-test/snappy-java-1.0.5.jar &lt;no signer certificates&gt;)
</span><span class='line'> sun.misc.Launcher$AppClassLoader@333cb1eb
</span><span class='line'> &lt;no principals&gt;
</span><span class='line'> java.security.Permissions@7377711 (
</span><span class='line'> ("java.io.FilePermission" "/home/hadoop/snappy-java-test/snappy-java-1.0.5.jar" "read")
</span><span class='line'> ("java.lang.RuntimePermission" "exitVM")
</span><span class='line'>)
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Exception in thread "main" java.lang.UnsatisfiedLinkError: org.xerial.snappy.SnappyNative.maxCompressedLength(I)I
</span><span class='line'>        at org.xerial.snappy.SnappyNative.maxCompressedLength(Native Method)
</span><span class='line'>        at org.xerial.snappy.Snappy.maxCompressedLength(Snappy.java:320)
</span><span class='line'>        at org.xerial.snappy.Snappy.rawCompress(Snappy.java:333)
</span><span class='line'>        at org.xerial.snappy.Snappy.compress(Snappy.java:92)
</span><span class='line'>        at Hello.main(Hello.java:8)
</span><span class='line'>      </span></code></pre></td></tr></table></div></figure>


<p>删掉jdbc-standalone后，hive-on-spark就ok了。如果你无法下手删除 hive-jdbc-1.2.1-standalone.jar ，那就把 <code>spark.io.compression.codec</code> 改成 <code>lz4</code> 等压缩也是可以的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 ~]$ hive
</span><span class='line'>
</span><span class='line'>Logging initialized using configuration in file:/home/hadoop/tools/apache-hive-1.2.1-bin/conf/hive-log4j.properties
</span><span class='line'>hive&gt; set hive.execution.engine=spark;
</span><span class='line'>hive&gt; select count(*) from t_info where edate=20160411;
</span><span class='line'>Query ID = hadoop_20160412205338_2c95c5fd-af50-42ba-8681-e154e4b74cb1
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>In order to change the average load for a reducer (in bytes):
</span><span class='line'>  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
</span><span class='line'>In order to limit the maximum number of reducers:
</span><span class='line'>  set hive.exec.reducers.max=&lt;number&gt;
</span><span class='line'>In order to set a constant number of reducers:
</span><span class='line'>  set mapreduce.job.reduces=&lt;number&gt;
</span><span class='line'>Starting Spark Job = 69afc030-fa1f-4fdf-81ef-12bdca411a4f
</span><span class='line'>
</span><span class='line'>Query Hive on Spark job[0] stages:
</span><span class='line'>0
</span><span class='line'>1
</span><span class='line'>
</span><span class='line'>Status: Running (Hive on Spark job[0])
</span><span class='line'>Job Progress Format
</span><span class='line'>CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
</span><span class='line'>2016-04-12 20:54:11,367 Stage-0_0: 0(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:14,421 Stage-0_0: 0(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:17,457 Stage-0_0: 0(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:19,486 Stage-0_0: 2(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:20,497 Stage-0_0: 3(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:21,509 Stage-0_0: 5(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:22,520 Stage-0_0: 6(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:23,532 Stage-0_0: 7(+2)/234    Stage-1_0: 0/1</span></code></pre></td></tr></table></div></figure>


<h2>小结</h2>

<p>第一，hive的assembly的包太tmd的坑了。第二，以后找java具体加载那个类，可以通过 class.getProtectionDomain 来获取了。第三，又多尝试一个环境部署hadoop。呵呵</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/08/puppet-install/">puppet4.4.1入门安装</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2016-04-08T11:49:32+00:00" pubdate data-updated="true">Fri 2016-04-08 11:49</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>网上资料比较多比较老，基本操作可以借鉴。安装Puppet最简单的方式就是用yum来安装(操作系统centos6），由于天朝的特殊环境最好建立本地仓库。本文记录我自己安装过程的过程，先介绍本地仓库创建，然后介绍Puppet环境的搭建。</p>

<p>操作系统：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 ~]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.5 (Final)</span></code></pre></td></tr></table></div></figure>


<h2>更新</h2>

<p>2016-4-28 15:42:32 - rpm强制安装puppetserver。依赖jdk8有点麻烦，自己安装jdk7就好了。
2016-5-3 09:39:40  - 更新puppetserver性能的部分，运行在Jetty之上不需要再折腾passenger了。见文章最后。</p>

<h2>本地仓库搭建</h2>

<p>Puppet4所有依赖都进行统一打包，其实通过rpm就能直接安装。为了体现下高大山、并且Puppet内部的项目之间是有依赖的。这里先使用createrepo创建本地库。</p>

<p>createrepo其实就是用来创建目录下rpm文件的索引数据(repodata)。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 ~]# yum install createrepo
</span><span class='line'>
</span><span class='line'># 下载系统对应的puppet-pc1的包: https://yum.puppetlabs.com/el/6/PC1/x86_64/ 全部最新版本
</span><span class='line'>[root@hadoop-master2 repo]# ls -1
</span><span class='line'>puppet-agent-1.4.1-1.el6.x86_64.rpm
</span><span class='line'>puppet-dashboard-1.2.23-0.1rc3.el6.noarch.rpm
</span><span class='line'>puppetdb-4.0.0-1.el6.noarch.rpm
</span><span class='line'>puppetdb-termini-3.2.4-1.el6.noarch.rpm
</span><span class='line'>puppetdb-terminus-3-1.el6.noarch.rpm
</span><span class='line'>puppetserver-2.3.1-1.el6.noarch.rpm
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 repo]# createrepo .
</span><span class='line'>Spawning worker 0 with 6 pkgs
</span><span class='line'>Workers Finished
</span><span class='line'>Gathering worker results
</span><span class='line'>
</span><span class='line'>Saving Primary metadata
</span><span class='line'>Saving file lists metadata
</span><span class='line'>Saving other metadata
</span><span class='line'>Generating sqlite DBs
</span><span class='line'>Sqlite DBs complete
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 puppetlabs]# cat /etc/yum.repos.d/puppet-local.repo 
</span><span class='line'>[puppet-local]
</span><span class='line'>name=Puppet Local
</span><span class='line'>baseurl=file:///opt/puppetlabs/repo
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span></code></pre></td></tr></table></div></figure>


<p>查看local下的rpm包：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 repo]# yum clean all
</span><span class='line'>Loaded plugins: fastestmirror, security
</span><span class='line'>Cleaning repos: base epel extras pgdg94 puppet-local updates
</span><span class='line'>Cleaning up Everything
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 repo]# yum list all | grep "puppet-local"
</span><span class='line'>puppet-agent.x86_64                         1.4.1-1.el6                  @puppet-local
</span><span class='line'>puppet-dashboard.noarch                     1.2.23-0.1rc3.el6            @puppet-local
</span><span class='line'>puppetdb.noarch                             4.0.0-1.el6                  @puppet-local
</span><span class='line'>puppetdb-termini.noarch                     3.2.4-1.el6                  @puppet-local
</span><span class='line'>puppetserver.noarch                         2.3.1-1.el6                  @puppet-local
</span><span class='line'>puppetdb-terminus.noarch                    3-1.el6                      puppet-local
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 repo]# yum search puppet</span></code></pre></td></tr></table></div></figure>


<p>网上资料还有安装 <code>yum-priorities</code> 来设置repo优先级的。我这里没有包冲突问题所以并没有安装这个。</p>

<h2>单机安装</h2>

<p>安装前翻一翻官网的文档： <a href="https://docs.puppet.com/puppetserver/latest/install_from_packages.html">https://docs.puppet.com/puppetserver/latest/install_from_packages.html</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 先看看 puppet-agent 和 puppetserver 的依赖
</span><span class='line'>[root@hadoop-master2 repo]# yum deplist puppet-agent
</span><span class='line'>Loaded plugins: fastestmirror, security
</span><span class='line'>Loading mirror speeds from cached hostfile
</span><span class='line'> * base: mirrors.aliyun.com
</span><span class='line'> * epel: ftp.cuhk.edu.hk
</span><span class='line'> * extras: mirrors.aliyun.com
</span><span class='line'> * updates: mirrors.aliyun.com
</span><span class='line'>Finding dependencies: 
</span><span class='line'>package: puppet-agent.x86_64 1.4.1-1.el6
</span><span class='line'>  dependency: tar
</span><span class='line'>   provider: tar.x86_64 2:1.23-13.el6
</span><span class='line'>  dependency: /bin/sh
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6_7.1
</span><span class='line'>  dependency: readline
</span><span class='line'>   provider: readline.i686 6.0-4.el6
</span><span class='line'>   provider: readline.x86_64 6.0-4.el6
</span><span class='line'>  dependency: util-linux
</span><span class='line'>   provider: util-linux-ng.i686 2.17.2-12.18.el6
</span><span class='line'>   provider: util-linux-ng.x86_64 2.17.2-12.18.el6
</span><span class='line'>  dependency: chkconfig
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-5.el6
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-5.el6_7.2
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 repo]# yum deplist puppetserver
</span><span class='line'>Loaded plugins: fastestmirror, security
</span><span class='line'>Loading mirror speeds from cached hostfile
</span><span class='line'> * base: mirrors.aliyun.com
</span><span class='line'> * epel: ftp.cuhk.edu.hk
</span><span class='line'> * extras: mirrors.aliyun.com
</span><span class='line'> * updates: mirrors.aliyun.com
</span><span class='line'>Finding dependencies: 
</span><span class='line'>package: puppetserver.noarch 2.3.1-1.el6
</span><span class='line'>  dependency: /bin/bash
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6_7.1
</span><span class='line'>  dependency: java-1.8.0-openjdk-headless
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.45-35.b13.el6
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.51-0.b16.el6_6
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.51-1.b16.el6_7
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.51-3.b16.el6_7
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.65-0.b17.el6_7
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.71-1.b15.el6_7
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.77-0.b03.el6_7
</span><span class='line'>  dependency: puppet-agent &gt;= 1.4.0
</span><span class='line'>   provider: puppet-agent.x86_64 1.4.1-1.el6
</span><span class='line'>  dependency: net-tools
</span><span class='line'>   provider: net-tools.x86_64 1.60-110.el6_2
</span><span class='line'>  dependency: /usr/bin/env
</span><span class='line'>   provider: coreutils.x86_64 8.4-37.el6
</span><span class='line'>   provider: coreutils.x86_64 8.4-37.el6_7.3
</span><span class='line'>  dependency: /bin/sh
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6_7.1
</span><span class='line'>  dependency: chkconfig
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-5.el6
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-5.el6_7.2
</span><span class='line'>
</span><span class='line'># 安装
</span><span class='line'>[root@hadoop-master2 repo]# yum install puppetserver
</span><span class='line'>
</span><span class='line'># jps查看进程，然后查看端口
</span><span class='line'>[root@hadoop-master2 repo]# netstat -anp | grep 4526
</span><span class='line'>tcp        0      0 :::8140                     :::*                        LISTEN      4526/java           
</span><span class='line'>
</span><span class='line'># 安装好后，查看各版本软件版本信息
</span><span class='line'>[root@hadoop-master2 repo]# puppet -V
</span><span class='line'>4.4.1
</span><span class='line'>[root@hadoop-master2 repo]# facter -v
</span><span class='line'>3.1.5 (commit b5c2cf9b2ac290cb17fcadea19b467a39e17c1fd)
</span><span class='line'>[root@hadoop-master2 repo]# puppetserver -v
</span><span class='line'>puppetserver version: 2.3.1</span></code></pre></td></tr></table></div></figure>


<p>puppetserver依赖puppet-agent，而puppet-agent是一个all-in-one的assembly的包。所以服务端安装puppetserver就行了。客户端仅安装puppet-agent即可。</p>

<p>Puppet4的目录进行比较大的调整，程序路径为 <code>/opt/puppetlabs</code> ，配置路径为 <code>/etc/puppetlabs</code> 。如果你看的是puppet3资料，对照查看官网 <a href="https://docs.puppet.com/puppet/4.4/reference/whered_it_go.html">Where Did Everything Go in Puppet 4.x?</a> 了解各程序的目录位置。</p>

<p>如果你单独安装了jdk(依赖的是jdk8也是挺烦的)，也可以使用rpm强制安装puppetserver，然后指定java程序的路径：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bash-4.1# yum deplist puppetserver
</span><span class='line'>Loaded plugins: fastestmirror, priorities
</span><span class='line'>Loading mirror speeds from cached hostfile
</span><span class='line'> * centos-local: 172.17.42.1:8888
</span><span class='line'>Finding dependencies: 
</span><span class='line'>package: puppetserver.noarch 2.3.1-1.el6
</span><span class='line'>  dependency: /bin/bash
</span><span class='line'>   provider: bash.x86_64 4.1.2-29.el6
</span><span class='line'>  dependency: java-1.8.0-openjdk-headless
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1.8.0.20-3.b26.el6
</span><span class='line'>  dependency: puppet-agent &gt;= 1.4.0
</span><span class='line'>   provider: puppet-agent.x86_64 1.4.1-1.el6
</span><span class='line'>  dependency: net-tools
</span><span class='line'>   provider: net-tools.x86_64 1.60-110.el6_2
</span><span class='line'>  dependency: /usr/bin/env
</span><span class='line'>   provider: coreutils.x86_64 8.4-37.el6
</span><span class='line'>  dependency: /bin/sh
</span><span class='line'>   provider: bash.x86_64 4.1.2-29.el6
</span><span class='line'>  dependency: chkconfig
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-2.el6_4.1
</span><span class='line'>
</span><span class='line'>bash-4.1# rpm -ivh http://172.17.42.1:8888/centos6/puppet/puppetserver-2.3.1-1.el6.noarch.rpm --nodeps --force
</span><span class='line'>Retrieving http://172.17.42.1:8888/centos6/puppet/puppetserver-2.3.1-1.el6.noarch.rpm
</span><span class='line'>warning: /var/tmp/rpm-tmp.7CAtn8: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY
</span><span class='line'>Preparing...                ########################################### [100%]
</span><span class='line'>usermod: no changes
</span><span class='line'>   1:puppetserver           ########################################### [100%]
</span><span class='line'>usermod: no changes
</span><span class='line'>bash-4.1# chkconfig --list | grep puppetserver
</span><span class='line'>puppetserver    0:off   1:off   2:on    3:on    4:on    5:on    6:off
</span><span class='line'>
</span><span class='line'>bash-4.1# cat /etc/sysconfig/puppetserver 
</span><span class='line'>...
</span><span class='line'>JAVA_BIN="/opt/jdk1.7.0_60/bin/java"
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>bash-4.1# netstat -a
</span><span class='line'>Active Internet connections (servers and established)
</span><span class='line'>Proto Recv-Q Send-Q Local Address               Foreign Address             State      
</span><span class='line'>tcp        0      0 *:8140                      *:*                         LISTEN      
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<h2>单机版HelloWorld</h2>

<p>单机模式不需要认证，当做学习调试环境挺好的：方便简单。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 manifests]# vi helloworld.pp
</span><span class='line'>notify { 'greeting':
</span><span class='line'>  message =&gt; 'Hello, world!'
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 manifests]# puppet apply helloworld.pp 
</span><span class='line'>Notice: Compiled catalog for hadoop-master2.localdomain in environment production in 0.03 seconds
</span><span class='line'>Notice: Hello, world!
</span><span class='line'>Notice: /Stage[main]/Main/Notify[greeting]/message: defined 'message' as 'Hello, world!'
</span><span class='line'>Notice: Applied catalog in 0.04 seconds
</span><span class='line'>
</span><span class='line'># 可以用resource根据当前环境生成配置
</span><span class='line'>[root@hadoop-master2 manifests]# puppet resource user hadoop
</span><span class='line'>user { 'hadoop':
</span><span class='line'>  ensure           =&gt; 'present',
</span><span class='line'>  gid              =&gt; '500',
</span><span class='line'>  home             =&gt; '/home/hadoop',
</span><span class='line'>  password         =&gt; 'XXXXXX',
</span><span class='line'>  password_max_age =&gt; '99999',
</span><span class='line'>  password_min_age =&gt; '0',
</span><span class='line'>  shell            =&gt; '/bin/bash',
</span><span class='line'>  uid              =&gt; '500',
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'># 状态变更
</span><span class='line'>[root@hadoop-master2 puppetlabs]# bin/puppet resource service puppet ensure=running enable=false
</span><span class='line'>Notice: /Service[puppet]/enable: enable changed 'true' to 'false'
</span><span class='line'>service { 'puppet':
</span><span class='line'>  ensure =&gt; 'running',
</span><span class='line'>  enable =&gt; 'false',
</span><span class='line'>}
</span><span class='line'>[root@hadoop-master2 puppetlabs]# chkconfig --list | grep puppet
</span><span class='line'>puppet          0:off   1:off   2:off   3:off   4:off   5:off   6:off
</span><span class='line'>puppetserver    0:off   1:off   2:on    3:on    4:on    5:on    6:off
</span></code></pre></td></tr></table></div></figure>


<h2>CS模式配置</h2>

<p>这里完全模拟生产环境情况(内网)，首先搭建两个本地仓库：centos，puppet。puppet依赖RPM根据具体情况下载即可，我这里用的是centos6.5。</p>

<p>搭建私有仓库：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>增加 java-1.8.0-openjdk-headless 和 tzdata-java-2014g(iso带的2013g不适配)
</span><span class='line'>[root@hadoop-master2 repo]# ll
</span><span class='line'>total 142344
</span><span class='line'>-rw-r--r-- 1 root root 33135156 Apr  9 21:47 java-1.8.0-openjdk-headless-1.8.0.51-3.b16.el6_7.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root 26740012 Apr  9 11:29 puppet-agent-1.4.1-1.el6.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root  4509000 Apr  9 11:29 puppet-dashboard-1.2.23-0.1rc3.el6.noarch.rpm
</span><span class='line'>-rw-r--r-- 1 root root 21866876 Apr  9 11:29 puppetdb-4.0.0-1.el6.noarch.rpm
</span><span class='line'>-rw-r--r-- 1 root root    25516 Apr  9 11:29 puppetdb-termini-3.2.4-1.el6.noarch.rpm
</span><span class='line'>-rw-r--r-- 1 root root     3676 Apr  9 11:29 puppetdb-terminus-3-1.el6.noarch.rpm
</span><span class='line'>-rw-r--r-- 1 root root 33412844 Apr  9 11:29 puppetserver-2.3.1-1.el6.noarch.rpm
</span><span class='line'>drwxr-xr-x 2 root root     4096 Apr  9 22:56 repodata
</span><span class='line'>-rw-r--r-- 1 root root   181196 Sep 17  2014 tzdata-java-2014g-1.el6.noarch.rpm
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 ~]# mount -t iso9660 -o loop CentOS-6.5-x86_64-bin-DVD1.iso /mnt/cdrom
</span><span class='line'># httpd 我的系统已经安装了
</span><span class='line'>[root@hadoop-master2 ~]# cd /var/www/html/
</span><span class='line'>[root@hadoop-master2 html]# ll
</span><span class='line'>total 820
</span><span class='line'>lrwxrwxrwx  1 root root     10 Apr  9 21:54 centos6_5 -&gt; /mnt/cdrom
</span><span class='line'>lrwxrwxrwx  1 root root     20 Mar 30 17:11 puppet -&gt; /opt/puppetlabs/repo</span></code></pre></td></tr></table></div></figure>


<p>启动docker实例，参考 <a href="http://www.winseliu.com/blog/2014/09/30/docker-ssh-on-centos/">docker的安装</a>。由于centos和puppet中有包冲突，需要安装 <code>yum-priorities</code> 。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 repo]# docker run -i -t centos:centos6 /bin/bash
</span><span class='line'>bash-4.1# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.5 (Final)
</span><span class='line'>
</span><span class='line'>bash-4.1# yum install yum-plugin-priorities-1.1.30-30.el6.noarch.rpm 
</span><span class='line'>
</span><span class='line'># 把默认的repo清理掉，添加puppet和centos
</span><span class='line'>bash-4.1# cat /etc/yum.repos.d/puppet-local.repo 
</span><span class='line'>[puppet-local]
</span><span class='line'>name=Puppet Local
</span><span class='line'>baseurl=http://172.17.42.1/puppet
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span><span class='line'>priority=1
</span><span class='line'>bash-4.1# cat /etc/yum.repos.d/centos-local.repo 
</span><span class='line'>[centos-local]
</span><span class='line'>name=Centos Local
</span><span class='line'>baseurl=http://172.17.42.1/centos6_5
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span><span class='line'>priority=2
</span><span class='line'>
</span><span class='line'>bash-4.1# yum install puppetserver
</span><span class='line'>
</span><span class='line'># 加载环境变量
</span><span class='line'>bash-4.1# source /etc/profile.d/puppet-agent.sh
</span><span class='line'># 查看puppet各程序版本
</span><span class='line'>bash-4.1# puppet -V
</span><span class='line'>4.4.1
</span><span class='line'>bash-4.1# puppetserver -v
</span><span class='line'>puppetserver version: 2.3.1
</span><span class='line'>bash-4.1# facter -v
</span><span class='line'>3.1.5 (commit b5c2cf9b2ac290cb17fcadea19b467a39e17c1fd)</span></code></pre></td></tr></table></div></figure>


<p>Agent安装：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bash-4.1# cat /etc/yum.repos.d/puppet-local.repo 
</span><span class='line'>[puppet-local]
</span><span class='line'>name=Puppet Local
</span><span class='line'>baseurl=http://172.17.42.1/puppet
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span><span class='line'>
</span><span class='line'>[centos-local]
</span><span class='line'>name=Centos Local
</span><span class='line'>baseurl=http://172.17.42.1/centos6_5
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span><span class='line'>
</span><span class='line'>bash-4.1# yum install puppet-agent -y</span></code></pre></td></tr></table></div></figure>


<p>配置：</p>

<ul>
<li>添加hosts</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bash-4.1# cat /etc/hosts
</span><span class='line'>172.17.0.4 puppet
</span><span class='line'>172.17.0.5 agent1
</span><span class='line'>172.17.0.6 agent2</span></code></pre></td></tr></table></div></figure>


<ul>
<li>master自测</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bash-4.1# puppet agent -t
</span><span class='line'>Info: Using configured environment 'production'
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Info: Caching catalog for 3e4b2ba27563.localdomain
</span><span class='line'>Info: Applying configuration version '1460222292'
</span><span class='line'>Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml
</span><span class='line'>Notice: Applied catalog in 0.01 seconds</span></code></pre></td></tr></table></div></figure>


<ul>
<li>agent连接服务器</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bash-4.1# puppet agent -t
</span><span class='line'>Info: Creating a new SSL key for 5a56be361905.localdomain
</span><span class='line'>Info: Caching certificate for ca
</span><span class='line'>Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
</span><span class='line'>Info: Creating a new SSL certificate request for 5a56be361905.localdomain
</span><span class='line'>Info: Certificate Request fingerprint (SHA256): 58:1A:2E:28:D3:D7:C5:7B:E3:1A:C2:0F:70:D0:46:C0:34:39:7F:EC:98:65:B1:09:96:D3:4B:A7:4B:32:A6:C6
</span><span class='line'>Info: Caching certificate for ca
</span><span class='line'>Exiting; no certificate found and waitforcert is disabled
</span><span class='line'>
</span><span class='line'># master查看/认证
</span><span class='line'>bash-4.1# puppet cert list
</span><span class='line'>  "5a56be361905.localdomain" (SHA256) 58:1A:2E:28:D3:D7:C5:7B:E3:1A:C2:0F:70:D0:46:C0:34:39:7F:EC:98:65:B1:09:96:D3:4B:A7:4B:32:A6:C6
</span><span class='line'>  "6516b8d0538b.localdomain" (SHA256) F7:49:CC:93:EA:5D:D9:A2:90:33:01:A9:74:86:97:0C:20:0C:EB:24:3A:13:85:64:5C:32:A8:D7:36:91:3C:77
</span><span class='line'>bash-4.1# puppet cert sign --all 
</span><span class='line'>Notice: Signed certificate request for 6516b8d0538b.localdomain
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest 6516b8d0538b.localdomain at '/etc/puppetlabs/puppet/ssl/ca/requests/6516b8d0538b.localdomain.pem'
</span><span class='line'>Notice: Signed certificate request for 5a56be361905.localdomain
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest 5a56be361905.localdomain at '/etc/puppetlabs/puppet/ssl/ca/requests/5a56be361905.localdomain.pem'
</span><span class='line'>
</span><span class='line'># agent再连
</span><span class='line'>bash-4.1# puppet agent -t
</span><span class='line'>Info: Caching certificate for 5a56be361905.localdomain
</span><span class='line'>Info: Caching certificate_revocation_list for ca
</span><span class='line'>Info: Caching certificate for 5a56be361905.localdomain
</span><span class='line'>Info: Using configured environment 'production'
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Info: Caching catalog for 5a56be361905.localdomain
</span><span class='line'>Info: Applying configuration version '1460222614'
</span><span class='line'>Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml
</span><span class='line'>Notice: Applied catalog in 0.02 seconds</span></code></pre></td></tr></table></div></figure>


<p>相比puppet那么多配置项，安装还是相对简单的。安装写到这些也差不多了，接下来要研究下监控和puppet的配置。</p>

<p>安装过程中也遇到一些问题，主要都是DNS导致。一开始 <strong>直接用hosts</strong> 来配置是最简便的，把server的ip指定为puppet域名。</p>

<p>再来个Hello：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># master
</span><span class='line'>bash-4.1# cd /etc/puppetlabs/code/environments/production/
</span><span class='line'>bash-4.1# ls
</span><span class='line'>environment.conf  hieradata  manifests  modules
</span><span class='line'>bash-4.1# cd manifests/
</span><span class='line'>bash-4.1# cat helloworld.pp 
</span><span class='line'>notify { 'Hello World' : 
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'># agent
</span><span class='line'>bash-4.1# puppet agent -t
</span><span class='line'>Info: Using configured environment 'production'
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Info: Caching catalog for 5a56be361905.localdomain
</span><span class='line'>Info: Applying configuration version '1460223248'
</span><span class='line'>Notice: Hello World
</span><span class='line'>Notice: /Stage[main]/Main/Notify[Hello World]/message: defined 'message' as 'Hello World'
</span><span class='line'>Notice: Applied catalog in 0.02 seconds
</span><span class='line'>bash-4.1# </span></code></pre></td></tr></table></div></figure>


<h2>最后说说PuppetServer性能</h2>

<ul>
<li><a href="https://docs.puppet.com/puppetserver/latest/">https://docs.puppet.com/puppetserver/latest/</a></li>
<li><a href="https://docs.puppet.com/puppetserver/latest/puppetserver_vs_passenger.html">puppetserver_vs_passenger</a></li>
<li>master与ca分离 <a href="https://docs.puppet.com/puppetserver/latest/external_ca_configuration.html">https://docs.puppet.com/puppetserver/latest/external_ca_configuration.html</a></li>
<li><a href="https://docs.puppet.com/puppetserver/latest/ssl_server_certificate_change_and_virtual_ips.html">https://docs.puppet.com/puppetserver/latest/ssl_server_certificate_change_and_virtual_ips.html</a></li>
</ul>


<p>晚上很多资料都是旧的，一般都是 puppetmaster + apache/nginx + passenger 。新版本使用puppetserver后，服务运行在JVM之上（ Puppet Server is hosted by a Jetty web server ），性能比原来ruby的方式更好（<a href="https://docs.puppet.com/puppetserver/latest/puppetserver_vs_passenger.html">反正官网是这么说的</a>）。所以没必要折腾其他ruby的东西了。</p>

<p><strong>题外话</strong>：搭上JVM（java）的车，对于大家都好^_^，现在大数据HADOOP都是基于java的，spark的scala也是运行在JVM之上。</p>

<blockquote><p>Because Puppet Server runs on the JVM, it takes a bit longer than the Apache/Passenger stack to start and get ready to accept HTTP connections.</p>

<p>Overall, Puppet Server performance is significantly better than a Puppet master running on the Apache/Passenger stack, but the initial startup is definitely slower.</p></blockquote>

<h2>参考</h2>

<ul>
<li><a href="https://docs.puppet.com/puppet/4.4/reference/">https://docs.puppet.com/puppet/4.4/reference/</a></li>
<li><a href="https://docs.puppetlabs.com/puppet/latest/reference/install_pre.html">https://docs.puppetlabs.com/puppet/latest/reference/install_pre.html</a></li>
<li><a href="https://docs.puppet.com/puppetserver/latest/install_from_packages.html">https://docs.puppet.com/puppetserver/latest/install_from_packages.html</a></li>
<li><a href="https://docs.puppet.com/puppet/4.4/reference/whered_it_go.html">https://docs.puppet.com/puppet/4.4/reference/whered_it_go.html</a></li>
<li><a href="https://github.com/puppetlabs/puppet-specifications/blob/master/file_paths.md">https://github.com/puppetlabs/puppet-specifications/blob/master/file_paths.md</a></li>
<li><a href="https://docs.puppet.com/puppet/4.4/reference/install_linux.html#installing-release-packages-on-yum-based-systems">https://docs.puppet.com/puppet/4.4/reference/install_linux.html#installing-release-packages-on-yum-based-systems</a></li>
<li><a href="https://yum.puppetlabs.com/el/6/PC1/x86_64/">https://yum.puppetlabs.com/el/6/PC1/x86_64/</a></li>
<li><a href="https://docs.puppetlabs.com/puppet/latest/reference/type.html#file">https://docs.puppetlabs.com/puppet/latest/reference/type.html#file</a></li>
<li><a href="https://docs.puppetlabs.com/puppet/latest/reference/config_important_settings.html">https://docs.puppetlabs.com/puppet/latest/reference/config_important_settings.html</a></li>
<li><a href="https://docs.puppetlabs.com/puppetdb/4.0/install_from_packages.html">https://docs.puppetlabs.com/puppetdb/4.0/install_from_packages.html</a></li>
<li><a href="https://docs.puppetlabs.com/puppet/4.4/reference/config_file_auth.html">https://docs.puppetlabs.com/puppet/4.4/reference/config_file_auth.html</a></li>
<li><p><a href="https://docs.puppetlabs.com/puppet/4.4/reference/config_file_autosign.html">https://docs.puppetlabs.com/puppet/4.4/reference/config_file_autosign.html</a></p></li>
<li><p>yum配置与各种使用 <a href="http://www.cnblogs.com/mchina/archive/2013/01/04/2842275.html">http://www.cnblogs.com/mchina/archive/2013/01/04/2842275.html</a></p></li>
<li><a href="http://kisspuppet.com/2014/01/26/puppet_create_repo/">http://kisspuppet.com/2014/01/26/puppet_create_repo/</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/08/dbcp-parameters/">DBCP参数在Hive JDBC上的实践</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2016-04-08T11:48:01+00:00" pubdate data-updated="true">Fri 2016-04-08 11:48</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>查询程序一开始只是简单使用dbcp来做连接的限制。在实践的过程中遇到各种问题，本文记录DBCP的参数优化提高程序健壮性的两次过程。</p>

<p>最开始的DBCP的配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;bean id="hiveDataSource" class="org.apache.commons.dbcp.BasicDataSource"
</span><span class='line'>  destroy-method="close" 
</span><span class='line'>  p:driverClassName="${hiveDriverClassName}"
</span><span class='line'>  p:url="${hiveUrl}" 
</span><span class='line'>  p:username="${hiveUsername}" 
</span><span class='line'>  p:password="${hivePassword}"
</span><span class='line'>  p:maxIdle="${hiveMaxIdle}" 
</span><span class='line'>  p:maxWait="${hiveMaxWait}" 
</span><span class='line'>  p:maxActive="${hiveMaxActive}" /&gt;
</span><span class='line'>
</span><span class='line'>&lt;bean id="hiveTemplate" class="org.springframework.jdbc.core.JdbcTemplate"&gt;
</span><span class='line'>  &lt;property name="dataSource"&gt;
</span><span class='line'>      &lt;ref bean="hiveDataSource" /&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>&lt;/bean&gt;</span></code></pre></td></tr></table></div></figure>


<p>第一个遇到的问题，就是每次hiveserver2重启后，这个查询程序也得重启。在实际使用过程中非常的麻烦！！</p>

<h4>重启问题（连接断开后不能重连）</h4>

<p>首先给出学习的链接 <a href="http://elf8848.iteye.com/blog/1931778">http://elf8848.iteye.com/blog/1931778</a> 巨详细，同时问题的场景都一模一样啊！！</p>

<p>添加三个参数：</p>

<ul>
<li>testOnBorrow = &ldquo;true&rdquo;       借出连接时不要测试，否则很影响性能。如果需要可以把validation语句搞个性能消耗最少的</li>
<li>testWhileIdle = &ldquo;true&rdquo;       指明连接是否被空闲连接回收器(如果有)进行检验.如果检测失败,则连接将被从池中去除.</li>
<li>validationQuery = &ldquo;show databases&rdquo; 验证连接是否可用，使用的SQL语句</li>
</ul>


<p>解释：</p>

<p>testWhileIdle = &ldquo;true&rdquo; 表示每 {timeBetweenEvictionRunsMillis} (默认-1，不执行)秒，取出 {numTestsPerEvictionRun} (默认值3)条连接，使用 {validationQuery} 进行测试 ，测试不成功就销毁连接。销毁连接后，连接数量就少了，如果小于minIdle数量，就新建连接。</p>

<p>testOnBorrow = &ldquo;true&rdquo; 它的默认值是true，如果测试失败会drop掉然后再borrow。false表示每次从连接池中取出连接时，不需要执行 {validationQuery} 中的SQL进行测试。若配置为true,对性能有非常大的影响，性能会下降7-10倍。所在一定要配置为false.</p>

<p>调整参数后hiveserver2重启，查询再连会先报错然后再连。在每次取连接的时刻使用 <code>show databases</code> 测试，如果失败则从pool中删掉这个连接，重新再取，实现了重连的效果。这里不用 <code>select 1</code> hive里面执行很慢， 同时testWhileIdle并没有生效，因为没有配置timeBetweenEvictionRunsMillis参数。</p>

<p>调整后的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;bean id="hiveDataSource" class="org.apache.commons.dbcp.BasicDataSource"
</span><span class='line'>destroy-method="close" 
</span><span class='line'>p:driverClassName="${hiveDriverClassName}"
</span><span class='line'>p:url="${hiveUrl}" 
</span><span class='line'>p:username="${hiveUsername}" 
</span><span class='line'>p:password="${hivePassword}"
</span><span class='line'>p:testOnBorrow="${hiveTestOnBorrow}"
</span><span class='line'>p:testWhileIdle="${hiveTestWhileIdle}" 
</span><span class='line'>p:validationQuery="${hiveValidationQuery}"
</span><span class='line'>p:maxIdle="${hiveMaxIdle}" 
</span><span class='line'>p:maxWait="${hiveMaxWait}" 
</span><span class='line'>p:maxActive="${hiveMaxActive}" 
</span><span class='line'>/&gt;</span></code></pre></td></tr></table></div></figure>


<p>问题又来了，由于测试切换tez和spark才配置了上面的重连。但是切换到spark后，启动的spark会一直保持(连接创建的session不会主动关闭)，直到hiveserver2 session超时(默认6h检查一次，7h idle就关闭)。</p>

<p>注意：有个隐忧，hive-on-spark每个连接都创建一个SESSION，这就退化到MR操作了。不能完全利用SPARK的优势！！例如业务中，即查询count、又获取一页数据，这里就是两个单独的spark程序！！N个session就N个 <strong>hive on spark</strong> 啊！！</p>

<h4>第二个问题，服务端session强制关闭</h4>

<p>问题其实和参考中的: <strong>MySQL8小时问题，Mysql服务器默认连接的“wait_timeout”是8小时，也就是说一个connection空闲超过8个小时，Mysql将自动断开该 connection</strong> 一模一样的。在增加 <strong>minEvictableIdleTimeMillis</strong> 和 <strong>timeBetweenEvictionRunsMillis</strong> 设置检查和回收的时间。</p>

<ul>
<li>timeBetweenEvictionRunsMillis = &ldquo;1800000&rdquo;  每30分钟运行一次空闲连接回收器，没必要那么频繁。</li>
<li>minEvictableIdleTimeMillis = &ldquo;3600000&rdquo;  池中的连接空闲1个小时后被回收，如果1个半小时没有操作，这个session就会被客户端关闭。可以通过yarn-8088的scheduler页面查看。</li>
</ul>


<p>设置后的最终效果：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;bean id="hiveDataSource" class="org.apache.commons.dbcp.BasicDataSource"
</span><span class='line'>destroy-method="close" 
</span><span class='line'>p:driverClassName="${hiveDriverClassName}"
</span><span class='line'>p:url="${hiveUrl}" 
</span><span class='line'>p:username="${hiveUsername}" 
</span><span class='line'>p:password="${hivePassword}"
</span><span class='line'>p:testOnBorrow="${hiveTestOnBorrow}"
</span><span class='line'>p:validationQuery="${hiveValidationQuery}"
</span><span class='line'>p:maxWait="${hiveMaxWait}" 
</span><span class='line'>p:maxIdle="${hiveMaxIdle}" 
</span><span class='line'>p:maxActive="${hiveMaxActive}" 
</span><span class='line'>p:testWhileIdle="${hiveTestWhileIdle}" 
</span><span class='line'>p:timeBetweenEvictionRunsMillis="${hiveTimeBetweenEvictionRunsMillis}" 
</span><span class='line'>p:minEvictableIdleTimeMillis="${hiveMinEvictableIdleTimeMillis}" 
</span><span class='line'>p:removeAbandoned="true"
</span><span class='line'>p:logAbandoned="true"
</span><span class='line'>/&gt;</span></code></pre></td></tr></table></div></figure>


<p>很多程序都有很多参数，大部分能通过文档明白，但是一些参数不到实践真的很难真正体会它的含义。参考的文章两次改进我查看了，但是第一次看的时刻根本没去加其他参数，因为对我来说没用，解决当前问题用不到嘛。</p>

<p>hadoop的参数更多，core/hdfs/mapred/yarn需要多用才能发现参数的功能和妙用。<strong>纸上得来终觉浅，绝知此事要躬行</strong> 。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/16">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/14">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2016/04/23/hadoop-guide-catalog/">[整理] Hadoop入门</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/29/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/26/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/17/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/10/08/docker-network-via-macvlan/">Docker多主机网络配置 - Macvlan</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/07/docker-network-via-pipework/">Docker多主机网络配置 - Pipework</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/06/staf-start-guide/">STAF Start Guide</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/05/respberrypi-connected-via/">连接树莓派</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/09/20/docker-manual-make-connect-each-other/">两台主机的docker通过route互联互通</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/09/19/os-install-via-usb/">使用U盘安装Centos7</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/09/18/redmine-deploy-and-install-plugins/">Redmine部署以及插件安装</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/09/17/docker-compose-hello/">Docker Compose入门</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/alluxio/'>alluxio</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/bigdata/'>bigdata</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (15) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/elasticsearch/'>elasticsearch</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/flume/'>flume</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (5) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (44) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (8) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jenkins/'>jenkins</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/k8s/'>k8s</a> (8) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kubeadm/'>kubeadm</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/nginx/'>nginx</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/puppet/'>puppet</a> (11) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/staf/'>staf</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (54) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/vagrant/'>vagrant</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/zookeeper/'>zookeeper</a> (1) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (192)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"winseliu"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->










</body>
</html>
