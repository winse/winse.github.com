
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="记录spark编译和打包成tar的整个流程。包括各个版本的编译过程，使用make-distribution脚本打包，搭建本地、standalone、yarn的spark环境。 2016-1 spark-1.6.0
2015-04 【Spark-1.3.0】单独附在最后，添加了spark- &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/15">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
  <li><a href="https://yunpan.cn/cuYhpFBPgQYgT" >Books[5aee]</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2014-10-16T20:02:46+08:00" pubdate data-updated="true">Thu 2014-10-16 20:02</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>记录spark编译和打包成tar的整个流程。包括各个版本的编译过程，使用make-distribution脚本打包，搭建本地、standalone、yarn的spark环境。</p>

<ul>
<li>2016-1 spark-1.6.0</li>
<li>2015-04 【Spark-1.3.0】单独附在最后，添加了spark-sql功能使用和spark-HA的配置</li>
</ul>


<h2>编译和打包</h2>

<ul>
<li>spark-1.6.0</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// java version "1.7.0_17" & Apache Maven 3.3.9 & CentOS release 6.6 (Final)
</span><span class='line'>[hadoop@cu2 spark-1.6.0]$ export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>[hadoop@cu2 spark-1.6.0]$ mvn package eclipse:eclipse -Phadoop-2.6 -Dhadoop.version=2.6.3 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 spark-1.6.0]$ vi make-distribution.sh 
</span><span class='line'>BUILD_COMMAND=("$MVN" package -DskipTests $@)
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 spark-1.6.0]$ ./make-distribution.sh --tgz --mvn "$(which mvn)"  -Dhadoop-2.6 -Dhadoop.version=2.6.3 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests 
</span><span class='line'>[hadoop@cu2 spark-1.6.0]$ ll spark-1.6.0-bin-2.6.3.tgz 
</span><span class='line'>
</span><span class='line'>// examples
</span><span class='line'>[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ export HADOOP_CONF_DIR=~/hadoop-2.6.3/etc/hadoop
</span><span class='line'>[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client lib/spark-examples-1.6.0-hadoop2.6.3.jar 10
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ export HADOOP_CONF_DIR=~/hadoop-2.6.3/etc/hadoop
</span><span class='line'>[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ export SPARK_PRINT_LAUNCH_COMMAND=true
</span><span class='line'>// export HADOOP_ROOT_LOGGER=DEBUG,console Spark的脚本不认这个变量
</span><span class='line'>[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ bin/spark-submit --master yarn --deploy-mode client --class org.apache.spark.examples.streaming.HdfsWordCount lib/spark-examples-1.6.0-hadoop2.6.3.jar /data
</span><span class='line'>
</span><span class='line'>// --driver-java-options "-Dhadoop.root.logger=WARN,console" 
</span><span class='line'>// --driver-java-options "-Dhadoop.root.logger=WARN,console -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"
</span><span class='line'>
</span><span class='line'>// org.apache.spark.deploy.yarn.Client#copyFileToRemote
</span><span class='line'>// --conf "spark.yarn.jar=hdfs://hadoop-master2:9000/spark-assembly-1.6.0-hadoop2.6.3.jar"
</span><span class='line'>
</span><span class='line'>// http://spark.apache.org/docs/latest/running-on-yarn.html</span></code></pre></td></tr></table></div></figure>


<ul>
<li>spark-1.5</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-- jdk8-x64 & spark-1.5.2 & maven-3.3.9
</span><span class='line'>set or export MAVEN_OPTS=-Xmx2g
</span><span class='line'>mvn package eclipse:eclipse -Phadoop-2.6 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</span><span class='line'>-- 注释掉pom.xml中的&lt;useZincServer&gt;true&lt;/useZincServer&gt; @see http://stackoverflow.com/questions/31844848/building-spark-with-maven-error-finding-javac-but-path-is-correct
</span><span class='line'>-- 公司网络不稳定，遇到下载maven包报错，多重试几次！！</span></code></pre></td></tr></table></div></figure>


<ul>
<li>spark-1.4.1</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 spark-1.4.1]$ export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 spark-1.4.1]$ mvn package -Phadoop-2.6 -Dhadoop.version=2.7.1 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</span><span class='line'>
</span><span class='line'>-- 打包：
</span><span class='line'>-- // 修改BUILD_COMMAND变量
</span><span class='line'>[hadoop@cu2 spark-1.4.1]$ vi make-distribution.sh 
</span><span class='line'>BUILD_COMMAND=("$MVN"  package -DskipTests $@)
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 spark-1.4.1]$ ./make-distribution.sh --mvn `which mvn` --tgz  --skip-java-test   -Phadoop-2.6 -Dhadoop.version=2.7.1 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests</span></code></pre></td></tr></table></div></figure>


<ul>
<li>spark-1.1.0</li>
</ul>


<p>官网提供的hadoop版本没有2.5的。这里我自己下载源码再进行编译。先下载spark-1.1.0.tgz，解压然后执行命令编译：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive -X -DskipTests clean package
</span><span class='line'>
</span><span class='line'>-- mvn package eclipse:eclipse -Phadoop-2.2 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests</span></code></pre></td></tr></table></div></figure>


<p>注意事项：用64位的JDK！！加上maven参数，不然很可能出现OOM（甚至各种稀奇古怪的问题）。编译的时间也挺长的，可以先去吃个饭。或者取消一些功能的编译（如hive）。</p>

<p>编译完后，在assembly功能下会生成包括所有spark及其依赖的jar文件。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker scala-2.10]# cd spark-1.1.0/assembly/target/scala-2.10/
</span><span class='line'>[root@docker scala-2.10]# ll -h
</span><span class='line'>total 135M
</span><span class='line'>-rw-r--r--. 1 root root 135M Oct 15 21:18 spark-assembly-1.1.0-hadoop2.5.1.jar</span></code></pre></td></tr></table></div></figure>


<p>打包:</p>

<p>上面我们已经编译好了spark程序，这里对其进行打包集成到一个压缩包。使用程序自带的make-distribution.sh即可。</p>

<p>为了减少重新编译的巨长的等待时间，修改下脚本<code>make-distribution.sh</code>的maven编译参数，去掉maven的clean阶段操作（最好直接注释掉mvn那行），修改最终结果如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#BUILD_COMMAND="mvn clean package -DskipTests $@"
</span><span class='line'>BUILD_COMMAND="mvn package -DskipTests $@"</span></code></pre></td></tr></table></div></figure>


<p>然后执行命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0]# sh -x make-distribution.sh --tgz  --skip-java-test -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive 
</span><span class='line'>[root@docker spark-1.1.0]# ll -h
</span><span class='line'>total 185M
</span><span class='line'>...
</span><span class='line'>-rw-r--r--. 1 root root 185M Oct 16 00:09 spark-1.1.0-bin-2.5.1.tgz</span></code></pre></td></tr></table></div></figure>


<p>最终会在目录行打包生成tgz的文件。</p>

<h2>本地运行</h2>

<p>把本机ip主机名写入到hosts，方便以后windows本机查看日志</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# echo 192.168.154.128 docker &gt;&gt; /etc/hosts
</span><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# cat /etc/hosts
</span><span class='line'>...
</span><span class='line'>192.168.154.128 docker</span></code></pre></td></tr></table></div></figure>


<ul>
<li>运行helloworld：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# bin/run-example SparkPi 10
</span><span class='line'>Spark assembly has been built with Hive, including Datanucleus jars on classpath
</span><span class='line'>...
</span><span class='line'>14/10/16 00:22:36 INFO SparkContext: Job finished: reduce at SparkPi.scala:35, took 2.848632007 s
</span><span class='line'>Pi is roughly 3.139344
</span><span class='line'>14/10/16 00:22:36 INFO SparkUI: Stopped Spark web UI at http://docker:4040
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<ul>
<li>交互式操作：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# bin/spark-shell --master local[2]
</span><span class='line'>Welcome to
</span><span class='line'>      ____              __
</span><span class='line'>     / __/__  ___ _____/ /__
</span><span class='line'>    _\ \/ _ \/ _ `/ __/  '_/
</span><span class='line'>   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0
</span><span class='line'>      /_/
</span><span class='line'>
</span><span class='line'>Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_60)
</span><span class='line'>...
</span><span class='line'>14/10/16 00:25:57 INFO SparkUI: Started SparkUI at http://docker:4040
</span><span class='line'>14/10/16 00:25:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>14/10/16 00:25:58 INFO Executor: Using REPL class URI: http://192.168.154.128:39385
</span><span class='line'>14/10/16 00:25:58 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@docker:57417/user/HeartbeatReceiver
</span><span class='line'>14/10/16 00:25:58 INFO SparkILoop: Created spark context..
</span><span class='line'>Spark context available as sc.
</span><span class='line'>
</span><span class='line'>scala&gt; 
</span></code></pre></td></tr></table></div></figure>


<p>说明下环境：我使用windows作为开发环境，使用虚拟机中的linux作为测试环境。同时通过ssh连接的隧道来实现windows无缝的访问虚拟机linux操作系统（可以通过浏览器socket5代理查看web页面）。</p>

<p>启动交互式访问后，就可以通过浏览器访问4040查看spark程序的状态。</p>

<p><img src="http://file.bmob.cn/M00/1E/4B/wKhkA1Q_3NOALefuAAEimqVy6-s418.png" alt="" /></p>

<p>任务已经启动，接下来就可以进行操作：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val textFile=sc.textFile("README.md")
</span><span class='line'>textFile: org.apache.spark.rdd.RDD[String] = README.md MappedRDD[1] at textFile at &lt;console&gt;:12
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.count()
</span><span class='line'>res0: Long = 141
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.first()
</span><span class='line'>res1: String = # Apache Spark
</span><span class='line'>
</span><span class='line'>scala&gt; val linesWithSpark = textFile.filter(line=&gt;line.contains("Spark"))
</span><span class='line'>linesWithSpark: org.apache.spark.rdd.RDD[String] = FilteredRDD[2] at filter at &lt;console&gt;:14
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.filter(line=&gt;line.contains("Spark")).count()
</span><span class='line'>res2: Long = 21
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)
</span><span class='line'>res3: Int = 15
</span><span class='line'>
</span><span class='line'>scala&gt; import java.lang.Math
</span><span class='line'>import java.lang.Math
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b)=&gt;Math.max(a,b))
</span><span class='line'>res4: Int = 15
</span><span class='line'>
</span><span class='line'>scala&gt; val wordCounts = textFile.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_)
</span><span class='line'>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at &lt;console&gt;:15
</span><span class='line'>
</span><span class='line'>scala&gt; wordCounts.collect()
</span><span class='line'>res5: Array[(String, Int)] = Array((means,1), (under,2), (this,4), (Because,1), (Python,2), (agree,1), (cluster.,1), (its,1), (follows.,1), (general,2), (have,2), (YARN,,3), (pre-built,1), (locally.,1), (locally,2), (changed,1), (MRv1,,1), (several,1), (only,1), (sc.parallelize(1,1), (This,2), (learning,,1), (basic,1), (requests,1), (first,1), (Configuration,1), (MapReduce,2), (CLI,1), (graph,1), (without,1), (documentation,1), ("yarn-client",1), ([params]`.,1), (any,2), (setting,2), (application,1), (prefer,1), (SparkPi,2), (engine,1), (version,3), (file,1), (documentation,,1), (&lt;http://spark.apache.org/&gt;,1), (MASTER,1), (entry,1), (example,3), (are,2), (systems.,1), (params,1), (scala&gt;,1), (provides,1), (refer,1), (MLLib,1), (Interactive,2), (artifact,1), (configure,1), (can,8), (&lt;art...
</span></code></pre></td></tr></table></div></figure>


<p>执行了上面一些操作后，通过网页查看状态变化：</p>

<p><img src="http://file.bmob.cn/M00/1E/4C/wKhkA1Q_3w6AM6njAAF-MCCYh2s170.png" alt="" /></p>

<h2>Spark-standalone集群</h2>

<p>部署集群需要用到多个服务器，这里我使用docker来进行部署。</p>

<p>本来应该早早完成本文的实践，但是在搭建docker-hadoop集群时花费了很多的时间。关于搭建集群dnsmasq处理域名问题参见下一篇文章。
最终实现可以参考：<a href="https://github.com/winse/docker-hadoop/tree/spark-yarn">docker-hadoop</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver1 -h slaver1 spark-yarn
</span><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver2 spark-yarn
</span><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name master -h master spark-yarn
</span><span class='line'>
</span><span class='line'>[root@docker docker-hadoop]# docker ps | grep spark | awk '{print $1}' | xargs -I{} docker inspect -f ' ' {} &gt; /etc/dnsmasq.hosts
</span><span class='line'>[root@docker docker-hadoop]# cat /etc/dnsmasq.hosts 
</span><span class='line'>172.17.0.29 master
</span><span class='line'>172.17.0.28 slaver2
</span><span class='line'>172.17.0.27 slaver1
</span><span class='line'>[root@docker docker-hadoop]# service dnsmasq restart
</span><span class='line'>[root@docker docker-hadoop]# ssh hadoop@master
</span><span class='line'>
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id master
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id localhost
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id slaver1
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id slaver2
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ sbin/start-all.sh 
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ /opt/jdk1.7.0_67/bin/jps  -m
</span><span class='line'>266 Jps -m
</span><span class='line'>132 Master --ip master --port 7077 --webui-port 8080
</span></code></pre></td></tr></table></div></figure>


<p>通过网页可以查看集群的状态：</p>

<p><img src="http://file.bmob.cn/M00/1E/F8/wKhkA1RClV2AE0biAAEmpXJlzTc914.png" alt="" /></p>

<p>运行任务连接到master：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-shell --master spark://master:7077
</span><span class='line'>...
</span><span class='line'>14/10/17 11:31:08 INFO BlockManagerMasterActor: Registering block manager slaver2:55473 with 265.4 MB RAM
</span><span class='line'>14/10/17 11:31:09 INFO BlockManagerMasterActor: Registering block manager slaver1:33441 with 265.4 MB RAM
</span><span class='line'>
</span><span class='line'>scala&gt; </span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmG-AO--XAAD84ATrCew955.png" alt="" /></p>

<p>从上图可以看到，程序已经正确连接到spark集群，master为driver，任务节点为slaver1和slaver2。下面运行下程序，然后通过网页查看运行的状态。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val textFile=sc.textFile("README.md")
</span><span class='line'>scala&gt; textFile.count()
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmdmAB3M9AAFIzMb4yk0370.png" alt="" /></p>

<p>系统安装好了，启动spark-standalone集群和hadoop-yarn一样。配置ssh、java，然后启动，配合网页8080/4040可以实时的了解任务的指标。</p>

<h2>yarn集群</h2>

<p>注意：如果你是按照前面的步骤来操作的，需要先把spark-standalone的集群停掉。端口8080和yarn web使用端口冲突，会导致yarn启动失败。</p>

<p>修改spark-env.sh，添加HADOOP_CONF_DIR参数。然后提交任务到yarn上执行就行了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ cat conf/spark-env.sh
</span><span class='line'>#!/usr/bin/env bash
</span><span class='line'>
</span><span class='line'>JAVA_HOME=/opt/jdk1.7.0_67 
</span><span class='line'>
</span><span class='line'>HADOOP_CONF_DIR=/opt/hadoop-2.5.1/etc/hadoop
</span><span class='line'>
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.1.0-hadoop2.5.1.jar  10</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/FD/wKhkA1RCszeAALCPAAK1Nzk6faQ330.png" alt="" /></p>

<p>运行的结果输出在driver的slaver2节点，对应输出型来说不是很直观。spark-yarn提供了另一种方式，driver直接本地运行<em>yarn-client</em>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client lib/spark-examples-1.1.0-hadoop2.5.1.jar  10
</span><span class='line'>...
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8248 ms on slaver1 (1/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 231 ms on slaver1 (2/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 158 ms on slaver1 (3/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 284 ms on slaver1 (4/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 175 ms on slaver1 (5/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 301 ms on slaver1 (6/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 175 ms on slaver1 (7/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 143 ms on slaver1 (8/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 164 ms on slaver1 (9/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO cluster.YarnClientSchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@slaver2:51923/user/Executor#1132577949] with ID 1
</span><span class='line'>14/10/17 13:31:04 INFO util.RackResolver: Resolved slaver2 to /default-rack
</span><span class='line'>14/10/17 13:31:04 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 397 ms on slaver1 (10/10)
</span><span class='line'>14/10/17 13:31:04 INFO cluster.YarnClientClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
</span><span class='line'>14/10/17 13:31:04 INFO scheduler.DAGScheduler: Stage 0 (reduce at SparkPi.scala:35) finished in 26.084 s
</span><span class='line'>14/10/17 13:31:04 INFO spark.SparkContext: Job finished: reduce at SparkPi.scala:35, took 28.31400558 s
</span><span class='line'>Pi is roughly 3.140248</span></code></pre></td></tr></table></div></figure>


<p>thrift连接yarn运行时时受容器内存最大值限制，需要修改yarn-site.xml。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat yarn-site.xml 
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;32000&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;32768&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;2048&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;32768&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>./sbin/start-thriftserver.sh --executor-memory 29g --master yarn-client</span></code></pre></td></tr></table></div></figure>


<p>不能把executor-memory的内存设置为等于最大值，否则会报错：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Exception in thread "main" java.lang.IllegalArgumentException: Required executor memory (30720+2150 MB) is above the max threshold (32768 MB) of this cluster!</span></code></pre></td></tr></table></div></figure>


<h2>总结</h2>

<p>本文主要是搭建spark的环境搭建，本地运行、以及在docker中搭建spark集群、yarn集群三种方式。本地运行最简单方便，但是没有模拟到集群环境；spark提供了yarn框架上的实现，直接提交任务到yarn即可；spark集群相对比较简单和方便，接下来的远程调试主要通过spark伪分布式集群方式来进行。</p>

<h2>参考</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/building-with-maven.html">Building Spark with Maven</a></li>
<li><a href="http://spark.apache.org/docs/latest/quick-start.html">Quick Start</a></li>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html">Spark Configuration</a></li>
<li><a href="http://www.07net01.com/linux/zuixindnsmasqanzhuangbushuxiangjie_centos6__653221_1381214991.html">DNS</a></li>
<li>[spark上安装mysql与hive](<a href="http://blog.csdn.net/hwssg/article/details/38424529">http://blog.csdn.net/hwssg/article/details/38424529</a>]</li>
</ul>


<h2>后记 Spark-1.3.0</h2>

<h3>编译1.3.0(cygwin)</h3>

<p>正式环境用的hadoop-2.2，不是开发环境，没有maven等工具。先本地编译后，再方式去。（由于是添加计算的工具，可以随便一点）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>mvn package eclipse:eclipse -Phadoop-2.2 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</span><span class='line'>
</span><span class='line'>-- // 删除生成的eclipse文件中的including
</span><span class='line'>find . -name ".classpath" | xargs -I{} sed -i 's/ including="\*\*\/\*\.java"//' {}
</span><span class='line'>
</span><span class='line'>dos2unix make-distribution.sh
</span><span class='line'>./make-distribution.sh --mvn `which mvn` --tgz  --skip-java-test -Phadoop-2.2 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</span><span class='line'>
</span><span class='line'>-- linux环境部署
</span><span class='line'>-- // 这个版本，windows-cygwin编译的shell文件也是**windows的换行符**！！需要注意下！
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ find bin/* -perm /u+x | xargs -I{} sed -i 's/^M//g' {} 
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ find sbin/* -perm /u+x | xargs -I{} sed -i 's/^M//g' {} 
</span></code></pre></td></tr></table></div></figure>


<h3>spark-1.3.0运行spark-sql</h3>

<ol>
<li><p>连接到hive-engine</p></li>
<li><p>依赖tez</p></li>
</ol>


<p>hive的<code>hive.execution.engine</code>的tez，添加tez的jar和hive-site到CLASSPATH。</p>

<p>包的导入以及配置：（如果使用meta-service的就不用这么麻烦）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ vi conf/spark-env.sh 
</span><span class='line'>...
</span><span class='line'>JAVA_HOME=/home/eshore/jdk1.7.0_60
</span><span class='line'>
</span><span class='line'># log4j
</span><span class='line'>
</span><span class='line'>__add_to_classpath() {
</span><span class='line'>
</span><span class='line'>  root=$1
</span><span class='line'>
</span><span class='line'>  if [ -d "$root" ] ; then
</span><span class='line'>    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
</span><span class='line'>      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
</span><span class='line'>      else
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$f
</span><span class='line'>      fi
</span><span class='line'>    done
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>__add_to_classpath "/home/eshore/tez-0.4.0-incubating"
</span><span class='line'>__add_to_classpath "/home/eshore/tez-0.4.0-incubating/lib"
</span><span class='line'>__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"
</span><span class='line'>
</span><span class='line'>export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
</span><span class='line'>export SPARK_CLASSPATH=/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'>不能直接把hive的包全部加进去，hive-0.13.1a和hive-0.13.1的部分包不一致！！
</span><span class='line'>
</span><span class='line'>  java.lang.NoSuchMethodException: org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(com.esotericsoftware.kryo.Kryo, java.io.InputStream, java.lang.Class)
</span><span class='line'>
</span><span class='line'>  private static java.lang.Object org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(org.apache.hive.com.esotericsoftware.kryo.Kryo,java.io.InputStream,java.lang.Class)
</span><span class='line'>
</span><span class='line'>* 如果不依赖tez，可以直接把datanucleus的三个包拷贝到lib目录下。
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ll lib
</span><span class='line'>total 262364
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop    339666 Mar 25 19:35 datanucleus-api-jdo-3.2.6.jar
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop   1890075 Mar 25 19:35 datanucleus-core-3.2.10.jar
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop   1809447 Mar 25 19:35 datanucleus-rdbms-3.2.9.jar
</span><span class='line'>-rwxr-xr-x 1 hadoop hadoop   4136686 Mar 31 13:05 spark-1.3.0-yarn-shuffle.jar
</span><span class='line'>-rwxr-xr-x 1 hadoop hadoop 154198768 Mar 31 13:05 spark-assembly-1.3.0-hadoop2.2.0.jar
</span><span class='line'>-rwxr-xr-x 1 hadoop hadoop 106275583 Mar 31 13:05 spark-examples-1.3.0-hadoop2.2.0.jar
</span><span class='line'>  
</span><span class='line'>[eshore@bigdatamgr1 conf]$ ll
</span><span class='line'>...
</span><span class='line'>lrwxrwxrwx 1 eshore biadmin   50 Mar 31 13:26 hive-site.xml -&gt; /home/eshore/apache-hive-0.13.1/conf/hive-site.xml
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  632 Mar 31 15:12 log4j.properties
</span><span class='line'>lrwxrwxrwx 1 eshore biadmin   44 Mar 31 10:20 slaves -&gt; /data/opt/ibm/biginsights/hadoop-conf/slaves
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 3380 Mar 31 16:17 spark-env.sh
</span><span class='line'>lrwxrwxrwx 1 eshore biadmin   62 Mar 31 16:17 tez-site.xml -&gt; /data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop/tez-site.xml</span></code></pre></td></tr></table></div></figure>


<p>上面用的是hive-site.xml直接连接数据库的方式。也可以起hive-metaserver，然后spark通过连接meta即可：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 起meta服务
</span><span class='line'>nohup bin/hive --service metastore &gt; metastore.log 2&gt;&1 &
</span><span class='line'>
</span><span class='line'># hive客户端配置
</span><span class='line'>vi hive-site.xml
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;hive.metastore.uris&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;thrift://DataNode2:9083&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;Thrift uri for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<ol>
<li>运行：</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$  bin/spark-sql 2&gt;sql.log
</span><span class='line'>SET spark.sql.hive.version=0.13.1
</span><span class='line'>spark-sql&gt; show databases;
</span><span class='line'>default
</span><span class='line'>neva2dta
</span><span class='line'>spark-sql&gt; show tables;
</span><span class='line'>pokes   false
</span><span class='line'>t_neva2_dps_xdr false
</span><span class='line'>t_neva2_ipdr_xdr        false
</span><span class='line'>spark-sql&gt; select count(*) from pokes;
</span><span class='line'>500
</span><span class='line'>spark-sql&gt; 
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 conf]$ vi spark-env.sh 
</span><span class='line'>#!/usr/bin/env bash
</span><span class='line'>
</span><span class='line'>JAVA_HOME=/home/eshore/jdk1.7.0_60
</span><span class='line'>SPARK_CLASSPATH='/home/eshore/apache-hive-0.13.1/lib/*:/home/eshore/tez-0.4.0-incubating/*:/home/eshore/tez-0.4.0-incubating/lib/*'
</span><span class='line'>
</span><span class='line'># 同步
</span><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat ~/spark-1.3.0-bin-2.2.0/conf/slaves` ; do rsync -vaz /data/opt/ibm/biginsights/hadoop-2.2.0 $h:/data/opt/ibm/biginsights/  ; done
</span></code></pre></td></tr></table></div></figure>


<p>运行hivesever服务</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ cat start_thrift.sh 
</span><span class='line'>#!/bin/bash
</span><span class='line'># hive-classpath已经在spark-env.sh中添加
</span><span class='line'>
</span><span class='line'>./sbin/start-thriftserver.sh --master spark://bigdatamgr1:7077 --executor-memory 16g
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ./start_thrift.sh 
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ bin/beeline -u jdbc:hive2://bigdatamgr1:10001 -n eshore -p '' 
</span></code></pre></td></tr></table></div></figure>


<p>在不依赖外部的jar时，spark的启动脚本是没有问题的，但是我们添加了很多依赖的jar这么写就有问题了，尽管thrift启动正常，但是shell总是打印错误：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>failed to launch org.apache.spark.sql.hive.thriftserver.HiveThriftServer2:
</span><span class='line'>  ========================================
</span><span class='line'>  
</span><span class='line'>full log in /home/eshore/spark-1.3.0-bin-2.2.0/sbin/../logs/spark-eshore-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-bigdatamgr1.out</span></code></pre></td></tr></table></div></figure>


<p>比较隐晦，问题在<code>sbin/spark-daemon.sh</code>，启动完后通过<code>if [[ ! $(ps -p "$newpid" -o args=) =~ $command ]]; then</code>（其中<code>=~</code>表示正则匹配，最终<code>spark-class.sh</code>调用java会加上classpath），而上面的classpath会很长，导致上面的匹配失败！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ vi bin/spark-class
</span><span class='line'>...
</span><span class='line'>  exec "$RUNNER" -cp "$CLASSPATH" $JAVA_OPTS "$@"
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>-- 匹配失败时的值
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ps -p 1925344 -o args=
</span><span class='line'>/home/eshore/jdk1.7.0_60/bin/java -cp :/home/eshore/spark-1.3.0-bin-2.2.0/sbin/../conf:/home/eshore/spark-1.3.0-bin-2.2.0/lib/spark-assembly-1.3.0-hadoop2.2.0.jar:/home/eshore/spark
</span></code></pre></td></tr></table></div></figure>


<h4>解决办法</h4>

<p>先看实验：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[dpi@dacs tmp]$ java -cp ~/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar:. JDBCConnTest
</span><span class='line'>
</span><span class='line'>[dpi@dacs tmp]$ echo $CLASSPATH
</span><span class='line'>.
</span><span class='line'>[dpi@dacs tmp]$ export CLASSPATH=~/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar
</span><span class='line'>[dpi@dacs tmp]$ java JDBCConnTest
</span><span class='line'>错误: 找不到或无法加载主类 JDBCConnTest
</span><span class='line'>[dpi@dacs tmp]$ java -cp . JDBCConnTest
</span><span class='line'>java.lang.ClassNotFoundException: com.mysql.jdbc.Driver
</span><span class='line'>
</span><span class='line'>[dpi@dacs tmp]$ echo $CLASSPATH
</span><span class='line'>/home/dpi/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar
</span><span class='line'>[dpi@dacs tmp]$ export CLASSPATH=~/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar:.
</span><span class='line'>[dpi@dacs tmp]$ java JDBCConnTest</span></code></pre></td></tr></table></div></figure>


<p>设置cp后会覆盖CLASSPATH。所以问题的解决方法：直接把cp的路径删掉（不添加），前面export的classpath路径。java程序会去主动获取改环境变量。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  export CLASSPATH
</span><span class='line'>  exec "$RUNNER" $JAVA_OPTS "$@"</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>效果如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>++ ps -p 1932338 -o args=
</span><span class='line'>+ [[ ! /home/eshore/jdk1.7.0_60/bin/java -XX:MaxPermSize=128m -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --executor-memory 48g spark-internal =~ org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 ]]</span></code></pre></td></tr></table></div></figure>


<ul>
<li>[=~的作用]<a href="http://bbs.chinaunix.net/thread-1623121-1-1.html">http://bbs.chinaunix.net/thread-1623121-1-1.html</a></li>
<li><a href="http://docs.oracle.com/javase/tutorial/essential/environment/paths.html">http://docs.oracle.com/javase/tutorial/essential/environment/paths.html</a></li>
<li><a href="https://docs.oracle.com/javase/8/docs/technotes/tools/windows/classpath.html">https://docs.oracle.com/javase/8/docs/technotes/tools/windows/classpath.html</a></li>
</ul>


<h3>Spark-HA</h3>

<p>仅需要配置，重启spark集群即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata8 spark-1.3.0-bin-2.2.0]$ cat conf/spark-env.sh
</span><span class='line'>...
</span><span class='line'>SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark"
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 conf]$ vi spark-defaults.conf 
</span><span class='line'>spark.master                     spark://bigdatamgr1:7077,bigdata8:7077
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>各个master要单独的启动:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ sbin/start-all.sh 
</span><span class='line'>[eshore@bigdata8 spark-1.3.0-bin-2.2.0]$ sbin/start-master.sh </span></code></pre></td></tr></table></div></figure>


<p>通过查看<a href="http://bigdata8:8080/">http://bigdata8:8080/</a>当前的状态为<strong>STANDBY</strong>。Workers列表为空。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ sbin/stop-master.sh </span></code></pre></td></tr></table></div></figure>


<p>停了bigdatamgr1后，刷新<code>bigdata8:8080</code>页面等1分钟左右就变成ALIVE，然后其他所有的节点也连接到bigdata8了。</p>

<ul>
<li><a href="http://www.cnblogs.com/byrhuangqiang/p/3937654.html">http://www.cnblogs.com/byrhuangqiang/p/3937654.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html#high-availability">http://spark.apache.org/docs/latest/spark-standalone.html#high-availability</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/12/spark-read-source-starter/">[读码] Spark1.1.0前篇&#8211;代码统计导入Eclipse</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-12T13:12:57+08:00" pubdate data-updated="true">Sun 2014-10-12 13:12</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>看过亚太研究院的spark在线教学视频，说spark1.0的源码仅有3w+的代码，蠢蠢欲动。先具体看下源码的量，估算估算；然后搭建eclipse读码环境。</p>

<h2>计算源码行数</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ git branch -v
</span><span class='line'>* (detached from v1.1.0) 2f9b2bd [maven-release-plugin] prepare release v1.1.0-rc4
</span><span class='line'>  master                 4d8ae70 [behind 1246] Cleanup on Connection and ConnectionManager
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ find . -name "*.scala" | grep 'src/main' | xargs sed  -e 's:\/\*.*\*\/::' -e  '/\/\*/, /\*\//{
</span><span class='line'>/\/\*/{
</span><span class='line'> s:\/\*.*::p
</span><span class='line'>}
</span><span class='line'>/\*\//{
</span><span class='line'> s:.*\*\/::p
</span><span class='line'>}
</span><span class='line'>d
</span><span class='line'>}' | sed -e '/^\s*$/d' -e '/^\s*\/\//d' | grep -v '^import' | grep -v '^package' | wc -l
</span><span class='line'>72967
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^scala^java
</span><span class='line'>1749
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^src/main^core/src/main
</span><span class='line'>877
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^java^scala
</span><span class='line'>38526
</span></code></pre></td></tr></table></div></figure>


<p>全部源码的数量（去掉测试）大概在7W左右，仅计算核心代码core下面的代码量在4W。从量上面来说还是比较乐观的，学习scala然后读spark的源码。</p>

<p>spark1.0.0的核心代码量在3w左右。1.1多了大概1w行！！</p>

<h2>Docker</h2>

<p>查看目录结构的时刻，看到spark1下面竟然有docker，不过看Dockerfile的内容只是简单的安装了scala、把本机的spark映射到docker容器、然后运行spark主从集群。</p>

<h2>导入eclipse</h2>

<p>spark使用主要使用scala编写，首先需要下载<a href="http://scala-ide.org/download/sdk.html">scala-ide</a>直接下载2.10的版本（基于eclipse，很多操作都类似）；然后下载<a href="https://github.com/apache/spark.git">spark的源码</a>检出v1.1.0的；然后使用maven生成eclipse工程文件。</p>

<p>(不推荐)使用<a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-Eclipse">sbt生成工程文件</a>。这种方式会缺少一些依赖的jar，处理比较麻烦，还不清楚到底是少了啥！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd sbt/
</span><span class='line'>$ sed -i 's/^M//g' *
</span><span class='line'>$ cd ..
</span><span class='line'>$ sbt/sbt eclipse -mem 512</span></code></pre></td></tr></table></div></figure>


<p>(推荐)使用MVN编译生成，<a href="http://spark.apache.org/docs/latest/building-with-maven.html">使用Maven生成官网文章</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ git clean -x -fd #清理非仓库代码
</span><span class='line'>
</span><span class='line'>$ echo $SCALA_HOME #指定scala-home
</span><span class='line'>/cygdrive/d/scala
</span><span class='line'>
</span><span class='line'># 这里我直接修改默认值，理论上加 -Phadoop-2.2 选项应该也是可以的
</span><span class='line'>$ vi pom.xml # hadoop.version 2.2.0
</span><span class='line'>$ mvn eclipse:eclipse
</span><span class='line'>
</span><span class='line'>$ find . -name ".classpath" | xargs sed -i -e 's/including="\*\*\/\*.java"//' -e 's/excluding="\*\*\/\*.java"//'
</span><span class='line'>
</span><span class='line'>#也可以把添加特性的操作/添加scala源码包操作批量处理掉</span></code></pre></td></tr></table></div></figure>


<p>然后导入到eclipse，然后再针对性的处理报错：</p>

<ul>
<li>先把每个工程都<strong>添加scala特性</strong></li>
<li>把含有python源码包的去掉（手动删除.classpath中classpathentry即可）</li>
<li>确认下并加上<code>src/test/scala</code>的源码包。</li>
</ul>


<p>注意，进行上面的步骤之前，由于scala源文件比较多，编译的时间会比较长，先把Project->Build Automatically去掉，然后一次性把问题处理掉后再手动build！</p>

<ul>
<li>手动使用<code>existing maven projects</code>导入yarn/stable，然后把<strong>yarn/common以链接的形式引入</strong>，并添加到源码包。</li>
</ul>


<p><img src="http://file.bmob.cn/M00/1C/E7/wKhkA1Q7jQ2AMhweAAOC-l-jcz4872.png" alt="" /></p>

<p>还有一个<strong> value q is not a member of StringContext </strong><a href="http://docs.scala-lang.org/overviews/quasiquotes/intro.html">quasiquotes</a>的错误，有些类需要在2.10添加编译组件才能正常编译，修改scala编译首选项。</p>

<p><img src="http://file.bmob.cn/M00/1D/07/wKhkA1Q76GyAFNYPAAEYJfk_ZGw816.png" alt="" /></p>

<p>添加依赖的编译组件后，整个功能就能正常编译通过了。接下来就能调试看源码了。</p>

<p><strong>备注：</strong>clean后发现target目录下并没有重新编译生成class，去掉<code>-Xshow-phases</code>才行。</p>

<blockquote><p> -Xshow-phases                  Print a synopsis of compiler phases.</p></blockquote>

<h2>Maven编译spark</h2>

<p>如果使用的hadoop版本在官网没有集成assembly版本，可以使用maven手动构建。至于打包可以查看下一篇文章。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>$ mvn -Pyarn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean package</span></code></pre></td></tr></table></div></figure>


<p><code>yarn</code>的profile能够编译成可执行的jar文件（包括所有依赖的spark），具体内容下一篇讲。</p>

<h2>小结</h2>

<p>断断续续的写了两天，字数统计弄了大半天，主要在于多行注释的处理。时间最主要都消耗在sbt、maven构建eclipse项目文件（生成、fixed）上。编译scala量上去后确实非常非常的慢，不管是maven还是eclipse都慢！</p>

<p>下一篇将使用docker搭建spark环境，并使用远程调试连接到helloworld程序。</p>

<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/24800129/scala-maven-builder-doesnt-understand-quasiquotes">Scala maven builder doesn&rsquo;t understand quasiquotes</a></li>
<li><a href="http://docs.scala-lang.org/overviews/macros/paradise.html">Macro Paradise</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/07/thinking/">思考</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-07T19:07:26+08:00" pubdate data-updated="true">Tue 2014-10-07 19:07</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>随着年龄的增大，很多原来不曾想的问题慢慢的都开始环绕在自己四周。开始让自己不得不反思，不得不去改变。</p>

<p>本人是一个性格比较极端，又很内向，所以对自己不关心、无自己原来没有直接联系的东西，很少体现积极主动的一面。时时刻刻展现着保守派的作风。自己又在学习能力方面自我感觉良好，对现状总是很不满，对一样事物的持续坚持的耐久力不足（倒不是不能吃苦、吃不了苦的问题）！</p>

<p>从出生到毕业，一直以来都有亲人朋友让我依靠，有很明确值得挑战和超越的目标（总体水平一般，在我前面的人乌压压一片）。出来工作后一直都很迷失，不知道自己能干啥，可以干啥，师范类专业连教师资格证都没有拿到（不是后悔，自己觉得不应该）！！现在想来其实自己太执拗，像极了不撞南墙死不改的蛮牛！！</p>

<p>年龄增加体力不及，开始思考着应该去锻炼锻炼了，但是一直各种借口无疾而终！觉得身体还行，以后再说。。。
工作资历增加直接辅导指导的大哥不再，开始各种瞎折腾，东一锤西一棒，终究是拣了芝麻丢了西瓜！觉得学习能力强以后都敢都来得及，以后再学呗。。。   <br/>
但是在运动场上，一直坚持运动的同学，打个3、4个小时的羽毛球气不喘一下，这时开始懊悔。
当原来一起协作的同事，开始在领域有所斩获，各种嫉妒羡慕的心里开始作祟。</p>

<p>星期一个个的开始了结束，自己却没有得到该有的锤炼和进度，在大势所趋下，自己却总是那么的慢慢吞吞！阅读一个类的千行源码，竟然断断续续花费了仅2个月！本来年前看tomcat原来的计划最终石沉大海！</p>

<p>总是对自己不够狠；狠下来一次后，总是各种理由，最终不能坚持！</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/30/docker-ssh-on-centos/">配置ssh登录docker-centos</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-09-30T00:10:02+08:00" pubdate data-updated="true">Tue 2014-09-30 00:10</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>上一篇写的是docker的入门知识，并没有进行实战。这些记录下使用ssh登录centos容器。</p>

<p>前文中参考的博客介绍了使用ssh登录tutorial容器（ubuntu），然后进行tomcat的安装，以及通过端口映射在客户机进行访问的例子。</p>

<h1>尝试</h1>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull learn/tutorial
</span><span class='line'>docker run -i -t learn/tutorial /bin/bash
</span><span class='line'>  apt-get update
</span><span class='line'>  apt-get install openssh-server
</span><span class='line'>  which sshd
</span><span class='line'>  /usr/sbin/sshd
</span><span class='line'>  mkdir /var/run/sshd
</span><span class='line'>  passwd #输入用户密码，我这里设置为123456，便于SSH客户端登陆使用
</span><span class='line'>  exit #退出
</span><span class='line'>docker ps -l
</span><span class='line'>docker commit 51774a81beb3 learn/tutorial # 提交后，下次启动就可以基于容器更改的系统
</span><span class='line'>docker run -d -p 49154:22 -p 80:8080 learn/tutorial /usr/sbin/sshd -D
</span><span class='line'>ssh root@127.0.0.1 -p 49154
</span><span class='line'>  # 在ubuntu 12.04上安装oracle jdk 7
</span><span class='line'>  apt-get install python-software-properties
</span><span class='line'>  add-apt-repository ppa:webupd8team/java
</span><span class='line'>  apt-get update
</span><span class='line'>  apt-get install -y wget
</span><span class='line'>  apt-get install oracle-java7-installer
</span><span class='line'>  java -version
</span><span class='line'>  # 下载tomcat 7.0.47
</span><span class='line'>  wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-7/v7.0.47/bin/apache-tomcat-7.0.47.tar.gz
</span><span class='line'>  # 解压，运行
</span><span class='line'>  tar xvf apache-tomcat-7.0.47.tar.gz
</span><span class='line'>  cd apache-tomcat-7.0.47
</span><span class='line'>  bin/startup.sh</span></code></pre></td></tr></table></div></figure>


<p>然而在centos上，运行是不成功的。总结操作如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker pull centos:centos6
</span><span class='line'>[root@docker ~]# docker run -i -t  centos:centos6 /bin/bash
</span><span class='line'>  yum install which openssh-server openssh-clients
</span><span class='line'>
</span><span class='line'>  /usr/sbin/sshd # 这里会报错，需要手动生成key
</span><span class='line'>  ssh-keygen -f /etc/ssh/ssh_host_rsa_key
</span><span class='line'>  ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key
</span><span class='line'>
</span><span class='line'>  vi /etc/pam.d/sshd  # 修改pam_loginuid.so为optional
</span><span class='line'>  # /bin/sed -i 's/.*session.*required.*pam_loginuid.so.*/session optional pam_loginuid.so/g' /etc/pam.d/sshd
</span><span class='line'>  
</span><span class='line'>  passwd # 添加密码
</span><span class='line'>  
</span><span class='line'>  rm -rf /etc/localtime
</span><span class='line'>  ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
</span><span class='line'>  cat &gt; /etc/sysconfig/clock  &lt;&lt;EOF
</span><span class='line'>  ZONE="Asia/Shanghai"
</span><span class='line'>  UTC=True
</span><span class='line'>  EOF</span></code></pre></td></tr></table></div></figure>


<ul>
<li>提交保存成果</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>[root@docker ~]# docker commit 3a7b6994bb2a winse/hadoop # 保存为自己使用的版本
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d winse/hadoop /usr/sbin/sshd
</span><span class='line'>f5cb57f6ec22dd9d257bf610322e2bd547ea0064262fcad63308b932c0490670
</span><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS                     PORTS               NAMES
</span><span class='line'>f5cb57f6ec22        winse/hadoop:latest   /usr/sbin/sshd      2 seconds ago       Exited (0) 2 seconds ago                       sharp_rosalind      
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d -p 8888:22 winse/hadoop /usr/sbin/sshd -D
</span><span class='line'>f9814253159373e8a8df3261904200a733b41c63f55708db3cb56a7ebf650cef
</span><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS                  NAMES
</span><span class='line'>f98142531593        winse/hadoop:latest   /usr/sbin/sshd -D   5 seconds ago       Up 4 seconds        0.0.0.0:8888-&gt;22/tcp   boring_bell         
</span><span class='line'>[root@docker ~]# ssh localhost -p 8888
</span><span class='line'>The authenticity of host '[localhost]:8888 ([::1]:8888)' can't be established.
</span><span class='line'>RSA key fingerprint is f5:5e:be:ae:ea:b1:ed:e8:49:43:28:9e:80:87:0d:86.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added '[localhost]:8888' (RSA) to the list of known hosts.
</span><span class='line'>root@localhost's password: 
</span><span class='line'>Last login: Mon Sep 29 14:48:23 2014 from localhost
</span><span class='line'>-bash-4.1# </span></code></pre></td></tr></table></div></figure>


<p>参数<code>-D</code>表示sshd运行在前台。这样当前的docker容器就会一直有程序在运行，不至于执行完指定的任务就被关闭掉了。</p>

<p>在centos配置ssh登录需要进行额外参数的设置。这个还是挺折腾人的。关于把<code>/etc/pam.d/sshd</code>中的<code>pam_loginuid.so</code>修改为optional，<a href="(http://stackoverflow.com/questions/21391142/why-is-it-needed-to-set-pam-loginuid-to-its-optional-value-with-docker">stackoverflow</a>)上的回答还是挺中肯的。</p>

<p>连上ssh后，下一步就和你远程操作服务器一样了。其实docker运行一个容器后，就会分配一个ip，你也可以根据这个ip来连接。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -t -i winse/hadoop /bin/bash
</span><span class='line'>bash-4.1# ssh localhost
</span><span class='line'>ssh: connect to host localhost port 22: Connection refused
</span><span class='line'>bash-4.1# service sshd start
</span><span class='line'>Starting sshd:                                             [  OK  ]
</span><span class='line'>bash-4.1# ifconfig
</span><span class='line'>eth0      Link encap:Ethernet  HWaddr 1E:2B:23:16:98:7E  
</span><span class='line'>          inet addr:172.17.0.31  Bcast:0.0.0.0  Mask:255.255.0.0
</span><span class='line'>          inet6 addr: fe80::1c2b:23ff:fe16:987e/64 Scope:Link
</span><span class='line'>
</span><span class='line'># 新开一个终端
</span><span class='line'>[root@docker ~]# ssh 172.17.0.31
</span><span class='line'>The authenticity of host '172.17.0.31 (172.17.0.31)' can't be established.
</span><span class='line'>RSA key fingerprint is f5:5e:be:ae:ea:b1:ed:e8:49:43:28:9e:80:87:0d:86.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added '172.17.0.31' (RSA) to the list of known hosts.
</span><span class='line'>root@172.17.0.31's password: 
</span><span class='line'>Last login: Mon Sep 29 14:48:23 2014 from localhost
</span><span class='line'>-bash-4.1#           </span></code></pre></td></tr></table></div></figure>


<h2>使用Dockerfile脚本安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# mkdir hadoop
</span><span class='line'>[root@docker ~]# cd hadoop/
</span><span class='line'>[root@docker hadoop]# touch Dockerfile
</span><span class='line'>[root@docker hadoop]# vi Dockerfile
</span><span class='line'>  # hadoop2 on docker-centos
</span><span class='line'>  FROM centos:centos6
</span><span class='line'>  MAINTAINER Winse &lt;fuqiuliu2006@qq.com&gt;
</span><span class='line'>  RUN yum install -y which openssh-clients openssh-server #-y表示交互都输入yes
</span><span class='line'>
</span><span class='line'>  RUN ssh-keygen -f /etc/ssh/ssh_host_rsa_key
</span><span class='line'>  RUN ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key
</span><span class='line'>
</span><span class='line'>  RUN echo 'root:hadoop' |chpasswd
</span><span class='line'>
</span><span class='line'>  RUN sed -i '/pam_loginuid.so/c session    optional     pam_loginuid.so'  /etc/pam.d/sshd
</span><span class='line'>
</span><span class='line'>  EXPOSE 22
</span><span class='line'>  CMD /usr/sbin/sshd -D
</span><span class='line'>  
</span><span class='line'>[root@docker hadoop]# docker build -t="winse/hadoop" .
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>winse/hadoop        latest              9d7f115ef0ec        5 minutes ago       289.1 MB
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker run -d --name slaver1 winse/hadoop
</span><span class='line'>[root@docker hadoop]# docker run -d --name slaver2 winse/hadoop
</span><span class='line'>[root@docker hadoop]# docker run -d --name master1 -P --link slaver1:slaver1 --link slaver2:slaver2  winse/hadoop
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker restart slaver1 slaver2 master1
</span><span class='line'>slaver1
</span><span class='line'>slaver2
</span><span class='line'>master1
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker port master1 22
</span><span class='line'>0.0.0.0:49159
</span><span class='line'>[root@docker hadoop]# ssh localhost -p 49159
</span><span class='line'>... 
</span><span class='line'>-bash-4.1# cat /etc/hosts
</span><span class='line'>172.17.0.31     7ef63f98e2d1
</span><span class='line'>127.0.0.1       localhost
</span><span class='line'>::1     localhost ip6-localhost ip6-loopback
</span><span class='line'>fe00::0 ip6-localnet
</span><span class='line'>ff00::0 ip6-mcastprefix
</span><span class='line'>ff02::1 ip6-allnodes
</span><span class='line'>ff02::2 ip6-allrouters
</span><span class='line'>172.17.0.29     slaver1
</span><span class='line'>172.17.0.30     slaver2</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.blogjava.net/yongboy/archive/2013/12/12/407498.html">Docker学习笔记之一，搭建一个JAVA Tomcat运行环境</a></li>
<li><a href="http://www.csdn123.com/html/topnews201408/36/1236.htm">Docker之配置Centos_ssh</a></li>
<li><a href="http://linux.die.net/man/8/pam_loginuid">pam_loginuid(8) - Linux man page</a></li>
<li><a href="http://stackoverflow.com/questions/21391142/why-is-it-needed-to-set-pam-loginuid-to-its-optional-value-with-docker">Why is it needed to set <code>pam_loginuid</code> to its <code>optional</code> value with docker?</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/27/docker-start-guide-on-centos/">Docker入门</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-09-27T20:28:24+08:00" pubdate data-updated="true">Sat 2014-09-27 20:28</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>docker进一年来火热，发现挺适合用来做运维系统发布的。如果用来捣鼓hadoop的系统部署感觉还是挺不错的。下面一起来学习下docker吧。</p>

<p>docker中提供了<a href="https://docs.docker.com/installation/windows/">windows的安装文档</a>，但是其实很坑爹啊。尽管<a href="https://github.com/boot2docker/windows-installer/releases">提供exe安装</a>，但是最终还是安装visualbox，然后启动带了docker的linux系统（iso）。</p>

<p>如果你已经安装了vmware，但没有安装linux，可以直接<a href="https://github.com/boot2docker/boot2docker/releases">下载iso</a>，然后通过iso来启动。</p>

<h2>安装</h2>

<p>如果你同时安装了vmware，又已经安装了linux，那下面简单列出安装配置docker中使用的命令。docker需要64位的linux操作系统，我这里使用的是centos6，具体的安装步骤看<a href="https://docs.docker.com/installation/centos/">官网的安装教程</a>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# yum install epel-release
</span><span class='line'>
</span><span class='line'>[root@docker ~]# yum install docker-io
</span><span class='line'>[root@docker ~]# service docker start
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run learn/tutorial /bin/echo hello world
</span><span class='line'>Unable to find image 'learn/tutorial' locally
</span><span class='line'>Pulling repository learn/tutorial
</span><span class='line'>8dbd9e392a96: Pulling fs layer 
</span><span class='line'>8dbd9e392a96: Download complete 
</span><span class='line'>hello world
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>learn/tutorial      latest              8dbd9e392a96        17 months ago       128 MB
</span><span class='line'>[root@docker ~]# docker images learn/tutorial 
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>learn/tutorial      latest              8dbd9e392a96        17 months ago       128 MB</span></code></pre></td></tr></table></div></figure>


<p>docker执行run命令时，如果指定的image本地不存在，会从<a href="https://registry.hub.docker.com/">hub服务器</a>获取。也可以先从服务器获取image，然后在执行。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull centos</span></code></pre></td></tr></table></div></figure>


<p>【注】：如果启动失败，1：重装一下docker； 2：还是不行，启动报<code>docker: relocation error: docker: symbol dm_task_get_info_with_deferred_remove, version Base not defined in file libdevmapper.so.1.02 with link time reference</code>，更新<code>yum upgrade device-mapper-libs</code>，然后启动<code>service docker start</code>（具体描述见文章末）</p>

<h2>简单入门</h2>

<p><a href="https://docs.docker.com/userguide/dockerizing/">HelloWorld教程</a></p>

<h4>单次执行</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run learn/tutorial /bin/echo 'hello world'
</span><span class='line'>hello world</span></code></pre></td></tr></table></div></figure>


<p>命令执行完后，容器就会关闭。</p>

<h4>交互式执行方式</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -t -i learn/tutorial /bin/bash
</span><span class='line'>root@274ede23baad:/# uptime
</span><span class='line'> 12:36:02 up  5:59,  0 users,  load average: 0.00, 0.00, 0.00
</span><span class='line'>root@9db219d2e98b:/# cat /etc/issue
</span><span class='line'>Ubuntu 12.04 LTS \n \l
</span><span class='line'>root@274ede23baad:/# pwd
</span><span class='line'>/
</span><span class='line'>root@274ede23baad:/# ls
</span><span class='line'>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  selinux  srv  sys  tmp  usr  var
</span><span class='line'>root@274ede23baad:/# exit
</span><span class='line'>exit</span></code></pre></td></tr></table></div></figure>


<ul>
<li>-t flag assigns a pseudo-tty or terminal inside our new container。</li>
<li>-i flag allows us to make an interactive connection by grabbing the standard in (STDIN) of the container.</li>
</ul>


<h4>后台任务</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -d learn/tutorial /bin/sh -c "while true; do echo hello world; sleep 1; done" 
</span><span class='line'>17e28b56e0cc4ddb5522736e2bcfd752d849a5b1d0b598478ee66b255801aa7c
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS               NAMES
</span><span class='line'>17e28b56e0cc        learn/tutorial:latest   /bin/sh -c 'while tr   2 minutes ago       Up 2 minutes                            trusting_wozniak    </span></code></pre></td></tr></table></div></figure>


<ul>
<li>-d flag tells Docker to run the container and put it in the background, to daemonize it.</li>
</ul>


<p>执行返回的是containter id(唯一ID)。通过ps可以查看当前的后台任务列表。ps列表中的containter id对应，可以查看相应的信息，最后的字段是一个随机指定的名字（也可以指定，后面再讲）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker logs trusting_wozniak
</span><span class='line'>hello world
</span><span class='line'>hello world
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker stop trusting_wozniak
</span><span class='line'>trusting_wozniak
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span></code></pre></td></tr></table></div></figure>


<p>可以通过logs查看容器的标准输出，通过stop来停止容器。</p>

<h2>深入容器</h2>

<p><a href="https://docs.docker.com/userguide/usingdocker/">Working with Containers</a></p>

<p>可以交互式的方式运行container，也可以后台任务的方式运行。</p>

<p>docker的命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Usage:  [sudo] docker [flags] [command] [arguments] ..
</span><span class='line'># Example:
</span><span class='line'>$ sudo docker run -i -t ubuntu /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>每个命令可以指定跟一系列的开关标识(flags)和参数(arguments)。</p>

<h4>各种参数</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker version
</span><span class='line'>
</span><span class='line'>$ docker run -d -P training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps -l
</span><span class='line'>CONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES
</span><span class='line'>bc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155-&gt;5000/tcp  nostalgic_morse
</span><span class='line'>
</span><span class='line'># docker run -d -p 6379 -v /home/hadoop/redis-2.8.13:/opt/redis-2.8.13 learn/tutorial /opt/redis-2.8.13/src/redis-server 
</span><span class='line'>be0b410f3601ea36070b3e519d9cc7cbe259caa2392f468c2dd2baebef42c4a8
</span><span class='line'>
</span><span class='line'># docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                     NAMES
</span><span class='line'>be0b410f3601        learn/tutorial:latest   /opt/redis-2.8.13/sr   10 seconds ago      Up 10 seconds       0.0.0.0:49153-&gt;6379/tcp   sad_colden          
</span><span class='line'>
</span><span class='line'># /home/hadoop/redis-2.8.13/src/redis-cli -p 49153
</span><span class='line'>127.0.0.1:49153&gt; keys *
</span><span class='line'>(empty list or set)
</span><span class='line'>127.0.0.1:49153&gt; </span></code></pre></td></tr></table></div></figure>


<ul>
<li>-P flag is new and tells Docker to map any required network ports inside our container to our host. This lets us view our web application.</li>
<li>-l tells the docker ps command to return the details of the last container started.</li>
<li>-a the docker ps command only shows information about running containers. If you want to see stopped containers too use the -a flag.</li>
<li>-p Network port bindings are very configurable in Docker. In our last example the -P flag is a shortcut for -p 5000 that maps port 5000 inside the container to a high port (from the range 49153 to 65535) on the local Docker host. We can also bind Docker containers to specific ports using the -p flag。</li>
<li>-v flag you can also mount a directory from your own host into a container.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker run -d -p 6379:6379 -v /home/hadoop/redis-2.8.13:/opt/redis-2.8.13 learn/tutorial /opt/redis-2.8.13/src/redis-server 
</span><span class='line'>2c50850c9437698769e54281a9f4154dc4120da2e113802454f1a23c83ab91fe
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                    NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   29 seconds ago      Up 28 seconds       0.0.0.0:6379-&gt;6379/tcp   naughty_yonath  
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker port naughty_yonath 6379
</span><span class='line'>0.0.0.0:6379
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker logs -f naughty_yonath
</span><span class='line'>...
</span><span class='line'>[1] 27 Sep 13:48:12.192 * The server is now ready to accept connections on port 6379
</span><span class='line'>[1] 27 Sep 13:50:33.228 * DB saved on disk
</span><span class='line'>[1] 27 Sep 13:50:43.730 * DB saved on disk</span></code></pre></td></tr></table></div></figure>


<ul>
<li>-f This time though we&rsquo;ve added a new flag, -f. This causes the docker logs command to act like the tail -f command and watch the container&rsquo;s standard out. We can see here the logs from Flask showing the application running on port 5000 and the access log entries for it.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker top naughty_yonath
</span><span class='line'>UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
</span><span class='line'>root                5015                1433                0                   21:48               ?                   00:00:00            /opt/redis-2.8.13/src/redis-server *:6379
</span><span class='line'>[root@docker redis-2.8.13]# docker inspect naughty_yonath
</span><span class='line'>...
</span><span class='line'>    "Volumes": {
</span><span class='line'>        "/opt/redis-2.8.13": "/home/hadoop/redis-2.8.13"
</span><span class='line'>    },
</span><span class='line'>    "VolumesRW": {
</span><span class='line'>        "/opt/redis-2.8.13": true
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker inspect -f '' naughty_yonath
</span><span class='line'>map[/opt/redis-2.8.13:/home/hadoop/redis-2.8.13]
</span></code></pre></td></tr></table></div></figure>


<h4>重启</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker stop naughty_yonath
</span><span class='line'>naughty_yonath
</span><span class='line'>[root@docker redis-2.8.13]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS                     PORTS               NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   8 minutes ago       Exited (0) 5 seconds ago                       naughty_yonath      
</span><span class='line'>[root@docker redis-2.8.13]# docker start naughty_yonath
</span><span class='line'>naughty_yonath
</span><span class='line'>[root@docker redis-2.8.13]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                    NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   8 minutes ago       Up 1 seconds        0.0.0.0:6379-&gt;6379/tcp   naughty_yonath</span></code></pre></td></tr></table></div></figure>


<h4>删除</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker stop naughty_yonath
</span><span class='line'>docker rm naughty_yonath</span></code></pre></td></tr></table></div></figure>


<h2>Images</h2>

<p><a href="https://docs.docker.com/userguide/dockerimages/">Working with Docker Images</a></p>

<h4>列出本地的images</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker images
</span><span class='line'># REPO[:TAG]
</span><span class='line'>docker run -t -i ubuntu:14.04 /bin/bash
</span><span class='line'>docker run -t -i ubuntu:latest /bin/bash</span></code></pre></td></tr></table></div></figure>


<h4>从Hub获取镜像Image</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull centos
</span><span class='line'>docker run -t -i centos /bin/bash
</span><span class='line'>docker search sinatra 
</span><span class='line'>docker pull training/sinatra</span></code></pre></td></tr></table></div></figure>


<h4>创建自己的images</h4>

<p>直接更新image</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -t -i training/sinatra /bin/bash
</span><span class='line'>root@0b2616b0e5a8:/# gem install json
</span><span class='line'>$ sudo docker commit -m="Added json gem" -a="Kate Smith" \
</span><span class='line'>  0b2616b0e5a8 ouruser/sinatra:v2
</span><span class='line'>$ docker images
</span><span class='line'>$ docker run -t -i ouruser/sinatra:v2 /bin/bash
</span><span class='line'>root@78e82f680994:/#</span></code></pre></td></tr></table></div></figure>


<p>通过DockerFile来添加功能，进行更新。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mkdir sinatra
</span><span class='line'>$ cd sinatra
</span><span class='line'>$ touch Dockerfile
</span><span class='line'>  # This is a comment
</span><span class='line'>  FROM ubuntu:14.04
</span><span class='line'>  MAINTAINER Kate Smith &lt;ksmith@example.com&gt;
</span><span class='line'>  RUN apt-get update && apt-get install -y ruby ruby-dev
</span><span class='line'>  RUN gem install sinatra
</span><span class='line'>
</span><span class='line'>$ docker build -t="ouruser/sinatra:v2" .
</span><span class='line'>$ docker run -t -i ouruser/sinatra:v2 /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>具体的DockerFile中各个指令的含义及其使用方法，参考<a href="https://docs.docker.com/userguide/dockerimages/">Building an image from a Dockerfile</a>和<a href="https://docs.docker.com/articles/dockerfile_best-practices/">Best Practices for Writing Dockerfiles</a>，以及<a href="https://docs.docker.com/reference/builder/">Dockerfile Reference</a>。具体例子<a href="https://github.com/perl/docker-perl/blob/r20140922.0/5.020.001-64bit,threaded/Dockerfile">docker-perl</a></p>

<h4>添加新标签Tag</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker tag 5db5f8471261 ouruser/sinatra:devel
</span><span class='line'>$ docker images ouruser/sinatra
</span><span class='line'>REPOSITORY          TAG     IMAGE ID      CREATED        VIRTUAL SIZE
</span><span class='line'>ouruser/sinatra     latest  5db5f8471261  11 hours ago   446.7 MB
</span><span class='line'>ouruser/sinatra     devel   5db5f8471261  11 hours ago   446.7 MB</span></code></pre></td></tr></table></div></figure>


<h4>上传分享到<a href="https://hub.docker.com/">hub</a></h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker push ouruser/sinatra</span></code></pre></td></tr></table></div></figure>


<h4>从本地删除</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker rmi training/sinatra</span></code></pre></td></tr></table></div></figure>


<h2>多container结合使用</h2>

<p><a href="https://docs.docker.com/userguide/dockerlinks/">Linking Containers Together</a></p>

<h4>端口映射</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d -P training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker ps nostalgic_morse
</span><span class='line'>CONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES
</span><span class='line'>bc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155-&gt;5000/tcp  nostalgic_morse
</span><span class='line'>
</span><span class='line'>docker run -d -p 5000:5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker run -d -p 127.0.0.1:5000:5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker run -d -p 127.0.0.1::5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'># The -p flag can be used multiple times to configure multiple ports.
</span><span class='line'>docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker port nostalgic_morse 5000
</span><span class='line'>127.0.0.1:49155</span></code></pre></td></tr></table></div></figure>


<h4>Container Linking</h4>

<p>docker想的还是很周到的。面临两个container互相访问，一个db，一个web，哪web怎么访问db的数据呢？</p>

<p>指定container的名称：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -d -P --name web training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps -l
</span><span class='line'>CONTAINER ID  IMAGE                  COMMAND        CREATED       STATUS       PORTS                    NAMES
</span><span class='line'>aed84ee21bde  training/webapp:latest python app.py  12 hours ago  Up 2 seconds 0.0.0.0:49154-&gt;5000/tcp  web
</span><span class='line'>
</span><span class='line'>$ docker inspect -f "" aed84ee21bde
</span><span class='line'>/web</span></code></pre></td></tr></table></div></figure>


<p>容器互通：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -d --name db training/postgres
</span><span class='line'>
</span><span class='line'>$ docker rm -f web
</span><span class='line'>$ docker run -d -P --name web --link db:db training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps
</span><span class='line'>CONTAINER ID  IMAGE                     COMMAND               CREATED             STATUS             PORTS                    NAMES
</span><span class='line'>349169744e49  training/postgres:latest  su postgres -c '/usr  About a minute ago  Up About a minute  5432/tcp                 db, web/db
</span><span class='line'>aed84ee21bde  training/webapp:latest    python app.py         16 hours ago        Up 2 minutes       0.0.0.0:49154-&gt;5000/tcp  web</span></code></pre></td></tr></table></div></figure>


<p>链接后，在web容器会添加DB的环境变量，同时把db的ip加入到/etc/hosts中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>$ docker run --rm --name web2 --link db:db training/webapp env
</span><span class='line'>    . . .
</span><span class='line'>    DB_NAME=/web2/db
</span><span class='line'>    DB_PORT=tcp://172.17.0.5:5432
</span><span class='line'>    DB_PORT_5432_TCP=tcp://172.17.0.5:5432
</span><span class='line'>    DB_PORT_5432_TCP_PROTO=tcp
</span><span class='line'>    DB_PORT_5432_TCP_PORT=5432
</span><span class='line'>    DB_PORT_5432_TCP_ADDR=172.17.0.5
</span><span class='line'>
</span><span class='line'>$ docker run -t -i --rm --link db:db training/webapp /bin/bash
</span><span class='line'>root@aed84ee21bde:/opt/webapp# cat /etc/hosts
</span><span class='line'>172.17.0.7  aed84ee21bde
</span><span class='line'>. . .
</span><span class='line'>172.17.0.5  db    </span></code></pre></td></tr></table></div></figure>


<p>You can see that Docker has created a series of environment variables with useful information about the source db container. Each variable is prefixed with <code>DB_</code>, which is populated from the alias you specified above. If the alias were db1, the variables would be prefixed with <code>DB1_</code>.</p>

<h2>存储</h2>

<p><a href="https://docs.docker.com/userguide/dockervolumes/">Managing Data in Containers</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Adding a data volume
</span><span class='line'>docker run -d -P --name web -v /webapp training/webapp python app.py
</span><span class='line'>
</span><span class='line'># Mount a Host Directory as a Data Volume
</span><span class='line'>docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py
</span><span class='line'># 只读
</span><span class='line'>docker run -d -P --name web -v /src/webapp:/opt/webapp:ro training/webapp python app.py
</span><span class='line'>
</span><span class='line'># Mount a Host File as a Data Volume
</span><span class='line'>docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash
</span><span class='line'>
</span><span class='line'># Creating and mounting a Data Volume Container
</span><span class='line'>docker run -d -v /dbdata --name dbdata training/postgres echo Data-only container for postgres
</span><span class='line'>docker run -d --volumes-from dbdata --name db1 training/postgres
</span><span class='line'>docker run -d --volumes-from dbdata --name db2 training/postgres
</span><span class='line'>docker run -d --name db3 --volumes-from db1 training/postgres
</span><span class='line'>
</span><span class='line'># Backup, restore, or migrate data volumes
</span><span class='line'>docker run --volumes-from dbdata -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata
</span><span class='line'>docker run -v /dbdata --name dbdata2 ubuntu /bin/bash
</span><span class='line'>docker run --volumes-from dbdata2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar
</span></code></pre></td></tr></table></div></figure>


<h2>回顾</h2>

<p>管理docker主要使用其提供的各种命令、以及参数来进行。</p>

<ul>
<li>本地的镜像管理: docker images / docker rmi [image identify]</li>
<li>容器管理： docker ps -a|-l / docker start|stop|rm|restart [image identify]</li>
<li>运行容器：docker run [images] [command]

<ul>
<li>-d 后台运行</li>
<li>-ti tty交互式运行</li>
<li>-P 把容器expose的端口映射到宿主机器端口。可以通过<code>docker port [container-name]</code>来查看端口映射关系。</li>
<li>-p [host-machine-port:container-machine-port]手动指定端口映射关系</li>
<li>-h [hostname] 实例操作系统的hostname</li>
<li>&ndash;name [name] 容器实例标识</li>
<li>-v [path] 建立目录</li>
<li>-v [host-machine-path:container-machine-path] 把宿主的文件路径映射到容器操作系统的指定目录</li>
<li>&ndash;link [container-name:name] 多容器之间互相访问。</li>
</ul>
</li>
</ul>


<p>还有很多辅助命令如：<code>top</code>, <code>logs</code>, <code>port</code>, <code>inspect</code>。以及进行版本管理的<code>pull</code>, <code>push</code>, <code>commit</code>, <code>tag</code>等等。</p>

<h2>更新</h2>

<ul>
<li>2015年3月3日00:29:44</li>
</ul>


<p>docker官网连不上，巨坑！从原来的docker导出</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker ps -a
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS                      PORTS               NAMES
</span><span class='line'>4a1ba5605868        learn/tutorial:latest   /bin/bash              15 seconds ago      Exited (0) 11 seconds ago                       loving_wilson        
</span><span class='line'>6e8a77ff8c26        centos:centos6          /bin/bash              10 minutes ago      Exited (0) 10 minutes ago                       determined_almeida  
</span><span class='line'>[root@docker ~]# docker export loving_wilson &gt; learn_tutorial.tar
</span><span class='line'>
</span><span class='line'>#===
</span><span class='line'>
</span><span class='line'>[root@localhost ~]# cat centos6.tar | docker import - centos:centos6
</span><span class='line'>876f82e7032a2ed567421298c6dd12a74ac7b37fc28ef4fd062ebb4678bd6821
</span><span class='line'>[root@localhost ~]# cat learn_tutorial.tar | docker import - learn/tutorial
</span><span class='line'>dc574b587de3479ecc3622c7b4f12227d894aa1461737612130122092a72bdb4
</span><span class='line'>[root@localhost ~]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED              VIRTUAL SIZE
</span><span class='line'>learn/tutorial      latest              dc574b587de3        23 seconds ago       128.2 MB
</span><span class='line'>centos              centos6             876f82e7032a        About a minute ago   212.7 MB</span></code></pre></td></tr></table></div></figure>


<ul>
<li>2015年8月5日11:04:14</li>
</ul>


<p>1 看看国内网站是否有对应的镜像： <a href="http://dockerpool.com/downloads">http://dockerpool.com/downloads</a></p>

<p>2 连不上可以<a href="https://registry.hub.docker.com/_/centos/">https://registry.hub.docker.com/_/centos/</a> , 直接到github上面下载对应的<a href="https://github.com/CentOS/sig-cloud-instance-images/tree/CentOS-6/docker">dockerfile</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# git clone -b CentOS-6  https://github.com/CentOS/sig-cloud-instance-images.git
</span><span class='line'>[root@localhost docker]# docker build . 
</span><span class='line'>
</span><span class='line'>[root@localhost docker]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>&lt;none&gt;              &lt;none&gt;              437c8a32e0c6        27 seconds ago      203.1 MB
</span><span class='line'>
</span><span class='line'># 启动登陆容器，安装sshd
</span><span class='line'>[root@localhost docker]# docker run -ti 437c8a32e0c6 /bin/bash
</span><span class='line'>[root@077cd71ff08f /]# yum install which openssh-server openssh-clients
</span><span class='line'>[root@077cd71ff08f /]# chkconfig --list
</span><span class='line'>iptables        0:off   1:off   2:on    3:on    4:on    5:on    6:off
</span><span class='line'>netconsole      0:off   1:off   2:off   3:off   4:off   5:off   6:off
</span><span class='line'>netfs           0:off   1:off   2:off   3:on    4:on    5:on    6:off
</span><span class='line'>network         0:off   1:off   2:on    3:on    4:on    5:on    6:off
</span><span class='line'>rdisc           0:off   1:off   2:off   3:off   4:off   5:off   6:off
</span><span class='line'>restorecond     0:off   1:off   2:off   3:off   4:off   5:off   6:off
</span><span class='line'>sshd            0:off   1:off   2:on    3:on    4:on    5:on    6:off
</span><span class='line'>udev-post       0:off   1:on    2:on    3:on    4:on    5:on    6:off
</span><span class='line'>[root@077cd71ff08f /]# service sshd status
</span><span class='line'>openssh-daemon is stopped
</span><span class='line'>[root@077cd71ff08f /]# service sshd start
</span><span class='line'>Generating SSH2 RSA host key:                              [  OK  ]
</span><span class='line'>Generating SSH1 RSA host key:                              [  OK  ]
</span><span class='line'>Generating SSH2 DSA host key:                              [  OK  ]
</span><span class='line'>Starting sshd:                                             [  OK  ]
</span><span class='line'>[root@077cd71ff08f /]# vi /etc/ssh/sshd_config 
</span><span class='line'>#UsePAM no
</span><span class='line'>#或者 sed -i '/pam_loginuid.so/c session    optional     pam_loginuid.so'  /etc/pam.d/sshd
</span><span class='line'>[root@077cd71ff08f /]# which sshd
</span><span class='line'>/usr/sbin/sshd
</span><span class='line'>[root@077cd71ff08f /]# passwd 记得添加密码
</span><span class='line'>
</span><span class='line'># 提交更新镜像
</span><span class='line'>[root@localhost ~]# docker ps -a
</span><span class='line'>CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS                      PORTS               NAMES
</span><span class='line'>077cd71ff08f        bigdata:latest        "/bin/bash"         4 minutes ago       Exited (0) 11 seconds ago                       desperate_bell       
</span><span class='line'>7195847a0166        437c8a32e0c6:latest   "/bin/bash"         3 hours ago         Up 5 minutes                                    determined_feynman   
</span><span class='line'>[root@localhost ~]# docker commit 077cd71ff08f bigdata
</span><span class='line'>[root@localhost ~]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>bigdata             latest              c2a336f22ff8        4 minutes ago       261.3 MB
</span><span class='line'>
</span><span class='line'># 启动新容器，使用ssh远程登陆
</span><span class='line'>[root@localhost ~]# docker run -d --dns 172.17.42.1 --name master -h master bigdata /usr/sbin/sshd -D
</span><span class='line'>[root@localhost ~]# docker run -d  --dns 172.17.42.1 --name slaver1 -h slaver1 bigdata /usr/sbin/sshd -D
</span><span class='line'>[root@localhost ~]# docker inspect master
</span><span class='line'>[root@localhost ~]# vi /etc/hosts
</span><span class='line'>[root@localhost ~]# service dnsmasq restart</span></code></pre></td></tr></table></div></figure>


<p>3 <a href="http://www.cnblogs.com/2018/p/4633940.html">自己制作</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost docker]# wget --no-check-certificate  https://raw.githubusercontent.com/docker/docker/master/contrib/mkimage-yum.sh
</span><span class='line'>[root@localhost docker]# chmod +x mkimage-yum.sh
</span><span class='line'>[root@localhost docker]# ./mkimage-yum.sh centos6
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>2015年3月2日16:13:12</li>
</ul>


<p>再在centos6.5上安装最新的，启动后报错：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# docker -d
</span><span class='line'>INFO[0000] +job serveapi(unix:///var/run/docker.sock)   
</span><span class='line'>INFO[0000] WARNING: You are running linux kernel version 2.6.32-431.el6.x86_64, which might be unstable running docker. Please upgrade your kernel to 3.8.0. 
</span><span class='line'>INFO[0000] Listening for HTTP on unix (/var/run/docker.sock) 
</span><span class='line'>docker: relocation error: docker: symbol dm_task_get_info_with_deferred_remove, version Base not defined in file libdevmapper.so.1.02 with link time reference</span></code></pre></td></tr></table></div></figure>


<p>需要再安装新的依赖（囧，md，用yum安装还要自己安装其他依赖！！）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]#  yum install device-mapper-event-libs</span></code></pre></td></tr></table></div></figure>


<ul>
<li>报错2：<code>cgroup.procs: invalid argument</code>[2015年8月6日11:11:49]</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# docker start 5ed45ce5ad3d
</span><span class='line'>Error response from daemon: Cannot start container 5ed45ce5ad3d: [8] System error: write /cgroup/freezer/docker/5ed45ce5ad3d085fe3c004f90eef7c774a722e84cf0c9d18c197cc5900bbc8ae/cgroup.procs: invalid argument
</span><span class='line'>FATA[0000] Error: failed to start one or more containers </span></code></pre></td></tr></table></div></figure>


<p>修改配置：<a href="http://blog.csdn.net/jollypigclub/article/details/40428095">http://blog.csdn.net/jollypigclub/article/details/40428095</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# vi /etc/sysconfig/docker
</span><span class='line'>...
</span><span class='line'>other_args="--exec-driver=lxc"
</span><span class='line'>#other_args=""
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<ul>
<li>docker本地存储的路径[@ 2015年8月5日11:19:17]</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost docker]# cd /var/lib/docker/
</span><span class='line'>[root@localhost docker]# ls
</span><span class='line'>containers  devicemapper  graph  init  linkgraph.db  repositories-devicemapper  tmp  trust  volumes
</span><span class='line'>[root@localhost docker]# cd graph/
</span><span class='line'>[root@localhost graph]# ll
</span><span class='line'>总用量 16
</span><span class='line'>drwx------ 2 root root 4096 8月   5 10:39 d5d33a6a321ae20a3ae4805b5643560ce9c16a49d2f1d32541b39e04ad083983
</span><span class='line'>drwx------ 2 root root 4096 8月   5 10:39 d8ed1be0a39bcc741aa1e95e59b844140d9294afc75082697184cdfbf2bc6a2d
</span><span class='line'>drwx------ 2 root root 4096 8月   5 09:48 f1b10cd842498c23d206ee0cbeaa9de8d2ae09ff3c7af2723a9e337a6965d639
</span><span class='line'>drwx------ 2 root root 4096 8月   5 10:39 _tmp
</span><span class='line'>
</span><span class='line'>[root@localhost docker]# cd devicemapper/devicemapper/
</span><span class='line'>[root@localhost devicemapper]# ll
</span><span class='line'>总用量 976024
</span><span class='line'>-rw------- 1 root root 107374182400 8月   5 09:38 data
</span><span class='line'>-rw------- 1 root root   2147483648 8月   5 09:38 metadata</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.blogjava.net/yongboy/archive/2013/12/12/407498.html">Docker学习笔记之一，搭建一个JAVA Tomcat运行环境</a></li>
<li><a href="http://www.inspires.cn/note/36">You are running linux kernel version 2.6.32-431.el6.x86_64(centos 6.5)</a></li>
<li><a href="http://blog.thoward37.me/articles/where-are-docker-images-stored/">Where are Docker images stored?(老版本，也值得一看)</a></li>
<li><a href="http://blog.csdn.net/xu470438000/article/details/43704469">Docker启动报错 relocation error libdevmapper</a></li>
<li><a href="http://www.programfish.com/blog/?p=9">docker镜像与容器存储结构分析</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/16">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/14">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2016/04/23/hadoop-guide-catalog/">[整理] Hadoop入门</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/06/17/ganglia-install-on-centos-with-puppet/">使用Puppet安装配置Ganglia</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/05/08/rrc-apache-spark-source-inside-shell/">[读读书]Apache Spark源码剖析-Shell</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/05/07/rrc-apache-spark-source-inside-preface/">[读读书]Apache Spark源码剖析-序</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/05/05/hdfs-heterogeneous-storage/">Hdfs异构存储实操</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/05/05/puppetboard-install/">Puppetboard Install</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/05/03/hiera-and-facts/">Hiera and Facts</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/04/28/mcollective-plugins/">MCollective Plugins</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/04/28/mcollective-quick-start/">MCollective安装配置</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/alluxio/'>alluxio</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (5) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (42) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/puppet/'>puppet</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (33) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (136)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'winseliu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
