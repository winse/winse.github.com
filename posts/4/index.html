
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="类C语言，继C++之后的最辉煌耀眼的明星都属Java，其中最突出的又数内存管理。JVM对运行在其上的程序进行内存自动化分配和管理，减少开发人员的工作量之外便于统一的维护和管理。JDK提供了各种各样的工具来让开发实施人员了解运行的运行状态。 jps -l -v -m
jstat -gcutil &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/4">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->

  <!--
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-43198550-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


  -->
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-25T21:12:13+08:00" pubdate data-updated="true">Mon 2014-08-25 21:12</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>类C语言，继C++之后的最辉煌耀眼的明星都属Java，其中最突出的又数内存管理。JVM对运行在其上的程序进行内存自动化分配和管理，减少开发人员的工作量之外便于统一的维护和管理。JDK提供了各种各样的工具来让开发实施人员了解运行的运行状态。</p>

<ul>
<li>jps -l -v -m</li>
<li>jstat -gcutil 2000 100</li>
<li>jmap</li>
<li>jinfo <a href="http://file.bmob.cn/M00/03/AD/wKhkA1PE2MGAB4-fAAGTqUeu-cE940.png">查看参数例子</a></li>
<li>jstack</li>
<li>jvisualvm/jconsole</li>
<li>mat(MemoryAnalyzer)</li>
<li>btrace</li>
<li>jclasslib（查看局部变量表）</li>
</ul>


<p>前段时间，接手(前面已经有成型的东西)使用Hadoop存储转换的项目，但是生产环境的程序总是隔三差五的OOM，同时使用的hive0.12.0也偶尔出现内存异常。这对于运维来说就是灭顶之灾！搞不好什么时刻程序就挂了！！必须咬咬牙把这个问题处理解决，开始把老古董们请出来，翻来基本不看的后半部分&ndash;Java内存管理。</p>

<ul>
<li>《Java程序性能优化-让你的Java程序更快、更稳定》第5章JVM调优/第6章Java性能调优工具</li>
<li>《深入理解Java虚拟机-JVM高级特性与最佳实践》第二部分自动内存管理机制</li>
</ul>


<p>这里用到的理论知识比较少。主要用Java自带的工具，加上内存堆分析工具（mat/jvisualvm）找出大对象，同时结合源代码定位问题。</p>

<p>下面主要讲讲实践，查找问题的思路。在本地进行测试的话，我们可以打断点，可以通过jvisualvm来查看整个运行过程内存的变化趋势图。但是到了linux服务器，并且还是生产环境的话，想要有本地一样的图形化工具来监控是比较困难的！一般服务器的内存都设置的比较大，而windows设置的内存又有限！所以内存达到1G左右，立马dump一个堆的内存快照然后下载到本地进行来分析（可以通过<code>-J-Xmx</code><a href="http://file.bmob.cn/M00/09/83/wKhkA1P7TV-ABDnOAAB-OnVBQic050.png">调整jvisualvm的内存</a>）。</p>

<ul>
<li>首先，由于报错是在Configuration加载配置文件时抛出OOM，第一反应肯定Configuraiton对象太多导致！同时查看dump的堆内存也佐证了这一点。直接把程序中的Configuration改成单例。</li>
</ul>


<p>程序对象内存占比排行（<code>jmap -histo PID</code>）：</p>

<p><img src="http://file.bmob.cn/M00/09/81/wKhkA1P7S8yARYSkAAiFW9cVN5w526.png" alt="" /></p>

<p>使用mat或者jvisualvm查看堆，确实Configuration对象过多（<code>jmap -dump:format=b,file=/tmp/bug.hprof PID</code>）：</p>

<p><img src="http://file.bmob.cn/M00/09/83/wKhkA1P7TbmAIdDEAAq3ktPBs6Q266.png" alt="" /></p>

<ul>
<li><p>修改后再次运行，但是没多大用！还是OOM！！</p></li>
<li><p>进一步分析，发现在Configuration中的属性/缓冲的都是弱引用是weakhashmap。</p></li>
</ul>


<p><img src="http://file.bmob.cn/M00/09/83/wKhkA1P7TfaAf4nwAAbcdgFiyXs804.png" alt="" /></p>

<p>OOM最终问题不在Configuration对象中的属性，哪谁hold住了Configuration对象呢？？</p>

<ul>
<li>再次从根源开始查找问题。程序中FileSystem对象使用<code>FileSystem.get(URI, Configuration, String)</code>获取，然后调用<code>get(URI,Configuration)</code>方法，其中的<strong>CACHE</strong>很是刺眼啊！</li>
</ul>


<p><img src="http://file.bmob.cn/M00/09/8D/wKhkA1P72pmAAMdnAAEYMjHFUAI853.png" alt="" /></p>

<p>缓冲FileSystem的Cache对象的Key是URI和UserGroupInformation两个属性来判断是否相等的。对于一个程序来说一般就读取一个HDFS的数据即URI前部分是确定的，重点在UserGroupInformation是通过<code>UserGroupInformation.getCurrentUser()</code>来获取的。</p>

<p>即获取在get时<code>UserGroupInformation.getBestUGI</code>得到的对象。而这个对象在UnSecure情况下每次都是调用<code>createRemoteUser</code>创建新的对象！也就是每调用一次<code>FileSystem.get(URI, Configuration, String)</code>就会缓存一个FileSystem对象，以及其hold住的Configuration都会被保留在内存中。
<img src="http://file.bmob.cn/M00/09/82/wKhkA1P7TBSAaYEoAAhzUA5j5MI991.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/09/82/wKhkA1P7TJ2AfwJzAAhEVFjK7Ek367.png" alt="" /></p>

<p>只消耗不释放终究会坐吃山空啊！到最后也就必然OOM了。从mat的UserGroupInformation的个数查询，以及Cache对象的总量可以印证。</p>

<p><img src="http://file.bmob.cn/M00/09/82/wKhkA1P7TNeAB7JAAAdMg-udeR8285.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/09/83/wKhkA1P7TU-ACoaCAApK4n-52hI027.png" alt="" /></p>

<h2>问题处理</h2>

<p>把程序涉及到FileSystem.get调用去掉user参数，使两个参数的方法。由于都使用getCurrentUser获取对象，也就是说程序整个运行过程中就一个FileSystem对象，但是与此同时就不能关闭获取到的FileSystem，如果当前运行的用户与集群所属用户不同，需要设置环境变量指定当前操作的用户！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>System.setProperty("HADOOP_USER_NAME", "hadoop");</span></code></pre></td></tr></table></div></figure>


<p>查找代码中调用了FileSystem#close是一个痛苦的过程，由于FileSystem实现的是Closeable的close方法，用<strong>Open Call Hierarchy</strong>基本是大海捞中啊，根本不知道那个代码是自己的！！这里用btrace神器让咋也高大上一把。。。</p>

<p>当时操作的步骤找不到了，下图是调用Cache#getInternal方法监控代码<a href="https://gist.github.com/winse/161f6fe9120f2ec6b024">GIST</a>：</p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7UD2AFk2cAAXRQWzniL0296.png" alt="" /></p>

<h2>hive0.12内存溢出问题</h2>

<p>hive0.12.0查询程序MR内容溢出</p>

<p><img src="http://file.bmob.cn/M00/09/81/wKhkA1P7StSAOgX1AAoW9v-Fd4s439.png" alt="" /></p>

<p>在hive-0.13前官网文档中有提到内存溢出这一点，可以对应到FileSystem中代码的判断。</p>

<p><img src="http://file.bmob.cn/M00/09/85/wKhkA1P7UP-ACRVdAAJHBKNTq94580.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>String disableCacheName = String.format("fs.%s.impl.disable.cache", scheme);
</span><span class='line'>if (conf.getBoolean(disableCacheName, false)) {
</span><span class='line'>  return createFileSystem(uri, conf);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>hive0.13.1处理</p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7T_CAcxVqAARr7CGiDvY177.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7T6KAKoUiAAvODPwh1po815.png" alt="" /></p>

<p>新版本在每次查询（session）结束后都会把本次涉及到的FileSystem关闭掉。</p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7T9uAQQB3AAWrj_efwZU495.png" alt="" /></p>

<h2>理论知识</h2>

<p>从GC类型开始讲，对自动化内存的垃圾收集有个整体的感知： 新生代/s0（survivor space0、from space）/s1（survivor space1、to space）/永久代。虚拟机参数<code>-Xmx</code>,<code>-Xms</code>,<code>-Xmn</code>（<code>-Xss</code>）来调节各个代的大小和比例。</p>

<ul>
<li><code>-Xss</code> 参数来设置栈的大小。栈的大小直接决定了函数的调用可达深度</li>
<li><code>-XX:PrintGCDetails -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=15 -Xms40m -Xmx40m -Xmn20m</code></li>
<li><code>-XX:NewSize</code>和<code>-XX:MaxNewSize</code></li>
<li><code>-XX:NewRatio</code>和<code>-XX:SurvivorRatio</code></li>
<li><code>-XX:PermSize=2m -XX:MaxPermSize=4m -XX:+PrintGCDetails</code></li>
<li><code>-verbose:gc</code></li>
<li><code>-XX:+PrintGC</code></li>
<li><code>-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/bug.hprof -XX:OnOutOfMemoryError=/reset.sh</code></li>
<li><code>jmap -dump:format=b,file=/tmp/bug.hprof PID</code></li>
<li><code>jmap -histo PID &gt; /tmp/s.txt</code></li>
<li><code>jstack -l PID</code></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/20/magical-use-java-equals-hashcode/">巧用Equals和Hashcode</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-20T11:03:54+08:00" pubdate data-updated="true">Wed 2014-08-20 11:03</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>java中让人疑惑的一点就是等于的判断，有使用<code>==</code>和<code>equals</code>， 有一些专门字符串初始化的资料来考你在是否已经真正的掌握判断两个对象是否一致。</p>

<p>同时，在重写equals时很多资料都强调要重写hashcode。</p>

<p>java中HashMap就是基于equals和hashcode来实现拉链的键值对。Map中存储了entry&lt;K,V>的数组，数组的下标是基于对象的hashcode再对entry长度[并]<code>&amp;</code>的结果。</p>

<p><img src="http://file.bmob.cn/M00/08/55/wKhkA1P0OO-AE9u3AABNzQK0pr8747.png" alt="" /></p>

<p>使用set/map来实现集合，并且对象重写了equals但没有重写hashcode的情况下，得到的结果与你臆想的不同。同时，在特定场景结合hashcode和equals可以实现很酷的效果。</p>

<ul>
<li>第一个例子A 重写了equals但是没有重写hashcode(ERROR)，此时判断元素是否在集合中结果可能并不是你想要的。</li>
<li>第二个B的例子 重写hashcode和equals对应后就正确了。</li>
<li>第三个AA是个很酷的例子 equals的条件更强，可以实现类似<code>map&lt;string, list&lt;string&gt;&gt;</code>的效果。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>static class A {
</span><span class='line'>
</span><span class='line'>  String name;
</span><span class='line'>  int age;
</span><span class='line'>
</span><span class='line'>  public A(String name, int age) {
</span><span class='line'>      this.name = name;
</span><span class='line'>      this.age = age;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public boolean equals(Object obj) {
</span><span class='line'>      return new EqualsBuilder().append(getClass(), obj.getClass()).append(name, ((A) obj).name).isEquals();
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testA() {
</span><span class='line'>  Set&lt;A&gt; set = new HashSet&lt;&gt;();
</span><span class='line'>
</span><span class='line'>  set.add(new A("abc", 12));
</span><span class='line'>  set.add(new A("abc", 14));
</span><span class='line'>
</span><span class='line'>  System.out.println(set.size());
</span><span class='line'>
</span><span class='line'>  System.out.println(set.contains(new A("abc", 0)));
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>static class B extends A {
</span><span class='line'>
</span><span class='line'>  public B(String name, int age) {
</span><span class='line'>      super(name, age);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public int hashCode() {
</span><span class='line'>      return this.name.hashCode();
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testB() {
</span><span class='line'>  /* Set&lt;A&gt; */Set&lt;B&gt; set = new HashSet&lt;&gt;();
</span><span class='line'>
</span><span class='line'>  set.add(new B("abc", 12));
</span><span class='line'>  set.add(new B("abc", 14));
</span><span class='line'>
</span><span class='line'>  System.out.println(set.size());
</span><span class='line'>
</span><span class='line'>  System.out.println(set.contains(new B("abc", 0)));
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>static class AA extends A {
</span><span class='line'>  public AA(String name, int age) {
</span><span class='line'>      super(name, age);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public boolean equals(Object obj) {
</span><span class='line'>      return new EqualsBuilder().append(getClass(), obj.getClass()).append(name, ((A) obj).name)
</span><span class='line'>              .append(age, ((A) obj).age).isEquals();
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public int hashCode() {
</span><span class='line'>      return this.name.hashCode();
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testAA() {
</span><span class='line'>  // 实现Map&lt;String, Set&lt;String&gt;&gt;的效果
</span><span class='line'>  Set&lt;AA&gt; set = new HashSet&lt;&gt;();
</span><span class='line'>
</span><span class='line'>  set.add(new AA("abc", 12));
</span><span class='line'>  set.add(new AA("abc", 14));
</span><span class='line'>
</span><span class='line'>  System.out.println(set.size());
</span><span class='line'>
</span><span class='line'>  System.out.println(set.contains(new AA("abc", 0)));
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://java.chinaitlab.com/base/879319.html">java中HashMap详解</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/06/read-hadoop-balancer-source-part1/">[读码] Hadoop2 Balancer磁盘空间平衡（上）</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-06T22:14:29+08:00" pubdate data-updated="true">Wed 2014-08-06 22:14</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>javadoc</h2>

<p>在一些节点满了或者加入了新的节点情况下，使用balancer工具可以平衡HDFS集群磁盘空间使用率。该功能作为一个单独的程序，能同时与集群的其他文件操作一起进行。</p>

<p>threshold（阀值）参数是个介于1%~100%的值，默认情况下是10%。指定了集群达到平衡的指标值。当节点的利用率（所在节点的磁盘使用率，只HDFS可利用的部分）与集群利用率（集群HDFS的使用率）之间的差不大于阀值threshold就表示集群已经处于平衡状态。所以，阀值threshold越小，集群各节点数据分布越平衡（集群越平衡），当然这也会耗费更多的时间来达到小的平衡阀值。同时，当数据同时又在进行读写操作时，可能平衡并不能达到非常小的阀值。</p>

<p><img src="http://file.bmob.cn/M00/0C/95/wKhkA1QJH2uAa8UEAABq7RCSLQ0452.png" alt="" /></p>

<p>这个工具依次地把磁盘使用率高的机器的块移动到使用率低的数据节点上。每次迭代移动/接受的数据小于10G或者节点容量的阀值百分比（In each iteration a datanode moves or receives no more than the lesser of 10G bytes or the threshold fraction of its capacity. ）。每次迭代不会大于20分钟。每次迭代完成后，balancer把数据节点信息更新到namenode，重新计算利用率后，再进行下一次迭代直到集群利用率阀值。</p>

<p>配置<code>dfs.balance.bandwidthPerSec</code>控制balancer操作传输的带宽，默认配置是1048576（1M/s）这个属性决定了一个块从一个数据节点移动到另一个节点的最大速率。默认是1M/s。bandwidth越高集群达到平衡越快，但是程序之间的竞争会更激烈。如果通过配置文件来修改这个属性，需要在下次启动HDFS才能生效。可以通过<code>hdfs dfsadmin -setBalancerBandwidth 10485760</code>来动态的设置。</p>

<p>每次迭代会输出开始时间，迭代的次数，上一次迭代移动的数据量，集群达到平衡还需要移动的数据量，该次迭代将移动的数据量。一般情况下，“Bytes Already Moved”将会增加同时“Bytes Left To Move”将会减少（但是如果此时有大数据量写入，那么Bytes Left To Move可能不减反增）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span></code></pre></td></tr></table></div></figure>


<p>多个balancer程序不能同时运行。</p>

<p>balancer程序会自动退出当存在以下情况：</p>

<ul>
<li>集群已经平衡</li>
<li>没有块将会被移动</li>
<li>连续5次的处理没有块移动</li>
<li>连接namenode时出现IOException</li>
<li>另一个balancer程序在跑</li>
</ul>


<p>根据上面的5中情况，balancer程序退出，同时会打印如下的信息：</p>

<ul>
<li>The cluster is balanced. Exiting</li>
<li>No block can be moved. Exiting&hellip;</li>
<li>No block has been moved for 5 iterations. Exiting&hellip;</li>
<li>Received an IO exception: failure reason. Exiting&hellip;</li>
<li>Another balancer is running. Exiting&hellip;</li>
</ul>


<p>当balancer运行时，管理员可以随时运行stop-balancer.sh来中断balancer程序。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/02/hadoop-datanode-config-should-equals/">Hadoop的datanode数据节点软/硬件配置应该一致</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-02T22:21:12+08:00" pubdate data-updated="true">Sat 2014-08-02 22:21</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>最好的就是集群的所有的datanode的节点的<strong>硬件配置一样</strong>！当然系统时间也的一致，hosts等等这些。机器配置一样时可以使用脚本进行批量处理，给维护带来很大的便利性。</p>

<p>今天收到运维的信息，说集群的一台机器硬盘爆了！上到环境查看<code>df -h</code>发现硬盘配置和其他datanode的不同！但是hadoop hdfs-site.xml的<code>dfs.datanode.data.dir</code>却是一样的！</p>

<p>经验： dir的配置应该是一个系统设备对应一个路径，而不是一个系统目录对应dir的一个路径！</p>

<h2>问题现象以及根源</h2>

<p>问题机器A的磁盘情况：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver8 ~]$ df -h
</span><span class='line'>文件系统              容量  已用  可用 已用%% 挂载点
</span><span class='line'>/dev/sda3             2.7T  2.5T   53G  98% /
</span><span class='line'>tmpfs                  32G  260K   32G   1% /dev/shm
</span><span class='line'>/dev/sda1              97M   32M   61M  35% /boot
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 /]$ ll
</span><span class='line'>总用量 170
</span><span class='line'>dr-xr-xr-x.   2 root   root    4096 2月  12 19:39 bin
</span><span class='line'>dr-xr-xr-x.   5 root   root    1024 2月  13 02:40 boot
</span><span class='line'>drwxr-xr-x.   2 root   root    4096 2月  23 2012 cgroup
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data1
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data10
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data11
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data12
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data13
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data14
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data15
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data2
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data3
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data4
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data5
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data6
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data7
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data8
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data9</span></code></pre></td></tr></table></div></figure>


<p>再看集群其他机器：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver1 ~]$ df -h
</span><span class='line'>文件系统              容量  已用  可用 已用%% 挂载点
</span><span class='line'>/dev/sda3             2.7T   32G  2.5T   2% /
</span><span class='line'>tmpfs                  32G   88K   32G   1% /dev/shm
</span><span class='line'>/dev/sda1              97M   32M   61M  35% /boot
</span><span class='line'>/dev/sdb1             1.8T  495G  1.3T  29% /data1
</span><span class='line'>/dev/sdb2             1.8T  485G  1.3T  28% /data2
</span><span class='line'>/dev/sdb3             1.8T  492G  1.3T  29% /data3
</span><span class='line'>/dev/sdb4             1.8T  488G  1.3T  28% /data4
</span><span class='line'>/dev/sdb5             1.8T  486G  1.3T  28% /data5
</span><span class='line'>/dev/sdb6             1.8T  480G  1.3T  28% /data6
</span><span class='line'>/dev/sdb7             1.8T  479G  1.3T  28% /data7
</span><span class='line'>/dev/sdb8             1.8T  474G  1.3T  28% /data8
</span><span class='line'>/dev/sdb9             1.8T  480G  1.3T  28% /data9
</span><span class='line'>/dev/sdb10            1.8T  478G  1.3T  28% /data10
</span><span class='line'>/dev/sdb11            1.8T  475G  1.3T  28% /data11
</span><span class='line'>/dev/sdb12            1.8T  489G  1.3T  29% /data12
</span><span class='line'>/dev/sdb13            1.8T  475G  1.3T  28% /data13
</span><span class='line'>/dev/sdb14            1.8T  476G  1.3T  28% /data14
</span><span class='line'>/dev/sdb15            1.8T  469G  1.3T  27% /data15</span></code></pre></td></tr></table></div></figure>


<p>出问题机器没有挂存储，仅仅是建立了对应的目录结构，并不是把目录挂载到单独的存储设备上。</p>

<p>同时查看50070的前面的信息，hadoop把每个逗号分隔后的路径默认都做一个磁盘设备来计算！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Node               Address             ..Admin State CC    Used  NU    RU(%) R(%)      Blocks Block  Pool Used Block Pool Used (%)
</span><span class='line'>hadoop-slaver1    192.168.32.21:50010 2   In Service  26.86   7.05    1.37    18.44   26.25       68.66   264844  7.05    26.25   
</span><span class='line'>hadoop-slaver8    192.168.32.28:50010 1   In Service  37.94   2.46    34.71   0.77    6.48        2.03    29637   2.46    6.48    </span></code></pre></td></tr></table></div></figure>


<p>配置容量是所有配置的路径所在盘容量的<strong>累加</strong>。总的剩余空间（余量）也是各个dir配置路径的剩余空间<strong>累加</strong>的！这样很容易出现问题！
最好的就是集群的所有的datanode的节点的<strong>硬件配置一样</strong>！当然系统时间也的一致，hosts等等这些。</p>

<h2>问题处理</h2>

<p>首先得把问题解决啊：</p>

<ul>
<li>把<code>dfs.datanode.data.dir</code>路径个数调整为磁盘个数！</li>
<li>修改该datanode的hdfs-site的配置，添加<code>dfs.datanode.du.reserved</code>，留给系统的空间设置为400多G。</li>
<li>冗余份数也没有必要3份，浪费空间。如果两台机器同时出现问题，还是同一份数据，那只能说是天意！你可以去趟澳门赌一圈了！</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
</span><span class='line'>&lt;value&gt;/data1/hadoop/data&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;
</span><span class='line'>&lt;value&gt;437438953472&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.replication&lt;/name&gt;
</span><span class='line'>&lt;value&gt;2&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>设置了reserved保留空间后，再看LIVE页面slaver8的容量变少了且正好等于(盘的容量2.7T-430G~=2.26T 计算容量的hdfs源码在<code>FsVolumeImpl.getCapacity()</code>)。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop-slaver8   192.168.32.28:50010 1   In Service  2.26    2.23    0.00    0.03    98.66</span></code></pre></td></tr></table></div></figure>


<p>datanode和blockpool的平衡处理，可以参考<a href="http://hadoop-master1:50070/dfsnodelist.jsp?whatNodes=LIVE">Live Datanodes</a>的容量和进行！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 ~]$ hdfs balancer -help
</span><span class='line'>Usage: java Balancer
</span><span class='line'>        [-policy &lt;policy&gt;]      the balancing policy: datanode or blockpool
</span><span class='line'>        [-threshold &lt;threshold&gt;]        Percentage of disk capacity
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 ~]$ hadoop-2.2.0/bin/hdfs getconf -confkey dfs.datanode.du.reserved
</span><span class='line'>137438953472</span></code></pre></td></tr></table></div></figure>


<p>删除一些没用的备份数据。配置好以后，重启当前slaver8节点，并进行数据平衡（如果觉得麻烦，直接丢掉原来的一个目录下的数据也行，可能更快！均衡器运行的太慢！！）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver8 ~]$  ~/hadoop-2.2.0/sbin/hadoop-daemon.sh stop datanode
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 ~]$  for i in 6 7 8 9 10 11 12 13 14 15; do  cd /data$i/hadoop/data/current/BP-1414312971-192.168.32.11-1392479369615/current/finalized;  find . -type f -exec mv {} /data1/hadoop/data/current/BP-1414312971-192.168.32.11-1392479369615/current/finalized/{} \;; done
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 ~]$  ~/hadoop-2.2.0/sbin/hadoop-daemon.sh start datanode
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ hdfs dfsadmin -setBalancerBandwidth 10485760
</span><span class='line'>[hadoop@hadoop-master1 ~]$ hdfs balancer -threshold 60
</span></code></pre></td></tr></table></div></figure>


<p>查看datanode的日志，由于移动数据，有些blk的id一样，会清理一些数据。对于均衡器程序的阀值越小集群越平衡！默认是10（%），会移动很多的数据（准备看下均衡器的源码，了解各个参数以及运行的逻辑）！</p>

<h2>参考</h2>

<ul>
<li><a href="http://blog.csdn.net/lingzihan1215/article/details/8700532">hadoop的datanode多磁盘空间处理</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-30T00:25:39+08:00" pubdate data-updated="true">Wed 2014-07-30 00:25</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>网上查了很多资料说的很复杂，要叉叉叉叉叉！其实hadoop2已经集成了hadoop-snappy，只要安装snappy即可。但是也没有一些文章说的只要编译snappy然后放到lib/native路径下即可，还需要重新编译libhadoop的library包。</p>

<p>查找hadoop-snappy的源码的时刻，在C代码里面定义了<code>HADOOP_SNAPPY_LIBRARY</code>，然后理着这个思路去查找，发现在CMakeFile文件中也定义了对应的变量，然后再查找pom.xml的native profile中定义了snappy.prefix的属性。最后就有了下面的步骤。</p>

<p>1) build snappy</p>

<p>编译Snappy，并把lib拷贝/同步到hadoop的native目录下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxf snappy-1.1.1.tar.gz 
</span><span class='line'>cd snappy-1.1.1
</span><span class='line'>./configure --prefix=/home/hadoop/snappy
</span><span class='line'>make
</span><span class='line'>make install
</span><span class='line'>
</span><span class='line'>cd snappy
</span><span class='line'>cd lib/
</span><span class='line'>rysnc -vaz * ~/hadoop-2.2.0/lib/native/</span></code></pre></td></tr></table></div></figure>


<p>2) rebuild hadoop common project</p>

<p>重新编译hadoop的lib，覆盖原来的文件。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop-common]$ mvn package -Dmaven.javadoc.skip=true -DskipTests -Dsnappy.prefix=/home/hadoop/snappy -Drequire.snappy=true -Pnative 
</span><span class='line'>
</span><span class='line'>[hadoop@master1 hadoop-common]$ cd ~/hadoop-2.2.0-src/hadoop-common-project/hadoop-common/
</span><span class='line'>[hadoop@master1 hadoop-common]$ cd target/native/target/usr/local/lib/
</span><span class='line'>[hadoop@master1 lib]$ ll
</span><span class='line'>total 1252
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 820824 Jul 30 00:18 libhadoop.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop     18 Jul 30 00:18 libhadoop.so -&gt; libhadoop.so.1.0.0
</span><span class='line'>-rwxrwxr-x. 1 hadoop hadoop 455542 Jul 30 00:18 libhadoop.so.1.0.0
</span><span class='line'>[hadoop@master1 lib]$ rsync -vaz * ~/hadoop-2.2.0/lib/native/
</span><span class='line'>sending incremental file list
</span><span class='line'>libhadoop.a
</span><span class='line'>libhadoop.so.1.0.0
</span><span class='line'>
</span><span class='line'>sent 409348 bytes  received 53 bytes  818802.00 bytes/sec
</span><span class='line'>total size is 1276384  speedup is 3.12
</span><span class='line'>[hadoop@master1 lib]$ </span></code></pre></td></tr></table></div></figure>


<p>3) check</p>

<p>检查程序snappy是否已经配置成功</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ hadoop checknative -a
</span><span class='line'>14/07/30 00:22:14 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
</span><span class='line'>14/07/30 00:22:14 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
</span><span class='line'>Native library checking:
</span><span class='line'>hadoop: true /home/hadoop/hadoop-2.2.0/lib/native/libhadoop.so.1.0.0
</span><span class='line'>zlib:   true /lib64/libz.so.1
</span><span class='line'>snappy: true /home/hadoop/hadoop-2.2.0/lib/native/libsnappy.so.1
</span><span class='line'>lz4:    true revision:43
</span><span class='line'>bzip2:  false 
</span><span class='line'>14/07/30 00:22:14 INFO util.ExitUtil: Exiting with status 1
</span><span class='line'>[hadoop@master1 ~]$ </span></code></pre></td></tr></table></div></figure>


<p>4) 跑一个压缩程序</p>

<p>先参考网上的，直接用hbase的带的测试类运行（前提：需要在hbase-env.sh中配置HADOOP_HOME，这样hbase才能找到hadoop下的lib本地库）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ hbase-0.98.3-hadoop2/bin/hbase org.apache.hadoop.hbase.util.CompressionTest file:///tmp/abc.snappy snappy
</span><span class='line'>2014-07-30 08:50:42,617 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
</span><span class='line'>SLF4J: Class path contains multiple SLF4J bindings.
</span><span class='line'>SLF4J: Found binding in [jar:file:/home/hadoop/hbase-0.98.3-hadoop2/lib/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
</span><span class='line'>SLF4J: Found binding in [jar:file:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
</span><span class='line'>SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
</span><span class='line'>2014-07-30 08:50:44,515 INFO  [main] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
</span><span class='line'>2014-07-30 08:50:44,522 INFO  [main] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
</span><span class='line'>2014-07-30 08:50:45,388 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
</span><span class='line'>2014-07-30 08:50:45,408 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
</span><span class='line'>2014-07-30 08:50:45,430 ERROR [main] hbase.KeyValue: Unexpected getShortMidpointKey result, fakeKey:testkey, firstKeyInBlock:testkey
</span><span class='line'>2014-07-30 08:50:47,088 INFO  [main] compress.CodecPool: Got brand-new decompressor [.snappy]
</span><span class='line'>SUCCESS
</span><span class='line'>[hadoop@master1 ~]$ </span></code></pre></td></tr></table></div></figure>


<p>看到最后的<strong>SUCCESS</strong>就说明安装配置成功了！</p>

<p>接下来自己写程序测试压缩/解压缩，首先编写java类：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import java.io.FileInputStream;
</span><span class='line'>import java.io.FileNotFoundException;
</span><span class='line'>import java.io.FileOutputStream;
</span><span class='line'>import java.io.IOException;
</span><span class='line'>
</span><span class='line'>import org.apache.commons.lang.StringUtils;
</span><span class='line'>import org.apache.hadoop.conf.Configuration;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionCodec;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionInputStream;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionOutputStream;
</span><span class='line'>import org.apache.hadoop.io.compress.SnappyCodec;
</span><span class='line'>import org.apache.hadoop.util.ReflectionUtils;
</span><span class='line'>import org.apache.zookeeper.common.IOUtils;
</span><span class='line'>
</span><span class='line'>public class SnappyCompressTest {
</span><span class='line'>
</span><span class='line'>        public static void main(String[] args) throws FileNotFoundException, IOException {
</span><span class='line'>                try {
</span><span class='line'>                        execute(args);
</span><span class='line'>                } catch (Exception e) {
</span><span class='line'>                        System.out.println("Usage: $0 read|write file[.snappy]");
</span><span class='line'>                }
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>        private static void execute(String[] args) throws FileNotFoundException, IOException {
</span><span class='line'>                String op = args[0];
</span><span class='line'>                String file = args[1];
</span><span class='line'>                String snappyFile = file + ".snappy";
</span><span class='line'>
</span><span class='line'>                Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
</span><span class='line'>                CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());
</span><span class='line'>
</span><span class='line'>                if (StringUtils.equalsIgnoreCase(op, "read")) {
</span><span class='line'>                        FileInputStream fin = new FileInputStream(snappyFile);
</span><span class='line'>                        CompressionInputStream in = codec.createInputStream(fin);
</span><span class='line'>                        FileOutputStream fout = new FileOutputStream(file);
</span><span class='line'>                        IOUtils.copyBytes(in, fout, 4096, true);
</span><span class='line'>                } else {
</span><span class='line'>                        FileInputStream fin = new FileInputStream(file);
</span><span class='line'>                        CompressionOutputStream out = codec.createOutputStream(new FileOutputStream(snappyFile));
</span><span class='line'>                        IOUtils.copyBytes(fin, out, 4096, true);
</span><span class='line'>                }
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>编译运行，测试读写功能。使用hadoop命令可以简化很多工作，把当前路径加入到<code>HADOOP_CLASSPATH</code>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 test]$ javac -cp `hadoop classpath` SnappyCompressTest.java 
</span><span class='line'>[hadoop@master1 test]$ export HADOOP_CLASSPATH=$PWD
</span><span class='line'>[hadoop@master1 test]$ hadoop SnappyCompressTest 
</span><span class='line'>Usage: $0 read|write file[.snappy]
</span><span class='line'>[hadoop@master1 test]$ hadoop SnappyCompressTest write test.txt 
</span><span class='line'>[hadoop@master1 test]$ ll
</span><span class='line'>total 16
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:23 test.txt
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
</span><span class='line'>[hadoop@master1 test]$ rm test.txt
</span><span class='line'>[hadoop@master1 test]$ hadoop SnappyCompressTest read test.txt 
</span><span class='line'>[hadoop@master1 test]$ ll
</span><span class='line'>total 16
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:28 test.txt
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
</span><span class='line'>[hadoop@master1 test]$ cat test.txt
</span><span class='line'>abc
</span><span class='line'>abc
</span><span class='line'>abc</span></code></pre></td></tr></table></div></figure>


<p>5) hbase中添加压缩</p>

<p>把所有library，以及hbase的配置同步其他所有从节点。对hbase的表使用Snappy压缩。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hbase(main):001:0&gt; create 'st1', 'f1'
</span><span class='line'>hbase(main):005:0&gt; alter 'st1', {NAME=&gt;'f1', COMPRESSION=&gt;'snappy'}
</span><span class='line'>Updating all regions with the new schema...
</span><span class='line'>0/1 regions updated.
</span><span class='line'>1/1 regions updated.
</span><span class='line'>Done.
</span><span class='line'>0 row(s) in 2.7880 seconds
</span><span class='line'>
</span><span class='line'>hbase(main):010:0&gt; create 'sst1','f1'
</span><span class='line'>0 row(s) in 0.5730 seconds
</span><span class='line'>
</span><span class='line'>=&gt; Hbase::Table - sst1
</span><span class='line'>hbase(main):011:0&gt; flush 'sst1'
</span><span class='line'>0 row(s) in 2.5380 seconds
</span><span class='line'>
</span><span class='line'>hbase(main):012:0&gt; flush 'st1'
</span><span class='line'>0 row(s) in 7.5470 seconds</span></code></pre></td></tr></table></div></figure>


<p>对于hbase来说，使用压缩消耗还是挺大的。插入10w数据中间进行compaction时停顿比较久。最后flush写数据的时间也长了很多！
下面是文件写入后的文件大小对比（由于是进行简单的测试，写入的数据重复比较多。具体比例没有参考价值）：</p>

<p><img src="http://file.bmob.cn/M00/05/5A/wKhkA1PYz9CAB-TdAAEWX8LGpUo149.png" alt="" /></p>

<p>6) 正式环境下解压snappy文件</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>import java.io.FileOutputStream;
</span><span class='line'>import java.io.IOException;
</span><span class='line'>import java.io.InputStream;
</span><span class='line'>
</span><span class='line'>import org.apache.hadoop.conf.Configuration;
</span><span class='line'>import org.apache.hadoop.fs.FileSystem;
</span><span class='line'>import org.apache.hadoop.fs.Path;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionCodec;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionInputStream;
</span><span class='line'>import org.apache.hadoop.io.compress.SnappyCodec;
</span><span class='line'>import org.apache.hadoop.util.ReflectionUtils;
</span><span class='line'>import org.apache.zookeeper.common.IOUtils;
</span><span class='line'>
</span><span class='line'>public class DecompressTest {
</span><span class='line'>  public static void main(String[] args) throws IOException {
</span><span class='line'>
</span><span class='line'>      Configuration conf = new Configuration();
</span><span class='line'>      Path path = new Path(args[0]);
</span><span class='line'>      FileSystem fs = path.getFileSystem(conf);
</span><span class='line'>
</span><span class='line'>      Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
</span><span class='line'>      CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());
</span><span class='line'>
</span><span class='line'>      InputStream fin = fs.open(path);
</span><span class='line'>      CompressionInputStream in = codec.createInputStream(fin);
</span><span class='line'>
</span><span class='line'>      IOUtils.copyBytes(in, System.out, 4096, true);
</span><span class='line'>
</span><span class='line'>      fin.close();
</span><span class='line'>
</span><span class='line'>      System.out.println("SUCCESS");
</span><span class='line'>
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>// build & run
</span><span class='line'>
</span><span class='line'>&gt;DecompressTest.java 
</span><span class='line'>vi DecompressTest.java 
</span><span class='line'>javac -cp `hadoop classpath`  DecompressTest.java 
</span><span class='line'>export HADOOP_CLASSPATH=.
</span><span class='line'># snappyfile on hdfs
</span><span class='line'>hadoop DecompressTest /user/hive/t_ods_access_log2/month=201408/day=20140828/hour=2014082808/t_ods_access_log2-2014082808.our.snappy.1409187524328
</span></code></pre></td></tr></table></div></figure>



</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/5">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/3">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2014/09/30/docker-ssh-on-centos/">配置ssh登录docker-centos</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/">在windows开发测试mapreduce几种方式</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/10/18/modify-hosts-build-hadoop-cluster-on-docker/">Dnsmasq解决docker集群节点互通问题</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/16/build-and-configuration-spark/">编译配置Spark</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/12/read-spark1-source-starter/">[读码] Spark1.1.0前篇</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/07/thinking/">思考</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/30/docker-ssh-on-centos/">配置ssh登录docker-centos</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/27/docker-start-guide-on-centos/">Docker入门</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/">在windows开发测试mapreduce几种方式</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/12/scala-wordcount-on-hadoop/">Scala Wordcount on Hadoop2</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (25) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (11) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (66)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.2.0</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.98.3</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-0.13.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.4.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'winseliu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
