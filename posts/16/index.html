
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="进程监管 supervisord
s6 supervisor使用ini的方式配置其实挺讨厌的，但有一点好的就是它不需要配置为boot进程（process id 1）。 在线安装 http://supervisord.org/installing.html 下载 https://pypi. &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/16">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
  <li><a href="https://yunpan.cn/cuYhpFBPgQYgT" >Books[5aee]</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2015-08-24T16:24:25+08:00" pubdate data-updated="true">Mon 2015-08-24 16:24</time>
        
      </p>
    
  </header>


  <div class="entry-content"><ul>
<li><a href="https://en.wikipedia.org/wiki/Process_supervision">进程监管</a>

<ul>
<li><a href="http://supervisord.org/">supervisord</a></li>
<li><a href="http://www.skarnet.org/software/s6/">s6</a></li>
</ul>
</li>
</ul>


<p>supervisor使用ini的方式配置其实挺讨厌的，但有一点好的就是它不需要配置为boot进程（process id 1）。</p>

<h2>在线安装</h2>

<ul>
<li><a href="http://supervisord.org/installing.html">http://supervisord.org/installing.html</a></li>
</ul>


<p>下载 <a href="https://pypi.python.org/pypi/supervisor">https://pypi.python.org/pypi/supervisor</a> ，并安装:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># yum的supervisor版本太低(2.1)了，使用tar.gz源码来安装
</span><span class='line'>[root@cu2 supervisor-3.2.3]# yum list supervisor
</span><span class='line'>Loaded plugins: fastestmirror, priorities
</span><span class='line'>Loading mirror speeds from cached hostfile
</span><span class='line'> * epel: ftp.cuhk.edu.hk
</span><span class='line'>Available Packages
</span><span class='line'>supervisor.noarch                                                                              2.1-9.el6                                                                               epel
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 不支持python3，安装之前先保证安装有python2.4+
</span><span class='line'>[root@cu2 ~]# python --version
</span><span class='line'>Python 2.6.6
</span><span class='line'>[root@cu2 ~]# cd supervisor-3.2.3/
</span><span class='line'>[root@cu2 supervisor-3.2.3]# python setup.py install
</span><span class='line'>...
</span><span class='line'>creating dist
</span><span class='line'>creating 'dist/supervisor-3.2.3-py2.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it
</span><span class='line'>removing 'build/bdist.linux-x86_64/egg' (and everything under it)
</span><span class='line'>Processing supervisor-3.2.3-py2.6.egg
</span><span class='line'>creating /usr/lib/python2.6/site-packages/supervisor-3.2.3-py2.6.egg
</span><span class='line'>Extracting supervisor-3.2.3-py2.6.egg to /usr/lib/python2.6/site-packages
</span><span class='line'>Adding supervisor 3.2.3 to easy-install.pth file
</span><span class='line'>Installing echo_supervisord_conf script to /usr/bin
</span><span class='line'>Installing pidproxy script to /usr/bin
</span><span class='line'>Installing supervisorctl script to /usr/bin
</span><span class='line'>Installing supervisord script to /usr/bin
</span><span class='line'>
</span><span class='line'>Installed /usr/lib/python2.6/site-packages/supervisor-3.2.3-py2.6.egg
</span><span class='line'>Processing dependencies for supervisor==3.2.3
</span><span class='line'>Searching for meld3&gt;=0.6.5
</span><span class='line'>Reading http://pypi.python.org/simple/meld3/
</span><span class='line'>Best match: meld3 1.0.2
</span><span class='line'>Downloading https://pypi.python.org/packages/45/a0/317c6422b26c12fe0161e936fc35f36552069ba8e6f7ecbd99bbffe32a5f/meld3-1.0.2.tar.gz#md5=3ccc78cd79cffd63a751ad7684c02c91
</span><span class='line'>Processing meld3-1.0.2.tar.gz
</span><span class='line'>Running meld3-1.0.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-nMSEmq/meld3-1.0.2/egg-dist-tmp-vkwrOp
</span><span class='line'>zip_safe flag not set; analyzing archive contents...
</span><span class='line'>Adding meld3 1.0.2 to easy-install.pth file
</span><span class='line'>
</span><span class='line'>Installed /usr/lib/python2.6/site-packages/meld3-1.0.2-py2.6.egg
</span><span class='line'>Finished processing dependencies for supervisor==3.2.3
</span></code></pre></td></tr></table></div></figure>


<h2>(离线安装)下载依赖以及安装包，并安装</h2>

<p>安装官网的版本下载: <a href="http://supervisord.org/installing.html#installing-to-a-system-without-internet-access">Installing To A System Without Internet Access</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>python -V
</span><span class='line'>
</span><span class='line'>tar zxvf setuptools-18.2.tar.gz 
</span><span class='line'>cd setuptools-18.2
</span><span class='line'>python setup.py install
</span><span class='line'>
</span><span class='line'>tar zxvf meld3-0.6.5.tar.gz 
</span><span class='line'>cd meld3-0.6.5
</span><span class='line'>python setup.py install
</span><span class='line'>
</span><span class='line'>tar zxvf elementtree-1.2.6-20050316.tar.gz 
</span><span class='line'>cd elementtree-1.2.6-20050316
</span><span class='line'>python setup.py install
</span><span class='line'>
</span><span class='line'>tar zxvf supervisor-3.1.3.tar.gz 
</span><span class='line'>cd supervisor-3.1.3
</span><span class='line'>python setup.py  install
</span></code></pre></td></tr></table></div></figure>


<h2>启动</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># http://supervisord.org/installing.html#creating-a-configuration-file
</span><span class='line'>[root@cu2 supervisor-3.2.3]# echo_supervisord_conf &gt;/etc/supervisord.conf
</span><span class='line'>[root@cu2 supervisor-3.2.3]# supervisord
</span><span class='line'>/usr/lib/python2.6/site-packages/supervisor-3.2.3-py2.6.egg/supervisor/options.py:296: UserWarning: Supervisord is running as root and it is searching for its configuration file in default locations (including its current working directory); you probably want to specify a "-c" argument specifying an absolute path to a configuration file for improved security.
</span><span class='line'>  'Supervisord is running as root and it is searching '
</span><span class='line'>
</span><span class='line'>[root@cu2 supervisor-3.2.3]# ps aux | grep supervisor | grep -v grep
</span><span class='line'>root     63599  0.0  0.1 196600 10404 ?        Ss   16:57   0:00 /usr/bin/python /usr/bin/supervisord
</span><span class='line'>
</span><span class='line'>[root@cu2 supervisor-3.2.3]# supervisorctl 
</span><span class='line'>supervisor&gt; help
</span><span class='line'>
</span><span class='line'>default commands (type help &lt;topic&gt;):
</span><span class='line'>=====================================
</span><span class='line'>add    exit      open  reload  restart   start   tail   
</span><span class='line'>avail  fg        pid   remove  shutdown  status  update 
</span><span class='line'>clear  maintail  quit  reread  signal    stop    version
</span><span class='line'>
</span><span class='line'>supervisor&gt; pid
</span><span class='line'>63599
</span><span class='line'>supervisor&gt; shutdown
</span><span class='line'>Really shut the remote supervisord process down y/N? y
</span><span class='line'>Shut down
</span><span class='line'>supervisor&gt; exit
</span></code></pre></td></tr></table></div></figure>


<p>ctl命令查看状态：</p>

<ul>
<li>reread: Reload the daemon&rsquo;s configuration files</li>
<li><strong>update</strong>: Reload config and add/remove as necessary</li>
<li>reload: Restart the remote supervisord.</li>
<li>pid: supervisord的进程号</li>
<li>status： Get all process status info</li>
<li>avail： Display all configured processes</li>
</ul>


<p>网页管理：</p>

<p><img src="/images/blogs/supervisord-web.png" alt="" /></p>

<h2>配置</h2>

<p><a href="http://supervisord.org/configuration.html">http://supervisord.org/configuration.html</a></p>

<p>默认server(supervisord)-client(supervisorctl)通过 <code>unix domain socket</code> 文件的方式来通信，为了方便网页查看<strong>同时</strong>开启web配置 <code>inet_http_server</code>。</p>

<p>program区域的配置要细读，主要配置工作都在这个上面。被管理的程序 <strong>不能后台运行</strong> (例如：java程序不要加 <code>nohup</code> 以及 <code>&amp;</code> )！！</p>

<blockquote><p>Controlled programs should themselves not be daemons, as supervisord assumes it is responsible for daemonizing its subprocesses</p></blockquote>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 supervisor-3.2.3]# vi /etc/supervisord.conf 
</span><span class='line'>...
</span><span class='line'>[inet_http_server]         ; inet (TCP) server disabled by default
</span><span class='line'>port=0.0.0.0:9001        ; (ip_address:port specifier, *:port for all iface)
</span><span class='line'>...
</span><span class='line'>[include]
</span><span class='line'>files = /etc/supervisord.d/*.ini
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# cat /etc/supervisord.d/redis.ini
</span><span class='line'>[program:redis]
</span><span class='line'>command=/home/hadoop/redis/bin/redis-server /home/hadoop/redis/redis.conf --port 1637%(process_num)01d
</span><span class='line'>process_name=%(program_name)s_1637%(process_num)01d
</span><span class='line'>numprocs=4
</span><span class='line'>numprocs_start=0
</span><span class='line'>priority=1
</span><span class='line'>autostart=true
</span><span class='line'>startsecs=0
</span><span class='line'>startretries=3
</span><span class='line'>autorestart=true
</span><span class='line'>directory=/home/hadoop/redis
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# supervisorctl shutdown
</span><span class='line'>Shut down
</span><span class='line'>[root@cu2 ~]# 
</span><span class='line'>[root@cu2 ~]# supervisord
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# ps aux | grep redis
</span><span class='line'>root     50458  0.1  0.0 137444  2384 ?        Sl   18:14   0:00 /home/hadoop/redis/bin/redis-server *:16372                                   
</span><span class='line'>root     50460  0.0  0.0 137444  2380 ?        Sl   18:14   0:00 /home/hadoop/redis/bin/redis-server *:16373                                   
</span><span class='line'>root     50461  0.0  0.0 137444  2384 ?        Sl   18:14   0:00 /home/hadoop/redis/bin/redis-server *:16370                                   
</span><span class='line'>root     50462  0.1  0.0 137444  2388 ?        Sl   18:14   0:00 /home/hadoop/redis/bin/redis-server *:16371                                   
</span><span class='line'>root     51853  0.0  0.0 103248   888 pts/2    S+   18:14   0:00 grep redis
</span><span class='line'>
</span><span class='line'># 测试：随便kill掉一个，再ps查看，进程没少，但是刚刚kill掉的redis-server进程号变了
</span><span class='line'>
</span><span class='line'># 可以通过supervisord.log查看启动的日志。默认在/tmp下面
</span><span class='line'>[root@cu2 tmp]# less supervisord.log 
</span><span class='line'>
</span><span class='line'># 如果在 配置program块 没有指定stdout和stderr的位置，可以在/tmp下找到对应的日志文件：
</span><span class='line'>[root@cu2 tmp]# ll | grep redis
</span><span class='line'>-rw------- 1 root   root      0 May  2 18:14 redis_16370-stderr---supervisor-pZ5pzl.log
</span><span class='line'>-rw------- 1 root   root   2649 May  2 18:17 redis_16370-stdout---supervisor-GyrHYw.log
</span><span class='line'>-rw------- 1 root   root      0 May  2 18:14 redis_16371-stderr---supervisor-XfyvMv.log
</span><span class='line'>-rw------- 1 root   root   5298 May  2 18:20 redis_16371-stdout---supervisor-NOmTQJ.log
</span><span class='line'>-rw------- 1 root   root      0 May  2 18:14 redis_16372-stderr---supervisor-SHMEi3.log
</span><span class='line'>-rw------- 1 root   root   4967 May  2 18:19 redis_16372-stdout---supervisor-fvKDa8.log
</span><span class='line'>-rw------- 1 root   root      0 May  2 18:14 redis_16373-stderr---supervisor-ncUWuE.log
</span><span class='line'>-rw------- 1 root   root   2326 May  2 18:14 redis_16373-stdout---supervisor-ZxiqIj.log
</span></code></pre></td></tr></table></div></figure>


<h2>supervisor服务</h2>

<ul>
<li><a href="https://github.com/Supervisor/initscripts/blob/master/redhat-init-equeffelec">https://github.com/Supervisor/initscripts/blob/master/redhat-init-equeffelec</a></li>
<li><a href="https://github.com/Supervisor/initscripts/blob/master/redhat-sysconfig-equeffelec">https://github.com/Supervisor/initscripts/blob/master/redhat-sysconfig-equeffelec</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 调整下启动脚本 start方法 中 --pidfile 的位置
</span><span class='line'>[root@cu2 ~]# ll /etc/init.d/supervisord 
</span><span class='line'>-rwxr-xr-x 1 root root 2977 May  2 19:29 /etc/init.d/supervisord
</span><span class='line'>[root@cu2 ~]# grep daemon /etc/init.d/supervisord 
</span><span class='line'>    daemon $supervisord --pidfile=${pidfile} $OPTIONS
</span><span class='line'>[root@cu2 ~]# ll /etc/sysconfig/supervisord 
</span><span class='line'>-rw-r--r-- 1 root root 723 May  2 19:17 /etc/sysconfig/supervisord
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# service supervisord status
</span><span class='line'>supervisord (pid  39549) is running...
</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://supervisord.org/installing.html#installing-to-a-system-without-internet-access">http://supervisord.org/installing.html#installing-to-a-system-without-internet-access</a></li>
<li><a href="http://supervisord.org/configuration.html#programx-section">http://supervisord.org/configuration.html#programx-section</a></li>
<li>工具使用的文章

<ul>
<li>supervisor

<ul>
<li><a href="http://supervisord.org/installing.html">http://supervisord.org/installing.html</a></li>
<li><a href="http://blog.csdn.net/heyjackie/article/details/12995187">http://blog.csdn.net/heyjackie/article/details/12995187</a></li>
</ul>
</li>
<li>s6 <a href="https://blog.tutum.co/2014/12/02/docker-and-s6-my-new-favorite-process-supervisor/">https://blog.tutum.co/2014/12/02/docker-and-s6-my-new-favorite-process-supervisor/</a></li>
<li>daemontools版本太老了，用s6吧

<ul>
<li><a href="https://isotope11.com/blog/manage-your-services-with-daemontools">https://isotope11.com/blog/manage-your-services-with-daemontools</a></li>
<li><a href="http://wrox.cn/article/100007143/">http://wrox.cn/article/100007143/</a></li>
</ul>
</li>
<li>runit <a href="http://jtimberman.housepub.org/blog/2012/12/29/process-supervision-solved-problem/">http://jtimberman.housepub.org/blog/2012/12/29/process-supervision-solved-problem/</a></li>
</ul>
</li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/08/21/logstash-elasticsearch-kibana-startguide/">Logstash Elasticsearch Kibana日志采集查询系统搭建</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-08-21T14:42:30+08:00" pubdate data-updated="true">Fri 2015-08-21 14:42</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>软件版本</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@master opt]# ll
</span><span class='line'>total 20
</span><span class='line'>drwxr-xr-x 7 root root 4096 Aug 21 01:23 elasticsearch-1.7.1
</span><span class='line'>drwxr-xr-x 8 uucp  143 4096 Mar 18  2014 jdk1.8.0_05
</span><span class='line'>drwxrwxr-x 7 1000 1000 4096 Aug 21 01:09 kibana-4.1.1-linux-x64
</span><span class='line'>drwxr-xr-x 5 root root 4096 Aug 21 05:58 logstash-1.5.3
</span><span class='line'>drwxrwxr-x 6 root root 4096 Aug 21 06:44 redis-3.0.3</span></code></pre></td></tr></table></div></figure>


<h2>安装运行脚本</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># java
</span><span class='line'>vi /etc/profile
</span><span class='line'>source /etc/profile
</span><span class='line'>
</span><span class='line'>cd /opt/elasticsearch-1.7.1
</span><span class='line'>bin/elasticsearch -p elasticsearch.pid -d
</span><span class='line'>
</span><span class='line'>curl localhost:9200/_cluster/nodes/172.17.0.4
</span><span class='line'>
</span><span class='line'>cd /opt/kibana-4.1.1-linux-x64/
</span><span class='line'>bin/kibana 
</span><span class='line'># http://master:5601
</span><span class='line'>
</span><span class='line'>cd /opt/redis-3.0.3
</span><span class='line'>yum install gcc
</span><span class='line'>yum install bzip2
</span><span class='line'>make MALLOC=jemalloc
</span><span class='line'>
</span><span class='line'># 也可以修改配置的daemon属性
</span><span class='line'>nohup src/redis-server & 
</span><span class='line'>
</span><span class='line'>cd /opt/logstash-1.5.3/
</span><span class='line'>bin/logstash -e 'input { stdin { } } output { stdout {} }'
</span><span class='line'>
</span><span class='line'>vi index.conf
</span><span class='line'>vi agent.conf
</span><span class='line'>
</span><span class='line'># agent可不加
</span><span class='line'>bin/logstash agent -f agent.conf &
</span><span class='line'>bin/logstash agent -f index.conf &</span></code></pre></td></tr></table></div></figure>


<h2>logstash配置</h2>

<p>由于程序都运行在一台机器(localhost)，redis、elasticsearch和kibana都使用默认配置。下面贴的是logstash的采集和过滤的配置：</p>

<p>(kibaba的配置config/kibana.yml, elasticsearch的配置config/elasticsearch.yml)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@master logstash-1.5.3]# cat agent.conf 
</span><span class='line'>input {
</span><span class='line'>  file {
</span><span class='line'>    path =&gt; "/var/log/yum.log"
</span><span class='line'>    start_position =&gt; beginning
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  redis {
</span><span class='line'>    key =&gt; "logstash.redis"
</span><span class='line'>    data_type =&gt; list
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  # 便于查看调试
</span><span class='line'>  stdout { }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@master logstash-1.5.3]# cat index.conf 
</span><span class='line'>input {
</span><span class='line'>  redis {
</span><span class='line'>    data_type =&gt; list
</span><span class='line'>    key =&gt; "logstash.redis"
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  elasticsearch {
</span><span class='line'>    host =&gt; "localhost"
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>注意要改动下被采集的原始文件！！然后启动相应的程序，打开浏览器<a href="http://master:5601">http://master:5601</a>配置一下索引项，就可以查看了。</p>

<p>至于input/output/filter(map,reduce)怎么配置，查看官方文档<a href="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html">filter-plugins</a></p>

<h2>filter</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu1 logstash-1.5.3]$ bin/logstash -e "
</span><span class='line'>input {
</span><span class='line'>stdin {}
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>filter {
</span><span class='line'>grok { 
</span><span class='line'>match =&gt; {\"message\" =&gt; \"%{WORD:content}\"}
</span><span class='line'>add_field =&gt; { \"foo_%{content}\" =&gt; \"helloworld\" }
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>stdout { codec =&gt; json }
</span><span class='line'>}
</span><span class='line'>"
</span><span class='line'>
</span><span class='line'>abc
</span><span class='line'>{"message":"abc","@version":"1","@timestamp":"2015-09-10T08:02:52.024Z","host":"cu1","content":"abc","foo_abc":"helloworld"}</span></code></pre></td></tr></table></div></figure>


<p>grok-pattern文件的位置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 logstash-1.5.3]$ less ./vendor/bundle/jruby/1.9/gems/logstash-patterns-core-0.1.10/patterns/grok-patterns 
</span><span class='line'>
</span><span class='line'>2015-09-06 15:23:53,027 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
</span><span class='line'>%{TIMESTAMP_ISO8601:time} %{LOGLEVEL:loglevel} %{GREEDYDATA:content}
</span><span class='line'>
</span><span class='line'>[2015-09-10 08:00:46,539][INFO ][cluster.metadata         ] [Jumbo Carnation] [logstash-2015.09.10] update_mapping [hbase-logs] (dynamic)
</span><span class='line'>\[%{TIMESTAMP_ISO8601:time}\]\[%{LOGLEVEL:loglevel}%{SPACE}\]%{GREEDYDATA:content}</span></code></pre></td></tr></table></div></figure>


<h2>学习</h2>

<p>过滤DEBUG/INFO日志</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu1 logstash-1.5.3]$ bin/logstash -e "
</span><span class='line'> input {
</span><span class='line'> stdin {}
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> filter {
</span><span class='line'> grok {
</span><span class='line'> match =&gt; { \"message\" =&gt; \"%{TIMESTAMP_ISO8601:time} %{LOGLEVEL:loglevel} %{GREEDYDATA:content}\" }
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> if [loglevel] == \"INFO\" { drop {} }
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> output {
</span><span class='line'> stdout {}
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> "</span></code></pre></td></tr></table></div></figure>


<p>用shell先预处理</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>    stdin {
</span><span class='line'>        type =&gt; "nginx"
</span><span class='line'>        format =&gt; "json_event"
</span><span class='line'>    }
</span><span class='line'>} 
</span><span class='line'>output {
</span><span class='line'>    amqp {
</span><span class='line'>        type =&gt; "nginx"
</span><span class='line'>        host =&gt; "10.10.10.10"
</span><span class='line'>        key  =&gt; "cdn"
</span><span class='line'>        name =&gt; "logstash"
</span><span class='line'>        exchange_type =&gt; "direct"
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>#!/bin/sh
</span><span class='line'>      tail -F /data/nginx/logs/access.json \
</span><span class='line'>    | sed 's/upstreamtime":-/upstreamtime":0/' \
</span><span class='line'>    | /usr/local/logstash/bin/logstash -f /usr/local/logstash/etc/agent.conf &</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.cnblogs.com/buzzlight/p/logstash_elasticsearch_kibana_log.html">http://www.cnblogs.com/buzzlight/p/logstash_elasticsearch_kibana_log.html</a></li>
<li><a href="http://www.cnblogs.com/ibook360/archive/2013/03/15/2961428.html">http://www.cnblogs.com/ibook360/archive/2013/03/15/2961428.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/index.html">https://www.elastic.co/guide/en/logstash/current/index.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/first-event.html">https://www.elastic.co/guide/en/logstash/current/first-event.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/working-with-plugins.html">https://www.elastic.co/guide/en/logstash/current/working-with-plugins.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html">https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html</a></li>
<li><p><a href="https://www.elastic.co/guide/en/logstash/current/codec-plugins.html">https://www.elastic.co/guide/en/logstash/current/codec-plugins.html</a></p></li>
<li><p><a href="http://blog.csdn.net/yeasy/article/details/45332493">http://blog.csdn.net/yeasy/article/details/45332493</a></p></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs/">Hadoop不同版本yarn和hdfs混搭，spark-yarn环境配置</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-06-10T18:48:19+08:00" pubdate data-updated="true">Wed 2015-06-10 18:48</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>hadoop分为存储和计算两个主要的功能，hdfs步入hadoop2后不论稳定性还是HA等等功能都比hadoop1要更吸引人。hadoop-2.2.0的hdfs已经比较稳定，但是yarn高版本有更加丰富的功能。本文主要关注spark-yarn下日志的查看，以及spark-yarn-dynamic的配置。</p>

<p>hadoop-2.2.0的hdfs原本已经在使用的环境，在这基础上搭建运行yarn-2.6.0，以及spark-1.3.0-bin-2.2.0。</p>

<ul>
<li>编译</li>
</ul>


<p>我是在虚拟机里面编译，共享了host主机的maven库。参考【VMware共享目录】，【VMware-Centos6 Build hadoop-2.6】注意<strong>cmake_symlink_library的异常，由于共享的windows目录下不能创建linux的软链接</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxvf ~/hadoop-2.6.0-src.tar.gz 
</span><span class='line'>cd hadoop-2.6.0-src/
</span><span class='line'>mvn package -Pdist,native -DskipTests -Dtar -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'># 由于hadoop-hdfs还是2.2的，这里编译spark需要用2.2版本！
</span><span class='line'># 如果用2.6会遇到[UnsatisfiedLinkError:org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray ](http://blog.csdn.net/zeng_84_long/article/details/44340441)
</span><span class='line'>cd spark-1.3.0
</span><span class='line'>export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=1g -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>mvn clean package -Phadoop-2.2 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</span><span class='line'>
</span><span class='line'>vi make-distribution.sh #注释掉BUILD_COMMAND那一行，不重复执行package！
</span><span class='line'>./make-distribution.sh  --mvn `which mvn` --tgz  --skip-java-test -Phadoop-2.6 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests</span></code></pre></td></tr></table></div></figure>


<ul>
<li><p>配置注意点</p></li>
<li><p>core-site不要全部拷贝原来的，只要一些主要的配置即可。</p></li>
<li>yarn-site的<code>yarn.resourcemanager.webapp.address</code>需要填写具体的地址，不能写<code>0.0.0.0</code>。</li>
<li>yarn-site的<code>yarn.nodemanager.aux-services</code>添加spark_shuffle服务。<a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation</a></li>
<li>把hive-site的文件拷贝/链接到spark的conf目录下。</li>
<li>spark-yarn-dynamic配置: <a href="https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation">https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ cat conf/spark-defaults.conf 
</span><span class='line'># spark.master                     spark://bigdatamgr1:7077,bigdata8:7077
</span><span class='line'># spark.eventLog.enabled           true
</span><span class='line'># spark.eventLog.dir               hdfs://namenode:8021/directory
</span><span class='line'># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
</span><span class='line'># spark.executor.extraJavaOptions       -Xmx16g -Xms16g -Xmn256m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:ParallelGCThreads=10
</span><span class='line'>spark.driver.memory              48g
</span><span class='line'>spark.executor.memory            48g
</span><span class='line'>spark.sql.shuffle.partitions     200
</span><span class='line'>
</span><span class='line'>#spark.scheduler.mode FAIR
</span><span class='line'>spark.serializer  org.apache.spark.serializer.KryoSerializer
</span><span class='line'>spark.driver.maxResultSize 8g
</span><span class='line'>#spark.kryoserializer.buffer.max.mb 2048
</span><span class='line'>
</span><span class='line'>spark.dynamicAllocation.enabled true
</span><span class='line'>spark.dynamicAllocation.minExecutors 4
</span><span class='line'>spark.shuffle.service.enabled true
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 conf]$ cat spark-env.sh 
</span><span class='line'>#!/usr/bin/env bash
</span><span class='line'>
</span><span class='line'>JAVA_HOME=/home/eshore/jdk1.7.0_60
</span><span class='line'>
</span><span class='line'># log4j
</span><span class='line'>
</span><span class='line'>__add_to_classpath() {
</span><span class='line'>
</span><span class='line'>  root=$1
</span><span class='line'>
</span><span class='line'>  if [ -d "$root" ] ; then
</span><span class='line'>    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
</span><span class='line'>      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
</span><span class='line'>      else
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$f
</span><span class='line'>      fi
</span><span class='line'>    done
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'># this add tail of SPARK_CLASSPATH
</span><span class='line'>__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"
</span><span class='line'>
</span><span class='line'>#export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
</span><span class='line'>export HADOOP_CONF_DIR=/home/eshore/hadoop-2.6.0/etc/hadoop
</span><span class='line'>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'># HA
</span><span class='line'>SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark" 
</span><span class='line'>
</span><span class='line'>SPARK_PID_DIR=${SPARK_HOME}/pids
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>同步</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for h in `cat slaves ` ; do rsync -vaz hadoop-2.6.0 $h:~/ --delete --exclude=work --exclude=logs --exclude=metastore_db --exclude=data --exclude=pids ; done</span></code></pre></td></tr></table></div></figure>


<ul>
<li>启动spark-hive-thrift</li>
</ul>


<p>./sbin/start-thriftserver.sh &ndash;executor-memory 29g &ndash;master yarn-client</p>

<p>对于多任务的集群来说，配置自动动态分配（类似资源池）更有利于资源的使用。可以通过【All Applications】-【ApplicationMaster】-【Executors】来观察执行进程的变化。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/04/18/tachyon-deep-source/">Tachyon剖析</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-04-18T23:13:01+08:00" pubdate data-updated="true">Sat 2015-04-18 23:13</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>要了解一个框架，一般都是从框架提供/开放的接口入手。先从最直接的方式下手，可以通过<code>tachyon tfs</code>和浏览器19999端口获取集群以及文件的相关信息。</p>

<ul>
<li>要了解tachyon首先就是其文件系统，可以从两个功能开始：命令行tachyon.command.TFsShell和web-servlet。</li>
<li>然后会深入tachyon.client包，了解<strong>TachyonFS</strong>和TachyonFile处理io的方式。以及tachyon.hadoop的包。</li>
<li>io处理：

<ul>
<li>写：BlockOutStream（#getLocalBlockTemporaryPath； MappedByteBuffer）、FileOutStream</li>
<li>读：RemoteBlockInStream、LocalBlockInStream</li>
</ul>
</li>
<li>了解thrift：MasterClient、MasterServiceHandler、WorkerClient、WorkerServiceHandler、ClientBlockInfo、ClientFileInfo。</li>
<li>看tachyon.example，巩固</li>
</ul>


<p>注：MappedByteBuffer在windows存在资源占用的bug！参见<a href="http://www.th7.cn/Program/java/2012/01/31/57401.shtml">http://www.th7.cn/Program/java/2012/01/31/57401.shtml</a>，</p>

<p>把整个io流理清之后，然后需要了解tachyon是怎么维护这些信息的：</p>

<ul>
<li>配置：WorkerConf、MasterConf、UserConf</li>
<li>了解thrift：MasterClient、MasterServiceHandler、ClientWorkerInfo、MasterInfo</li>
<li>TachyonMaster和TachyonWorker的启动</li>
<li>Checkpoint、Image、Journal</li>
<li>内存淘汰策略</li>
<li>DataServer在哪里用到（nio/netty）：TachyonFile#readRemoteByteBuffer、RemoteBlockInStream#read(byte[], int, int)</li>
<li>HA</li>
<li>Dependency（不知道怎么用）</li>
</ul>


<p>远程调试Worker以及tfs：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ cat conf/tachyon-env.sh 
</span><span class='line'>export TACHYON_WORKER_JAVA_OPTS="$TACHYON_JAVA_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070"
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ export TACHYON_JAVA_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8070"
</span><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ bin/tachyon tfs lsr /</span></code></pre></td></tr></table></div></figure>


<h2>IO/client</h2>

<ul>
<li>TachyonFS是client与Master/Worker的纽带，请求<strong>文件系统和文件</strong>的元数据CRUD操作。其中的WorkerClient仅用于写（保存）文件。</li>
<li>TachyonFile是文件的抽象处理集合，可以获取文件的基本属性（元数据），同时提供了IO的接口用于文件内容的读写。</li>
<li>InStream读、获取文件内容

<ul>
<li>EmptyBlockInStream(文件包括的块为0）</li>
<li>BlockInStream(文件包括的块为1）

<ul>
<li>LocalBlockInStream 仅当是localworker且该块在本机时，通过MappedByteBuffer获取数据（数据在ramdisk也就是内存盘上哦）。</li>
<li>RemoteBlockInStream （通过nio从远程的worker#DataServer机器获取数据#retrieveByteBufferFromRemoteMachine，如果readtype设置为cache同时把数据缓冲到本地worker）</li>
</ul>
</li>
<li>FileInStream(文件包括的块为1+，可以理解为BlocksInStream。通过mCurrentPosition / mBlockCapacity来获取当前的blockindex最终还是调用BlockInStream）</li>
</ul>
</li>
<li>FileOutStream写，写数据入口就是只有FileOutStream

<ul>
<li>BlockOutStream（WriteType设置了需要缓冲，会写到本地localworker。<strong>由于需要进行分块，会复杂些#appendCurrentBuffer</strong>）</li>
<li>UnderFileSystem（如果WriteType设置了Through，则把数据写一份到underfs文件系统）</li>
</ul>
</li>
</ul>


<h2>Master</h2>

<p>TachyonMaster是master的启动类，所有的服务都是在这个类里面初始化启动的。</p>

<ul>
<li>HA：LeaderSelectorClient</li>
<li>EditLog：EditLogProcessor、Journal。

<ul>
<li>如果是HA模式，leader调用setup方法把EditLogProcessor停掉。也就是说在HA模式下，standby才会运行EditLogProcessor实时处理editlog。</li>
<li>leader和非HA master则仅在启动时通过调用MasterInfo#init处理editlog一次。</li>
</ul>
</li>
<li>Thrift: TServer、MasterServiceHandler；与MasterClient对应的服务端。</li>
<li>Web: UIWebServer，使用jetty的内嵌服务器。</li>
<li>MasterInfo</li>
</ul>


<h2>Worker</h2>

<h2>Thrift</h2>

<h2>HA</h2>

<p>当配置<code>tachyon.usezookeeper</code>设置为true时，启动master时会初始化LeaderSelectorClient对象。使用curator连接到zookeeper服务器进行leader的选举。</p>

<p><strong>LeaderSelectorClient</strong>实现了LeaderSelectorListener接口，创建LeaderSelector并传入当前实例作为监听实例，当选举完成后，被选leader触发takeLeadership事件。</p>

<blockquote><p>public void takeLeadership(CuratorFramework client) throws Exception
Called when your instance has been granted leadership. This method should not return until you wish to release leadership</p></blockquote>

<p>takeLeadership方法中把<code>mIsLeader</code>设置为true（master自己判断）、创建<code>mLeaderFolder + mName</code>路径（客户端获取master leader）；然后隔5s的死循环（运行在LeaderSelector单独的线程池）。</p>

<h2>Checkpoint</h2>

<h2>Journal</h2>

<hr />

<h2>问题</h2>

<ul>
<li>程序没有返回内容，没有响应（v1.1.0已经没有这个问题了）</li>
</ul>


<p>tfs 默认是CACHE_THROUGH，会缓冲同时写ufs。如果改成must则只写cache，然后清理内存，再获取数据，一直没有内容返回，不知道为什么？</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ export TACHYON_JAVA_OPTS="-Dtachyon.user.file.writetype.default=MUST_CACHE "
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs copyFromLocal LICENSE /LICENSE2
</span><span class='line'>Copied LICENSE to /LICENSE2
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs free /LICENSE2
</span><span class='line'>/LICENSE2 was successfully freed from memory.
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs cat /LICENSE2</span></code></pre></td></tr></table></div></figure>


<p>v1.1</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 alluxio-1.1.0-SNAPSHOT]$ bin/alluxio fs ls /README.txt                                            
</span><span class='line'>1366.00B  04-16-2016 07:56:24:499  In Memory      /README.txt
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.1.0-SNAPSHOT]$ bin/alluxio fs free /README.txt
</span><span class='line'>/README.txt was successfully freed from memory.
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.1.0-SNAPSHOT]$ bin/alluxio fs cat /README.txt 
</span><span class='line'>Block 402653184 is not available in Alluxio</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/04/15/tachyon-quickstart/">Tachyon入门指南</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-04-15T22:56:09+08:00" pubdate data-updated="true">Wed 2015-04-15 22:56</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>tachyon程序是在HDFS与程序之间缓冲，相当于CPU与磁盘设备之间内存的功能。tachyon提供了TachyonFS、TachyonFile等API使操作起来更像一个文件系统；同时实现了HDFS的FileSystem接口，方便原有程序的迁移，只要把url的模式（schema）hdfs改成tachyon。</p>

<p>tachyon和HDFS一样也是master-slaver（worker）结构：master保存元数据，worker节点使用内存盘缓冲数据。</p>

<h2>部署集群</h2>

<p>下载tachyon的编译文件后，按下面的步骤部署：</p>

<ul>
<li>解压</li>
<li>修改conf/tachyon-env.sh（JAVA_HOME，TACHYON_UNDERFS_ADDRESS，TACHYON_MASTER_ADDRESS）</li>
<li>修改conf/worker</li>
<li>同步代码到workers子节点</li>
<li>格式化tachyon（建立master和worker所需的各种目录）</li>
<li>挂载内存盘</li>
<li>启动集群</li>
<li>通过19999端口访问</li>
</ul>


<p>如果hadoop集群的版本不是最新的2.6.0，需要手工编译源码：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mvn clean package assembly:single -Dhadoop.version=2.2.0 -DskipTests -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<p>同步程序的脚本如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do  rsync -vaz tachyon-0.6.1 $h:~/ --exclude=logs --exclude=underfs --exclude=journal ; done</span></code></pre></td></tr></table></div></figure>


<p>用tachyon用户格式化：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/tachyon format</span></code></pre></td></tr></table></div></figure>


<p>使用root挂载内存盘：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/tachyon-mount.sh Mount workers
</span><span class='line'>for h in `cat slaves ` ; do  ssh $h "chmod 777 /mnt/ramdisk; chmod 777 /mnt/tachyon_default_home"  ; done</span></code></pre></td></tr></table></div></figure>


<p>确认下worker节点是否有underfs/tmp/tachyon/data，如果没有手动创建下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do ssh $h mkdir -p ~/tachyon-0.6.1/underfs/tmp/tachyon/data ; done</span></code></pre></td></tr></table></div></figure>


<p>启动集群：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 tachyon-0.6.1]$ bin/tachyon-start.sh all NoMount</span></code></pre></td></tr></table></div></figure>


<p>上传文件到tachyon：（注意，这里是在worker节点！）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs copyFromLocal README.md /
</span><span class='line'>Copied README.md to /</span></code></pre></td></tr></table></div></figure>


<h2>集成到Spark</h2>

<p>注意，这里是在worker节点，使用local本地集群的方式（spark集群资源全部被spark-sql占用了，导致提交的任务分配不到资源！）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 spark-1.3.0-bin-2.2.0]$ export SPARK_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar 
</span><span class='line'>[eshore@bigdata1 spark-1.3.0-bin-2.2.0]$ bin/spark-shell --master local[1] -Dspark.ui.port=4041
</span><span class='line'>scala&gt; val s = sc.textFile("tachyon://bigdatamgr1:19998/README.md")
</span><span class='line'>s: org.apache.spark.rdd.RDD[String] = tachyon://bigdatamgr1:19998/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:21
</span><span class='line'>
</span><span class='line'>scala&gt; s.count()
</span><span class='line'>15/04/03 11:13:09 WARN : tachyon.home is not set. Using /mnt/tachyon_default_home as the default value.
</span><span class='line'>res0: Long = 45
</span><span class='line'>
</span><span class='line'>scala&gt; val wordCounts = s.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
</span><span class='line'>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:23
</span><span class='line'>
</span><span class='line'>scala&gt; wordCounts.saveAsTextFile("tachyon://bigdatamgr1:19998/wordcount-README")
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 tachyon-0.6.1]$ bin/tachyon tfs ls /wordcount-README/
</span><span class='line'>1407.00 B 04-03-2015 11:16:05:483  In Memory      /wordcount-README/part-00000
</span><span class='line'>0.00 B    04-03-2015 11:16:05:787  In Memory      /wordcount-README/_SUCCESS</span></code></pre></td></tr></table></div></figure>


<p>为啥要在worker节点运行呢？不能在master节点运行？运行肯定是可以的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ export SPARK_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ bin/spark-shell --master local[1] --jars /home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>
</span><span class='line'>scala&gt; val s = sc.textFile("tachyon://bigdatamgr1:19998/NOTICE")
</span><span class='line'>s: org.apache.spark.rdd.RDD[String] = tachyon://bigdatamgr1:19998/NOTICE MapPartitionsRDD[1] at textFile at &lt;console&gt;:15
</span><span class='line'>
</span><span class='line'>scala&gt; s.count()
</span><span class='line'>15/04/13 16:05:45 WARN BlockReaderLocal: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
</span><span class='line'>15/04/13 16:05:45 WARN : tachyon.home is not set. Using /mnt/tachyon_default_home as the default value.
</span><span class='line'>java.io.IOException: The machine does not have any local worker.
</span><span class='line'>        at tachyon.client.BlockOutStream.&lt;init&gt;(BlockOutStream.java:94)
</span><span class='line'>        at tachyon.client.BlockOutStream.&lt;init&gt;(BlockOutStream.java:65)
</span><span class='line'>        at tachyon.client.RemoteBlockInStream.read(RemoteBlockInStream.java:204)
</span><span class='line'>        at tachyon.hadoop.HdfsFileInputStream.read(HdfsFileInputStream.java:142)
</span><span class='line'>        at java.io.DataInputStream.read(DataInputStream.java:100)
</span><span class='line'>        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)
</span><span class='line'>        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
</span><span class='line'>        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)
</span><span class='line'>        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)
</span><span class='line'>        at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:245)
</span><span class='line'>        at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:212)
</span><span class='line'>        at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
</span><span class='line'>        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
</span><span class='line'>        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
</span><span class='line'>        at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1466)
</span><span class='line'>        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
</span><span class='line'>        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
</span><span class='line'>        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
</span><span class='line'>        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
</span><span class='line'>        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
</span><span class='line'>        at org.apache.spark.scheduler.Task.run(Task.scala:64)
</span><span class='line'>        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span><span class='line'>res0: Long = 2</span></code></pre></td></tr></table></div></figure>


<p>两个点：</p>

<ul>
<li>这里是运行的spark local集群；</li>
<li>运行当然没有问题，但是会打印不和谐的<strong>The machine does not have any local worker</strong>警告日志。这与FileSystem的获取输入流<code>ReadType.CACHE</code>实现有关（见源码HdfsFileInputStream）。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mTachyonFileInputStream = mTachyonFile.getInStream(ReadType.CACHE);</span></code></pre></td></tr></table></div></figure>


<p>如果master为spark集群，spark-driver不管运行在哪台集群都没有问题。因为，此时运行任务的spark-worker就是tachyon-worker节点啊，当然就有local worker了。</p>

<p>为了更深入的了解，还可以试验一下<code>ReadType.CACHE</code>的作用：原本不在内存的数据，计算后就会被载入到缓冲（内存）！！</p>

<p>可以再试一次，先从内存中删掉（此处underfs配置存储在HDFS）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs free /NOTICE
</span><span class='line'>/NOTICE was successfully freed from memory.
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs fileinfo /NOTICE
</span><span class='line'>/NOTICE with file id 2 has the following blocks: 
</span><span class='line'>ClientBlockInfo(blockId:2147483648, offset:0, length:62, locations:[NetAddress(mHost:bigdata8, mPort:-1, mSecondaryPort:-1), NetAddress(bigdata6, mPort:-1, mSecondaryPort:-1), NetAddress(mHost:bigdata5, mPort:-1, mSecondaryPort:-1)])</span></code></pre></td></tr></table></div></figure>


<p>再次运行count：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; s.count()
</span><span class='line'>res1: Long = 2</span></code></pre></td></tr></table></div></figure>


<p>再次查看文件状态：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs fileinfo /NOTICE
</span><span class='line'>/NOTICE with file id 2 has the following blocks: 
</span><span class='line'>ClientBlockInfo(blockId:2147483648, offset:0, length:62, locations:[NetAddress(mHost:bigdata1, mPort:29998, mSecondaryPort:29999)])</span></code></pre></td></tr></table></div></figure>


<p>此时文件对应的block所在机器变成了bigdata1，也就是spark-worker运行的节点（这里用local，worker和driver都在bigdata1上）。</p>

<p>参考</p>

<ul>
<li><a href="http://tachyon-project.org/Running-Tachyon-on-a-Cluster.html">http://tachyon-project.org/Running-Tachyon-on-a-Cluster.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html">http://spark.apache.org/docs/latest/configuration.html</a></li>
<li><a href="http://tachyon-project.org/Running-Spark-on-Tachyon.html">http://tachyon-project.org/Running-Spark-on-Tachyon.html</a></li>
</ul>


<h2>集成到Hadoop集群</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ export HADOOP_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount -libjars /home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar tachyon://bigdatamgr1:19998/NOTICE tachyon://bigdatamgr1:19998/NOTICE-wordcount
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs cat /NOTICE-wordcount/part-r-00000
</span><span class='line'>2012-2014       1
</span><span class='line'>Berkeley        1
</span><span class='line'>California,     1
</span><span class='line'>Copyright       1
</span><span class='line'>Tachyon 1
</span><span class='line'>University      1
</span><span class='line'>of      1</span></code></pre></td></tr></table></div></figure>


<h2>后记</h2>

<p>当前apache开源大部分集群的部署都是同一种模式，源码也基本都是用maven来进行构建。部署其实没有什么难度，如果是应用到spark、hadoop这样的平台，其实只要部署，然后用FileSystem的接口就一切ok了。但是要了解其原理，官网的文档也不是很全，那得需要深入源码。</p>

<p>入门写到这里，差不多了，下一篇从TachyonFS角度解析tachyon。</p>

<h2>附录</h2>

<ul>
<li>spark-env.sh</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>JAVA_HOME=/home/eshore/jdk1.7.0_60
</span><span class='line'>
</span><span class='line'># log4j
</span><span class='line'>
</span><span class='line'>__add_to_classpath() {
</span><span class='line'>
</span><span class='line'>  root=$1
</span><span class='line'>
</span><span class='line'>  if [ -d "$root" ] ; then
</span><span class='line'>    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
</span><span class='line'>      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
</span><span class='line'>      else
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$f
</span><span class='line'>      fi
</span><span class='line'>    done
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>__add_to_classpath "/home/eshore/tez-0.4.0-incubating"
</span><span class='line'>__add_to_classpath "/home/eshore/tez-0.4.0-incubating/lib"
</span><span class='line'>__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"
</span><span class='line'>
</span><span class='line'>export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
</span><span class='line'>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'># HA
</span><span class='line'>SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark"
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do rsync -vaz spark-1.3.0-bin-2.2.0 $h:~/ --exclude=logs --exclude=metastore_db --exclude=work --delete ; done</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/17">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/15">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2016/04/23/hadoop-guide-catalog/">[整理] Hadoop入门</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/02/25/k8s-docker-multinode/">K8s集群部署</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/02/08/k8s-minikube-on-windows/">K8s Minikube on Windows</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/02/06/docker-http-proxy-and-save-reload/">Docker代理配置以及导入导出</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/02/04/privoxy-http-proxy-for-shadowsocks/">使用Privoxy把shadowsocks转换为Http代理</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/01/27/vnc-server-on-centos7/">在Centos7上安装VNC Server</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/01/25/develop-environment-prepare/">[整理] 环境准备工具集</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/01/21/jarsperreports-pdf-chinese/">jarsperreports生成PDF中文问题</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/01/20/nginx-https-with-tomcat-http/">Nginx Https With Tomcat Http</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/alluxio/'>alluxio</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/bigdata/'>bigdata</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (8) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/elasticsearch/'>elasticsearch</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/flume/'>flume</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (5) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (42) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/k8s/'>k8s</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/nginx/'>nginx</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/puppet/'>puppet</a> (10) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (11) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (44) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/zookeeper/'>zookeeper</a> (1) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (163)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"winseliu"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->










</body>
</html>
