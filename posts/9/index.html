
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="hadoop2中已经不使用masters来启动secondnamenode了！
SECONDARY_NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -secondarynamenodes 2&gt;/dev/null | sed 's/^M//g' ) &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winse.github.io/posts/9">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->

  <!--
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-43198550-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


  -->
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winse.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/03/30/masters-is-to-start-secondarynamenode/">Masters配置是用来启动hadoop1 Secondarynamenode</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-30T00:00:00+08:00" pubdate data-updated="true">Sat 2013-03-30 00:00</time>
        
      </p>
    
  </header>


  <div class="entry-content"><blockquote><p>hadoop2中已经不使用masters来启动secondnamenode了！
<code>SECONDARY_NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -secondarynamenodes 2&gt;/dev/null | sed 's/^M//g' )</code></p></blockquote>

<hr />

<p><a href="http://blog.csdn.net/dajuezhao/article/details/5987580">http://blog.csdn.net/dajuezhao/article/details/5987580</a> 写道</p>

<blockquote><p>2、secondarynamenode一般来说不应该和namenode在一起，所以，我把它配置到了datanode上。配置到datanode上，一般来说需要改以下配置文件。conf/master、conf/hdfs-site.xml和conf/core-site.xml这3个配置文件，修改部分如下: <br/>
master：一般的安装手册都是说写上namenode机器的IP或是名称。这里要说明一下，这个master不决定哪个是namenode，而决定的是secondarynamenode（决定谁是namenode的关键配置是core-site.xml中的fs.default.name这个参数）。</p></blockquote>

<p>《SecondaryNamenode应用摘记 <a href="http://blog.csdn.net/dajuezhao/article/details/5987580%E3%80%8B">http://blog.csdn.net/dajuezhao/article/details/5987580%E3%80%8B</a></p>

<p>今天看hadoop/bin目录下的shell时，发现start-dfs.sh中有如下的命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>"$bin"/hadoop-daemon.sh --config $HADOOP_CONF_DIR start namenode $nameStartOpt
</span><span class='line'>"$bin"/hadoop-daemons.sh --config $HADOOP_CONF_DIR start datanode $dataStartOpt
</span><span class='line'>"$bin"/hadoop-daemons.sh --config $HADOOP_CONF_DIR --hosts masters start secondarynamenode</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/03/DE/wKhkA1PMcNyAaz78AACVrXwbfzc895.png" alt="" /></p>

<p>也就是，使用start-dfs.sh时，是启动<strong>该机器</strong>的namenode服务，<strong>masters</strong>指定的secondarynamenode服务！
正如最上面引用中说的一样！</p>

<p>其实masters和slaves文件中定义的内容，就是用于shell来启动对应机器的服务的！！
如果你自己管理他们，压根不需要这些文件！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ find . -type f | while read file;do  if cat $file | grep -E "masters|slaves" | grep -v "^#" ; then echo "=== $file"; echo "" ; fi;  done
</span><span class='line'>....
</span><span class='line'>    export HOSTLIST="${HADOOP_CONF_DIR}/slaves"
</span><span class='line'>=== ./slaves.sh
</span><span class='line'>
</span><span class='line'>"$bin"/hadoop-daemons.sh --config $HADOOP_CONF_DIR --hosts masters start secondarynamenode
</span><span class='line'>=== ./start-dfs.sh
</span><span class='line'>
</span><span class='line'>"$bin"/hadoop-daemons.sh --config $HADOOP_CONF_DIR --hosts masters stop secondarynamenode
</span><span class='line'>=== ./stop-dfs.sh</span></code></pre></td></tr></table></div></figure>


<hr />

<p><a href="http://winseclone.iteye.com/blog/1839126">【原文地址】</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/03/27/run-on-hadoop-on-ant/">Ant实现hadoop插件Run-on-Hadoop</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-27T18:53:00+08:00" pubdate data-updated="true">Wed 2013-03-27 18:53</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>撇开eclipse的插件不说，如果直接在eclipse运行main方法，运行的时刻会提示map，reduce找不到的错误。其实就是没有把需要类的包提供给集群环境。</p>

<p>看过使用hadoop-eclipse-plugin插件（<a href="/blog/1837035">http://winseclone.iteye.com/blog/1837035</a>）最后解析的Run-on-Hadoop的实现，不难得出下面的方法步骤：</p>

<ul>
<li>首先打包jar，</li>
<li>然后把jar的路径给Main的-Dmapred.jar参数。</li>
</ul>


<p>这样，就可以把环境需要的class上传到hadoop了。</p>

<p>主要的ant代码如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property name="exported.jar" value="${build.dir}/tmp-runonhadoop-${now}.jar"&gt;&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;target name="jar" depends="build" description="Make tmp-run.jar"&gt;
</span><span class='line'>  &lt;jar jarfile="${exported.jar}" basedir="${build.classes}"&gt;
</span><span class='line'>      &lt;fileset dir="${build.classes}" includes="**/example/*" /&gt;
</span><span class='line'>      &lt;exclude name="**/core-site.xml"/&gt;
</span><span class='line'>  &lt;/jar&gt;
</span><span class='line'>&lt;/target&gt;
</span><span class='line'>
</span><span class='line'>&lt;target name="WordCount" depends="build, jar" &gt;
</span><span class='line'>  &lt;java classname="com.winse.hadoop.examples.WordCount" failonerror="true" fork="yes"&gt;
</span><span class='line'>      &lt;arg line="-fs=${fs.default.name} -jt=${mapred.job.tracker} -Dmapred.jar=${exported.jar} /test/input /test/output"/&gt;
</span><span class='line'>
</span><span class='line'>      &lt;classpath refid="runon.classpath"/&gt;
</span><span class='line'>  &lt;/java&gt;
</span><span class='line'>&lt;/target&gt;</span></code></pre></td></tr></table></div></figure>


<h2><a href="http://dl.iteye.com/topics/download/267b989c-3422-3fe8-b1c8-c4550accac6b">源码</a>：</h2>

<p>其实就<a href="https://gist.github.com/winse/9564525">build.xml</a>重要，其他就是exmaples里面的wordcount的源码而已。
其实build.xml也不重要，重要的思路！思路清晰了做事情就八九不离十了！</p>

<hr />

<p><a href="http://winseclone.iteye.com/blog/1837531">【原文地址】</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/03/27/hadoop-eclipse-plugin-use/">使用hadoop-eclipse-plugin插件</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-27T01:16:00+08:00" pubdate data-updated="true">Wed 2013-03-27 01:16</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>一直使用hadoop但都没有用过hadoop-plugins插件，倒不是看不上这个插件的意思。只是个人感觉使用SecureCRT太好用了。上传一个jar直接一拉进去使用lrzsz（z-moden）就直接搞定了。</p>

<p>但，身边搞hadoop的大部分都使用，今天略有兴致的弄了一弄，把环境hadoop-eclipse-plugins整起来了。在公司本来就使用插件来开发的，同时也看看Run-on-Hadoop的大致的实现。</p>

<h2>环境准备：</h2>

<ul>
<li>eclipse-jee-3.7</li>
<li>jdk7</li>
<li>hadoop-1.0.0</li>
</ul>


<h2>插件编译-导出-安装</h2>

<p>打开eclipse，把hadoop-1.0.0\src\contrib\eclipse-plugin整个工程导入（本来就是一个eclipse的project）。工程的lib目录下面已经包括了hadoop-core-1.0.0.jar包，但是hadoop-core-1.0.0.jar需要其他的依赖的jar包。我把这些依赖的jar从hadoop-1.0.0\lib下面复制到eclipse的/MapReduceTools/lib目录下。</p>

<p>然后，打开eclipse的/MapReduceTools/META-INF/MANIFEST.MF文件，切换到Runtime标签页，在classpath区域点击“Add&hellip;”添加lib下面的所有jar。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2577/d8eb44b7-008d-3bbb-9695-d4d200432100.png" alt="" /></p>

<p>然后，再切换到Overview标签页，导出插件。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2579/d631e742-2d17-3fe8-8fdc-04617b2ff7f6.png" alt="" /></p>

<p>导出以后，把导出的插件拷贝都<strong>eclipse/dropins</strong>目录下。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2568/bfa52d58-6556-3419-8d90-1a129d637b7d.png" alt="" /></p>

<p>然后重启。</p>

<h2>配置环境：</h2>

<ol>
<li><p>设置HADOOP_HOME的位置。</p>

<p> 设置Preferences中的Hadoop Map/Reduce的<strong>Hadoop installation directory</strong>为你hadoop的主目录。我这里是C:\cygwin\home\Winseliu\hadoop-1.0.0（主要是用来把hadoop/lib下的包加入到MapRed Project的classpath）。</p></li>
<li><p>切换到Mapreduce透视图（Window | Open Perspective | other | Map/Reduce），然后新建一个Hadoop Location。操作如图：</p>

<p> <img src="http://dl.iteye.com/upload/attachment/0082/2570/39e0d163-8a83-36ae-9896-50a0fb5e087b.png" alt="" /></p></li>
<li><p>然后，如果集群启动了，就可以看到下图的效果了。</p>

<p> <img src="http://dl.iteye.com/upload/attachment/0082/2572/370005b7-3ccf-30c0-832f-8f987cb8f426.png" alt="" /></p>

<p> mapred插件提供的MapReduce Driver类还是不错的，把从写JobMain繁琐的工作中稍稍解放了一点。</p>

<p> <img src="http://dl.iteye.com/upload/attachment/0082/2581/662d38e3-2196-3daf-814a-1a217d4e9892.png" alt="" /></p></li>
</ol>


<h2>Run-on-Hadoop的实现</h2>

<p>一般我们在设置Job（JobConf）.setJarByClass(WordCount.class)都是设置Main对应的主类。</p>

<p>查看源码，就可以得知。其实最终，设置了conf的<strong>mapred.jar</strong>属性！在提交的job时刻，会把该属性对应的jar拷贝到HDFS，最后发布到每台运行的机器上。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2599/f1c7a584-a703-3850-aaf0-6189e81897fa.png" alt="" /></p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2594/7af83776-ebcd-3783-8101-4b3ec748e6cc.png" alt="" /></p>

<p>所以，也就是说，只要把需要的<strong>class导出为jar</strong>，然后把该<strong>jar对应的路径给mapred.jar的属性</strong>即可。</p>

<p>理着这思路，在源码中有org.apache.hadoop.eclipse.server.JarModule类调用JDT的JarPackageData来处理jar的导出工作。</p>

<p>JarModule类在<code>org.apache.hadoop.eclipse.servers.RunOnHadoopWizard.performFinish()</code>方法中被调用。也就是点击Run-on-Hadoop菜单后弹出的对话框的<strong>Finish按钮</strong>被点击时。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2605/cccce4cc-1056-3315-bcdc-1c9453fdb23a.png" alt="" /></p>

<p>然后，会把该jar的路径写入到core-site.xml的配置文件中。该core-site.xml文件的路径会被加入到classpath，也就说Main方法的Configuration会加载这个配置文件！把conf的目录加入到classpath:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  classPath =
</span><span class='line'>          iConf.getAttribute(
</span><span class='line'>              IJavaLaunchConfigurationConstants.ATTR_CLASSPATH,
</span><span class='line'>              new ArrayList());
</span><span class='line'>  IPath confIPath = new Path(confDir.getAbsolutePath());
</span><span class='line'>      IRuntimeClasspathEntry cpEntry =
</span><span class='line'>          JavaRuntime.newArchiveRuntimeClasspathEntry(confIPath);
</span><span class='line'>      classPath.add(0, cpEntry.getMemento());</span></code></pre></td></tr></table></div></figure>


<p>后面的步骤就和一个普通的java类被调用一样的效果咯！</p>

<p>发布到集群运行的包涉及影响的文件：</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/2638/d113ef06-1bbb-387f-9ebe-e7e4580c023b.png" alt="" /></p>

<h2>参考资源：</h2>

<ul>
<li><a href="http://my.oschina.net/zhujinbao/blog/56236">http://my.oschina.net/zhujinbao/blog/56236</a></li>
</ul>


<hr />

<p><a href="http://winseclone.iteye.com/blog/1837035">【原文地址】</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/03/24/pseudo-distributed-hadoop-in-windows/">Windows配置hadoop伪分布式环境(续)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-24T00:00:00+08:00" pubdate data-updated="true">Sun 2013-03-24 00:00</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在前一篇文章中，介绍了一写常见问题的解决方法。</p>

<p>但是，当我重装系统，再次按照<a href="/blog/2012/11/25/windows-install-pseudo-distributed-hadoop1/">前面一篇文章</a>  <strong>安装cygwin和hadoop-1</strong>时，发现伪分布式环境使用mapred时，总是报错。（忘了，但是好像当时没有遇到过这种情况。就当是安装win8送给自己的礼物吧！）。</p>

<p>怀疑了很多东西，配置有问题，重新自定hadoop.tmp.dir，把hadoop-1.1.0换成hadoop-1.0.0等等。</p>

<p>错误日志如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ hhadoop fs -rmr /test/output ; hhadoop jar hadoop-examples-1.0.0.jar wordcount /test/input /test/output
</span><span class='line'>Deleted hdfs://WINSE:9000/test/output
</span><span class='line'>13/03/23 22:46:07 INFO input.FileInputFormat: Total input paths to process : 1
</span><span class='line'>13/03/23 22:46:08 INFO mapred.JobClient: Running job: job_201303232144_0002
</span><span class='line'>13/03/23 22:46:09 INFO mapred.JobClient:  map 0% reduce 0%
</span><span class='line'>13/03/23 22:46:16 INFO mapred.JobClient: Task Id : attempt_201303232144_0002_m_000002_0, Status : FAILED
</span><span class='line'>java.lang.Throwable: Child Error
</span><span class='line'>        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:272)
</span><span class='line'>Caused by: java.io.IOException: Task process exit with nonzero status of -1.
</span><span class='line'>        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:259)
</span><span class='line'>
</span><span class='line'>13/03/23 22:46:16 WARN mapred.JobClient: Error reading task outputhttp://WINSE:50060/tasklog?plaintext=true&attemptid=attempt_201303232144_0002_m_000002_0&filter=stdout
</span><span class='line'>13/03/23 22:46:16 WARN mapred.JobClient: Error reading task outputhttp://WINSE:50060/tasklog?plaintext=true&attemptid=attempt_201303232144_0002_m_000002_0&filter=stderr
</span><span class='line'>13/03/23 22:46:22 INFO mapred.JobClient: Task Id : attempt_201303232144_0002_m_000002_1, Status : FAILED</span></code></pre></td></tr></table></div></figure>


<p>经过不断的修改源码，加入sysout打印，算是最终找出程序出现错误的地方！</p>

<p>org.apache.hadoop.mapred.DefaultTaskController.java #launchTask
org.apache.hadoop.mapred.JvmManager.java #runChild
org.apache.hadoop.mapred.TaskRunner.java #launchJvmAndWait</p>

<p>org.apache.hadoop.fs.FileUtil.java #checkReturnValue
org.apache.hadoop.fs.RawLocalFileSystem.java  #setPermission  #mkdirs</p>

<p>发现在org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(Path)方法中，建立文件的路径方法检查attempt_201303232144_0002_m_000001_0<strong>是否为文件夹</strong>时会失败！</p>

<p>而在cygwin中查看文件属性：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Winseliu@WINSE ~/hadoop/logs/userlogs/job_201303232144_0002
</span><span class='line'>$ ll
</span><span class='line'>总用量 9
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000001_0 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000001_0
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000001_1 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000001_1
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000001_2 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000001_2
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000001_3 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000001_3
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000002_0 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000002_0
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000002_1 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000002_1
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000002_2 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000002_2
</span><span class='line'>lrwxrwxrwx 1 Winseliu None  89 3月  23 22:46 attempt_201303232144_0002_m_000002_3 -&gt; /cluster/mapred/local/userlogs/job_201303232144_0002/attempt_201303232144_0002_m_000002_3
</span><span class='line'>-rwxr-xr-x 1 Winseliu None 404 3月  23 22:46 job-acls.xml</span></code></pre></td></tr></table></div></figure>


<p>对于linux来说，这些就是引用到另一个文件夹，它本身应该也是文件夹！但是window的jdk不认识这些东西！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public boolean mkdirs(Path f) throws IOException {
</span><span class='line'>    Path parent = f.getParent();
</span><span class='line'>    File p2f = pathToFile(f);
</span><span class='line'>    return (parent == null || mkdirs(parent)) &&
</span><span class='line'>      (p2f.mkdir() || p2f.isDirectory());
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>所以在判断p2f.isDirectory()返回false，然后会抛出IOException，最终以-1的状态退出Map Child的程序！</p>

<p>其使用org.apache.hadoop.mapred.TaskRunner.prepareLogFiles(TaskAttemptID, boolean)方法来指定输出日志的位置。在最终执行的会在shell命令中把sysout和syserr输出到日志文件中（ $COMMAND  1>>$stdout  2>>$stderr ，本文最后有贴运行时的cmd）。</p>

<p>而userlogs的父目录是使用hadoop.log.dir系统属性来进行配置的！</p>

<p>mapred.DefaultTaskController.launchTask()</p>

<p>|&ndash;mapred.TaskLog.buildCommandLine()</p>

<h2>临时解决方法：</h2>

<p>把hadoop.log.dir定位到真正mapred日志的目录( mapred.local.dir : ${hadoop.tmp.dir}/mapred/local )！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export HADOOP_LOG_DIR=/cluster/mapred/local</span></code></pre></td></tr></table></div></figure>


<p>最终的效果，运行的程序会把日志输出到attempt目录下的stdout,stderr文件中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Winseliu@WINSE /cluster/mapred/local/userlogs/job_201303240035_0001/attempt_201303240035_0001_m_000000_0
</span><span class='line'>$ ll
</span><span class='line'>总用量 6
</span><span class='line'>lrwxrwxrwx  1 Winseliu None   89 3月  24 00:36 attempt_201303240035_0001_m_000000_0 -&gt; /cluster/mapred/local/userlogs/job_201303240035_0001/attempt_201303240035_0001_m_000000_0
</span><span class='line'>----------+ 1 Winseliu None  136 3月  24 00:36 log.index
</span><span class='line'>-rw-r--r--+ 1 Winseliu None    0 3月  24 00:36 stderr
</span><span class='line'>-rw-r--r--+ 1 Winseliu None    0 3月  24 00:36 stdout
</span><span class='line'>----------+ 1 Winseliu None 1238 3月  24 00:36 syslog</span></code></pre></td></tr></table></div></figure>


<p>上面的软链接是调用org.apache.hadoop.mapred.TaskLog.createTaskAttemptLogDir()生成的，本文最后有贴运行时ln命令及参数。</p>

<p>ln当linkname是一个已经存在文件夹时，会在linkname的文件夹下建立一个以targetname作为名称的链接。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Winseliu@WINSE ~/tt
</span><span class='line'>$ mkdir f1
</span><span class='line'>
</span><span class='line'>Winseliu@WINSE ~/tt
</span><span class='line'>$ ln -s f1 f1
</span><span class='line'>
</span><span class='line'>Winseliu@WINSE ~/tt
</span><span class='line'>$ ls -Rl
</span><span class='line'>.:
</span><span class='line'>总用量 0
</span><span class='line'>drwxr-xr-x+ 1 Winseliu None 0 3月  24 13:56 f1
</span><span class='line'>
</span><span class='line'>./f1:
</span><span class='line'>总用量 1
</span><span class='line'>lrwxrwxrwx 1 Winseliu None 2 3月  24 13:56 f1 -&gt; f1
</span></code></pre></td></tr></table></div></figure>


<p>把windows的/cluster映射到cygwin(linux)的/cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Winseliu@WINSE ~/hadoop
</span><span class='line'>$ ll /cygdrive/c | grep cluster
</span><span class='line'>drwxr-xr-x+ 1 Winseliu         None                      0 3月  24 00:08 cluster
</span><span class='line'>
</span><span class='line'>Winseliu@WINSE ~/hadoop
</span><span class='line'>$ ll / | grep cluster
</span><span class='line'>lrwxrwxrwx   1 Winseliu None     19 3月  23 09:39 cluster -&gt; /cygdrive/c/cluster</span></code></pre></td></tr></table></div></figure>


<p>但是，运行wordcount的例子时，还是不正常！查看tasktracker的日志时，发现有String转成Integer的NumberFormatException异常!</p>

<p>修改org.apache.hadoop.mapred.JvmManager.JvmManagerForType.<strong>JvmRunner</strong>.kill()方法。添加pidStr为空字符串的检查！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>String pidStr = jvmIdToPid.get(jvmId);
</span><span class='line'>if (pidStr != null && !pidStr.isEmpty()) {</span></code></pre></td></tr></table></div></figure>


<p>然后，终于看到Finish咯！在/test/output/part-r-00000中也看到了结果。</p>

<h2>其他一些简化处理，即配置文件：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>alias startCluster="~/hadoop/bin/start-all.sh"
</span><span class='line'>alias stopCluster="~/hadoop/bin/stop-all.sh; ~/hadoop/bin/stop-all.sh"
</span><span class='line'>
</span><span class='line'>alias hhadoop="~/hadoop/bin/hadoop"
</span><span class='line'>
</span><span class='line'>Winseliu@WINSE ~
</span><span class='line'>$ ll | grep hadoop
</span><span class='line'>lrwxrwxrwx  1 Winseliu None    12 3月  23 10:44 hadoop -&gt; hadoop-1.0.0
</span><span class='line'>drwx------+ 1 Winseliu None     0 3月  24 00:06 hadoop-1.0.0</span></code></pre></td></tr></table></div></figure>


<p>配置文件：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;!-- core-site.xml --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;fs.default.name&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hdfs://WINSE:9000&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span><span class='line'>&lt;value&gt;/cluster&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- hdfs-site.xml --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.replication&lt;/name&gt;
</span><span class='line'>&lt;value&gt;1&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.permissions&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;None&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.safemode.extension&lt;/name&gt;
</span><span class='line'>&lt;value&gt;1000&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- mapred-site.xml --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;mapred.job.tracker&lt;/name&gt;
</span><span class='line'>&lt;value&gt;WINSE:9001&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;</span></code></pre></td></tr></table></div></figure>


<p>关于查看启动的进程，看可以通过任务管理器来查看：</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/1102/e0e310ee-731c-37c3-8e74-30e425d374f5.png" alt="" /></p>

<p>还可以看看pid的修改时间，来确认服务的启动：
<img src="http://dl.iteye.com/upload/attachment/0082/2365/d6277aa6-8821-354a-831a-b9b518272b10.png" alt="" /></p>

<p>我是经常通过50070和50030来查看的~~ 看到50030的Nodes为1时，就说明集群启动正常了。</p>

<p> 执行时的日志：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cmd : ln -s /cluster/mapred/local\userlogs\job_201303241340_0001\attempt_201303241340_0001_m_000000_0 C:\cluster\mapred\local\userlogs\job_201303241340_0001\attempt_201303241340_0001_m_000000_0
</span><span class='line'>
</span><span class='line'>cmdLine : export HADOOP_CLIENT_OPTS="-Dhadoop.tasklog.taskid=attempt_201303241340_0001_m_000000_0 -Dhadoop.tasklog.iscleanup=false -Dhadoop.tasklog.totalLogFileSize=0"
</span><span class='line'>export SHELL="/bin/bash"
</span><span class='line'>export HADOOP_WORK_DIR="\cluster\mapred\local\taskTracker\Winseliu\jobcache\job_201303241340_0001\attempt_201303241340_0001_m_000000_0\work"
</span><span class='line'>export HOME="/homes/"
</span><span class='line'>export LOGNAME="Winseliu"
</span><span class='line'>export HADOOP_TOKEN_FILE_LOCATION="/cluster/mapred/local/taskTracker/Winseliu/jobcache/job_201303241340_0001/jobToken"
</span><span class='line'>export HADOOP_ROOT_LOGGER="INFO,TLA"
</span><span class='line'>export LD_LIBRARY_PATH="\cluster\mapred\local\taskTracker\Winseliu\jobcache\job_201303241340_0001\attempt_201303241340_0001_m_000000_0\work"
</span><span class='line'>export USER="Winseliu"
</span><span class='line'>
</span><span class='line'>exec '/cygdrive/c/Java/jdk1.7.0_02/jre/bin/java' '-Djava.library.path=/home/Winseliu/hadoop-1.0.0/lib/native/Windows_NT_unknown-x86-32;\cluster\mapred\local\
</span><span class='line'>taskTracker\Winseliu\jobcache\job_201303241340_0001\attempt_201303241340_0001_m_000000_0\work' '-Xmx200m' '-Djava.net.preferIPv4Stack=true' '-Dhadoop.metrics
</span><span class='line'>.log.level=WARN' '-Djava.io.tmpdir=/cluster/mapred/local/taskTracker/Winseliu/jobcache/job_201303241340_0001/attempt_201303241340_0001_m_000000_0/work/tmp' '
</span><span class='line'>-classpath' 'C:\cygwin\home\Winseliu\conf;C:\Java\jdk1.7.0_02\lib\tools.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\;C:\cygwin\home\Winseliu\hadoop-1.0.0\hadoop
</span><span class='line'>-core-1.0.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\asm-3.2.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\aspectjrt-1.6.5.jar;C:\cygwin\home\Winseliu\had
</span><span class='line'>oop-1.0.0\lib\aspectjtools-1.6.5.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-beanutils-1.7.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-be
</span><span class='line'>anutils-core-1.8.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-cli-1.2.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-codec-1.4.jar;C:\cygwin\
</span><span class='line'>home\Winseliu\hadoop-1.0.0\lib\commons-collections-3.2.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-configuration-1.6.jar;C:\cygwin\home\Winseliu\h
</span><span class='line'>adoop-1.0.0\lib\commons-daemon-1.0.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-digester-1.8.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-e
</span><span class='line'>l-1.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-httpclient-3.0.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-lang-2.4.jar;C:\cygwin\home\
</span><span class='line'>Winseliu\hadoop-1.0.0\lib\commons-logging-1.1.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-logging-api-1.0.4.jar;C:\cygwin\home\Winseliu\hadoop-1.0
</span><span class='line'>.0\lib\commons-math-2.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\commons-net-1.4.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\core-3.1.1.jar;C:\cygwin\
</span><span class='line'>home\Winseliu\hadoop-1.0.0\lib\hadoop-capacity-scheduler-1.0.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\hadoop-fairscheduler-1.0.0.jar;C:\cygwin\home\Win
</span><span class='line'>seliu\hadoop-1.0.0\lib\hadoop-thriftfs-1.0.0.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\hsqldb-1.8.0.10.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jackso
</span><span class='line'>n-core-asl-1.0.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jackson-mapper-asl-1.0.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jasper-compiler-5.5.12.ja
</span><span class='line'>r;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jasper-runtime-5.5.12.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jdeb-0.8.jar;C:\cygwin\home\Winseliu\hadoop-1.0
</span><span class='line'>.0\lib\jersey-core-1.8.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jersey-json-1.8.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jersey-server-1.8.jar;C:\cyg
</span><span class='line'>win\home\Winseliu\hadoop-1.0.0\lib\jets3t-0.6.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jetty-6.1.26.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jetty-
</span><span class='line'>util-6.1.26.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jsch-0.1.42.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\junit-4.5.jar;C:\cygwin\home\Winseliu\hadoo
</span><span class='line'>p-1.0.0\lib\kfs-0.2.2.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\log4j-1.2.15.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\mockito-all-1.8.5.jar;C:\cygwin\
</span><span class='line'>home\Winseliu\hadoop-1.0.0\lib\mysql-connector-java-5.1.10-bin.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\ojdbc6.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\l
</span><span class='line'>ib\oro-2.0.8.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\servlet-api-2.5-20081211.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\slf4j-api-1.4.3.jar;C:\cygwin
</span><span class='line'>\home\Winseliu\hadoop-1.0.0\lib\slf4j-log4j12-1.4.3.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\sqoop-1.4.1-incubating.jar;C:\cygwin\home\Winseliu\hadoop-1.
</span><span class='line'>0.0\lib\xmlenc-0.52.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jsp-2.1\jsp-2.1.jar;C:\cygwin\home\Winseliu\hadoop-1.0.0\lib\jsp-2.1\jsp-api-2.1.jar;\cluste
</span><span class='line'>r\mapred\local\taskTracker\Winseliu\jobcache\job_201303241340_0001\jars\classes;\cluster\mapred\local\taskTracker\Winseliu\jobcache\job_201303241340_0001\jar
</span><span class='line'>s;\cluster\mapred\local\taskTracker\Winseliu\jobcache\job_201303241340_0001\attempt_201303241340_0001_m_000000_0\work' '-Dhadoop.log.dir=C:\cluster\mapred\lo
</span><span class='line'>cal' '-Dhadoop.root.logger=INFO,TLA' '-Dhadoop.tasklog.taskid=attempt_201303241340_0001_m_000000_0' '-Dhadoop.tasklog.iscleanup=false' '-Dhadoop.tasklog.tota
</span><span class='line'>lLogFileSize=0' 'org.apache.hadoop.mapred.Child' '127.0.0.1' '49203' 'attempt_201303241340_0001_m_000000_0' 'C:\cluster\mapred\local\userlogs\job_20130324134
</span><span class='line'>0_0001\attempt_201303241340_0001_m_000000_0' '-1682417583'  &lt; /dev/null  1&gt;&gt; /cygdrive/c/cluster/mapred/local/userlogs/job_201303241340_0001/attempt_20130324
</span><span class='line'>1340_0001_m_000000_0/stdout 2&gt;&gt; /cygdrive/c/cluster/mapred/local/userlogs/job_201303241340_0001/attempt_201303241340_0001_m_000000_0/stderr</span></code></pre></td></tr></table></div></figure>


<p>linux的版本的日志目录结构：</p>

<p><img src="http://dl.iteye.com/upload/attachment/0082/1222/e1eeb8b1-cc16-3439-bc0f-2758e283012a.png" alt="" /></p>

<hr />

<p><a href="http://winseclone.iteye.com/blog/1835591">【原文地址】</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/03/15/compile-hadoop-source-and-modify-jsp/">编译hadoop的jsp源码</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-15T23:21:00+08:00" pubdate data-updated="true">Fri 2013-03-15 23:21</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>从apache下载的tar.gz的hadoop-1.1.0包中本来就包括了src的源码。可以方便我们查看源码调试。</p>

<p><del>题外话： 从github上下载了最新的hadoop-common的源码，发现hadoop-2.0已经是使用maven管理代码了。</del></p>

<p>在eclipse中新建java project，去掉<strong>Use Default location</strong>的复选框的勾，项目目录为hadoop-1.1.0程序所在的位置。然后点击finish即可。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7334/643c83b4-e123-362e-bc40-d801805584f4.png" alt="" /></p>

<p>完成后，项目下面的lib包，以及Source Folder源码包都已经正确的配置好了。如下图。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7344/64575adf-3ee5-3a83-a184-bcff4f9df4d8.png" alt="" /></p>

<p>编译hadoop的源码，需要用到sed，sh的linux shell命令（根据网上的资料）。安装好了cygwin，把c:\cygwin\bin加入到PATH环境变量。然后直接使用eclipse ant（eclipse自带）编译。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Winseliu@WINSE ~
</span><span class='line'>$ cygcheck -c cygwin
</span><span class='line'>Cygwin Package Information
</span><span class='line'>Package              Version        Status
</span><span class='line'>cygwin               1.7.17-1       OK
</span></code></pre></td></tr></table></div></figure>


<p><img src="http://dl.iteye.com/upload/attachment/0081/7351/e1b2d853-fcaf-3ccc-8663-8f579c67755f.png" alt="" /></p>

<p>由于linux和windows的换行符的不同（同事周帅哥在导数据也遇到这样的问题），直接编译会失败。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7353/29b31fa1-7f02-3979-b72d-fc3019f355dd.png" alt="" /></p>

<p>需要对src/saveVersion.sh的shell文件进行修改：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-  user=`whoami`
</span><span class='line'>+  user=`whoami | tr -d '\r'` </span></code></pre></td></tr></table></div></figure>


<p>然后再编译一次就ok了！</p>

<hr />

<p>经过上面步骤已经可以正确的编译hadoop-core的源码了。</p>

<p>在监控集群的时刻，我们一般都在自己常用的windows系统上面通过50030和50070来了解集群的情况。但是如果没有域名服务器，那，我们就不得不修改hosts文件。在出现访问失败的情况下，我们可以使用ip地址替换URL中对应的hostname来访问，但是比较麻烦。</p>

<p>如果在服务器响应请求的时刻，解析生成html的时刻就已经是ip地址那就最好不过了！
其实，直接看看jsp的源码，修改起来不算太难。把jsp里面的hostname转换为IP地址即可。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7361/951ae5f2-9dc3-31cd-aef3-657608a93e00.png" alt="" /></p>

<p>把上图的hostname通过InetAddress获取转换为IpAddress地址。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-    String namenodeHost = jspHelper.nameNodeAddr.getHostName();
</span><span class='line'>+    String namenodeHost = jspHelper.nameNodeAddr.getAddress().getHostAddress();
</span><span class='line'>
</span><span class='line'>-              InetAddress.getByName(namenodeHost).getCanonicalHostName() + ":" +
</span><span class='line'>+              InetAddress.getByName(namenodeHost).getHostAddress() + ":" +
</span></code></pre></td></tr></table></div></figure>


<p>全部修改完成后，再次运行hadoop-1.1.0 build.xml的ant命令，会调用自定义的jsp-compile把jsp转换成java类保存到build/src目录下面。然后javac再编译build/src目录下的源码。</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7370/421dc6d3-4a8a-340a-8bca-705369c0a057.png" alt="" /></p>

<p>如果你只想编译这些jsp，把javac中的srcdir的目录只保留build.src应该就可以咯。</p>

<p>我是直接把build/src作为Source Folder，然后把这个Source Folder下的编译文件放置的特定的目录，然后覆盖原来jar里面的class即可！</p>

<p><img src="http://dl.iteye.com/upload/attachment/0081/7396/7b3ccd33-936a-30ce-8082-d82f34d768bf.png" alt="" /></p>

<h2>参考：</h2>

<ul>
<li><a href="http://wenku.baidu.com/view/c1ad44323968011ca3009199.html">Hadoop源代码eclipse编译教程</a></li>
</ul>


<hr />

<p><a href="http://winse.iteye.com/blog/1830311">【原文地址】</a></p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/10">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/8">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://file.bmob.cn/M00/03/DE/wKhkA1PMkwyAH4pHAAHJu9ZKfYc646.png"><i class="fa fa-weixin">winseliu</i></a></p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/09/07/expect-automate-and-batch-config-ssh/">Expect-批量实现SSH无密钥登录</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/05/read-hadoop-balancer-source-part3/">[读码] Hadoop2 Balancer磁盘空间平衡（下）</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/05/read-hadoop-balancer-source-part2/">[读码] Hadoop2 Balancer磁盘空间平衡（中）</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/04/scala-quadratic-sum-of-odd-num-in-100/">计算出从1到100之间所有奇数的平方之和</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/03/linux-shell-shebang-tanjinghao/">Scala Shell #! 惊叹号井号</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/01/hadoop2-mapreduce-compress/">Hadoop2 Mapreduce输入输出压缩</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/08/29/xamarin-application-use-adt-eclipse-debug-java-code/">用ADT调试Xamarin程序中的Java库</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (23) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (11) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (58)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.2.0</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.98.3</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-0.13.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.4.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253103971'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1253103971%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'winseliu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
