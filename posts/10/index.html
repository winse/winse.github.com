
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="从原始的Eclipse右键导出打包，到后面使用maven打包，就单自己一个人使用开发部署是完全没问题的。现在的jenkins是对工具的封装、可视化和自动化，对于团队合作还是有一定的作用的，时时刻刻告诉我们代码是可运行的。 但是如果一个很久前的项目，又需要新加/修改功能，一下子还捡不起来， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/10">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/06/04/jenkins-start-guide/">Jenkins Start Guide</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2017-06-04T18:19:23+08:00" pubdate data-updated="true">Sun 2017-06-04 18:19</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>从原始的Eclipse右键导出打包，到后面使用maven打包，就单自己一个人使用开发部署是完全没问题的。现在的jenkins是对工具的封装、可视化和自动化，对于团队合作还是有一定的作用的，时时刻刻告诉我们代码是可运行的。</p>

<p>但是如果一个很久前的项目，又需要新加/修改功能，一下子还捡不起来，不放心啊还得验证一把。还有就是，测试有时刻他们自己打包，不会的还的教她们使用工具，人家烦自己也累。</p>

<p>jenkins是一个持续集成的工具，原来也接触过，但是都没用起来，都是搞开发，大部分时刻都能自己搞定。当下由于情况比较特殊，很多代码都直接在生产改，测试环境就不顾上了，但是测试环境不能总是旧代码啊，就想着有个自动化的东西来进行部署。</p>

<p>主要就是完成一个代码自动化部署的工作：自己搭建一个jenkins，从oschina上拉代码，编译后部署到tomcat并重启。</p>

<h2><a href="https://jenkins.io/download/">安装Jenkins</a></h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war
</span><span class='line'>nohup java -jar jenkins.war --httpPort=8081 &gt;/var/log/jenkins.log 2&gt;&1 & </span></code></pre></td></tr></table></div></figure>


<h2>建立到oschina的无密钥登录</h2>

<p>由于项目是私有的，通过https需要输入密码，还是git方式无密钥登录方式便捷一些。本地linux执行ssh-keygen，然后把 id_rsa.pub 的内容拷贝到项目的公钥处进行配置。可以参考文档： <a href="http://git.mydoc.io/?t=154712">http://git.mydoc.io/?t=154712</a>。</p>

<p>也可以结合 本地ssh-agent 和 ssh-forward 来弄。</p>

<h2>配置项目</h2>

<p>第一次登录需要进行一些配置，默认创建的admin密码会保存在 ~/.jenkins/secrets/initialAdminPassword 。（在初始化页面创建新用户报错，也不知道啥原因。登录后再建吧）</p>

<p>新版本的按照默认安装插件还不够，需要再添加一些。登录成功后，添加如下插件：</p>

<ul>
<li>Deploy to container Plugin  把war发布到容器tomcat&hellip;</li>
<li>Nexus Artifact Uploader  上传jar到私服</li>
<li><p>Maven Integration plugin 集成maven</p></li>
<li><p>ThinBackup 备份也是有必要的，用的越久越是必要！！</p></li>
</ul>


<p>配置maven：</p>

<p>自己下载个maven解压后，在jenkins - Global Tool Configuration上面配置maven地址即可（把 自动安装 的勾去掉就可以填地址了）</p>

<p>然后配置JOB：</p>

<ul>
<li>构建一个maven项目：填任务的名称，然后点击左下角的OK</li>
<li>源码管理git: 填写地址，然后新增Credentials - SSH Username with private key - From the Jenkins master ~/.ssh 起一个容易区分的名字</li>
<li>构建触发器： Build periodically - 0 0 * * * 每天一次</li>
<li>Build：web/pom.xml ; clean package -Papp,dist -DskipTests 就是mvn命令的一串参数</li>
<li>Post Steps: Run only if build succeeds - Execute Shell</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/opt/apache-tomcat-8.0.26/bin/shutdown.sh ; sleep 1 
</span><span class='line'>rm -rf /opt/apache-tomcat-8.0.26/webapps/app.war 
</span><span class='line'>cp $WORKSPACE/web/app/target/app.war /opt/apache-tomcat-8.0.26/webapps 
</span><span class='line'>cd /opt/apache-tomcat-8.0.26/webapps ; ./deploy.sh 
</span><span class='line'>BUILD_ID=dontKillMe nohup /opt/apache-tomcat-8.0.26/bin/startup.sh & 
</span><span class='line'>sleep 3</span></code></pre></td></tr></table></div></figure>


<p>注意：这里的BUILD_ID挺有意思的！！！</p>

<p>也可以配置 <strong>构建后操作</strong> 把包发布到tomcat manager（呵呵，无奈原始包webapps下的都被我删了)，就用脚本弄了。</p>

<h2>构建</h2>

<p>完成上面的操作后，就可以执行跑一次看看效果了。其他的还有很多功能：权限等。</p>

<h2>多节点(集群)</h2>

<p>如果只有一台jenkins的时刻，远程发布项目一般都scp或者使用tomcat-manager进行处理，如果把部署的机器作为jenkins node的话，就可以把部署的任务放到该节点本地跑，就不需要考虑远程部署的问题了。</p>

<p>配置节点： <a href="http://blog.csdn.net/e295166319/article/details/54134487">windows</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>用法   : 只允许运行绑定到这台机器的Job
</span><span class='line'>  启动方法 ： Launch slave agents via SSH（在主机配置无密钥登录，填用户、Private key：From the Jenkins master ~/.ssh）</span></code></pre></td></tr></table></div></figure>


<p>配置好后，在界面点击 <code>Launch agent</code>，会把agent拷贝到机器并启动agent。</p>

<p>然后任务的话，配置 <strong> Restrict where this project can be run </strong> 。</p>

<h2>参考</h2>

<ul>
<li><a href="http://www.cnblogs.com/gao241/archive/2013/03/20/2971416.html">Jenkins配置基于角色的项目权限管理</a></li>
<li><a href="http://www.cnblogs.com/zz0412/p/jenkins_jj_14.html">Jenkins进阶系列之——14配置Jenkins用户和权限</a></li>
<li><a href="https://wiki.jenkins-ci.org/display/JENKINS/Spawning+processes+from+build">https://wiki.jenkins-ci.org/display/JENKINS/Spawning+processes+from+build</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/java/j-lo-jenkins/">https://www.ibm.com/developerworks/cn/java/j-lo-jenkins/</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/06/01/optimize-java-on-production-environment/">追生产的一次优化</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2017-06-01T08:36:33+08:00" pubdate data-updated="true">Thu 2017-06-01 08:36</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>注：最后就是升级到JDK8(哭笑)&hellip;</p>

<p>最近闲得慌啊，本来不是自己职能范围内的。但是看着一台机器每天负载50+的跑，不舒服，就想去折腾折腾把负载降下来。</p>

<p>进程图：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>top - 08:01:24 up 1203 days,  9:06,  4 users,  load average: 31.41, 32.97, 32.38
</span><span class='line'>Tasks: 569 total,  11 running, 558 sleeping,   0 stopped,   0 zombie
</span><span class='line'>Cpu(s): 20.1%us, 68.1%sy,  0.0%ni,  6.0%id,  0.1%wa,  0.0%hi,  5.7%si,  0.0%st
</span><span class='line'>Mem:  49420852k total, 31831356k used, 17589496k free,   358748k buffers
</span><span class='line'>Swap: 33791992k total,   519332k used, 33272660k free, 18614472k cached
</span><span class='line'>
</span><span class='line'>  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                   
</span><span class='line'> 2340 omc       20   0 29.2g 8.5g  11m S 598.1 18.1   3436:40 java                     
</span><span class='line'>31349 omc       20   0 8071m 563m  11m S 348.4  1.2   1735:33 java                     
</span><span class='line'>28147 omc       20   0 15.5g 1.5g  14m S 341.9  3.2   1959:42 java                     
</span><span class='line'>   61 root      20   0     0    0    0 S 48.9  0.0  83728:05 ksoftirqd/14              
</span><span class='line'>   73 root      20   0     0    0    0 S 48.2  0.0  82342:12 ksoftirqd/17              
</span><span class='line'>    9 root      20   0     0    0    0 S 46.9  0.0  85312:03 ksoftirqd/1               
</span><span class='line'>   13 root      20   0     0    0    0 S 46.6  0.0  84297:57 ksoftirqd/2               
</span><span class='line'>   25 root      20   0     0    0    0 S 45.3  0.0  82811:49 ksoftirqd/5               
</span><span class='line'>   89 root      20   0     0    0    0 S 45.3  0.0  84608:31 ksoftirqd/21              
</span><span class='line'>   65 root      20   0     0    0    0 S 44.9  0.0  83475:48 ksoftirqd/15              
</span><span class='line'>   17 root      20   0     0    0    0 R 44.6  0.0  83990:21 ksoftirqd/3               
</span><span class='line'>   57 root      20   0     0    0    0 S 44.6  0.0  84625:38 ksoftirqd/13              
</span><span class='line'>   33 root      20   0     0    0    0 R 44.0  0.0  80537:34 ksoftirqd/7               
</span><span class='line'>    4 root      20   0     0    0    0 R 43.3  0.0  81489:54 ksoftirqd/0               
</span><span class='line'>   41 root      20   0     0    0    0 R 42.0  0.0  82651:17 ksoftirqd/9               
</span><span class='line'>   37 root      20   0     0    0    0 S 40.0  0.0  82636:26 ksoftirqd/8               
</span><span class='line'>   85 root      20   0     0    0    0 S 39.7  0.0  84557:49 ksoftirqd/20              
</span><span class='line'>   21 root      20   0     0    0    0 S 38.7  0.0  83271:24 ksoftirqd/4               
</span><span class='line'>   53 root      20   0     0    0    0 R 36.1  0.0  82083:15 ksoftirqd/12              
</span><span class='line'>   45 root      20   0     0    0    0 R 35.8  0.0  86230:39 ksoftirqd/10              
</span><span class='line'>   93 root      20   0     0    0    0 R 35.4  0.0  86416:12 ksoftirqd/22              
</span><span class='line'>   69 root      20   0     0    0    0 R 35.1  0.0  82726:46 ksoftirqd/16              
</span><span class='line'>   29 root      20   0     0    0    0 S 34.8  0.0  78415:22 ksoftirqd/6               
</span><span class='line'>   77 root      20   0     0    0    0 R 33.1  0.0  82419:34 ksoftirqd/18              
</span><span class='line'>   81 root      20   0     0    0    0 S 30.2  0.0  80141:58 ksoftirqd/19              
</span><span class='line'>   97 root      20   0     0    0    0 R 21.3  0.0  85993:03 ksoftirqd/23              
</span><span class='line'>   49 root      20   0     0    0    0 S 21.0  0.0  86742:13 ksoftirqd/11              
</span><span class='line'>28418 nobody    20   0  855m  32m 1144 S 20.7  0.1  72:23.66 gmetad</span></code></pre></td></tr></table></div></figure>


<p>线程图：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>top - 08:03:20 up 1203 days,  9:08,  4 users,  load average: 31.07, 32.36, 32.23
</span><span class='line'>Tasks: 940 total,  31 running, 909 sleeping,   0 stopped,   0 zombie
</span><span class='line'>Cpu(s): 20.0%us, 70.0%sy,  0.0%ni,  4.6%id,  0.0%wa,  0.0%hi,  5.4%si,  0.0%st
</span><span class='line'>Mem:  49420852k total, 31845576k used, 17575276k free,   358776k buffers
</span><span class='line'>Swap: 33791992k total,   519332k used, 33272660k free, 18615376k cached
</span><span class='line'>
</span><span class='line'>  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND             
</span><span class='line'>28174 omc       20   0 15.5g 1.5g  14m R 59.9  3.2 307:28.86 java                
</span><span class='line'>28203 omc       20   0 15.5g 1.5g  14m S 55.7  3.2 272:43.21 java                
</span><span class='line'> 2416 omc       20   0 29.2g 8.5g  11m R 55.4 18.1 274:31.07 java                
</span><span class='line'>31384 omc       20   0 8071m 563m  11m R 53.7  1.2 240:45.47 java                
</span><span class='line'> 2409 omc       20   0 29.2g 8.5g  11m S 53.1 18.1 245:56.03 java                
</span><span class='line'>28197 omc       20   0 15.5g 1.5g  14m S 52.4  3.2 279:04.35 java                
</span><span class='line'> 2406 omc       20   0 29.2g 8.5g  11m R 51.8 18.1 249:00.25 java                
</span><span class='line'>28208 omc       20   0 15.5g 1.5g  14m R 51.8  3.2 300:50.49 java                
</span><span class='line'> 2412 omc       20   0 29.2g 8.5g  11m S 51.5 18.1 232:11.81 java                
</span><span class='line'> 2415 omc       20   0 29.2g 8.5g  11m R 51.5 18.1 234:57.25 java                
</span><span class='line'> 2391 omc       20   0 29.2g 8.5g  11m R 51.1 18.1 301:52.48 java                
</span><span class='line'>28175 omc       20   0 15.5g 1.5g  14m R 51.1  3.2 299:18.11 java                
</span><span class='line'>31383 omc       20   0 8071m 563m  11m R 50.8  1.2 242:23.43 java                
</span><span class='line'>16662 omc       20   0 29.2g 8.5g  11m R 49.5 18.1   3:26.22 java                
</span><span class='line'>31381 omc       20   0 8071m 563m  11m R 49.5  1.2 237:05.25 java                
</span><span class='line'>   41 root      20   0     0    0    0 S 48.9  0.0  82652:00 ksoftirqd/9         
</span><span class='line'>   17 root      20   0     0    0    0 S 47.9  0.0  83990:59 ksoftirqd/3         
</span><span class='line'>   65 root      20   0     0    0    0 S 47.9  0.0  83476:26 ksoftirqd/15        
</span><span class='line'> 2408 omc       20   0 29.2g 8.5g  11m R 47.9 18.1 249:43.27 java                
</span><span class='line'>31382 omc       20   0 8071m 563m  11m R 47.9  1.2 237:07.76 java                
</span><span class='line'>   49 root      20   0     0    0    0 S 47.3  0.0  86743:04 ksoftirqd/11        
</span><span class='line'>   89 root      20   0     0    0    0 R 46.6  0.0  84609:14 ksoftirqd/21        
</span><span class='line'>   81 root      20   0     0    0    0 S 46.3  0.0  80142:39 ksoftirqd/19        
</span><span class='line'>   61 root      20   0     0    0    0 R 46.0  0.0  83728:50 ksoftirqd/14        
</span><span class='line'>31376 omc       20   0 8071m 563m  11m R 45.3  1.2 306:00.66 java                
</span><span class='line'>   33 root      20   0     0    0    0 R 45.0  0.0  80538:15 ksoftirqd/7         
</span><span class='line'>31385 omc       20   0 8071m 563m  11m R 45.0  1.2 272:52.36 java                
</span><span class='line'>   13 root      20   0     0    0    0 S 44.7  0.0  84298:42 ksoftirqd/2         
</span><span class='line'>   73 root      20   0     0    0    0 S 43.7  0.0  82342:53 ksoftirqd/17        
</span><span class='line'>   53 root      20   0     0    0    0 R 43.4  0.0  82083:54 ksoftirqd/12        
</span><span class='line'>   97 root      20   0     0    0    0 S 43.4  0.0  85993:53 ksoftirqd/23        
</span><span class='line'>   45 root      20   0     0    0    0 R 42.4  0.0  86231:24 ksoftirqd/10        
</span><span class='line'>   77 root      20   0     0    0    0 S 42.1  0.0  82420:20 ksoftirqd/18        
</span><span class='line'> 2407 omc       20   0 29.2g 8.5g  11m R 41.1 18.1 240:01.88 java                
</span><span class='line'> 2410 omc       20   0 29.2g 8.5g  11m R 40.8 18.1 227:49.76 java                
</span><span class='line'>   85 root      20   0     0    0    0 R 40.5  0.0  84558:37 ksoftirqd/20        
</span><span class='line'>28196 omc       20   0 15.5g 1.5g  14m R 38.2  3.2 276:56.00 java                
</span><span class='line'>   29 root      20   0     0    0    0 S 37.9  0.0  78416:08 ksoftirqd/6         
</span><span class='line'>   37 root      20   0     0    0    0 S 37.9  0.0  82637:15 ksoftirqd/8         
</span><span class='line'> 2411 omc       20   0 29.2g 8.5g  11m R 37.9 18.1 247:22.02 java                
</span><span class='line'> 2360 omc       20   0 29.2g 8.5g  11m S 37.6 18.1 179:49.10 java                
</span><span class='line'> 2413 omc       20   0 29.2g 8.5g  11m S 36.9 18.1 233:48.03 java                
</span><span class='line'>   69 root      20   0     0    0    0 R 36.3  0.0  82727:24 ksoftirqd/16        
</span><span class='line'>    4 root      20   0     0    0    0 R 35.6  0.0  81490:34 ksoftirqd/0         
</span><span class='line'>31369 omc       20   0 8071m 563m  11m R 35.6  1.2 192:32.42 java                
</span><span class='line'>   21 root      20   0     0    0    0 S 35.0  0.0  83272:02 ksoftirqd/4         
</span><span class='line'>28167 omc       20   0 15.5g 1.5g  14m R 33.7  3.2 197:02.78 java                
</span><span class='line'>   25 root      20   0     0    0    0 S 27.2  0.0  82812:29 ksoftirqd/5         
</span><span class='line'>   93 root      20   0     0    0    0 R 25.9  0.0  86416:55 ksoftirqd/22</span></code></pre></td></tr></table></div></figure>


<p>按照网络上的文档，查cpu时间很长的、占用很高的线程，然后拿着ID转成16进程到jstack里面去对：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[omc@cu-omc1 ~]$ jstack 28147
</span><span class='line'>2017-06-01 08:07:13
</span><span class='line'>Full thread dump Java HotSpot(TM) 64-Bit Server VM (23.7-b01 mixed mode):
</span><span class='line'>...
</span><span class='line'>"Timer-0" daemon prio=10 tid=0x00007fbd84850000 nid=0x6e27 in Object.wait() [0x00007fbe31f98000]
</span><span class='line'>   java.lang.Thread.State: TIMED_WAITING (on object monitor)
</span><span class='line'>        at java.lang.Object.wait(Native Method)
</span><span class='line'>        at java.util.TimerThread.mainLoop(Timer.java:552)
</span><span class='line'>        - locked &lt;0x0000000767760360&gt; (a java.util.TaskQueue)
</span><span class='line'>        at java.util.TimerThread.run(Timer.java:505)
</span><span class='line'>
</span><span class='line'>"schedulerFactory_QuartzSchedulerThread" prio=10 tid=0x00007fbd843f6000 nid=0x6e26 in Object.wait() [0x00007fbe32099000]
</span><span class='line'>   java.lang.Thread.State: TIMED_WAITING (on object monitor)
</span><span class='line'>        at java.lang.Object.wait(Native Method)
</span><span class='line'>        at org.quartz.core.QuartzSchedulerThread.run(QuartzSchedulerThread.java:311)
</span><span class='line'>        - locked &lt;0x0000000767770098&gt; (a java.lang.Object)
</span><span class='line'>
</span><span class='line'>"schedulerFactory_Worker-2" prio=10 tid=0x00007fbd848cd000 nid=0x6e25 in Object.wait() [0x00007fbe3219a000]
</span><span class='line'>   java.lang.Thread.State: TIMED_WAITING (on object monitor)
</span><span class='line'>        at java.lang.Object.wait(Native Method)
</span><span class='line'>        at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:568)
</span><span class='line'>        - locked &lt;0x00000007677ebb38&gt; (a java.lang.Object)
</span><span class='line'>
</span><span class='line'>"schedulerFactory_Worker-1" prio=10 tid=0x00007fbd848b3000 nid=0x6e24 in Object.wait() [0x00007fbe3229b000]
</span><span class='line'>   java.lang.Thread.State: TIMED_WAITING (on object monitor)
</span><span class='line'>        at java.lang.Object.wait(Native Method)
</span><span class='line'>        at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:568)
</span><span class='line'>        - locked &lt;0x00000007677ec710&gt; (a java.lang.Object)
</span><span class='line'>...
</span><span class='line'>"GC task thread#17 (ParallelGC)" prio=10 tid=0x00007fbe64035000 nid=0x6e06 runnable 
</span><span class='line'>
</span><span class='line'>"VM Periodic Task Thread" prio=10 tid=0x00007fbe6411f800 nid=0x6e0e waiting on condition 
</span><span class='line'>
</span><span class='line'>JNI global references: 321
</span><span class='line'>
</span><span class='line'>[omc@cu-omc1 ~]$ echo "obase=16;28203" | bc
</span><span class='line'>6E2B
</span><span class='line'>[omc@cu-omc1 ~]$ cat | while read id ; do echo "obase=16;$id" | bc ; done &lt;&lt;EOF
</span><span class='line'>28174
</span><span class='line'>28203
</span><span class='line'>28197
</span><span class='line'>28208
</span><span class='line'>28175
</span><span class='line'>28196
</span><span class='line'>28167
</span><span class='line'>EOF
</span><span class='line'>
</span><span class='line'>6E0E
</span><span class='line'>6E2B
</span><span class='line'>6E25
</span><span class='line'>6E30
</span><span class='line'>6E0F
</span><span class='line'>6E24
</span><span class='line'>6E07
</span></code></pre></td></tr></table></div></figure>


<p><img src="/images/blogs/linux-jdk7-jstack.png" alt="" /></p>

<p>基本都是sleep，wait的线程占用cpu很大。并且导致了系统cpu软中断处理进程ksoftirqd占用了大部分系统资源。系统不停的在处理上下文，负载奇高：</p>

<p><img src="/images/blogs/linux-jdk7-vmstat.png" alt="" /></p>

<p>ksoftirqd 不知道干嘛的，/proc/interrupts 看不懂，查了sleep和wait的区别，strace、iostat、jmap、jstack、jstat、vmstat、pidstat、还有看到内存补齐的一些文章，反正就是找不到北。</p>

<p>一开始以为是quartz的问题，对比了其他机器的quartz应用，有怀疑过版本问题（quartz-1.8.6, 2.2.0）；有试着去减少simplethreadpool的默认线程数（org.quartz.threadPool.threadCount），CPU占用是会少一点点，但是ksoftirqd还是压力很大，系统还是很大部分消耗在上下文切换，路子不对。</p>

<p>问题环境：</p>

<ul>
<li>Red Hat Enterprise Linux Server release 6.3 (Santiago)/2.6.32-279.el6.x86_64</li>
<li>Spring + quartz-2.2.&frac12;</li>
<li>jdk1.7.0_17</li>
</ul>


<p>完全没辙，不是功能代码的问题啊。搞到12点，困死了，回去睡个觉。今天一早起来，想想，不如换个 <strong>JDK8</strong> 试试吧（按照部署要求jdk放local目录下，要ROOT密码的昨晚就没动）。我勒个去，重启了感觉世界都变亮了。上下文切换cs 1w不到，us、sy基本忽略不计啊。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ vmstat -a 1
</span><span class='line'>procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----
</span><span class='line'> r  b   swpd   free  inact active   si   so    bi    bo   in   cs us sy id wa st
</span><span class='line'> 0  0 517160 25052176 12486824 10402340    0    0     2    31    0    0  4  9 86  0  0
</span><span class='line'> 0  0 517160 25052732 12486972 10401996    0    0     0  4676 4548 4806  0  0 99  0  0
</span><span class='line'> 1  0 517160 25052468 12486972 10402356    0    0     0     0 4044 4419  0  0 99  0  0
</span><span class='line'> 0  1 517160 25053664 12486852 10401992    0    0     0  8100 7608 5311  0  0 95  4  0
</span><span class='line'> 0  1 517160 25054800 12486852 10402084    0    0     0  8228 7847 5408  1  1 95  4  0
</span><span class='line'> 0  1 517160 25054924 12486852 10402380    0    0     0  8200 8075 4929  0  0 95  4  0
</span><span class='line'> 1  1 517160 25054868 12486852 10402112    0    0     0  7484 7898 5754  1  1 94  4  0
</span><span class='line'> 2  1 517160 25055544 12486848 10402148    0    0     0  8224 7537 4428  0  0 95  4  0</span></code></pre></td></tr></table></div></figure>


<p>好吧，以后优化的第一步就是换JDK .__. 。就像优化数据库第一步就建索引 V.V 。应该是JDK8对object.wait调用linux系统调用进行了优化。</p>

<h2>有点意思</h2>

<ul>
<li><a href="http://coderplay.iteye.com/blog/1481211">从Java视角理解CPU上下文切换(Context Switch)</a></li>
<li><a href="http://coderplay.iteye.com/blog/1485760">从Java视角理解CPU缓存(CPU Cache)</a></li>
<li><a href="http://coderplay.iteye.com/blog/1486649">从Java视角理解伪共享(False Sharing)</a></li>
<li><a href="https://github.com/LMAX-Exchange/disruptor">disruptor</a></li>
<li><a href="http://www.cnblogs.com/zhiranok/archive/2012/08/13/context_switch_1.html">http://www.cnblogs.com/zhiranok/archive/2012/08/13/context_switch_1.html</a></li>
<li><a href="http://www.bijishequ.com/detail/60264?p=">http://www.bijishequ.com/detail/60264?p=</a></li>
<li><a href="http://9leg.com/java/2016/08/09/cpu-consumption-analysis.html">http://9leg.com/java/2016/08/09/cpu-consumption-analysis.html</a></li>
</ul>


<blockquote><p>us过高
当us值过高时，表示运行的应用消耗了大部分的cpu。在这种情况下，对于java应用而言，最重要的是找到具体消耗cpu的线程所执行的代码，可以采用如下方法。</p>

<p>首先通过linux命令top命令查看us过高的pid值</p>

<p>通过top -Hp pid查看该pid进程下的线程的cpu消耗状况，得到具体pid值</p>

<p>将pid值转化为16进制，这个转化后的值对应nid值的线程</p>

<p>通过jstack pid grep -C 20 “16进制的值” 命令查看运行程序的线程信息</p>

<p>该线程就是消耗cpu的线程，在采样时须多执行几次上述的过程，以确保找到真实的消耗cpu的线程。</p>

<p>java应用造成us过高的原因主要是线程一直处于可运行的状态Runnable，通常是这些线程在执行无阻塞、循环、正则或纯粹的计算等动作造成。 另外一个可能会造成us过高的原因是频繁的gc。如每次请求都需要分配较多内存，当访问量高时就导致不断的进行gc，系统响应速度下降， 进而造成堆积的请求更多，消耗的内存严重不足，最严重的时候会导致系统不断进行FullGC，对于频繁的gc需要通过分析jvm内存的消耗来查找原因。</p>

<p>sy过高
当sy值过高时，表示linux花费了更多的时间在进行线程切换。java应用造成这种现象的主要原因是启动的线程比较多， 且这些线程多处于不断的阻塞（例如锁等待，io等待）和执行状态的变化过程中，这就导致了操作系统要不断的切换执行的线程， 产生大量的上下文切换。在这种情况下，对java应用而言，最重要的是找出不断切换状态的原因， 可采用的方法为通过kill -3 pid 或jstack -l pid的方法dump出java应用程序的线程信息，查看线程的状态信息以及锁信息， 找出等待状态或锁竞争过多的线程。</p></blockquote>

<ul>
<li><a href="http://yaocoder.blog.51cto.com/2668309/1543352">http://yaocoder.blog.51cto.com/2668309/1543352</a></li>
</ul>


<p>strace -T -r -c -p pid
pstack pid
trace -p tid</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/05/23/spark-on-hive-speculation-shit-bug/">Hive on Spark预测性执行BUG一枚</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2017-05-23T20:11:49+08:00" pubdate data-updated="true">Tue 2017-05-23 20:11</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>为了平复难以平复的痛苦，难以掩饰的激动，把这次遇到并解决的记录下。尽管最终解决的patch是官网的: <a href="https://issues.apache.org/jira/browse/HIVE-13066">Hive on Spark gives incorrect results when speculation is on</a>。</p>

<p>版本说明下：</p>

<ul>
<li>hive-1.2.1</li>
<li>spark-1.3.1</li>
</ul>


<p>在没有启动spark.speculation前，有个别任务执行非常慢，非常之讨厌。而启用预测性执行后，时不时任务会有些会失败，让人很烦躁。但是吧，也不算故障，说来也奇怪，重启下后再次查询问题就不出现了，也就没太在意。</p>

<p>今天数据量比较大，并且是上头检查。妈蛋，搞成了故障，没得办法，必须把原因找出来了。下来就帖日志了：</p>

<p>应用SQL查询报错日志：啥也看不到，就知道Hive查询报错，只能拿着时间去查Hive日志</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[ERROR] 14:19:56.685 [RMI TCP Connection(7)-192.168.31.11] c.e.z.h.s.BaseHiveQueryService | Error while processing statement: FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
</span><span class='line'>java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
</span><span class='line'>        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
</span><span class='line'>        at org.apache.hive.jdbc.HiveStatement.executeQuery(HiveStatement.java:392)
</span><span class='line'>        at org.apache.hive.jdbc.HivePreparedStatement.executeQuery(HivePreparedStatement.java:109)
</span><span class='line'>        at org.apache.commons.dbcp.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:96)
</span><span class='line'>        at org.apache.commons.dbcp.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:96)
</span><span class='line'>        at com.eshore.zhfx.hbase.service.BaseHiveQueryService.listIteratorInternal(BaseHiveQueryService.java:101)
</span><span class='line'>        at com.eshore.zhfx.hbase.service.BaseHiveQueryService.listIterator(BaseHiveQueryService.java:80)
</span><span class='line'>        at com.eshore.zhfx.hbase.QueryService.getAccessLogIterator(QueryService.java:140)
</span><span class='line'>        at com.eshore.zhfx.hbase.QueryService$$FastClassByCGLIB$$a60bf6f7.invoke(&lt;generated&gt;)
</span><span class='line'>        at net.sf.cglib.proxy.MethodProxy.invoke(MethodProxy.java:191)
</span><span class='line'>        at org.springframework.aop.framework.Cglib2AopProxy$CglibMethodInvocation.invokeJoinpoint(Cglib2AopProxy.java:688)
</span><span class='line'>        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:150)
</span><span class='line'>        at org.springframework.aop.framework.adapter.AfterReturningAdviceInterceptor.invoke(AfterReturningAdviceInterceptor.java:50)
</span><span class='line'>        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)
</span><span class='line'>        at org.springframework.aop.framework.adapter.MethodBeforeAdviceInterceptor.invoke(MethodBeforeAdviceInterceptor.java:50)
</span><span class='line'>        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)
</span><span class='line'>        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:89)
</span><span class='line'>        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)
</span><span class='line'>        at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:621)
</span><span class='line'>        at com.eshore.zhfx.hbase.QueryService$$EnhancerByCGLIB$$9a4ab584.getAccessLogIterator(&lt;generated&gt;)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>        at java.lang.reflect.Method.invoke(Method.java:601)
</span><span class='line'>        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:309)
</span><span class='line'>        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:183)
</span><span class='line'>        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:150)
</span><span class='line'>        at org.springframework.remoting.support.RemoteInvocationTraceInterceptor.invoke(RemoteInvocationTraceInterceptor.java:77)
</span><span class='line'>        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)
</span><span class='line'>        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:202)
</span><span class='line'>        at com.sun.proxy.$Proxy22.getAccessLogIterator(Unknown Source)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</span></code></pre></td></tr></table></div></figure>


<p>HIVE服务日志：rename错了，但是也好像看不到啥。知道那个节点有问题了，去查节点日志</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2017-05-23 14:19:20,509 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) - 2017-05-23 14:19:20,508 WARN  [task-result-getter-1] scheduler.TaskSetManager: Lost task 2199.1 in stage 2.0 (TID 4517, hadoop-slaver41): java.lang.IllegalStateException: Hit error while closing operators - failing tree: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://zfcluster/hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_task_tmp.-ext-10001/_tmp.002199_0 to: hdfs://zfcluster/hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_tmp.-ext-10001/002199_0.snappy
</span><span class='line'>2017-05-23 14:19:20,509 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.close(SparkMapRecordHandler.java:195)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.closeRecordProcessor(HiveMapFunctionResultList.java:58)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:106)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at scala.collection.Iterator$class.foreach(Iterator.scala:727)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$2.apply(AsyncRDDActions.scala:114)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$2.apply(AsyncRDDActions.scala:114)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1576)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1576)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.spark.scheduler.Task.run(Task.scala:64)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at java.lang.Thread.run(Thread.java:722)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) - Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://zfcluster/hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_task_tmp.-ext-10001/_tmp.002199_0 to: hdfs://zfcluster/hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_tmp.-ext-10001/002199_0.snappy
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.commit(FileSinkOperator.java:237)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.access$200(FileSinkOperator.java:143)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:1051)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:616)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:630)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:630)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:630)
</span><span class='line'>2017-05-23 14:19:20,510 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:630)
</span><span class='line'>2017-05-23 14:19:20,511 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.close(SparkMapRecordHandler.java:172)
</span><span class='line'>2017-05-23 14:19:20,511 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) -  ... 15 more
</span><span class='line'>2017-05-23 14:19:20,511 INFO  client.SparkClientImpl (SparkClientImpl.java:run(569)) - </span></code></pre></td></tr></table></div></figure>


<p>Task错误节点错误日志：这日志没啥。重名，拿名称去查namenode日志看看是啥子？</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>17/05/23 14:19:18 INFO exec.FileSinkOperator: FS[24]: records written - 0
</span><span class='line'>17/05/23 14:19:18 INFO exec.FileSinkOperator: Final Path: FS hdfs://zfcluster/hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_tmp.-ext-10001/002199_0
</span><span class='line'>17/05/23 14:19:18 INFO exec.FileSinkOperator: Writing to temp file: FS hdfs://zfcluster/hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_task_tmp.-ext-10001/_tmp.002199_0
</span><span class='line'>17/05/23 14:19:18 INFO exec.FileSinkOperator: New Final Path: FS hdfs://zfcluster/hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_tmp.-ext-10001/002199_0.snappy
</span><span class='line'>17/05/23 14:19:19 INFO compress.CodecPool: Got brand-new compressor [.snappy]
</span><span class='line'>org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://zfcluster/hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_task_tmp.-ext-10001/_tmp.002199_0 to: hdfs://zfcluster/hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_tmp.-ext-10001/002199_0.snappy
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.commit(FileSinkOperator.java:237)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.access$200(FileSinkOperator.java:143)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:1051)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:616)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:630)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:630)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:630)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:630)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.close(SparkMapRecordHandler.java:172)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.closeRecordProcessor(HiveMapFunctionResultList.java:58)
</span><span class='line'>        at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:106)
</span><span class='line'>        at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)
</span><span class='line'>        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
</span><span class='line'>        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
</span><span class='line'>        at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$2.apply(AsyncRDDActions.scala:114)
</span><span class='line'>        at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$2.apply(AsyncRDDActions.scala:114)
</span><span class='line'>        at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1576)
</span><span class='line'>        at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1576)
</span><span class='line'>        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
</span><span class='line'>        at org.apache.spark.scheduler.Task.run(Task.scala:64)
</span><span class='line'>        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:722)</span></code></pre></td></tr></table></div></figure>


<p>Namenode日志：有点点线索了，分配了两次，导致了第二个任务写入的时刻报错！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ grep '_tmp.002199' hadoop/logs/hadoop-hadoop-namenode-hadoop-master2.log.1
</span><span class='line'>2017-05-23 14:19:01,591 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_task_tmp.-ext-10001/_tmp.002199_0. BP-1414312971-192.168.32.11-1392479369615 blk_1219124858_145508182{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-ad2eac59-1e38-4019-a5ac-64c465366186:NORMAL:192.168.32.93:50010|RBW], ReplicaUnderConstruction[[DISK]DS-90c8cbe3-fd70-4ad7-938a-4248b4435df7:NORMAL:192.168.32.136:50010|RBW], ReplicaUnderConstruction[[DISK]DS-9da76df9-47f0-4e25-b375-e1bf32f4cf52:NORMAL:192.168.36.58:50010|RBW]]}
</span><span class='line'>2017-05-23 14:19:14,939 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_task_tmp.-ext-10001/_tmp.002199_0 is closed by DFSClient_attempt_201705231411_0000_m_001585_0_1316598676_51
</span><span class='line'>2017-05-23 14:19:20,368 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_task_tmp.-ext-10001/_tmp.002199_0. BP-1414312971-192.168.32.11-1392479369615 blk_1219125517_145508841{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-4d4c90f0-1ddf-4800-b33a-e776e58dc744:NORMAL:192.168.32.61:50010|RBW], ReplicaUnderConstruction[[DISK]DS-948cd823-5a4c-4673-8ace-99f02a26522b:NORMAL:192.168.32.52:50010|RBW], ReplicaUnderConstruction[[DISK]DS-7818addb-3881-446e-abb3-2c178be6bb63:NORMAL:192.168.32.176:50010|RBW]]}
</span><span class='line'>2017-05-23 14:19:20,478 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_task_tmp.-ext-10001/_tmp.002199_0 is closed by DFSClient_attempt_201705231411_0000_m_001345_1_1292482540_51
</span><span class='line'>2017-05-23 14:19:20,480 WARN org.apache.hadoop.hdfs.StateChange: DIR* FSDirectory.unprotectedRenameTo: failed to rename /hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_task_tmp.-ext-10001/_tmp.002199_0 to /hive/scratchdir/hadoop/64801461-94aa-4e17-afee-494e77b49998/hive_2017-05-23_14-18-38_278_4858034238266422677-2/-mr-10000/.hive-staging_hive_2017-05-23_14-18-38_278_4858034238266422677-2/_tmp.-ext-10001/002199_0.snappy because destination exists</span></code></pre></td></tr></table></div></figure>


<p>好了，看到这里，驴脑袋还没怀疑到是预测性执行导致的问题。当时想为啥会出现同一个文件名呢：SPARK ON HIVE多个stage执行导致的? 但是重启后报一样的错误，002199是哪里产生，怎么产生的？</p>

<p>MAP太多了000000又循环了一轮？看了执行的map数也就2600啊，不应该啊。</p>

<p>那么这个文件名是哪里产生的呢？然后就搞了下远程调试：没啥用，错误是在task上发生的，调试hive-driver没啥用，但是有意外收获</p>

<ul>
<li><a href="http://www.winseliu.com/blog/2014/06/21/upgrade-hive/">http://www.winseliu.com/blog/2014/06/21/upgrade-hive/</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started">https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 hive]$ DEBUG=true bin/hive
</span><span class='line'>Listening for transport dt_socket at address: 8000
</span><span class='line'>
</span><span class='line'>Logging initialized using configuration in file:/home/hadoop/apache-hive-1.2.1-bin/conf/hive-log4j.properties
</span><span class='line'>hive&gt; set hive.execution.engine=spark; '查询之前需要设置下引擎，故障得先处理。搞成默认的mr跑是成功的
</span><span class='line'>hive&gt;                                  'SQLSQLSQL...执行刚报错的SQL
</span><span class='line'>Query ID = hadoop_20170523173748_7660d9fb-9683-4792-8315-a51f6dcc270b
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>In order to change the average load for a reducer (in bytes):
</span><span class='line'>  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
</span><span class='line'>In order to limit the maximum number of reducers:
</span><span class='line'>  set hive.exec.reducers.max=&lt;number&gt;
</span><span class='line'>In order to set a constant number of reducers:
</span><span class='line'>  set mapreduce.job.reduces=&lt;number&gt;
</span><span class='line'>Starting Spark Job = 48a8668b-1c59-4cbf-b1e2-e19612ee77d0
</span><span class='line'>
</span><span class='line'>Query Hive on Spark job[0] stages:
</span><span class='line'>0
</span><span class='line'>
</span><span class='line'>Status: Running (Hive on Spark job[0])
</span><span class='line'>Job Progress Format
</span><span class='line'>CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
</span><span class='line'>2017-05-23 17:38:15,730 Stage-0_0: 0/2609
</span><span class='line'>2017-05-23 17:38:16,739 Stage-0_0: 0(+159)/2609
</span><span class='line'>...
</span><span class='line'>2017-05-23 17:39:23,182 Stage-0_0: 2162(+447)/2609
</span><span class='line'>2017-05-23 17:39:24,188 Stage-0_0: 2167(+608)/2609
</span><span class='line'>2017-05-23 17:39:25,195 Stage-0_0: 2201(+836,-1)/2609
</span><span class='line'>2017-05-23 17:39:26,201 Stage-0_0: 2215(+832,-2)/2609
</span><span class='line'>2017-05-23 17:39:27,207 Stage-0_0: 2227(+820,-2)/2609
</span><span class='line'>2017-05-23 17:39:28,213 Stage-0_0: 2250(+797,-2)/2609
</span><span class='line'>2017-05-23 17:39:29,219 Stage-0_0: 2280(+767,-2)/2609
</span><span class='line'>2017-05-23 17:39:30,224 Stage-0_0: 2338(+709,-2)/2609
</span><span class='line'>2017-05-23 17:39:31,230 Stage-0_0: 2350(+696,-3)/2609
</span><span class='line'>2017-05-23 17:39:32,236 Stage-0_0: 2359(+684,-6)/2609
</span><span class='line'>2017-05-23 17:39:33,243 Stage-0_0: 2363(+676,-10)/2609
</span><span class='line'>2017-05-23 17:39:34,249 Stage-0_0: 2365(+673,-12)/2609
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>有报错了，赶紧去web页面看了下结果，好家伙，全部是Speculation的报错：</p>

<p><img src="/images/blogs/hive-on-spark-speculation.jpg" alt="" /></p>

<p>在结合前面的namenode的日志，基本就走到正道上面。然后 <strong> hive spark speculation </strong> 一股沟，没错第一条就是hive官网的bug啊。</p>

<ul>
<li><a href="https://issues.apache.org/jira/browse/HIVE-13066">https://issues.apache.org/jira/browse/HIVE-13066</a></li>
</ul>


<p>然后就是打patch修改HivePairFlatMapFunction，验证是OK的。至少原来出错的语句完美跑完。</p>

<h2>总结下</h2>

<p>就是前段集成攻城狮把网络回环的问题处理了，导致网络状态好的不要不要的啊！把那些有备用10M网卡全部停了，集群的机器的网络好了N倍。第二个就是数据量实在大，其实speculation有启动，但是最先完成的还是先启动的，又没有把预测执行kill掉并且还运行完了最终还保存到同名文件。最后让我又一次体验了一把找开源软件BUG激情四射的半天。</p>

<p>记录聊以慰藉！！</p>

<hr />

<p>other : SparkClientImpl LeaseExpiredException No lease on  File does not exist</p>

<ul>
<li><a href="https://stackoverflow.com/questions/26842933/leaseexpiredexception-no-lease-error-on-hdfs-failed-to-close-file">LeaseExpiredException: No lease error on HDFS (Failed to close file)</a></li>
<li><a href="https://stackoverflow.com/questions/7559880/leaseexpiredexception-no-lease-error-on-hdfs">LeaseExpiredException: No lease error on HDFS</a></li>
<li><a href="http://www.jianshu.com/p/f5ec6c7bb176">http://www.jianshu.com/p/f5ec6c7bb176</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/05/05/puppet-automate-deploy-hosts/">Puppet批量自动化部署实战</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2017-05-05T08:33:37+08:00" pubdate data-updated="true">Fri 2017-05-05 08:33</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>断断续续使用Puppet近一年，多次体验到Puppet的强大：SSH更新、需ROOT权限批量处理等等。这次集群新上架了又爽了一把。把整个过程记录下来，方便今后参考。</p>

<p>运维的同事也想了解puppet，在docker容器上安装了一遍，把具体的内容附上：<a href="/files/expect+puppet.txt">expect+puppet.txt</a></p>

<p>这次操作是对以前零零碎碎积累的一次检验和温习。需要用到的工具比较多：</p>

<ul>
<li>RPM打包、本地YUM仓库 - RPMBUILD、CREATEREPO</li>
<li>SSH无密钥登录 - EXPECT&amp;FOR</li>
<li>时间同步、host配置 - SCP、SSH&amp;FOR</li>
<li>创建用户、新用户无密钥等 - PUPPET</li>
<li>ssh_known_hosts - PUPPETDB</li>
<li>rhel.repo、gmond、时区设置 - PUPPET</li>
</ul>


<p>远程配置机器首先当然是进行无密钥登录的设置，这样才能进行批量操作，不然几百台机器每次都需要干预太烦人、工作量太大。无密钥登录使用原来写好的EXPECT脚本，使用FOR循环执行，等待结果即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master1 ~]# cat ssh-copy-id.expect 
</span><span class='line'>#!/usr/bin/expect  
</span><span class='line'>
</span><span class='line'>## Usage $0 [user@]host password
</span><span class='line'>
</span><span class='line'>set host [lrange $argv 0 0];
</span><span class='line'>set password [lrange $argv 1 1] ;
</span><span class='line'>
</span><span class='line'>set timeout 30;
</span><span class='line'>
</span><span class='line'>spawn ssh-copy-id $host ;
</span><span class='line'>
</span><span class='line'>expect {
</span><span class='line'>  "(yes/no)?" { send yes\n; exp_continue; }
</span><span class='line'>  "password:" { send $password\n; exp_continue; }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>exec sleep 1;
</span><span class='line'>
</span><span class='line'># 用for，不要用while
</span><span class='line'>for h in `cat /etc/hosts | grep -v '^#' | grep slaver | grep -E '\.36\.|\.37\.' | awk '{print $2}' ` ; do 
</span><span class='line'>  ./ssh-copy-id.expect $h 'PASSWD';
</span><span class='line'>done
</span></code></pre></td></tr></table></div></figure>


<p>做好无密钥登录，拷贝 /etc/hosts, /etc/cron.daily/ntp.cron, /etc/yum.repos.d/puppet.repo 到全部的新机器。这里puppet.repo是自己编译搭建的私有仓库（具体编译配置步骤查看puppet分类下的文章），通过 <code>yum install mcollective-plugins-simple</code> 就可以把mcolletive和puppet-agent安装好。把所有步骤封装到一个prepare.sh脚本，内容如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'># must be hostname!!
</span><span class='line'>HOSTS="$@"
</span><span class='line'>PASSWD=${PASSWD:-'root'}
</span><span class='line'>PUPPETSERVER="hadoop-master1"
</span><span class='line'>
</span><span class='line'>for h in $HOSTS ; do ./ssh-copy-id.expect $h "$PASSWD" ; done
</span><span class='line'>
</span><span class='line'>for h in $HOSTS ; do
</span><span class='line'>scp /etc/hosts $h:/etc ;
</span><span class='line'>scp /etc/yum.repos.d/puppet.repo $h:/etc/yum.repos.d/ ;
</span><span class='line'>scp /etc/cron.daily/ntp.cron $h:/etc/cron.daily/ ;
</span><span class='line'>
</span><span class='line'>ssh $h '
</span><span class='line'>#ntpdate cu-omc1 #着重注意
</span><span class='line'>rm -rf /etc/yum.repos.d/CentOS-*
</span><span class='line'>yum install mcollective-plugins-simple -y
</span><span class='line'>' ;
</span><span class='line'>
</span><span class='line'>scp /etc/puppetlabs/mcollective/server.cfg $h:/etc/puppetlabs/mcollective/
</span><span class='line'>ssh $h "
</span><span class='line'>sed -i '/HOSTNAME/ {
</span><span class='line'>i \
</span><span class='line'>HOSTNAME=$h
</span><span class='line'>d
</span><span class='line'>} ' /etc/sysconfig/network
</span><span class='line'>hostname $h
</span><span class='line'>
</span><span class='line'>echo -e '\n\n[agent]\nserver = $PUPPETSERVER\ncertname=$h' &gt; /etc/puppetlabs/puppet/puppet.conf
</span><span class='line'>chkconfig mcollective on
</span><span class='line'>service mcollective start
</span><span class='line'>"
</span><span class='line'>
</span><span class='line'>done
</span></code></pre></td></tr></table></div></figure>


<p>然后执行 <code>./prepare.sh hadoop-slaver{200..500}</code> 就可以了。</p>

<p>接下来重点讲讲PUPPET配置的编写。</p>

<p>首先根据当前需要创建的用户、组把创建用户的配置写好：</p>

<ul>
<li><a href="https://docs.puppet.com/puppet/4.10/quick_start_user_group.html">https://docs.puppet.com/puppet/4.10/quick_start_user_group.html</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master1 ~]# puppet resource -e group hadoop
</span><span class='line'>group { 'hadoop':
</span><span class='line'>  ensure =&gt; 'present',
</span><span class='line'>  gid    =&gt; '501',
</span><span class='line'>}
</span><span class='line'>[root@hadoop-master1 ~]# puppet resource -e user hadoop
</span><span class='line'>user { 'hadoop':
</span><span class='line'>  ensure           =&gt; 'present',
</span><span class='line'>  gid              =&gt; '501',
</span><span class='line'>  groups           =&gt; ['wheel'],
</span><span class='line'>  home             =&gt; '/home/hadoop',
</span><span class='line'>  password         =&gt; '$6$AfnA...uIhHC9I.',
</span><span class='line'>  password_max_age =&gt; '99999',
</span><span class='line'>  password_min_age =&gt; '0',
</span><span class='line'>  shell            =&gt; '/bin/bash',
</span><span class='line'>  uid              =&gt; '501',
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>添加require、groups，然后删除uid、gid。最后需要添加 managehome => true, 否则用户目录就不会自动创建：</p>

<ul>
<li><a href="http://www.dbalex.com/category/devops/puppet">http://www.dbalex.com/category/devops/puppet</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 默认不创建用户目录
</span><span class='line'>[root@hadoop-slaver200 ~]# su - hadoop
</span><span class='line'>su: warning: cannot change directory to /home/hadoop: No such file or directory
</span><span class='line'>-bash-4.1$ 
</span><span class='line'>
</span><span class='line'># 创建用户配置成品
</span><span class='line'>group { 'hadoop':
</span><span class='line'>  ensure =&gt; 'present',
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>user { 'hadoop':
</span><span class='line'>  ensure           =&gt; 'present',
</span><span class='line'>  groups           =&gt; ['hadoop', 'wheel'],
</span><span class='line'>  home             =&gt; '/home/hadoop',
</span><span class='line'>  password         =&gt; '$6$Af...IhHC9I.',
</span><span class='line'>  password_max_age =&gt; '99999',
</span><span class='line'>  password_min_age =&gt; '0',
</span><span class='line'>  shell            =&gt; '/bin/bash',
</span><span class='line'>  managehome       =&gt; true,
</span><span class='line'>  require          =&gt; Group['hadoop'],
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li><a href="https://ask.puppet.com/question/15753/how-can-i-chown-directories-recursivley/">https://ask.puppet.com/question/15753/how-can-i-chown-directories-recursivley/</a></li>
<li><a href="https://serverfault.com/questions/542947/issue-with-changing-permission-and-owner-recursively-on-files-with-puppet-and-va">https://serverfault.com/questions/542947/issue-with-changing-permission-and-owner-recursively-on-files-with-puppet-and-va</a></li>
<li><a href="https://serverfault.com/questions/416254/adding-an-existing-user-to-a-group-with-puppet">https://serverfault.com/questions/416254/adding-an-existing-user-to-a-group-with-puppet</a></li>
</ul>


<p>添加好用户后，就是把无密钥登录也让PUPPET来弄。其实就是把 id_rsa.pub 的内容写入都行机器的 authorized_keys ，PUPPET已经自带了这个类：ssh_authorized_key。把id_ras.pub的内容（中间的内容）赋值给 key 属性即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh_authorized_key {'root@hadoop-master1':
</span><span class='line'>  user =&gt; 'root',
</span><span class='line'>  type =&gt; 'ssh-rsa',
</span><span class='line'>  key =&gt; 'AAAAB3NzaC1y...O1Q==',
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>ssh_authorized_key {'hadoop@hadoop-master1':
</span><span class='line'>  user =&gt; 'hadoop',
</span><span class='line'>  type =&gt; 'ssh-rsa',
</span><span class='line'>  key =&gt; 'AAAAB3Nza...IZYPw==',
</span><span class='line'>  require  =&gt; User['hadoop'],
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>无密钥登录比较容易，没有涉及到收集节点信息。仅仅把公钥写入新机器还不够，还得把 known_hosts 也处理好，不然第一次连接新机器都需要输入一下yes。内容如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver200 ~]$ ssh hadoop-slaver202
</span><span class='line'>The authenticity of host 'hadoop-slaver202 (192.168.36.59)' can't be established.
</span><span class='line'>RSA key fingerprint is fe:7e:26:c4:56:ea:f4:21:61:82:6d:9b:4a:72:93:a4.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? </span></code></pre></td></tr></table></div></figure>


<ul>
<li><a href="https://docs.puppet.com/puppet/4.4/lang_virtual.html">https://docs.puppet.com/puppet/4.4/lang_virtual.html</a></li>
<li><a href="https://docs.puppet.com/puppet/4.4/lang_collectors.html">https://docs.puppet.com/puppet/4.4/lang_collectors.html</a></li>
<li><a href="https://docs.puppet.com/puppet/4.4/lang_exported.html">https://docs.puppet.com/puppet/4.4/lang_exported.html</a></li>
<li><a href="https://docs.puppet.com/puppet/4.4/lang_resources_advanced.html#amending-attributes-with-a-collector">https://docs.puppet.com/puppet/4.4/lang_resources_advanced.html#amending-attributes-with-a-collector</a></li>
<li><a href="https://docs.puppet.com/puppet/latest/types/ssh_authorized_key.html">https://docs.puppet.com/puppet/latest/types/ssh_authorized_key.html</a></li>
<li><a href="https://www.puppetcookbook.com/posts/install-package.html">https://www.puppetcookbook.com/posts/install-package.html</a></li>
<li><a href="https://docs.puppet.com/puppet/4.10/lang_conditional.html">https://docs.puppet.com/puppet/4.10/lang_conditional.html</a></li>
</ul>


<p>正如上面官网介绍的，需要用到虚拟资源，自动把新机器指纹（fingerprint）写入到机器需要PUPPETDB的支持，安装配置又需要PGSQL的配合。需要耗费一番功夫，但是还是划得来的（具体安装步骤查看puppet分类下的文章）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>if $hostname =~ /^hadoop-/ {
</span><span class='line'>
</span><span class='line'>  $host_aliases = [ $ipaddress, $hostname ]
</span><span class='line'>  
</span><span class='line'>  # Export hostkeys from all hosts.
</span><span class='line'>  @@sshkey { $::fqdn:
</span><span class='line'>    ensure =&gt; present,
</span><span class='line'>    host_aliases =&gt; $host_aliases,
</span><span class='line'>    type =&gt; 'ssh-rsa',
</span><span class='line'>    key =&gt; $sshrsakey,
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  if $hostname =~ /^hadoop-master/ {
</span><span class='line'>    # realize all exported
</span><span class='line'>    Sshkey &lt;&lt;| |&gt;&gt;
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>先在所有slaver机器运行一遍 puppet agent -t ，然后再在master节点把收集的指纹写入到 /etc/ssh/ssh_known_hosts 。</p>

<p>这里说个插曲：机器的hosts和hostname是通过 FOR&amp;SSH 命令来统一修改的，有些可能没有配置好导致机器的主机名有重复。通过执行配置known_hosts竟然帮我找出了hostname重复的机器，意外的收获。该问题的处理我是直接登录到PGSQL改了对应表的数据处理的。</p>

<p>到这里机器基本能用了。主机名、hosts、时间同步、hadoop用户以及master到该用户的无密钥登录都已经配置好了。</p>

<p>接下来把实战过程中安装gmond的步骤帖出来：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$$ cd /etc/puppetlabs/code/environments/production/manifests/
</span><span class='line'>
</span><span class='line'>$$ vi change_site.sh
</span><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'>## Usage:
</span><span class='line'>##  ./change_site.sh nrpe.site
</span><span class='line'>##
</span><span class='line'>
</span><span class='line'>[[ $# != 1 ]] && exit 1
</span><span class='line'>
</span><span class='line'>cd $(cd $(dirname $0); pwd)
</span><span class='line'>
</span><span class='line'>rm -rf site.pp
</span><span class='line'>ln -s $1 site.pp
</span><span class='line'>
</span><span class='line'>$$ vi pexec.sh
</span><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'>## Usage:
</span><span class='line'>##   ./pexec.sh /cu-ud/ sudo_revert.site 
</span><span class='line'>##
</span><span class='line'>
</span><span class='line'>case $# in
</span><span class='line'>1)
</span><span class='line'>  FUNC="$1"
</span><span class='line'>  HOST_PARAM=
</span><span class='line'>  ;;
</span><span class='line'>2)
</span><span class='line'>  FUNC="$2"
</span><span class='line'>  HOST_PARAM="-I $1"
</span><span class='line'>  ;;
</span><span class='line'>*)
</span><span class='line'>  while [ $# -gt 1 ] ; do 
</span><span class='line'>    HOST_PARAM="$HOST_PARAM -I $1"
</span><span class='line'>    shift
</span><span class='line'>  done
</span><span class='line'>  FUNC=$1
</span><span class='line'>  ;;
</span><span class='line'>esac
</span><span class='line'>
</span><span class='line'>cd $(cd $(dirname $0); pwd)
</span><span class='line'>
</span><span class='line'>./change_site.sh "$FUNC"
</span><span class='line'>
</span><span class='line'>if [[ "$HOST_PARAM" != "" && ! "$HOST_PARAM" =~ */* ]] ; then
</span><span class='line'>  mco shell $HOST_PARAM run -- `which puppet` agent -t
</span><span class='line'>else
</span><span class='line'>  mco puppet $HOST_PARAM runall 20
</span><span class='line'>fi</span></code></pre></td></tr></table></div></figure>


<p>由于机器增加比较多，且网络环境变的复杂化。把原来的2个分组修改成4个。不同的网络段和功能分别设置不同的广播端口。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ./pexec.sh /hadoop-slaver.$/ gmond.site 
</span><span class='line'>
</span><span class='line'># 采集数据的节点重启后，其他发送数据的节点貌似都需要重启。
</span><span class='line'>$ screen
</span><span class='line'>$ for ((i=1;i&lt;=53;i++)); do  mco shell -I /hadoop-slaver${i}.$/ run -- ' service gmond restart ' ; done 
</span><span class='line'># 这个确认搞的很麻烦，
</span><span class='line'># 想通过ganglia-web获取数据然后判断是否有数据进行重启。</span></code></pre></td></tr></table></div></figure>


<p>Ganglia删除某节点后，如果要从rrds上去掉改节点的信息，需要：重启对应收集的gmond，对应集群的rrds目录，然后重启gmetad。或者等够一段时间，gmetad会自动去掉。</p>

<h2>总结</h2>

<p>现在添加机器，直接连上puppetserver机器然后执行几个命令就可以搞定；</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>HOST=new-host-name 
</span><span class='line'># 无密钥登录和puppet/mco
</span><span class='line'>PASSWD=new-host-root-password ./prepare.sh $HOST
</span><span class='line'>
</span><span class='line'>./pexec.sh $HOST new-hadoop.site
</span><span class='line'>./pexec.sh $HOST gmond.site # 当前需要到web界面确认新节点的数据是否被采集</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/04/14/k8s-hadoop-deploy/">K8s Hadoop Deploy</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2017-04-14T10:56:39+08:00" pubdate data-updated="true">Fri 2017-04-14 10:56</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>折磨了一个多星期，最后还是调通了。折磨源于不自知，源于孤单，源于自负，后来通过扩展、查阅资料、请教同事顺利解决。简单部署可以查看<a href="https://github.com/winse/docker-hadoop">README.md</a> 。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install docker-engine-1.12.6 docker-engine-selinux-1.12.6 -y
</span><span class='line'>
</span><span class='line'>cd kube-deploy
</span><span class='line'>vi hosts
</span><span class='line'>vi k8s.profile
</span><span class='line'># 把deploy同步到其他实体机，同时把k8s.profile映射到/etc/profile.d
</span><span class='line'>./rsync-deploy.sh
</span><span class='line'>
</span><span class='line'>cd docker-multinode/
</span><span class='line'>./master.sh or ./worker.sh
</span><span class='line'>
</span><span class='line'>docker save gcr.io/google_containers/etcd-amd64:3.0.4 | docker-bs load
</span><span class='line'>docker save quay.io/coreos/flannel:v0.6.1-amd64 | docker-bs load
</span><span class='line'>
</span><span class='line'>cd kube-deploy/hadoop/kubenetes/
</span><span class='line'>./prepare.sh
</span><span class='line'>kubectl create -f hadoop-master2.yaml
</span><span class='line'>kubectl create -f hadoop-slaver.yaml </span></code></pre></td></tr></table></div></figure>


<p>Tip：其实使用一套配置就可以启动多个集群，在 <code>kubectl create</code> 后面加上 <code>-n namespace</code> 即可。</p>

<p>比如：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 kubenetes]# kubectl create namespace hd1
</span><span class='line'>[root@cu2 kubenetes]# kubectl create namespace hd2
</span><span class='line'>
</span><span class='line'>[root@cu2 kubenetes]# ./prepare.sh hd1
</span><span class='line'>[root@cu2 kubenetes]# kubectl create -f hadoop-master2.yaml -n hd1
</span><span class='line'>[root@cu2 kubenetes]# kubectl create -f hadoop-slaver.yaml -n hd1
</span><span class='line'>[root@cu2 kubenetes]# ./prepare.sh hd2
</span><span class='line'>[root@cu2 kubenetes]# kubectl create -f hadoop-master2.yaml -n hd2
</span><span class='line'>[root@cu2 kubenetes]# kubectl create -f hadoop-slaver.yaml -n hd2
</span><span class='line'>
</span><span class='line'>[root@cu2 kubenetes]# kubectl get pods --all-namespaces
</span><span class='line'>NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE
</span><span class='line'>hd1           hadoop-master2                          1/1       Running   0          28s
</span><span class='line'>hd1           slaver-rc-fdcsw                         1/1       Running   0          18s
</span><span class='line'>hd1           slaver-rc-qv964                         1/1       Running   0          18s
</span><span class='line'>hd2           hadoop-master2                          1/1       Running   0          26s
</span><span class='line'>hd2           slaver-rc-0vdfk                         1/1       Running   0          17s
</span><span class='line'>hd2           slaver-rc-r7g84                         1/1       Running   0          17s
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>现在想来其实就是 <strong> dockerd &ndash;ip-masq=false </strong>的问题（所有涉及的dockerd都需要加）。 还有就是一台机器单机下的容器互相访问，源IP都错也是安装了openvpn所导致，对所有过eth0的都加了MASQUERADE。</p>

<p>根源就在于请求的源地址被替换，也就是iptables的转发进行了SNAT。关于iptables转发这篇文章讲的非常清晰；<a href="http://fancyxinyu.blog.163.com/blog/static/18232136620136185434661/">IPtables之四：NAT原理和配置  </a> 。</p>

<h2>所遇到的问题</h2>

<p>没加ip-masq之前，namenode收到datanode的请求后，源地址是flannel.0的ip: 10.1.98.0。</p>

<p>namenode对应的日志为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2017-04-09 07:22:06,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.1.98.0, datanodeUuid=5086c549-f3bb-4ef6-8f56-05b1f7adb7d3, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-522174fa-6e7b-4c3f-ae99-23c3018e35d7;nsid=1613705851;c=0) storage 5086c549-f3bb-4ef6-8f56-05b1f7adb7d3
</span><span class='line'>2017-04-09 07:22:06,920 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/10.1.98.0:50010
</span><span class='line'>2017-04-09 07:22:06,921 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.1.98.0:50010</span></code></pre></td></tr></table></div></figure>


<p>一开始以为是flannel的问题，换成yum安装，然后同时flannel把backend切换成vxlan后，还是一样的问题。</p>

<p>最后请教搞网络的同事，应该是请求的源地址被替换了，也就定位到iptables。然后通过查看文档，其实前面也有看到过对应的文章，但是看不明白不知道缘由。</p>

<ul>
<li><a href="https://groups.google.com/d/msg/kubernetes-users/P4uh7y383oo/bPzIRaxhs5gJ">Networking Problem in creating HDFS cluster. - Eugene Yakubovich </a></li>
<li><a href="https://groups.google.com/d/msg/kubernetes-users/P4uh7y383oo/a1GIV4hcAgAJ">Networking Problem in creating HDFS cluster. - Huihui He </a></li>
<li><a href="https://developer.ibm.com/recipes/tutorials/networking-your-docker-containers-using-docker0-bridge/">Networking your docker containers using docker0 bridge</a></li>
</ul>


<p>iptables的部分相关信息：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# iptables -S -t nat
</span><span class='line'>...
</span><span class='line'>-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
</span><span class='line'>-A PREROUTING -j PREROUTING_direct
</span><span class='line'>-A PREROUTING -j PREROUTING_ZONES_SOURCE
</span><span class='line'>-A PREROUTING -j PREROUTING_ZONES
</span><span class='line'>-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
</span><span class='line'>-A OUTPUT -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
</span><span class='line'>-A OUTPUT -j OUTPUT_direct
</span><span class='line'>-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
</span><span class='line'>-A POSTROUTING -s 10.1.34.0/24 ! -o docker0 -j MASQUERADE
</span><span class='line'>-A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
</span><span class='line'>-A POSTROUTING -j POSTROUTING_direct
</span><span class='line'>-A POSTROUTING -j POSTROUTING_ZONES_SOURCE
</span><span class='line'>-A POSTROUTING -j POSTROUTING_ZONES
</span><span class='line'>-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000
</span><span class='line'>-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
</span><span class='line'>-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -m mark --mark 0x4000/0x4000 -j MASQUERADE
</span><span class='line'>-A KUBE-SEP-75CPIAPDB4MAVFWI -s 10.1.40.3/32 -m comment --comment "kube-system/kube-dns:dns-tcp" -j KUBE-MARK-MASQ
</span><span class='line'>-A KUBE-SEP-75CPIAPDB4MAVFWI -p tcp -m comment --comment "kube-system/kube-dns:dns-tcp" -m tcp -j DNAT --to-destination 10.1.40.3:53
</span><span class='line'>-A KUBE-SEP-IWNPEB4T46P6VG5J -s 192.168.0.148/32 -m comment --comment "default/kubernetes:https" -j KUBE-MARK-MASQ
</span><span class='line'>-A KUBE-SEP-IWNPEB4T46P6VG5J -p tcp -m comment --comment "default/kubernetes:https" -m recent --set --name KUBE-SEP-IWNPEB4T46P6VG5J --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 192.168.0.148:6443
</span><span class='line'>-A KUBE-SEP-UYUINV25NDNSKNUW -s 10.1.40.3/32 -m comment --comment "kube-system/kube-dns:dns" -j KUBE-MARK-MASQ
</span><span class='line'>-A KUBE-SEP-UYUINV25NDNSKNUW -p udp -m comment --comment "kube-system/kube-dns:dns" -m udp -j DNAT --to-destination 10.1.40.3:53
</span><span class='line'>-A KUBE-SEP-XDHL2OHX2ICPQHKI -s 10.1.40.2/32 -m comment --comment "kube-system/kubernetes-dashboard:" -j KUBE-MARK-MASQ
</span><span class='line'>-A KUBE-SEP-XDHL2OHX2ICPQHKI -p tcp -m comment --comment "kube-system/kubernetes-dashboard:" -m tcp -j DNAT --to-destination 10.1.40.2:9090
</span><span class='line'>-A KUBE-SERVICES -d 10.0.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y
</span><span class='line'>-A KUBE-SERVICES -d 10.0.0.95/32 -p tcp -m comment --comment "kube-system/kubernetes-dashboard: cluster IP" -m tcp --dport 80 -j KUBE-SVC-XGLOHA7QRQ3V22RZ
</span><span class='line'>-A KUBE-SERVICES -d 10.0.0.10/32 -p udp -m comment --comment "kube-system/kube-dns:dns cluster IP" -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU
</span><span class='line'>-A KUBE-SERVICES -d 10.0.0.10/32 -p tcp -m comment --comment "kube-system/kube-dns:dns-tcp cluster IP" -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4
</span><span class='line'>-A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
</span><span class='line'>-A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment "kube-system/kube-dns:dns-tcp" -j KUBE-SEP-75CPIAPDB4MAVFWI
</span><span class='line'>-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment "default/kubernetes:https" -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-IWNPEB4T46P6VG5J --mask 255.255.255.255 --rsource -j KUBE-SEP-IWNPEB4T46P6VG5J
</span><span class='line'>-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment "default/kubernetes:https" -j KUBE-SEP-IWNPEB4T46P6VG5J
</span><span class='line'>-A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment "kube-system/kube-dns:dns" -j KUBE-SEP-UYUINV25NDNSKNUW
</span><span class='line'>-A KUBE-SVC-XGLOHA7QRQ3V22RZ -m comment --comment "kube-system/kubernetes-dashboard:" -j KUBE-SEP-XDHL2OHX2ICPQHKI</span></code></pre></td></tr></table></div></figure>


<p>在dockerd服务脚本加上 <code>--ip-masq=false</code> 后，<code>-A POSTROUTING -s 10.1.34.0/24 ! -o docker0 -j MASQUERADE</code> 这一句就没有了，也就是不会进行源地址重写了，这样请求发送到namenode后还是datanode容器的IP。问题解决，原因简单的让人欲哭无泪啊。</p>

<p>写yaml遇到的一些其他问题：</p>

<ul>
<li><a href="http://andykdocs.de/development/Docker/Fixing+the+Docker+TERM+variable+issue">Fixing the Docker TERM variable issue</a></li>
<li><a href="http://stackoverflow.com/questions/27195466/hdfs-datanode-denied-communication-with-namenode-because-hostname-cannot-be-reso">hdfs Datanode denied communication with namenode because hostname cannot be resolved</a></li>
</ul>


<p>当然还有很多其他的问题，这篇就写这么多，优化工作后面的弄好了再写。</p>

<h2>中间过程步骤记录</h2>

<p>主要就是记录心路历程，如果以后遇到同样的问题能让自己快速回想起来。如果仅仅为了部署，可以跳过该部分，直接后最后的常用命令。</p>

<p>记录下中间 <strong>通过yum安装etcd和flanneld</strong> 的过程。物理机安装flanneld会把配置docker环境变量（/run/flannel/subnet.env）加入启动脚本。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>安装docker-v1.12
</span><span class='line'>https://docs.docker.com/v1.12/
</span><span class='line'>https://docs.docker.com/v1.12/engine/installation/linux/centos/
</span><span class='line'>
</span><span class='line'># 删掉原来的
</span><span class='line'>yum-config-manager --disable docker-ce*
</span><span class='line'>yum remove -y docker-ce*
</span><span class='line'>
</span><span class='line'>sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'
</span><span class='line'>[dockerrepo]
</span><span class='line'>name=Docker Repository
</span><span class='line'>baseurl=https://yum.dockerproject.org/repo/main/centos/7/
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=1
</span><span class='line'>gpgkey=https://yum.dockerproject.org/gpg
</span><span class='line'>EOF
</span><span class='line'>
</span><span class='line'>https://yum.dockerproject.org/repo/main/centos/7/Packages/
</span><span class='line'>[root@cu3 ~]# yum --showduplicates list docker-engine | expand
</span><span class='line'>docker-engine.x86_64             1.12.6-1.el7.centos                  dockerrepo
</span><span class='line'>
</span><span class='line'>[root@cu3 yum.repos.d]# yum install docker-engine-1.12.6 docker-engine-selinux-1.12.6
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>https://kubernetes.io/docs/getting-started-guides/centos/centos_manual_config/
</span><span class='line'>
</span><span class='line'>cat &gt; /etc/yum.repos.d/virt7-docker-common-release.repo &lt;&lt;EOF
</span><span class='line'>[virt7-docker-common-release]
</span><span class='line'>name=virt7-docker-common-release
</span><span class='line'>baseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/
</span><span class='line'>gpgcheck=0
</span><span class='line'>EOF
</span><span class='line'>
</span><span class='line'>yum -y install --enablerepo=virt7-docker-common-release etcd flannel
</span><span class='line'>yum -y install --enablerepo=virt7-docker-common-release flannel
</span><span class='line'>
</span><span class='line'>- ETCD配置
</span><span class='line'>[root@cu3 docker-multinode]# 
</span><span class='line'>etcdctl mkdir /kube-centos/network
</span><span class='line'>etcdctl set /kube-centos/network/config "{ \"Network\": \"10.1.0.0/16\", \"SubnetLen\": 24, \"Backend\": { \"Type\": \"vxlan\" } }"
</span><span class='line'>
</span><span class='line'>- FlANNEL
</span><span class='line'>[root@cu3 ~]# cat /etc/sysconfig/flanneld
</span><span class='line'># Flanneld configuration options  
</span><span class='line'>
</span><span class='line'># etcd url location.  Point this to the server where etcd runs
</span><span class='line'>FLANNEL_ETCD_ENDPOINTS="http://cu3:2379"
</span><span class='line'>
</span><span class='line'># etcd config key.  This is the configuration key that flannel queries
</span><span class='line'># For address range assignment
</span><span class='line'>FLANNEL_ETCD_PREFIX="/kube-centos/network"
</span><span class='line'>
</span><span class='line'># Any additional options that you want to pass
</span><span class='line'>#FLANNEL_OPTIONS=""
</span><span class='line'>
</span><span class='line'>[root@cu2 yum.repos.d]# systemctl daemon-reload
</span><span class='line'>
</span><span class='line'>[root@cu2 yum.repos.d]# cat /run/flannel/subnet.env
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# systemctl cat docker
</span><span class='line'>...
</span><span class='line'># /usr/lib/systemd/system/docker.service.d/flannel.conf
</span><span class='line'>[Service]
</span><span class='line'>EnvironmentFile=-/run/flannel/docker </span></code></pre></td></tr></table></div></figure>


<p>测试过程中有yaml配置中启动sshd，然后启动容器后，通过手动启动namenode、datanode的方式来测试：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd hadoop-2.6.5
</span><span class='line'>gosu hadoop mkdir /data/bigdata
</span><span class='line'>gosu hadoop sbin/hadoop-daemon.sh start datanode 
</span><span class='line'>
</span><span class='line'>cd hadoop-2.6.5/
</span><span class='line'>gosu hadoop  bin/hadoop namenode -format 
</span><span class='line'>gosu hadoop sbin/hadoop-daemon.sh start namenode</span></code></pre></td></tr></table></div></figure>


<p>后来发现问题出在iptables后，又回到原来的docker-bootstrap启动，需要删除flannel.1的网络：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># yum安装flanneld后停止 https://kubernetes.io/docs/getting-started-guides/scratch/
</span><span class='line'>ip link set flannel.1 down
</span><span class='line'>ip link delete flannel.1
</span><span class='line'>route -n
</span><span class='line'>
</span><span class='line'>rm /usr/lib/systemd/system/docker.service.d/flannel.conf </span></code></pre></td></tr></table></div></figure>


<p>开了防火墙的话，把容器的端加入到信任列表：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>systemctl enable firewalld && systemctl start firewalld
</span><span class='line'>
</span><span class='line'>firewall-cmd --zone=trusted --add-source=10.0.0.0/8 --permanent 
</span><span class='line'>firewall-cmd --zone=trusted --add-source=192.168.0.0/16 --permanent 
</span><span class='line'>firewall-cmd --reload</span></code></pre></td></tr></table></div></figure>


<h2>一些有趣的命令</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>查看用了哪些镜像
</span><span class='line'>
</span><span class='line'>[root@cu2 /]# kubectl get pods --all-namespaces -o jsonpath="{..image}" |\
</span><span class='line'> tr -s '[[:space:]]' '\n' |\
</span><span class='line'> sort |\
</span><span class='line'> uniq -c
</span><span class='line'>      2 gcr.io/google_containers/dnsmasq-metrics-amd64:1.0
</span><span class='line'>      2 gcr.io/google_containers/exechealthz-amd64:1.2
</span><span class='line'>     12 gcr.io/google_containers/hyperkube-amd64:v1.5.5
</span><span class='line'>      2 gcr.io/google_containers/kube-addon-manager-amd64:v6.1
</span><span class='line'>      2 gcr.io/google_containers/kubedns-amd64:1.9
</span><span class='line'>      2 gcr.io/google_containers/kube-dnsmasq-amd64:1.4
</span><span class='line'>      2 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.0
</span><span class='line'>    
</span><span class='line'>      
</span><span class='line'>修改默认kubectl的配置
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# vi $KUBECONFIG 
</span><span class='line'>apiVersion: v1
</span><span class='line'>kind: Config
</span><span class='line'>preferences: {}
</span><span class='line'>current-context: default
</span><span class='line'>clusters:
</span><span class='line'>- cluster:
</span><span class='line'>    server: http://localhost:8080
</span><span class='line'>  name: default
</span><span class='line'>contexts:
</span><span class='line'>- context:
</span><span class='line'>    cluster: default
</span><span class='line'>    user: ""
</span><span class='line'>    namespace: kube-system
</span><span class='line'>  name: default
</span><span class='line'>users: {}
</span><span class='line'>
</span><span class='line'>如果kubectl没有下载，可以从镜像启动的容器里面获取
</span><span class='line'>
</span><span class='line'>[root@cu2 docker-multinode]# docker exec -ti 0c0360bcc2c3 bash
</span><span class='line'>root@cu2:/# cp kubectl /var/run/
</span><span class='line'>
</span><span class='line'>[root@cu2 run]# mv kubectl /data/kubernetes/kube-deploy/docker-multinode/
</span><span class='line'>
</span><span class='line'>获取容器IP
</span><span class='line'>
</span><span class='line'>https://kubernetes.io/docs/user-guide/jsonpath/
</span><span class='line'>[root@cu2 ~]# kubectl get pods -o wide -l run=redis -o jsonpath={..podIP}
</span><span class='line'>10.1.75.2 10.1.75.3 10.1.58.3 10.1.58.2 10.1.33.3
</span><span class='line'>
</span><span class='line'>网络共用: --net
</span><span class='line'>
</span><span class='line'>docker run -ti --entrypoint=sh --net=container:8e9f21956469f4ef7e5b9d91798788ab83f380795d2825cdacae0ed28f5ba03b gcr.io/google_containers/skydns-amd64:1.0
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>格式化输出
</span><span class='line'>
</span><span class='line'>kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}"  
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# export POD_COL="custom-columns=NAME:.metadata.name,RESTARTS:.status.containerStatuses[*].restartCount,CONTAINERS:.spec.containers[*].name,IP:.status.podIP,HOST:.spec.nodeName"
</span><span class='line'>[root@cu2 ~]# kubectl get pods -o $POD_COL 
</span><span class='line'>
</span><span class='line'>kubectl get po -l k8s-app=kube-dns -o=custom-columns=NAME:.metadata.name,CONTAINERS:.spec.containers[*].name
</span><span class='line'>
</span><span class='line'>[root@cu2 kubernetes]# kubectl get po --all-namespaces -o=custom-columns=NAME:.metadata.name,CONTAINERS:.spec.containers[*].name
</span><span class='line'>
</span><span class='line'>kubectl get po --all-namespaces {range .items[*]}{.metadata.name}{“\t”}{end}
</span><span class='line'>
</span><span class='line'>备份
</span><span class='line'>
</span><span class='line'>echo "$(docker ps  | grep -v IMAGE | awk '{print $2}' )
</span><span class='line'>$(docker-bs ps | grep -v IMAGE | awk '{print $2}' )" | sort -u | while read image ; do docker save $image&gt;$(echo $image | tr '[/:]' _).tar ; done
</span><span class='line'>
</span><span class='line'>加Label
</span><span class='line'>
</span><span class='line'>cat /etc/hosts | grep -E "\scu[0-9]\s" | awk '{print "kubectl label nodes "$1" hostname="$2}' | while read line ; do sh -c "$line" ; done
</span><span class='line'>
</span><span class='line'>扩容
</span><span class='line'>
</span><span class='line'>[root@cu2 kubernetes]# kubectl run redis --image=redis:3.2.8 
</span><span class='line'>[root@cu2 kubernetes]# kubectl scale --replicas=9 deployment/redis
</span><span class='line'>
</span><span class='line'> echo " $( kubectl describe pods hadoop-master2 | grep -E "Node|Container ID" | awk -F/ '{print $NF}' | tr '\n' ' ' | awk '{print "ssh "$1" \rdocker exec -ti "$2" bash"}' ) "
</span><span class='line'> </span></code></pre></td></tr></table></div></figure>


<p>测试DNS是否成功：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 kube-deploy]# vi busybox.yaml
</span><span class='line'>apiVersion: v1
</span><span class='line'>kind: Pod
</span><span class='line'>metadata:
</span><span class='line'>  name: busybox
</span><span class='line'>  namespace: default
</span><span class='line'>spec:
</span><span class='line'>  containers:
</span><span class='line'>  - image: busybox
</span><span class='line'>    command:
</span><span class='line'>      - sleep
</span><span class='line'>      - "3600"
</span><span class='line'>    imagePullPolicy: IfNotPresent
</span><span class='line'>    name: busybox
</span><span class='line'>  restartPolicy: Always
</span><span class='line'>
</span><span class='line'>[root@cu3 kube-deploy]# kubectl create -f busybox.yaml 
</span><span class='line'>pod "busybox" created
</span><span class='line'>[root@cu3 kube-deploy]# kubectl get pods 
</span><span class='line'>NAME      READY     STATUS              RESTARTS   AGE
</span><span class='line'>busybox   0/1       ContainerCreating   0          11s
</span><span class='line'>[root@cu3 kube-deploy]# kubectl get pods 
</span><span class='line'>NAME      READY     STATUS    RESTARTS   AGE
</span><span class='line'>busybox   1/1       Running   0          1m
</span><span class='line'>[root@cu3 kube-deploy]# kubectl exec -ti busybox -- nslookup kubernetes.default
</span><span class='line'>Server:    10.0.0.10
</span><span class='line'>Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local
</span><span class='line'>
</span><span class='line'>Name:      kubernetes.default
</span><span class='line'>Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local
</span><span class='line'>
</span><span class='line'>用容器的MYSQL的做客户端
</span><span class='line'>
</span><span class='line'>kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h mysql -ppassword
</span></code></pre></td></tr></table></div></figure>


<p>小结一点：日志的重要性！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 kubenetes]# docker ps -a | grep kubelet
</span><span class='line'>[root@cu2 kubenetes]# docker logs --tail=200 7432da457558
</span><span class='line'>
</span><span class='line'>E0417 11:39:40.194844   22528 configmap.go:174] Couldn't get configMap hadoop/dta-hadoop-config: configmaps "dta-hadoop-config" not found
</span><span class='line'>E0417 11:39:40.194910   22528 configmap.go:174] Couldn't get configMap hadoop/dta-bin-config: configmaps "dta-bin-config" not found
</span></code></pre></td></tr></table></div></figure>


<p>监控heapster的一些错误，还没调好</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# kubectl exec -ti heapster-564189836-shn2q -n kube-system -- sh
</span><span class='line'>/ # 
</span><span class='line'>/ # 
</span><span class='line'>没pod的数据
</span><span class='line'>/ # /heapster --source=https://kubernetes.default --sink=log --heapster-port=8083 -v 10
</span><span class='line'>
</span><span class='line'>E0329 10:11:53.823641       1 reflector.go:203] k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:100: Failed to list *api.Node: Get https://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>$heapster/metrics
</span><span class='line'>$heapster/api/v1/model/debug/allkeys
</span></code></pre></td></tr></table></div></figure>


<p>其他一些配置</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>other_args=" --registry-mirror=https://docker.mirrors.ustc.edu.cn "
</span><span class='line'>
</span><span class='line'>--insecure-registry gcr.io 
</span><span class='line'>
</span><span class='line'>iptables -S -t nat
</span></code></pre></td></tr></table></div></figure>


<h2>其他一些资源</h2>

<ul>
<li><a href="https://kubernetes.io/docs/concepts/cluster-administration/resource-usage-monitoring/">https://kubernetes.io/docs/concepts/cluster-administration/resource-usage-monitoring/</a></li>
<li><p><a href="https://github.com/kubernetes/heapster/tree/v1.3.0/deploy/kube-config/influxdb">https://github.com/kubernetes/heapster/tree/v1.3.0/deploy/kube-config/influxdb</a></p></li>
<li><p><a href="https://github.com/kubernetes/heapster/blob/master/docs/debugging.md">https://github.com/kubernetes/heapster/blob/master/docs/debugging.md</a></p></li>
<li><p><a href="https://docs.docker.com/v1.12/engine/installation/linux/centos/">https://docs.docker.com/v1.12/engine/installation/linux/centos/</a></p></li>
<li><p><a href="https://github.com/CodisLabs/codis/blob/release3.2/Dockerfile">https://github.com/CodisLabs/codis/blob/release3.2/Dockerfile</a></p></li>
<li><a href="https://github.com/sporkmonger/redis-k8s/blob/master/redis.yaml">https://github.com/sporkmonger/redis-k8s/blob/master/redis.yaml</a></li>
<li><a href="https://github.com/sobotklp/kubernetes-redis-cluster/blob/master/redis-cluster.yml">https://github.com/sobotklp/kubernetes-redis-cluster/blob/master/redis-cluster.yml</a></li>
</ul>


<p>statefulset</p>

<ul>
<li><a href="https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/">https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/stateful-application/run-stateful-application/">https://kubernetes.io/docs/tutorials/stateful-application/run-stateful-application/</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/stateful-application/run-replicated-stateful-application/">https://kubernetes.io/docs/tutorials/stateful-application/run-replicated-stateful-application/</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/11">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/9">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2019/04/10/try-k8s/">Try K8s</a>
			</li>
		
			<li class="post">
				<a href="/blog/2018/08/25/video-auto-translate/">视频自动翻译</a>
			</li>
		
			<li class="post">
				<a href="/blog/2018/06/09/reasonable-way-to-access-the-internet/">科学上网（续）</a>
			</li>
		
			<li class="post">
				<a href="/blog/2018/01/20/gitalk-on-octopress/">Gitalk on Octopress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/11/16/sphinx-generate-docs/">使用Sphinx生成/管理文档</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/10/30/windows-run-ubuntu/">Windows Run Ubuntu</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/07/30/kubeadm-install-kubenetes-on-centos7/">Kubeadm部署kubernetes</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/07/08/casperjs-crawler/">爬虫之CasperJS</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/05/23/spark-on-hive-speculation-shit-bug/">Hive on Spark预测性执行BUG一枚</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/01/27/vnc-server-on-centos7/">在Centos7上安装VNC Server</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/01/25/develop-environment-prepare/">[整理] 环境准备工具集</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/01/19/nginx-https/">Nginx配置https</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/09/19/163-open-movies-download/">批量下载163-open的视频</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/04/23/hadoop-guide-catalog/">[整理] Hadoop入门</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/04/04/rpm-build-your-package/">RPM打包</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/11/22/gfw-ladder/">搭梯笔记</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/02/23/quickly-open-program-in-windows/">[Windows运行]快速打开程序</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2021/09/15/k2-again/">斐讯K2刷机padavan</a>
      </li>
    
      <li class="post">
        <a href="/blog/2020/05/11/redmine-on-arm-pi/">在树莓派上部署redmine</a>
      </li>
    
      <li class="post">
        <a href="/blog/2020/04/12/appium-android-auto-test/">appium-Android自动化测试</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/07/26/android-linux-via-termux/">Android Linux via Termux</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/04/10/try-bk-dot-tencent-dot-com/">Try bk.tencent.com</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/04/10/try-k8s/">Try K8s</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/10/20/jcef-build-on-win64/">编译JCEF - Win64</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/08/25/video-auto-translate/">视频自动翻译</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/alluxio/'>alluxio</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/bigdata/'>bigdata</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/deprecated/'>deprecated</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/devops/'>devops</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (15) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/elasticsearch/'>elasticsearch</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/es/'>es</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/flume/'>flume</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (5) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (44) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (8) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (13) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (8) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jenkins/'>jenkins</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/k2/'>k2</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/k8s/'>k8s</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kubeadm/'>kubeadm</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/logstash/'>logstash</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/map/'>map</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/nginx/'>nginx</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/puppet/'>puppet</a> (11) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (27) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/staf/'>staf</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (69) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/vagrant/'>vagrant</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/wsl/'>wsl</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/zookeeper/'>zookeeper</a> (1) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (217)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
<!--
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
&#8211;>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2021 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

<script>

var time=location.pathname.substring(6).substring(0,11);
var eName=location.pathname.substring(17);
var gitalk = new Gitalk({
  clientID: 'c14f68eac6330d15d984',
  clientSecret: '73b7c1fffa98e299ff0cdd332821201933858e6e',
  repo: 'winse.github.com',
  owner: 'winse',
  admin: ['winse'],
  id: eName,
  labels: ['Gitalk', time],
  body: "http://winseliu.com" + location.pathname,
  createIssueManually: true,
  
  // facebook-like distraction free mode
  distractionFreeMode: false
})

gitalk.render('gitalk-container')

</script>



<script>
/*
$.ajax({
  type: "POST",
  url: "http://log.winseliu.com:20000",
  data: JSON.stringify({
    title: document.title,
    location: JSON.stringify(location),
    referrer: document.referrer,
    userAgent: navigator.userAgent
  }),
  contentType: "application/json; charset=utf-8",
  dataType: "json"
});
*/
</script>









</body>
</html>
