
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="flume-1.6依赖的kafka、elasticsearch的版本与我这使用程序的版本不一致，部分jar依赖需要替换，flume-elasticsearch-sink源码需要进行一些修改来适配elasticsearch-2.2。 flume-1.6.0
kafka_2.11-0.9.0.1( &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/2">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
  <li><a href="https://yunpan.cn/cuYhpFBPgQYgT" >Books[5aee]</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/06/28/flume-kafka-elasticsearch-for-analyse/">使用 Flume+kafka+elasticsearch 处理数据</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-06-28T09:50:05+08:00" pubdate data-updated="true">Tue 2016-06-28 09:50</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>flume-1.6依赖的kafka、elasticsearch的版本与我这使用程序的版本不一致，部分jar依赖需要替换，flume-elasticsearch-sink源码需要进行一些修改来适配elasticsearch-2.2。</p>

<ul>
<li>flume-1.6.0</li>
<li>kafka_2.11-0.9.0.1(可以与0.8.2客户端通信, flume-kafka-channel-1.6.0不改)</li>
<li>elasticsearch-2.2.0</li>
</ul>


<p>由于版本的差异，需要替换/添加以下jar到 <code>flume/lib</code> 下：</p>

<p>使用 <code>mvn dependecy:copy-dependencies</code> 导出所需依赖的包</p>

<p>jackson一堆，hppc-0.7.1.jar，t-digest-3.0.jar，jsr166e-1.1.0.jar，guava-18.0.jar，lucene一堆，elasticsearch-2.2.0.jar。</p>

<p>远程调试配置：</p>

<p>source由于项目上的一些特殊规则，需要自己编写。通过远程DEBUG来打断点来排查BUG。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 flume]$ vi conf/flume-env.sh
</span><span class='line'>export JAVA_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8092"</span></code></pre></td></tr></table></div></figure>


<h2>实战1</h2>

<p>KafkaChannel：考虑到其他功能也需要用到这些数据。</p>

<p>先写一个配置把flume自带功能跑通，这里用 netcat 作为输入运行：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 flumebin]$ cat dta.flume 
</span><span class='line'>dta.sources=s1
</span><span class='line'>dta.channels=c1
</span><span class='line'>dta.sinks=k1
</span><span class='line'>
</span><span class='line'>dta.channels.c1.type=org.apache.flume.channel.kafka.KafkaChannel
</span><span class='line'>dta.channels.c1.capacity=10000
</span><span class='line'>dta.channels.c1.transactionCapacity=1000
</span><span class='line'>dta.channels.c1.brokerList=cu5:9093
</span><span class='line'>dta.channels.c1.topic=flume_cmdid_1234
</span><span class='line'>dta.channels.c1.groupId=flume_dta
</span><span class='line'>dta.channels.c1.zookeeperConnect=cu3:2181/kafka_0_9
</span><span class='line'>dta.channels.c1.parseAsFlumeEvent=false
</span><span class='line'>
</span><span class='line'>dta.sources.s1.channels=c1
</span><span class='line'>dta.sources.s1.type=netcat
</span><span class='line'>dta.sources.s1.bind=0.0.0.0
</span><span class='line'>dta.sources.s1.port=6666
</span><span class='line'>dta.sources.s1.max-line-length=88888888
</span><span class='line'>
</span><span class='line'>dta.sinks.k1.channel=c1
</span><span class='line'>dta.sinks.k1.type=elasticsearch
</span><span class='line'>dta.sinks.k1.hostNames=cu2:9300
</span><span class='line'>dta.sinks.k1.indexName=foo_index
</span><span class='line'>dta.sinks.k1.indexType=idcisp
</span><span class='line'>dta.sinks.k1.clusterName=eshore-cu
</span><span class='line'>dta.sinks.k1.batchSize=500
</span><span class='line'>dta.sinks.k1.ttl=5d
</span><span class='line'>dta.sinks.k1.serializer=com.eshore.zhfx.collector.InfoSecurityLogIndexRequestBuilderFactory
</span><span class='line'>dta.sinks.k1.serializer.idcispUrlBase64=true
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 flumebin]$ bin/flume-ng agent --classpath flume-dta-source-2.1.jar  -n dta -c conf -f dta.flume
</span><span class='line'>
</span><span class='line'># 新开一个窗口
</span><span class='line'>[hadoop@cu2 ~]$ nc localhost 6666
</span></code></pre></td></tr></table></div></figure>


<p>kafka的主题、ES的索引可以不要手动建，当然为了更好的控制ES索引创建可以添加一个索引名的template。</p>

<p>InfoSecurityLogIndexRequestBuilderFactory 实现 ElasticSearchIndexRequestBuilderFactory 把原始记录转换成 ES 的JSON对象。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private Counter allRecordMetric = MetricManager.getInstance().counter("all_infosecurity");
</span><span class='line'>  private Counter errorRecordMetric = MetricManager.getInstance().counter("error_infosecurity");
</span><span class='line'>  
</span><span class='line'>  public IndexRequestBuilder createIndexRequest(Client client, String indexPrefix, String indexType, Event event)
</span><span class='line'>      throws IOException {
</span><span class='line'>    allRecordMetric.inc();
</span><span class='line'>
</span><span class='line'>    String record = new String(event.getBody(), outputCharset);
</span><span class='line'>
</span><span class='line'>    context.put(ElasticSearchSinkConstants.INDEX_NAME, indexPrefix);
</span><span class='line'>    indexNameBuilder.configure(context);
</span><span class='line'>    IndexRequestBuilder indexRequestBuilder = client.prepareIndex(indexNameBuilder.getIndexName(event), indexType);
</span><span class='line'>
</span><span class='line'>    try {
</span><span class='line'>      Gson gson = new Gson();
</span><span class='line'>      IdcIspLog log = parseRecord(record);
</span><span class='line'>      BytesArray data = new BytesArray(gson.toJson(log));
</span><span class='line'>
</span><span class='line'>      indexRequestBuilder.setSource(data);
</span><span class='line'>      indexRequestBuilder.setRouting(log.commandld);
</span><span class='line'>    } catch (Exception e) {
</span><span class='line'>      LOG.error(e.getMessage(), e);
</span><span class='line'>      errorRecordMetric.inc();
</span><span class='line'>
</span><span class='line'>      indexRequestBuilder.setSource(record.getBytes(outputCharset));
</span><span class='line'>      // 保留错误的数据
</span><span class='line'>      indexRequestBuilder.setRouting("error");
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    return indexRequestBuilder;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<h2>实战2</h2>

<p>测试自定义的Source：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>dta.sources=s1
</span><span class='line'>dta.channels=c1
</span><span class='line'>dta.sinks=k1
</span><span class='line'>
</span><span class='line'>dta.channels.c1.type=memory
</span><span class='line'>dta.channels.c1.capacity=1000000
</span><span class='line'>dta.channels.c1.transactionCapacity=1000000
</span><span class='line'>dta.channels.c1.byteCapacity=7000000000
</span><span class='line'>
</span><span class='line'>dta.sources.s1.channels=c1
</span><span class='line'>dta.sources.s1.type=com.eshore.zhfx.collector.CollectSource
</span><span class='line'>dta.sources.s1.spoolDir=/home/hadoop/flume/data/
</span><span class='line'>dta.sources.s1.trackerDir=/tmp/dtaspool
</span><span class='line'>
</span><span class='line'>dta.sinks.k1.channel=c1
</span><span class='line'>dta.sinks.k1.type=logger</span></code></pre></td></tr></table></div></figure>


<p>CollectSource 实现PollableSource 继承AbstractSource类。参考Flume开发文档: <a href="http://flume.apache.org/FlumeDeveloperGuide.html#source">http://flume.apache.org/FlumeDeveloperGuide.html#source</a>  <code>org.apache.flume.source.SequenceGeneratorSource</code> 类。</p>

<p>方法process主逻辑代码如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public Status process() throws EventDeliveryException {
</span><span class='line'>    Status status = Status.READY;
</span><span class='line'>
</span><span class='line'>    try {
</span><span class='line'>      List&lt;Event&gt; events = readEvent(batchSize);
</span><span class='line'>      if (!events.isEmpty()) {
</span><span class='line'>        sourceCounter.addToEventReceivedCount(events.size());
</span><span class='line'>        sourceCounter.incrementAppendBatchReceivedCount();
</span><span class='line'>
</span><span class='line'>        getChannelProcessor().processEventBatch(events);
</span><span class='line'>        // 记录文件已经处理的位置
</span><span class='line'>        commit();
</span><span class='line'>
</span><span class='line'>        sourceCounter.addToEventAcceptedCount(events.size());
</span><span class='line'>        sourceCounter.incrementAppendBatchAcceptedCount();
</span><span class='line'>      }
</span><span class='line'>    } catch (ChannelException | IOException e) {
</span><span class='line'>      status = Status.BACKOFF;
</span><span class='line'>      Throwables.propagate(e);
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    return status;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<h2>实例：Flume+Kafka+ES</h2>

<p>把两个实例整合起来，把实例1的Source替换下即可。</p>

<h2>附-kafka基本操作</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu5 kafka_2.11-0.9.0.1]$ bin/kafka-server-start.sh config/server1.properties 
</span><span class='line'>
</span><span class='line'>[hadoop@cu5 kafka_2.11-0.9.0.1]$ cat config/server1.properties 
</span><span class='line'>listeners=PLAINTEXT://:9093
</span><span class='line'>log.dirs=/tmp/kafka-logs1
</span><span class='line'>num.partitions=1
</span><span class='line'>zookeeper.connect=cu3,cu4,cu5/kafka_0_9
</span><span class='line'>
</span><span class='line'>[hadoop@cu5 kafka_2.11-0.9.0.1]$ bin/kafka-topics.sh --create --zookeeper cu3:2181/kafka_0_9 --replication 1 --partitions 1 --topic flume
</span><span class='line'>Created topic "flume".
</span><span class='line'>
</span><span class='line'>[hadoop@cu5 kafka_2.11-0.9.0.1]$ bin/kafka-topics.sh --list --zookeeper cu3:2181/kafka_0_9
</span><span class='line'>flume
</span><span class='line'>
</span><span class='line'>[hadoop@cu5 kafka_2.11-0.9.0.1]$ bin/kafka-console-producer.sh --broker-list cu5:9093 --topic flume
</span><span class='line'>
</span><span class='line'>[hadoop@cu5 kafka_2.11-0.9.0.1]$ bin/kafka-console-consumer.sh --zookeeper cu3:2181/kafka_0_9 --topic flume --from-beginning
</span></code></pre></td></tr></table></div></figure>


<h2>附-Flume操作</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>https://flume.apache.org/FlumeUserGuide.html#fan-out-flow
</span><span class='line'>
</span><span class='line'>export FLUME_JAVA_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8092"
</span><span class='line'>bin/flume-ng agent --classpath "flume-dta-libs/*" -Dflume.root.logger=DEBUG,console  -n dta -c conf -f accesslog.flume
</span><span class='line'>
</span><span class='line'># with ganglia
</span><span class='line'>[ud@cu-ud1 apache-flume-1.6.0-bin]$ bin/flume-ng agent --classpath "/home/ud/collector/common-lib/*"  -Dflume.root.logger=Debug,console -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=239.2.11.71:8649 -n dta -c conf -f accesslog.flume 
</span><span class='line'>
</span><span class='line'># windows
</span><span class='line'>bin\flume-ng.cmd agent -n agent -c conf -f helloworld.flume -property "flume.root.logger=INFO,console"</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/06/17/ganglia-install-on-centos-with-puppet/">使用Puppet安装配置Ganglia</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-06-17T09:30:50+08:00" pubdate data-updated="true">Fri 2016-06-17 09:30</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>前面写过完全纯手工和用yum安装依赖来安装ganglia的文章，最近生产安装了puppet，既然已经手上已有牛杀鸡就不用再取菜刀了。今天记录下前几天使用puppet安装ganglia的经历。</p>

<h2>前提（自己操作过）</h2>

<ul>
<li>配置过私有仓库 (createrepo)</li>
<li>安装好puppet</li>
<li>编译过自己的rpm (rpmbuild)</li>
</ul>


<h2>编译gmetad，gmond，gweb</h2>

<p>点击链接下载SPEC：</p>

<ul>
<li><a href="/files/ganglia-puppet/gmetad.spec">gmetad.spec</a></li>
<li><a href="/files/ganglia-puppet/gmond.spec">gmond.spec</a></li>
<li><a href="/files/ganglia-puppet/gweb.spec">gweb.spec</a></li>
</ul>


<p>然后编译打包：</p>

<p>先手动编译安装 ganglia ，把依赖的问题处理好。然后再使用 rpmbuild 编译生成 rpm ！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 1&gt; 建立目录结构
</span><span class='line'>mkdir ganglia-build
</span><span class='line'>cd ganglia-build
</span><span class='line'>mkdir BUILD RPMS SOURCES SPECS SRPMS
</span><span class='line'>
</span><span class='line'># 2&gt; 修改配置
</span><span class='line'># ganglia-web-3.7.1.tar.gz的makefile、conf_default.php.in修改下，根据等下要配置gmetad的参数进行修改
</span><span class='line'>
</span><span class='line'>less ganglia-web-3.7.1/Makefile 
</span><span class='line'>  # Location where gweb should be installed to (excluding conf, dwoo dirs).
</span><span class='line'>  GDESTDIR = /var/www/html/ganglia
</span><span class='line'>
</span><span class='line'>  # Location where default apache configuration should be installed to.
</span><span class='line'>  GCONFDIR = /usr/local/ganglia/etc/
</span><span class='line'>
</span><span class='line'>  # Gweb statedir (where conf dir and Dwoo templates dir are stored)
</span><span class='line'>  GWEB_STATEDIR = /var/www/html/ganglia
</span><span class='line'>
</span><span class='line'>  # Gmetad rootdir (parent location of rrd folder)
</span><span class='line'>  GMETAD_ROOTDIR = /data/ganglia
</span><span class='line'>
</span><span class='line'>  APACHE_USER = apache
</span><span class='line'>
</span><span class='line'># 连外网太慢，下载放到本地
</span><span class='line'>less ganglia-web-3.7.1/conf_default.php.in 
</span><span class='line'>  #$conf['cubism_js_path'] = "js/cubism.v1.min.js";
</span><span class='line'>  $conf['jquery_js_path'] = "js/jquery.min.js";
</span><span class='line'>  $conf['jquerymobile_js_path'] = "js/jquery.mobile.min.js";
</span><span class='line'>  $conf['jqueryui_js_path'] = "js/jquery-ui.min.js";
</span><span class='line'>  $conf['rickshaw_js_path'] = "js/rickshaw.min.js";
</span><span class='line'>  $conf['cubism_js_path'] = "js/cubism.v1.min.js";
</span><span class='line'>  $conf['d3_js_path'] = "js/d3.min.js";
</span><span class='line'>  $conf['protovis_js_path'] = "js/protovis.min.js";
</span><span class='line'>
</span><span class='line'># 3&gt; 源文件
</span><span class='line'># 把文件放到SOURCES目录下，
</span><span class='line'>ls SOURCES/
</span><span class='line'>  ganglia-3.7.2.tar.gz  ganglia-web-3.7.1.tar.gz
</span><span class='line'>
</span><span class='line'># 4&gt; 编译生成RPM
</span><span class='line'>rpmbuild -v -ba SPECS/gmetad.spec 
</span><span class='line'>rpmbuild -v -ba SPECS/gmond.spec 
</span><span class='line'>rpmbuild -v -ba SPECS/gweb.spec 
</span><span class='line'>
</span><span class='line'># 5&gt; 查看内容
</span><span class='line'>rpm -qpl RPMS/x86_64/ganglia-3.7.2-1.el6.x86_64.rpm </span></code></pre></td></tr></table></div></figure>


<h2>本地仓库</h2>

<p>这里假设已经把系统光盘做成了本地仓库。</p>

<p><strong>先安装httpd、php、createrepo</strong>，然后按照下面的步骤创建本地仓库：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 系统带的可以从光盘拷贝，直接映射到httpd的目录下即可
</span><span class='line'>[hadoop@hadoop-master1 rhel6.3]$ ls 
</span><span class='line'>Packages  repodata
</span><span class='line'>[hadoop@hadoop-master1 html]$ pwd
</span><span class='line'>/var/www/html
</span><span class='line'>[hadoop@hadoop-master1 html]$ ll
</span><span class='line'>lrwxrwxrwx.  1 root root   20 2月  15 2014 rhel6.3 -&gt; /opt/rhel6.3
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ sudo mkdir -p /opt/dta/repo
</span><span class='line'>[hadoop@hadoop-master1 ~]$ cd /opt/dta/repo
</span><span class='line'>[hadoop@hadoop-master1 repo]$ ls *.rpm
</span><span class='line'>gmetad-3.7.2-1.el6.x86_64.rpm  gmond-3.7.2-1.el6.x86_64.rpm  gweb-3.7.1-1.el6.x86_64.rpm  libconfuse-2.7-4.el6.x86_64.rpm
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 repo]$ sudo createrepo .
</span><span class='line'>3/3 - libconfuse-2.7-4.el6.x86_64.rpm                                           
</span><span class='line'>Saving Primary metadata
</span><span class='line'>Saving file lists metadata
</span><span class='line'>Saving other metadata
</span><span class='line'>
</span><span class='line'># 映射到httpd目录下
</span><span class='line'>[hadoop@hadoop-master1 yum.repos.d]$ cd /var/www/html/
</span><span class='line'>[hadoop@hadoop-master1 html]$ sudo ln -s /opt/dta/repo dta
</span><span class='line'>
</span><span class='line'># 加入本地仓库源
</span><span class='line'>[hadoop@hadoop-master1 yum.repos.d]$ sudo cp puppet.repo dta.repo
</span><span class='line'>[hadoop@hadoop-master1 yum.repos.d]$ sudo vi dta.repo 
</span><span class='line'>[dta]
</span><span class='line'>name=DTA Local
</span><span class='line'>baseurl=http://hadoop-master1:801/dta
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span></code></pre></td></tr></table></div></figure>


<p>注意： 在安装的时刻找不到gmond，可以先清理yum的缓冲： <code>yum clean all</code></p>

<h2>puppet模块</h2>

<p>添加了三个模块，用于主机添加repo配置和sudo配置，以及安装配置gmond。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master1 modules]# tree $PWD
</span><span class='line'>/etc/puppetlabs/code/environments/production/modules
</span><span class='line'>├── dtarepo
</span><span class='line'>│   ├── manifests
</span><span class='line'>│   │   └── init.pp
</span><span class='line'>│   └── templates
</span><span class='line'>│       └── dta.repo
</span><span class='line'>├── gmond
</span><span class='line'>│   ├── manifests
</span><span class='line'>│   │   └── init.pp
</span><span class='line'>│   └── templates
</span><span class='line'>│       └── gmond.conf
</span><span class='line'>└── sudo
</span><span class='line'>    ├── manifests
</span><span class='line'>    │   └── init.pp
</span><span class='line'>    └── templates
</span><span class='line'>        └── sudo.erb</span></code></pre></td></tr></table></div></figure>


<p>都比较简单，通过init.pp来进行配置，然后加载模板，写入到同步主机本地文件中。</p>

<ul>
<li>dtarepo</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./dtarepo/manifests/init.pp
</span><span class='line'>class dtarepo {
</span><span class='line'>
</span><span class='line'>file{'/etc/yum.repos.d/dta.repo':
</span><span class='line'>  ensure =&gt; file,
</span><span class='line'>  content =&gt; template('dtarepo/dta.repo'),
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>./dtarepo/templates/dta.repo
</span><span class='line'>[dta]
</span><span class='line'>name=DTA Local
</span><span class='line'>baseurl=http://hadoop-master1:801/dta
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0</span></code></pre></td></tr></table></div></figure>


<ul>
<li>sudo</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./sudo/manifests/init.pp
</span><span class='line'>class sudo {
</span><span class='line'>
</span><span class='line'>if ( $::hostname =~ /(^cu-omc)/ ) {
</span><span class='line'>  $user = 'omc'
</span><span class='line'>} elsif ( $::hostname =~ /(^cu-uc)/ ) {
</span><span class='line'>  $user = 'uc'
</span><span class='line'>} elsif ( $::hostname =~ /(^cu-ud)/ ) {
</span><span class='line'>  $user = 'ud'
</span><span class='line'>} elsif ( $::hostname =~ /(^cu-db)/ ) {
</span><span class='line'>  $user = 'mysql'
</span><span class='line'>} else {
</span><span class='line'>  $user = 'hadoop'
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>file { "/etc/sudoers.d/10_$user":
</span><span class='line'>  ensure =&gt; file,
</span><span class='line'>  mode =&gt; '0440', 
</span><span class='line'>  content =&gt; template('sudo/sudo.erb'),
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>./sudo/templates/sudo.erb
</span><span class='line'>&lt;%= scope.lookupvar('sudo::user') %&gt; ALL=(ALL) NOPASSWD: ALL</span></code></pre></td></tr></table></div></figure>


<ul>
<li>gmond</li>
</ul>


<p>在默认的gmond.conf基础上修改一下两个配置: globals.deaf, cluster.name</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./gmond/manifests/init.pp
</span><span class='line'>class gmond {
</span><span class='line'>
</span><span class='line'>$deaf = $::hostname ? {
</span><span class='line'>  'hadoop-master1' =&gt; 'no',
</span><span class='line'>  'cu-omc1' =&gt; 'no',
</span><span class='line'>  default =&gt; 'yes',
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>if ( $::hostname =~ /(^cu-)/ ) {
</span><span class='line'>  $cluster_name = 'CU'
</span><span class='line'>} else {
</span><span class='line'>  $cluster_name = 'HADOOP'
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>package { 'gmond':
</span><span class='line'>  ensure =&gt; present,
</span><span class='line'>  before =&gt; File['/usr/local/ganglia/etc/gmond.conf'],
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>file { '/usr/local/ganglia/etc/gmond.conf':
</span><span class='line'>  ensure =&gt; file,
</span><span class='line'>  content =&gt; template('gmond/gmond.conf'),
</span><span class='line'>  notify =&gt; Service['gmond'],
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>service { 'gmond':
</span><span class='line'>  ensure    =&gt; running,
</span><span class='line'>  enable    =&gt; true,
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>./gmond/templates/gmond.conf
</span><span class='line'>/* This configuration is as close to 2.5.x default behavior as possible
</span><span class='line'>   The values closely match ./gmond/metric.h definitions in 2.5.x */
</span><span class='line'>globals {
</span><span class='line'>...
</span><span class='line'>  mute = no
</span><span class='line'>  deaf = &lt;%= scope.lookupvar('gmond::deaf') %&gt;
</span><span class='line'>  allow_extra_data = yes
</span><span class='line'>...
</span><span class='line'>cluster {
</span><span class='line'>  name = "&lt;%= scope.lookupvar('gmond::cluster_name') %&gt;"</span></code></pre></td></tr></table></div></figure>


<p>参考下逻辑即可（也可以通过hiera配置）。</p>

<p>最后在 site.pp 引用加载编写的Module：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master1 modules]# cd ../manifests/
</span><span class='line'>[root@hadoop-master1 manifests]# cat site.pp 
</span><span class='line'>file{'/etc/puppetlabs/mcollective/facts.yaml':
</span><span class='line'>  owner    =&gt; root,
</span><span class='line'>  group    =&gt; root,
</span><span class='line'>  mode     =&gt; '400',
</span><span class='line'>  loglevel =&gt; debug, # reduce noise in Puppet reports
</span><span class='line'>  content  =&gt; inline_template("&lt;%= scope.to_hash.reject { |k,v| k.to_s =~ /(uptime_seconds|timestamp|free)/ }.to_yaml %&gt;"), # exclude rapidly changing facts
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>include dtarepo
</span><span class='line'>include gmond
</span><span class='line'>
</span><span class='line'># include sudo
</span></code></pre></td></tr></table></div></figure>


<h2>一键安装</h2>

<p>安装gmetad：</p>

<p>首先在主机上安装gmetad，由于只需要在一台机器安装，配置没有整成模板，这里直接手动弄。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master1 dtarepo]# mco rpc package install package=gmetad -I cu-omc1
</span><span class='line'>
</span><span class='line'># 注意：主机多网卡时可能需要添加route
</span><span class='line'>[root@cu-omc1 ~]# route add -host 239.2.11.71 dev bond0
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 ~]# /etc/ganglia/gmetad.conf 注意!! 这里的rrd_rootdir配置与上面gweb/makefile是对应的！！
</span><span class='line'>data_source "HADOOP" hadoop-master1
</span><span class='line'>data_source "CU" cu-omc1
</span><span class='line'>gridname "CQCU"
</span><span class='line'>rrd_rootdir "/data/ganglia/rrds"
</span></code></pre></td></tr></table></div></figure>


<p>安装gmond：</p>

<p>在cu-omc2上安装gmond（正则表达式，想怎么匹配就怎么写）：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master1 production]# mco shell -I /^cu-omc2/ run -- "/opt/puppetlabs/bin/puppet agent -t"</span></code></pre></td></tr></table></div></figure>


<p>puppet同步好后，就安装好puppet，以及启动gmond服务。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/06/15/elasticsearch-startguide/">Elasticsearch Startguide</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-06-15T08:40:28+08:00" pubdate data-updated="true">Wed 2016-06-15 08:40</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>如果有Lucene的使用经历，elasticsearch的入门还是比较简单的。直接解压启动命令就安装好了，然后就是添加一些plugins就OK了。</p>

<h2>安装</h2>

<p>从官网下载 <a href="https://www.elastic.co/downloads/elasticsearch">TAR包</a> ，解压后，运行 elasticsearch 脚本启动服务。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># -d 表示 daemonize 后台运行
</span><span class='line'>[hadoop@cu2 elasticsearch-2.2.0]$ bin/elasticsearch -d</span></code></pre></td></tr></table></div></figure>


<h2>插件</h2>

<p>大部分插件都是ajax方式的静态页面，可以通过plugin脚本安装，或者直接解压文件到plugins目录下面。</p>

<p>安装已经下载到本地的插件需要加file协议，不然程序会从官网下载。或者直接解压到plugins目录下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 elasticsearch-2.2.0]$ bin/plugin install file:///home/hadoop/elasticsearch-head-master.zip 
</span><span class='line'>-&gt; Installing from file:/home/hadoop/elasticsearch-head-master.zip...
</span><span class='line'>Trying file:/home/hadoop/elasticsearch-head-master.zip ...
</span><span class='line'>Downloading .........DONE
</span><span class='line'>Verifying file:/home/hadoop/elasticsearch-head-master.zip checksums if available ...
</span><span class='line'>NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
</span><span class='line'>Installed head into /home/hadoop/elasticsearch-2.2.0/plugins/head
</span></code></pre></td></tr></table></div></figure>


<p>windows</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\local\usr\share\elasticsearch-2.3.3\bin&gt;plugin.bat install file:///D:/SOFTWARE/elasticsearch/elasticsearch-plugin/elasticsearch-head-master.zip</span></code></pre></td></tr></table></div></figure>


<p>安装好plugin后，打开浏览器查看索引情况： <a href="http://localhost:9200/_plugin/head/">http://localhost:9200/_plugin/head/</a></p>

<h2>插件高阶</h2>

<p>有些插件版本比较旧需要改一改，需要了解新版本的 elasticsearch-plugin 的规范：</p>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/installation.html">https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/installation.html</a></p>

<p>新版本插件主要是需要增加一个描述文件：</p>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/breaking_20_plugin_and_packaging_changes.html#_plugins_require_descriptor_file">Plugins require descriptor file</a></p>

<p>遇到想安装的旧版本的plugin，描述文件写法可以参考 <a href="https://github.com/mobz/elasticsearch-head">elasticsearch-head</a> 。</p>

<p>可选插件：</p>

<ul>
<li>paramedic <a href="https://github.com/karmi/elasticsearch-paramedic">https://github.com/karmi/elasticsearch-paramedic</a></li>
<li>head <a href="https://github.com/mobz/elasticsearch-head">https://github.com/mobz/elasticsearch-head</a></li>
<li>kopf <a href="https://github.com/lmenezes/elasticsearch-kopf">https://github.com/lmenezes/elasticsearch-kopf</a></li>
</ul>


<h2>常用URL请求</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html
</span><span class='line'># 创建
</span><span class='line'>$ curl -XPUT 'http://localhost:9200/t_ods_idc_isp_log2/' -d '{
</span><span class='line'>    "settings" : {
</span><span class='line'>        "index" : {
</span><span class='line'>            "number_of_shards" : 3,
</span><span class='line'>            "number_of_replicas" : 0
</span><span class='line'>        }
</span><span class='line'>    }
</span><span class='line'>}'
</span><span class='line'>{"acknowledged":true}
</span><span class='line'>
</span><span class='line'># https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html
</span><span class='line'># 更新
</span><span class='line'># mapping.json
</span><span class='line'>{
</span><span class='line'>  "properties": {
</span><span class='line'>      "author": {
</span><span class='line'>          "type": "string"
</span><span class='line'>      },
</span><span class='line'>...
</span><span class='line'>      "year": {
</span><span class='line'>          "type": "long",
</span><span class='line'>          "ignore_malformed": false,
</span><span class='line'>          "index": "analyzed"
</span><span class='line'>      },
</span><span class='line'>      "avaiable": {
</span><span class='line'>          "type": "boolean"
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>$ curl -XPUT 'localhost:9200/t_ods_idc_isp_log2/_mapping/default' -d @mapping.json
</span><span class='line'>
</span><span class='line'>$ curl -XPUT 'localhost:9200/t_ods_idc_isp_log2/_mapping/default' -d '
</span><span class='line'>{
</span><span class='line'>  "properties": {
</span><span class='line'>    "fDIID": {
</span><span class='line'>      "type": "string"
</span><span class='line'>    },
</span><span class='line'>...
</span><span class='line'>    "gatherTime": {
</span><span class='line'>      "type": "long"
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>'
</span><span class='line'>
</span><span class='line'># https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html
</span><span class='line'># 索引
</span><span class='line'># documents.json
</span><span class='line'>{ "index": {      "_index": "library",        "_type": "book",        "_id": "1"  } }
</span><span class='line'>{     "title": "All Quiet on the Western Front",  "otitle": "Im Westen nichts Neues",     "author": "Erich Maria Remarque",   "year": 1929,   "characters": ["Paul Baumer",   "Albert Kropp",     "Haie Westhus",     "Fredrich Muller",  "Stanislaus Katczinsky",    "Tjaden"],  "tags": ["novel"],  "copies": 1,    "available": true,  "section": 3 }
</span><span class='line'>{     "index": {      "_index": "library",        "_type": "book",        "_id": "2"  } }
</span><span class='line'>{     "title": "Catch-22",    "author": "Joseph Heller",  "year": 1961,   "characters": ["John Yossarian",    "Captain Aardvark",     "Chaplain Tappman",     "Colonel Cathcart",     "Doctor Daneeka"],  "tags": ["novel"],  "copies": 6,    "available": false,     "section": 1 }
</span><span class='line'>
</span><span class='line'>$ curl -s -XPOST localhost:9200/_bulk --data-binary @documents.json
</span><span class='line'>
</span><span class='line'># 删除
</span><span class='line'>$ curl -XDELETE 'http://localhost:9200/t_ods_idc_isp_log2/'
</span><span class='line'>{"acknowledged":true}
</span><span class='line'>
</span><span class='line'># https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html
</span><span class='line'># https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-field-mapping.html
</span><span class='line'># 状态查看
</span><span class='line'>http://localhost:9200/_cat/health?v
</span><span class='line'>http://localhost:9200/_cat/nodes?v
</span><span class='line'>http://localhost:9200/_cat/indices?v
</span><span class='line'>
</span><span class='line'>curl -XGET 'http://localhost:9200/_all/_mapping/book/field/author'
</span><span class='line'>curl -XHEAD -i 'http://localhost:9200/twitter/tweet'
</span><span class='line'>curl localhost:9200/_stats
</span><span class='line'>curl -XGET 'http://localhost:9200/_all/_mapping/[type]'
</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/05/08/rrc-apache-spark-source-inside-shell/">[读读书]Apache Spark源码剖析-Shell</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-05-08T21:41:01+08:00" pubdate data-updated="true">Sun 2016-05-08 21:41</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本来第二篇应该是与 [第1章 初识Spark] 有关，但是通过 spark-shell 跑个helloworld出结果了，我还是不知道那些脚本干了啥，整个的运行逻辑是什么样的？而且，在开发环境运行shell来启动总觉得怪怪的，哪调试怎么调呢？</p>

<p>所以，我准备把 spark/bin 目录下面的脚本理清楚，然后再去写开发环境的helloworld。<strong> 其实每个大数据的框架，shell脚本都是通用出口，也是研读源码的第一个突破口 </strong>。</p>

<p>官网 <strong> Quick Start </strong> 提供的简短例子都是通过 spark-shell 来运行的。还有另一种，自己打包一个jar通过 spark-submit 提交任务给集群。</p>

<p>spark-shell，spark-submit 就是两个非常重要的脚本，这里就来理一下这些脚本。</p>

<p>1) spark-shell - [3.1 spark-shell]</p>

<p>spark-shell 脚本的内容相对多一些，主要代码如下（其他代码都是为了兼容cygwin弄的，我们这里不关注）：</p>

<pre><code>SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"
trap onExit INT # 程序终止(interrupt)信号, 在用户键入INTR字符(通常是Ctrl C)时发出 来自: http://man.linuxde.net/trap

export SPARK_SUBMIT_OPTS
"${SPARK_HOME}"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"
</code></pre>

<p>最终调用 spark-submit 脚本。</p>

<p>提交我们自己helloworld命令如下：</p>

<pre><code>$ YOUR_SPARK_HOME/bin/spark-submit \
  --class "SimpleApp" \
  --master local[4] \
  target/scala-2.10/simple-project_2.10-1.0.jar
</code></pre>

<p>其实 spark-shell 与我们自己提交程序一样，不过 spark-shell 提交运行的类是spark自带，不需要额外的jar 。</p>

<p>2) spark-submit</p>

<p>submit脚本其实就是在输入参数前面添加 org.apache.spark.deploy.SparkSubmit ，然后传递给 spark-class 。</p>

<pre><code>exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"
</code></pre>

<p>3) spark-class</p>

<p>spark-shell 调用 spark-submit ， spark-submit 又调用 spark-class 。</p>

<p>spark-class脚本最终启动 java 调用 launcher模块(org.apache.spark.launcher.Main)解析参数计算出 <strong> 最终执行的命令 </strong>，然后通过 exec 来运行。</p>

<p>bin路径下脚本之间调用关系：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-sql
</span><span class='line'>spark-shell
</span><span class='line'>  spark-submit
</span><span class='line'>      spark-class
</span><span class='line'>          (java)org.apache.spark.launcher.Main</span></code></pre></td></tr></table></div></figure>


<p>要讲清楚 spark-shell 相对复杂点，通过 <strong>脚本</strong> 和 <strong>程序</strong> 两部分来分别说明。</p>

<p>3.1 脚本</p>

<p>脚本的主要作用是调用 <strong>Launcher模块</strong> 代码产生最终的执行命令(N行字符串)，然后把输出的字符串数组提供给 exec 执行。</p>

<p>spark-class先加载环境变量配置文件，再获取assembly.jar位置，然后调用 org.apache.spark.launcher.Main ， Main类根据环境变量和传入参数算出真正执行的命令。</p>

<p>这里把核心的脚本内容列出来：</p>

<pre><code>. "${SPARK_HOME}"/bin/load-spark-env.sh # 我这里把它展开
    . "${user_conf_dir}/spark-env.sh"

    # 通过ASSEMBLY路径来判断SPARK_SCALA_VERSION，通过tar打包不需要这个变量
    ASSEMBLY_DIR1="${SPARK_HOME}/assembly/target/scala-2.10" 
    export SPARK_SCALA_VERSION="2.10"

RUNNER="${JAVA_HOME}/bin/java"

SPARK_ASSEMBLY_JAR=
if [ -f "${SPARK_HOME}/RELEASE" ]; then
  ASSEMBLY_DIR="${SPARK_HOME}/lib"
else
  ASSEMBLY_DIR="${SPARK_HOME}/assembly/target/scala-$SPARK_SCALA_VERSION"
fi
ASSEMBLY_JARS="$(ls -1 "$ASSEMBLY_DIR" | grep "^spark-assembly.*hadoop.*\.jar$" || true)"
SPARK_ASSEMBLY_JAR="${ASSEMBLY_DIR}/${ASSEMBLY_JARS}"
LAUNCH_CLASSPATH="$SPARK_ASSEMBLY_JAR"

export _SPARK_ASSEMBLY="$SPARK_ASSEMBLY_JAR"

CMD=()
while IFS= read -d '' -r ARG; do
  CMD+=("$ARG")
done &lt; &lt;("$RUNNER" -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@")
exec "${CMD[@]}"
</code></pre>

<p>前面的脚本内容都是准备环境变量，就最后几行代码比较复杂。我这里debug一下，在脚本 while 循环打印每个输出的值看下 Main类 输出的是什么：</p>

<pre><code># 修改后的效果
[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ tail bin/spark-class
CMD=()
while IFS= read -d '' -r ARG; do
  echo "[DEBUG] $ARG"
  CMD+=("$ARG")
done &lt; &lt;(set -x; "$RUNNER" -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@")
echo "${CMD[@]}"
exec "${CMD[@]}"

# 启动 spark-shell 查看输出的调试信息
[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ bin/spark-shell 
++ /opt/jdk1.8.0/bin/java -cp /home/hadoop/spark-1.6.0-bin-2.6.3/lib/spark-assembly-1.6.0-hadoop2.6.3-ext-2.1.jar org.apache.spark.launcher.Main org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name 'Spark shell'
[DEBUG] /opt/jdk1.8.0/bin/java
[DEBUG] -cp
[DEBUG] /home/hadoop/spark/lib/mysql-connector-java-5.1.34.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/conf/:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/spark-assembly-1.6.0-hadoop2.6.3-ext-2.1.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-rdbms-3.2.9.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-core-3.2.10.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-api-jdo-3.2.6.jar:/home/hadoop/hadoop/etc/hadoop/
[DEBUG] -Dscala.usejavacp=true
[DEBUG] -Xms512m
[DEBUG] -Xmx512m
[DEBUG] org.apache.spark.deploy.SparkSubmit
[DEBUG] --class
[DEBUG] org.apache.spark.repl.Main
[DEBUG] --name
[DEBUG] Spark shell
[DEBUG] spark-shell
/opt/jdk1.8.0/bin/java -cp /home/hadoop/spark/lib/mysql-connector-java-5.1.34.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/conf/:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/spark-assembly-1.6.0-hadoop2.6.3-ext-2.1.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-rdbms-3.2.9.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-core-3.2.10.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-api-jdo-3.2.6.jar:/home/hadoop/hadoop/etc/hadoop/ -Dscala.usejavacp=true -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name 'Spark shell' spark-shell
...
</code></pre>

<p>从上面的调试信息可以看出：org.apache.spark.launcher.Main 把环境变量和传入参数整理后重新输出，输出的内容被脚本保存到 CMD[@] 数组中，最后使用exec来执行。</p>

<p>上面使用 spark-shell 启动任务，都可以直接用调用的类和参数在idea里面运行：</p>

<p><img src="/images/blogs/idea-spark-shell.png" alt="" /></p>

<p>3.2 Launcher模块</p>

<p>launcher模块功能 其实 用shell完全可以全部实现的。如果shell和launcher的代码你都看了的话，会发现 shell 和 java代码 功能逻辑非常类似。如下面获取java路径的代码：</p>

<pre><code>  List&lt;String&gt; buildJavaCommand(String extraClassPath) throws IOException {
    ...
    if (javaHome != null) {
      cmd.add(join(File.separator, javaHome, "bin", "java"));
    } else if ((envJavaHome = System.getenv("JAVA_HOME")) != null) {
        cmd.add(join(File.separator, envJavaHome, "bin", "java"));
    } else {
        cmd.add(join(File.separator, System.getProperty("java.home"), "bin", "java"));
    }
    ...
  }
</code></pre>

<p>而shell脚本里面是这样子写的：</p>

<pre><code># Find the java binary
if [ -n "${JAVA_HOME}" ]; then
  RUNNER="${JAVA_HOME}/bin/java"
else
  if [ `command -v java` ]; then
    RUNNER="java"
  else
    echo "JAVA_HOME is not set" &gt;&amp;2
    exit 1
  fi
fi
</code></pre>

<p>对比两者，其实是用脚本更加直观。但是使用java编写的 launcher模块 更便于管理和扩展，稍微调整下就能复用代码：输出给windows-cmd脚本、或者为了兼容多个操作系统多语言(python，r 等)。所以提取一个公共 launcher模块 出来其实是个挺不错的选择。同时对于不是很熟悉shell的程序员来说也更方便了解系统运作。</p>

<p>按功能可以分为 CommandBuilder 和 SparkLauncher 两个部分。</p>

<p>3.2.1 CommandBuilder</p>

<p>SparkSubmitCommandBuilder解析用户输入的参数并输出命令给脚本使用，而SparkClassCommandBuilder主要为后台进程产生启动命令（sbin目录下面的脚本）。</p>

<p>主要的类以及参数：</p>

<ul>
<li>Main ： 统一入口</li>
<li>AbstractCommandBuilder : 提供构造命令的公共方法

<ul>
<li>buildJavaCommand

<ul>
<li>buildClassPath

<ul>
<li>SPARK_CLASSPATH</li>
<li>extraClassPath</li>
<li>getConfDir : 等于环境变量 $SPARK_CONF_DIR 或者 $SPARK_HOME/conf 的值</li>
<li>classes

<ul>
<li>SPARK_PREPEND_CLASSES</li>
<li>SPARK_TESTING</li>
</ul>
</li>
<li>findAssembly : 获取 spark-assembly-1.6.0-hadoop2.6.3.jar 的路径，lib 或者 assembly/target/scala-$SPARK_SCALA_VERSION 路径下

<ul>
<li>_SPARK_ASSEMBLY</li>
</ul>
</li>
<li>datanucleus-* : 从 lib / lib_managed/jars 目录下获取</li>
<li>HADOOP_CONF_DIR</li>
<li>YARN_CONF_DIR</li>
<li>SPARK_DIST_CLASSPATH</li>
</ul>
</li>
</ul>
</li>
<li>getEffectiveConfig : 获取 spark-defaults.conf 的内容</li>
</ul>
</li>
<li>SparkSubmitCommandBuilder

<ul>
<li>构造函数调用OptionParser解析参数，解析handle有处理specialClasses！</li>
<li>buildSparkSubmitCommand

<ul>
<li>getEffectiveConfig</li>
<li>extraClassPath : spark.driver.extraClassPath</li>
<li>SPARK_SUBMIT_OPTS</li>
<li>SPARK_JAVA_OPTS</li>
<li>client模式下加载配置

<ul>
<li>spark.driver.memory / SPARK_DRIVER_MEMORY / SPARK_MEM / DEFAULT_MEM(1g)</li>
<li>DRIVER_EXTRA_JAVA_OPTIONS</li>
<li>DRIVER_EXTRA_LIBRARY_PATH</li>
</ul>
</li>
<li>buildSparkSubmitArgs ： 把参数键值对存储List，用于输出</li>
</ul>
</li>
</ul>
</li>
<li>SparkSubmitCommandBuilder$OptionParser -> SparkSubmitOptionParser(子类需要实现handle方法)

<ul>
<li><code>bin/spark-submit -h</code> 帮助文档会输出可以<strong>设置的参数</strong></li>
<li>或者直接查看<a href="http://spark.apache.org/docs/latest/submitting-applications.html">官网文档</a></li>
</ul>
</li>
<li>SparkClassCommandBuilder

<ul>
<li>org.apache.spark.deploy.master.Master

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_MASTER_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.deploy.worker.Worker

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_WORKER_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.deploy.history.HistoryServer

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_HISTORY_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.executor.CoarseGrainedExecutorBackend

<ul>
<li>SPARK_JAVA_OPTS</li>
<li>SPARK_EXECUTOR_OPTS</li>
<li>SPARK_EXECUTOR_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.executor.MesosExecutorBackend

<ul>
<li>SPARK_EXECUTOR_OPTS</li>
<li>SPARK_EXECUTOR_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.deploy.ExternalShuffleService / org.apache.spark.deploy.mesos.MesosExternalShuffleService

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_SHUFFLE_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.tools.

<ul>
<li>extraClassPath : spark-tools_.*.jar</li>
<li>SPARK_JAVA_OPTS</li>
<li>DEFAULT_MEM(1g)</li>
</ul>
</li>
<li>other

<ul>
<li>SPARK_JAVA_OPTS</li>
<li>SPARK_DRIVER_MEMORY</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>3.2.2 SparkLauncher</p>

<p>SparkLauncher提供了在程序中提交application的方式。通过Driver端的支持获取程序执行动态，为实现后端管理应用提供一种可行的方式。</p>

<p>launcher提交还是使用spark-submit脚本，绕一圈又回到上面的参数解析生成命令然后exec执行。但是launcher中通过启动 LauncherServer(socketserver)，Driver(LauncherBackend)监听这个端口会把程序的最新状态通过socket推给launcher。</p>

<p><img src="/images/blogs/rrc-spark/spark-launcher.jpg" alt="" /></p>

<p>代码包括：</p>

<pre><code>* SparkLauncher
* LauncherServer
* LauncherConnection
* LauncherProtocol
* ChildProcAppHandle : SparkAppHandle
</code></pre>

<p>具体下载代码 <a href="https://github.com/winse/spark-examples/blob/master/src/main/scala/com/github/winse/spark/HelloWorldLauncher.scala">HelloWorldLauncher.scala</a> ，然后本地调试一步步的查看。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/05/07/rrc-apache-spark-source-inside-preface/">[读读书]Apache Spark源码剖析-序</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-05-07T23:58:57+08:00" pubdate data-updated="true">Sat 2016-05-07 23:58</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>今天去广州图书馆办了证，借了几本关于大数据的书。老实说，国家提供的便民基础设施应该发挥她的价值，国家建那么多公共设施，还有很多人在后台让这些服务运作起来。借书是一种最高性价比学习的方式，第一：不能乱写乱画必须做笔记或者背下来，把最有价值的东西汇集；第二：有时间限制，好书逼着我们持续的去读；第三：自然是读到烂书也不用花钱，有价值的书必然也是最多人看的，看到翻的很旧的新书你就借了吧。</p>

<p>其中一个《Apache Spark源码剖析-徐鹏》，大致翻了一下，老实说作者很牛逼啊，从那么多的代码里面挑出和主题相关的，不比鸡蛋里面挑石头容易，跟着作者的思路去读应该不错。打算每天读点代码，同时把看书和看代码也记录下来，每天一小结，同时希望对别人有些参考作用。</p>

<p>Spark更新的很快，书本介绍的是 spark-1.0 ，不过书中介绍的主要是思路，我们这里选择比较新的版本 1.6.0 来读（生产用的是1.6）。</p>

<p><strong> 说到思路，如果你对Redis也感兴趣，强烈推荐读读 《Redis设计与实现-黄建宏》 </strong></p>

<h3>使用环境说明</h3>

<p>和作者不同，我选择直接在windows来读/调试代码，为了读个代码还得整一套linux的开发环境挺累的（原来也试过整linux开发环境后来放弃了），Windows 积累的经验已经可以让我自由调试和看代码了。</p>

<p>吐槽下sbt，很讨厌这玩意又慢还用ivy，我X，大数据不都用 maven 嘛，难道我还得为 spark 整一套完全一样的jar本地缓冲？不过还好 spark-1.6 已经是用 maven 来管理了。</p>

<ul>
<li>win10 + cygwin</li>
<li>jdk8_x64（内存可以调到大于1G）</li>
<li>maven3</li>
<li>scala_2.10</li>
<li>spark_1.6.0</li>
<li>hive_1.2.1</li>
<li>hadoop_2.6.3</li>
<li>JetBrains idea 看代码确实不错</li>
</ul>


<h3>Spark开发环境搭建 - [附录A Spark源码调试]</h3>

<p>1) 配置 idea-scala</p>

<p>1.1 优化idea启动参数</p>

<p>安装 <strong>最新版idea</strong> (当前最新版本是15.0.5)。在程序安装的 bin 目录下，有x64配置文件 idea64.exe.vmoptions ，在配置文件开头添加jdk8内存配置：</p>

<pre><code>-server
-Xms1g
-Xmx2g
-XX:MetaspaceSize=256m
-XX:MaxMetaspaceSize=256m
</code></pre>

<p>由于机器 eclipse 原来使用的 jdk_x86，为了兼容，单独编写 idea64.exe 的启动脚本 <strong> idea.bat </strong>：</p>

<pre><code>set JAVA_HOME=D:\Java\jdk1.8.0_40
D:
cd "D:\Program Files\JetBrains\IntelliJ IDEA Community Edition 15.0.5\bin"
start idea64.exe"

exit
</code></pre>

<p><strong> 快键：idea可以适配eclipse的快键集，通过 Settings -> Keymap -> Keymaps 设置。 </strong></p>

<p>1.2 安装scala插件</p>

<p>第一种方式，当然最好就是通过plugins的搜索框就能安装，但这在中国得看运气。 第二种方式，首先下载好插件，然后选择从硬盘安装插件。</p>

<ul>
<li>A 从网络安装</li>
</ul>


<p>打开 plugins 管理页面：（也可以通过 File -> Settings&hellip; -> Plugins 打开）</p>

<p><img src="/images/blogs/rrc-spark/idea-start-configure.png" alt="" /></p>

<p>弹出的 Plugins 对话框显示了当前已经安装的插件：</p>

<p><img src="/images/blogs/rrc-spark/idea-plugins-list.png" alt="" /></p>

<p>在 Plugins 对话框页面选择 <strong> Browse repositories&hellip; </strong> 按钮，再在弹出的对话框中查找 Scala 相关的插件：</p>

<p><img src="/images/blogs/rrc-spark/idea-browse-plugins.png" alt="" /></p>

<p>选择安装 Scala ，当然你也可以同时安装上 SBT 。</p>

<ul>
<li>B 从硬盘安装</li>
</ul>


<p>运气好就算可以直接从网络安装，但是下载过程其实也挺慢的。我们还可以先自己下载好插件再安装（或者从其他同学获取、迅雷分分钟下完）。</p>

<p>首先需要查看自己 idea 的版本，再在 <a href="https://plugins.jetbrains.com/?idea_ce">https://plugins.jetbrains.com/?idea_ce</a> 查找下载符合自己版本的 <a href="https://plugins.jetbrains.com/plugin/1347?pr=idea_ce">scala 插件</a>，最后通过 <code>Install plugin from disk...</code> 安装，然后重启idea即可。</p>

<p><img src="/images/blogs/rrc-spark/idea-version.png" alt="" />
<img src="/images/blogs/rrc-spark/download-scala-plugin.png" alt="" />
<img src="/images/blogs/rrc-spark/idea-scala-from-disk.png" alt="" /></p>

<p>2) 下载 spark 源码，并导入idea</p>

<p>2.1 下载源码，检出 1.6.0 版本</p>

<pre><code>$ git clone https://github.com/apache/spark.git
$ git checkout v1.6.0
</code></pre>

<p>如果你只想看 1.6.0 的内容，可以直接在clone命令添加参数指定版本：</p>

<pre><code>$ git clone https://github.com/apache/spark.git -b v1.6.0
</code></pre>

<p>2.2 导入idea</p>

<p>导入之前先要生成arvo的java类(这里直接package编译一下)：</p>

<pre><code>E:\git\spark\external\flume-sink&gt;mvn package -DskipTests
</code></pre>

<p>由于我使用 hadoop-2.6.3 ，并且导入的时刻不能修改环境变量，直接修改 pom.xml 里面 hadoop.version 属性的值。</p>

<p><img src="/images/blogs/rrc-spark/spark-hadoop-version.png" alt="" /></p>

<p>启动idea，使用 <strong> Import Project </strong> 导入源代码； 然后选择 E:/git/spark （刚刚下载的源码位置）；然后选择导入maven项目；在 profile 页把必要的都选上（当然也可以后期通过 <code>Maven Projects</code> 面板来修改）：</p>

<p><img src="/images/blogs/rrc-spark/spark-import-profile.png" alt="" /></p>

<p>导入完成后，依赖关系maven已经处理好了，直接就能用了。也可以 Make Projects 再编译一次，并把运行application的make去掉，免得浪费编译时间）。</p>

<p><strong> 注意：mvn idea:idea 其实不咋的，生成的配置不是很兼容最好不要用！！ </strong></p>

<p>2.3 调试/测试</p>

<p>在调试运行之前，先了解和解决 idea maven-provided 的问题：在idea里面直接运行 src/main/java 下面的类会被当做在生产环境运行，所以idea不会把这些 provided的依赖 加入到运行的classpath。</p>

<ul>
<li><a href="https://youtrack.jetbrains.com/issue/IDEA-54595">https://youtrack.jetbrains.com/issue/IDEA-54595</a></li>
<li><a href="http://stackoverflow.com/questions/30453269/maven-provided-dependency-will-cause-noclassdeffounderror-in-intellij">http://stackoverflow.com/questions/30453269/maven-provided-dependency-will-cause-noclassdeffounderror-in-intellij</a></li>
</ul>


<p><img src="/images/blogs/rrc-spark/idea-maven-provided.png" alt="" /></p>

<p>idea运行时从 examples/spark-examples_2.10.iml 文件中读取classpath的配置，所以我们直接把 spark-examples_2.10.iml 中的 <code>scope="PROVIDED"</code> 全部删掉（也可以说是替换成空格）。</p>

<pre><code># 一次全部删掉！
winse@Lenovo-PC ~/git/spark
$ find . -name "*.iml"  | xargs -I{} sed -i 's/scope="PROVIDED"//' {}
</code></pre>

<p>首先右键 <strong> Run &lsquo;LogQuery&rsquo; </strong> 运行（由于缺少master的配置会报错的），生成启动的 LogQuery Configuration：</p>

<p><img src="/images/blogs/rrc-spark/spark-logquery-firststart.png" alt="" /></p>

<p>然后选择上图中的 <strong> Edit Configurations&hellip; </strong> 下拉选项，在弹出配置对话框为中为 LogQuery 添加 <strong> VM options </strong> 配置: <code>-Dspark.master=local</code> ，最后就可以打断点Debug调试了。</p>

<p><img src="/images/blogs/rrc-spark/spark-logquery-config.png" alt="" /></p>

<p>运行结果如下：</p>

<p><img src="/images/blogs/rrc-spark/spark-logquery-result.png" alt="" /></p>

<p>遇到idea导入maven依赖有问题的，可以参考下 <a href="http://stackoverflow.com/questions/11454822/import-maven-dependencies-in-intellij-idea">Import Maven dependencies in IntelliJ IDEA</a> 。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/3">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/index.html">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2016/04/23/hadoop-guide-catalog/">[整理] Hadoop入门</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/08/19/j2ee-maven-resources-compress/">Maven压缩js/css功能实践</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/08/17/redis-batch-operate/">Redis批量操作</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/28/redis-optimise/">Redis使用优化</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/19/xss-blocked-by-naxsi/">使用 Naxsi 处理 XSS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/14/codis-guide/">Codis简单使用</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/06/28/flume-kafka-elasticsearch-for-analyse/">使用 Flume+kafka+elasticsearch 处理数据</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/06/17/ganglia-install-on-centos-with-puppet/">使用Puppet安装配置Ganglia</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/06/15/elasticsearch-startguide/">Elasticsearch Startguide</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/alluxio/'>alluxio</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/elasticsearch/'>elasticsearch</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/flume/'>flume</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (5) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (42) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/nginx/'>nginx</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/puppet/'>puppet</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (36) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (144)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'winseliu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
