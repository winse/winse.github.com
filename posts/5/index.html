
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="说 Puppet 入门配置过程中 90% 的问题与有关毫不为过！！因为节点之间的通信都需要证书验证，而证书验证和域名绑定。 主要讲讲 FQDN(Fully Qualified Domain Name) 查看和配置，以及 Puppet4.4 认证相关的操作。 环境说明 测试环境是几台云主机 ， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/5">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
  <li><a href="https://yunpan.cn/cuYhpFBPgQYgT" >Books[5aee]</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/21/puppet-domain-fdqn/">Puppet入门之域名证书</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-21T00:06:06+08:00" pubdate data-updated="true">Thu 2016-04-21 00:06</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>说 Puppet 入门配置过程中 90% 的问题与有关毫不为过！！因为节点之间的通信都需要证书验证，而证书验证和域名绑定。</p>

<p>主要讲讲 FQDN(Fully Qualified Domain Name) 查看和配置，以及 <strong>Puppet4.4</strong> 认证相关的操作。</p>

<h1>环境说明</h1>

<p>测试环境是几台云主机 ，主机名根据项目情况命名（也就是说云主机内网域名解析是不行的）。操作系统没特殊说明那么使用的是 Centos6 。</p>

<ul>
<li>cu2： 服务端master，证书服务器ca</li>
<li>cu1/cu3/cu4/cu5:  agent</li>
</ul>


<p>这里列出来的是部署之前的域名情况。一步步的处理域名代码的麻烦。如果想避免不必要的烦恼，请使用 FQDN 加上 <strong>域</strong> 。</p>

<h1>服务节点证书重新签名</h1>

<p>安装后直接测试，默认连接的服务器是 puppet 。所以要么指定 puppet 对应主机，要么加上 &ndash;server 参数。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 默认的 puppet 服务器找不到对应的主机
</span><span class='line'>[root@cu2 ~]# puppet agent --test
</span><span class='line'>Warning: Unable to fetch my node definition, but the agent run will continue:
</span><span class='line'>Warning: getaddrinfo: Name or service not known
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/facts.d]: Failed to generate additional resources using 'eval_generate': getaddrinfo: Name or service not known
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/facts.d]: Could not evaluate: Could not retrieve file metadata for puppet:///pluginfacts: getaddrinfo: Name or service not known
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/lib]: Failed to generate additional resources using 'eval_generate': getaddrinfo: Name or service not known
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/lib]: Could not evaluate: Could not retrieve file metadata for puppet:///plugins: getaddrinfo: Name or service not known
</span><span class='line'>Error: Could not retrieve catalog from remote server: getaddrinfo: Name or service not known
</span><span class='line'>Warning: Not using cache on failed catalog
</span><span class='line'>Error: Could not retrieve catalog; skipping run
</span><span class='line'>Error: Could not send report: getaddrinfo: Name or service not known
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 加上 域 后不通，DNS服务器不识别自定义的主机名
</span><span class='line'>[root@cu2 ~]# cat /etc/resolv.conf 
</span><span class='line'>; generated by /sbin/dhclient-script
</span><span class='line'>search ds.ctyun
</span><span class='line'>nameserver 192.168.0.1
</span><span class='line'>[root@cu2 ~]# puppet agent --server cu2.ds.ctyun --test
</span><span class='line'>Warning: Unable to fetch my node definition, but the agent run will continue:
</span><span class='line'>Warning: getaddrinfo: Name or service not known
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/facts.d]: Failed to generate additional resources using 'eval_generate': getaddrinfo: Name or service not known
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/facts.d]: Could not evaluate: Could not retrieve file metadata for puppet:///pluginfacts: getaddrinfo: Name or service not known
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/lib]: Failed to generate additional resources using 'eval_generate': getaddrinfo: Name or service not known
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/lib]: Could not evaluate: Could not retrieve file metadata for puppet:///plugins: getaddrinfo: Name or service not known
</span><span class='line'>Error: Could not retrieve catalog from remote server: getaddrinfo: Name or service not known
</span><span class='line'>Warning: Not using cache on failed catalog
</span><span class='line'>Error: Could not retrieve catalog; skipping run
</span><span class='line'>Error: Could not send report: getaddrinfo: Name or service not known
</span><span class='line'>[root@cu2 ~]# ping cu2.ds.ctyun
</span><span class='line'>ping: unknown host cu2.ds.ctyun
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 传说中用的 -f 参数没L用
</span><span class='line'>[root@cu2 ~]# hostname -i
</span><span class='line'>192.168.0.x
</span><span class='line'>[root@cu2 ~]# hostname -f
</span><span class='line'>cu2
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 加自定义 域 ，并重新设定 FQDN hostname。 修改主机hostname的步骤可以替换成在 /etc/resolv.conf 加 **domain eshore.cn**
</span><span class='line'>[root@cu2 ~]# vi /etc/hosts
</span><span class='line'>192.168.0.x cu1 cu1.eshore.cn
</span><span class='line'>192.168.0.x cu2 cu2.eshore.cn
</span><span class='line'>
</span><span class='line'>192.168.0.x cu3 cu3.eshore.cn
</span><span class='line'>192.168.0.x cu4 cu4.eshore.cn
</span><span class='line'>192.168.0.x cu5 cu5.eshore.cn
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# vi /etc/sysconfig/network
</span><span class='line'>NETWORKING=yes
</span><span class='line'>HOSTNAME=cu2.eshore.cn
</span><span class='line'>[root@cu2 ~]# hostname cu2.eshore.cn
</span><span class='line'>[root@cu2 ~]# hostname
</span><span class='line'>cu2.eshore.cn
</span><span class='line'>
</span><span class='line'># 确认
</span><span class='line'>[root@cu2 ~]# puppet config print certname
</span><span class='line'>cu2.eshore.cn
</span><span class='line'>
</span><span class='line'>[root@cu2 puppet]# dnsdomainname -v
</span><span class='line'>gethostname()=`cu2.eshore.cn'
</span><span class='line'>Resolving `cu2.eshore.cn' ...
</span><span class='line'>Result: h_name=`cu2'
</span><span class='line'>Result: h_aliases=`cu2.eshore.cn'
</span><span class='line'>Result: h_addr_list=`192.168.0.214'
</span><span class='line'>
</span><span class='line'>[root@cu2 puppet]# hostname -f -v
</span><span class='line'>gethostname()=`cu2.eshore.cn'
</span><span class='line'>Resolving `cu2.eshore.cn' ...
</span><span class='line'>Result: h_name=`cu2'
</span><span class='line'>Result: h_aliases=`cu2.eshore.cn'
</span><span class='line'>Result: h_addr_list=`192.168.0.214'
</span><span class='line'>cu2
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 清理已经为本机签发的证书
</span><span class='line'>[root@cu2 ~]# puppet cert list -all
</span><span class='line'>+ "cu2.ds.ctyun" (SHA256) A6:30:6D:80:A8:04:60:56:4C:F3:D5:3C:9A:5C:2A:11:6C:A6:A9:F7:6E:5E:A5:37:59:28:5B:B6:E3:D3:73:D5 (alt names: "DNS:puppet", "DNS:cu2.ds.ctyun")
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# puppet cert clean cu2.ds.ctyun
</span><span class='line'>Notice: Revoked certificate with serial 2
</span><span class='line'>Notice: Removing file Puppet::SSL::Certificate cu2.ds.ctyun at '/etc/puppetlabs/puppet/ssl/ca/signed/cu2.ds.ctyun.pem'
</span><span class='line'>Notice: Removing file Puppet::SSL::Certificate cu2.ds.ctyun at '/etc/puppetlabs/puppet/ssl/certs/cu2.ds.ctyun.pem'
</span><span class='line'>Notice: Removing file Puppet::SSL::Key cu2.ds.ctyun at '/etc/puppetlabs/puppet/ssl/private_keys/cu2.ds.ctyun.pem'
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 由于是server节点的证书变更，重启puppetserver会重新生成/签发证书
</span><span class='line'>[root@cu2 ~]# service puppetserver restart
</span><span class='line'>Stopping puppetserver:                                     [  OK  ]
</span><span class='line'>Starting puppetserver:                                     [  OK  ]
</span><span class='line'>
</span><span class='line'>[root@cu2 puppet]# tree /etc/puppetlabs/puppet/ssl
</span><span class='line'>/etc/puppetlabs/puppet/ssl
</span><span class='line'>├── ca
</span><span class='line'>│   ├── ca_crl.pem
</span><span class='line'>│   ├── ca_crt.pem
</span><span class='line'>│   ├── ca_key.pem
</span><span class='line'>│   ├── ca_pub.pem
</span><span class='line'>│   ├── inventory.txt
</span><span class='line'>│   ├── private
</span><span class='line'>│   ├── requests
</span><span class='line'>│   ├── serial
</span><span class='line'>│   └── signed
</span><span class='line'>│       └── cu2.eshore.cn.pem
</span><span class='line'>├── certificate_requests
</span><span class='line'>├── certs
</span><span class='line'>│   ├── ca.pem
</span><span class='line'>│   └── cu2.eshore.cn.pem
</span><span class='line'>├── crl.pem
</span><span class='line'>├── private
</span><span class='line'>├── private_keys
</span><span class='line'>│   └── cu2.eshore.cn.pem
</span><span class='line'>└── public_keys
</span><span class='line'>    └── cu2.eshore.cn.pem
</span><span class='line'>
</span><span class='line'>9 directories, 12 files
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# puppet agent --server cu2.eshore.cn --test
</span><span class='line'>Info: Using configured environment 'production'
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Info: Caching catalog for cu2.eshore.cn
</span><span class='line'>Info: Applying configuration version '1461149778'
</span><span class='line'>Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml
</span><span class='line'>Notice: Applied catalog in 0.01 seconds
</span></code></pre></td></tr></table></div></figure>


<h1>Agent 重新签名</h1>

<p>涉及到客户端域名错误，需要服务端配合清理签名请求等操作。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 首先同步 /etc/hosts 到所有agent节点
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># cu1 连接 服务器cu2
</span><span class='line'>[root@cu1 ~]# puppet agent --server cu2.eshore.cn --test
</span><span class='line'>Info: Creating a new SSL key for cu1.ds.ctyun
</span><span class='line'>Info: Caching certificate for ca
</span><span class='line'>Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
</span><span class='line'>Info: Creating a new SSL certificate request for cu1.ds.ctyun
</span><span class='line'>Info: Certificate Request fingerprint (SHA256): 4F:D6:DC:25:22:D9:44:E5:70:9F:9B:B1:0F:99:B2:AC:F5:5F:50:CE:B7:C3:AF:65:F4:E2:DF:D5:2D:6F:96:07
</span><span class='line'>Info: Caching certificate for ca
</span><span class='line'>Exiting; no certificate found and waitforcert is disabled
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 在没有修改 域 的情况下，已经发送了 ds.ctyun 域 的签名请求
</span><span class='line'># 修改主机域，再发送请求
</span><span class='line'>[root@cu1 ~]# vi /etc/resolv.conf 
</span><span class='line'>; generated by /sbin/dhclient-script
</span><span class='line'>domain eshore.cn
</span><span class='line'>search ds.ctyun
</span><span class='line'>nameserver 192.168.0.1
</span><span class='line'>
</span><span class='line'>[root@cu1 ~]#  puppet config print certname
</span><span class='line'>cu1.eshore.cn
</span><span class='line'>
</span><span class='line'>[root@cu1 ~]# puppet agent --server cu2.eshore.cn --test
</span><span class='line'>Info: Creating a new SSL key for cu1.eshore.cn
</span><span class='line'>Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
</span><span class='line'>Info: Creating a new SSL certificate request for cu1.eshore.cn
</span><span class='line'>Info: Certificate Request fingerprint (SHA256): B8:A1:65:B6:FE:02:87:B1:8D:0A:62:2E:FE:30:DD:B3:3B:C9:A2:B2:A1:50:11:D3:FE:03:6A:81:A6:84:C0:6B
</span><span class='line'>Exiting; no certificate found and waitforcert is disabled
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 此时服务端cu2已包括了 cu1 的两个签名请求信息
</span><span class='line'>[root@cu2 puppet]# puppet cert list -all
</span><span class='line'>  "cu1.ds.ctyun"  (SHA256) 4F:D6:DC:25:22:D9:44:E5:70:9F:9B:B1:0F:99:B2:AC:F5:5F:50:CE:B7:C3:AF:65:F4:E2:DF:D5:2D:6F:96:07
</span><span class='line'>  "cu1.eshore.cn" (SHA256) B8:A1:65:B6:FE:02:87:B1:8D:0A:62:2E:FE:30:DD:B3:3B:C9:A2:B2:A1:50:11:D3:FE:03:6A:81:A6:84:C0:6B
</span><span class='line'>+ "cu2.eshore.cn" (SHA256) 3D:8E:4E:18:45:F4:8C:9B:71:7C:13:45:0D:8A:6F:A5:6E:22:D5:0E:B1:B0:54:29:47:02:AE:95:8B:E6:A6:B7 (alt names: "DNS:puppet", "DNS:cu2.eshore.cn")
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 本地清理 无效的签名请求 或者直接删除ssl目录： rm -rf /var/lib/puppet/ssl
</span><span class='line'>[root@cu1 ~]# puppet certificate_request destroy cu1.ds.ctyun
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest cu1.ds.ctyun at '/etc/puppetlabs/puppet/ssl/certificate_requests/cu1.ds.ctyun.pem'
</span><span class='line'>1
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 服务端清理 特定客户端无效请求
</span><span class='line'># http://serverfault.com/questions/574976/puppet-trying-to-configure-puppet-client-for-first-use-but-got-some-problems-wi
</span><span class='line'>[root@cu2 puppet]# puppet node clean cu1.ds.ctyun 
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest cu1.ds.ctyun at '/etc/puppetlabs/puppet/ssl/ca/requests/cu1.ds.ctyun.pem'
</span><span class='line'>cu1.ds.ctyun
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 服务端签名，客户端agent同步manifest
</span><span class='line'>[root@cu2 puppet]# puppet cert sign cu1.eshore.cn
</span><span class='line'>Notice: Signed certificate request for cu1.eshore.cn
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest cu1.eshore.cn at '/etc/puppetlabs/puppet/ssl/ca/requests/cu1.eshore.cn.pem'
</span><span class='line'>
</span><span class='line'>[root@cu1 ~]# puppet agent --server cu2.eshore.cn --test
</span><span class='line'>Info: Caching certificate_revocation_list for ca
</span><span class='line'>Info: Using configured environment 'production'
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Info: Caching catalog for cu1.eshore.cn
</span><span class='line'>Info: Applying configuration version '1461156849'
</span><span class='line'>Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml
</span><span class='line'>Notice: Applied catalog in 0.01 seconds
</span></code></pre></td></tr></table></div></figure>


<p>其他修改主机域后统一签名：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 puppet]# puppet cert list 
</span><span class='line'>  "cu3.eshore.cn" (SHA256) 16:CB:A3:6D:21:69:78:D0:0D:37:1F:A7:C1:86:2E:55:7F:B1:60:77:05:EC:F5:37:81:12:28:73:61:1A:4F:20
</span><span class='line'>  "cu4.eshore.cn" (SHA256) CB:80:64:BD:B8:56:56:43:90:11:D4:B2:A9:7B:D8:DC:E4:0C:8D:5A:71:0B:FF:97:65:20:F5:B4:D7:15:11:B6
</span><span class='line'>  "cu5.eshore.cn" (SHA256) D6:9A:B0:93:98:94:D2:D2:E3:A9:55:24:EC:7A:E0:13:48:5B:26:16:6C:5A:B6:11:F5:7C:F2:56:E4:DA:D8:31
</span><span class='line'>[root@cu2 puppet]# puppet cert sign --all
</span><span class='line'>Notice: Signed certificate request for cu5.eshore.cn
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest cu5.eshore.cn at '/etc/puppetlabs/puppet/ssl/ca/requests/cu5.eshore.cn.pem'
</span><span class='line'>Notice: Signed certificate request for cu4.eshore.cn
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest cu4.eshore.cn at '/etc/puppetlabs/puppet/ssl/ca/requests/cu4.eshore.cn.pem'
</span><span class='line'>Notice: Signed certificate request for cu3.eshore.cn
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest cu3.eshore.cn at '/etc/puppetlabs/puppet/ssl/ca/requests/cu3.eshore.cn.pem'
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 最终效果
</span><span class='line'>[root@cu2 puppet]# puppet cert list -all
</span><span class='line'>+ "cu1.eshore.cn" (SHA256) 46:69:EE:A8:E5:F9:FB:E3:59:63:C5:FC:52:AF:14:43:70:EF:D0:42:70:C4:0E:D2:14:E4:1C:D9:94:F8:9E:E7
</span><span class='line'>+ "cu2.eshore.cn" (SHA256) 3D:8E:4E:18:45:F4:8C:9B:71:7C:13:45:0D:8A:6F:A5:6E:22:D5:0E:B1:B0:54:29:47:02:AE:95:8B:E6:A6:B7 (alt names: "DNS:puppet", "DNS:cu2.eshore.cn")
</span><span class='line'>+ "cu3.eshore.cn" (SHA256) 58:ED:A3:CC:B9:53:34:4B:64:3C:2A:B4:91:AD:0D:8F:AF:EA:B0:5C:A7:73:06:F1:A7:4B:D2:E2:06:B5:21:39
</span><span class='line'>+ "cu4.eshore.cn" (SHA256) DD:A2:B9:86:53:29:DB:12:A3:0C:AA:9C:11:68:72:70:72:E2:16:36:8E:20:AC:E5:48:12:36:E2:80:6C:F0:E6
</span><span class='line'>+ "cu5.eshore.cn" (SHA256) EE:E6:FB:D2:1A:04:AD:C3:5B:1F:4F:79:C3:B6:36:15:B5:AC:8B:8B:5D:CB:A4:AA:AF:7B:FB:50:0B:83:7E:38
</span></code></pre></td></tr></table></div></figure>


<h1>自动签名配置文件</h1>

<p>反正都是学习，在无尽的折腾成长。如果是生产环境最好不要清理服务端的已签名证书，不但客户端要重新签，如果安装了puppetdb等其他程序需要签名都得重新配置签名。</p>

<p>注意： 如果已经安装官网的步骤安装 PuppetDB ，清理服务端的证书建议通过命令 puppet cert clean DOMAIN 来清理。否则 PuppetDB 中还有对应的证书缓存信息。</p>

<p><img src="/images/blogs/puppet-puppetdb-resign.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># https://tickets.puppetlabs.com/browse/PUP-1426
</span><span class='line'># 貌似不支持全部清除已签名证书
</span><span class='line'>[root@cu2 ~]# puppet cert clean --all 
</span><span class='line'>Error: Refusing to revoke all certs, provide an explicit list of certs to revoke
</span><span class='line'>
</span><span class='line'># 直接删掉ssl目录
</span><span class='line'>[root@cu2 ~]# puppet master --configprint ssldir
</span><span class='line'>/etc/puppetlabs/puppet/ssl
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# cd /etc/puppetlabs/puppet
</span><span class='line'>[root@cu2 puppet]# ll
</span><span class='line'>...
</span><span class='line'>drwxrwx--x 8 puppet puppet 4096 Apr 20 15:10 ssl
</span><span class='line'>
</span><span class='line'># 注意ssl目录的权限。这里仅删除目录里面的文件
</span><span class='line'>[root@cu2 puppet]# service puppetserver stop
</span><span class='line'>Stopping puppetserver:                                     [  OK  ]
</span><span class='line'>[root@cu2 puppet]# 
</span><span class='line'>[root@cu2 puppet]# rm -rf ssl/*
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 先启动服务看看原来已签名的再连服务器是什么情况
</span><span class='line'>[root@cu2 puppet]# service puppetserver start
</span><span class='line'>Starting puppetserver:                                     [  OK  ]
</span><span class='line'>
</span><span class='line'>[root@cu2 puppet]# tree ssl/
</span><span class='line'>ssl/
</span><span class='line'>├── ca
</span><span class='line'>│   ├── ca_crl.pem
</span><span class='line'>│   ├── ca_crt.pem
</span><span class='line'>│   ├── ca_key.pem
</span><span class='line'>│   ├── ca_pub.pem
</span><span class='line'>│   ├── inventory.txt
</span><span class='line'>│   ├── requests
</span><span class='line'>│   ├── serial
</span><span class='line'>│   └── signed
</span><span class='line'>│       └── cu2.eshore.cn.pem
</span><span class='line'>├── certificate_requests
</span><span class='line'>├── certs
</span><span class='line'>│   ├── ca.pem
</span><span class='line'>│   └── cu2.eshore.cn.pem
</span><span class='line'>├── crl.pem
</span><span class='line'>├── private
</span><span class='line'>├── private_keys
</span><span class='line'>│   └── cu2.eshore.cn.pem
</span><span class='line'>└── public_keys
</span><span class='line'>    └── cu2.eshore.cn.pem
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># agent 再请求，会报错。删除 ssl 后，再签名
</span><span class='line'>[root@cu3 ~]# puppet agent --server cu2.eshore.cn --test
</span><span class='line'>Warning: Unable to fetch my node definition, but the agent run will continue:
</span><span class='line'>Warning: SSL_connect returned=1 errno=0 state=error: certificate verify failed: [unable to get local issuer certificate for /CN=cu2.eshore.cn]
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/facts.d]: Failed to generate additional resources using 'eval_generate': SSL_connect returned=1 errno=0 state=error: certificate verify failed: [unable to get local issuer certificate for /CN=cu2.eshore.cn]
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/facts.d]: Could not evaluate: Could not retrieve file metadata for puppet:///pluginfacts: SSL_connect returned=1 errno=0 state=error: certificate verify failed: [unable to get local issuer certificate for /CN=cu2.eshore.cn]
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/lib]: Failed to generate additional resources using 'eval_generate': SSL_connect returned=1 errno=0 state=error: certificate verify failed: [unable to get local issuer certificate for /CN=cu2.eshore.cn]
</span><span class='line'>Error: /File[/opt/puppetlabs/puppet/cache/lib]: Could not evaluate: Could not retrieve file metadata for puppet:///plugins: SSL_connect returned=1 errno=0 state=error: certificate verify failed: [unable to get local issuer certificate for /CN=cu2.eshore.cn]
</span><span class='line'>Error: Could not retrieve catalog from remote server: SSL_connect returned=1 errno=0 state=error: certificate verify failed: [unable to get local issuer certificate for /CN=cu2.eshore.cn]
</span><span class='line'>Warning: Not using cache on failed catalog
</span><span class='line'>Error: Could not retrieve catalog; skipping run
</span><span class='line'>Error: Could not send report: SSL_connect returned=1 errno=0 state=error: certificate verify failed: [unable to get local issuer certificate for /CN=cu2.eshore.cn]
</span><span class='line'>
</span><span class='line'>[root@cu3 ~]# puppet agent --configprint ssldir
</span><span class='line'>/etc/puppetlabs/puppet/ssl
</span><span class='line'>[root@cu3 ~]# cd /etc/puppetlabs/puppet
</span><span class='line'>[root@cu3 puppet]# rm -rf ssl/*
</span><span class='line'>[root@cu3 puppet]# puppet agent --server cu2.eshore.cn --test
</span><span class='line'>Info: Creating a new SSL key for cu3.eshore.cn
</span><span class='line'>Info: Caching certificate for ca
</span><span class='line'>Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
</span><span class='line'>Info: Creating a new SSL certificate request for cu3.eshore.cn
</span><span class='line'>Info: Certificate Request fingerprint (SHA256): 9D:58:14:C0:CA:DD:51:77:0B:3F:EB:09:02:9B:D6:67:04:FD:48:7A:6E:CB:83:43:8D:5B:A9:78:0C:89:90:5B
</span><span class='line'>Info: Caching certificate for ca
</span><span class='line'>Exiting; no certificate found and waitforcert is disabled
</span><span class='line'>
</span><span class='line'>[root@cu2 puppet]# puppet cert list -all
</span><span class='line'>  "cu3.eshore.cn" (SHA256) 9D:58:14:C0:CA:DD:51:77:0B:3F:EB:09:02:9B:D6:67:04:FD:48:7A:6E:CB:83:43:8D:5B:A9:78:0C:89:90:5B
</span><span class='line'>+ "cu2.eshore.cn" (SHA256) BA:C4:C9:CC:92:6E:45:2E:B1:7F:BC:15:49:0A:2C:BB:5F:C6:B0:73:EB:6C:21:EA:C8:A6:DD:2D:FE:DF:67:70 (alt names: "DNS:puppet", "DNS:cu2.eshore.cn")
</span><span class='line'>[root@cu2 puppet]# puppet cert sign --all
</span><span class='line'>Notice: Signed certificate request for cu3.eshore.cn
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest cu3.eshore.cn at '/etc/puppetlabs/puppet/ssl/ca/requests/cu3.eshore.cn.pem'
</span><span class='line'>
</span><span class='line'>[root@cu3 puppet]# puppet agent --server cu2.eshore.cn --test
</span><span class='line'>Info: Caching certificate for cu3.eshore.cn
</span><span class='line'>Info: Caching certificate_revocation_list for ca
</span><span class='line'>Info: Caching certificate for cu3.eshore.cn
</span><span class='line'>Info: Using configured environment 'production'
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Info: Caching catalog for cu3.eshore.cn
</span><span class='line'>Info: Applying configuration version '1461205206'
</span><span class='line'>Notice: Applied catalog in 0.01 seconds
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 配置autosign
</span><span class='line'># https://docs.puppet.com/puppet/4.4/reference/ssl_autosign.html
</span><span class='line'># 在CA的服务器配置的master节点下配置autosign: Naïve Autosigning
</span><span class='line'>[root@cu2 puppet]# vi puppet.conf 
</span><span class='line'>...
</span><span class='line'>autosign = true
</span><span class='line'># 或者添加配置文件: Basic Autosigning (autosign.conf)
</span><span class='line'>[root@cu2 puppet]# vi autosign.conf
</span><span class='line'>*.eshore.cn
</span><span class='line'>
</span><span class='line'>[root@cu2 puppet]# service puppetserver restart
</span><span class='line'>Stopping puppetserver:                                     [  OK  ]
</span><span class='line'>Starting puppetserver:                                     [  OK  ]
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># agent 自动重新签名
</span><span class='line'>[root@cu1 ~]# cd /etc/puppetlabs/puppet/
</span><span class='line'>[root@cu1 puppet]# rm -rf ssl/*
</span><span class='line'>[root@cu1 puppet]# 
</span><span class='line'>[root@cu1 puppet]# puppet agent --server cu2.eshore.cn --test
</span><span class='line'>Info: Creating a new SSL key for cu1.eshore.cn
</span><span class='line'>Info: Caching certificate for ca
</span><span class='line'>Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
</span><span class='line'>Info: Creating a new SSL certificate request for cu1.eshore.cn
</span><span class='line'>Info: Certificate Request fingerprint (SHA256): D1:F5:6D:A4:91:57:DF:92:47:98:B7:C6:78:E5:C5:E0:AA:DA:70:90:0D:68:48:09:81:FA:65:98:02:F0:84:A9
</span><span class='line'>Info: Caching certificate for cu1.eshore.cn
</span><span class='line'>Info: Caching certificate_revocation_list for ca
</span><span class='line'>Info: Caching certificate for ca
</span><span class='line'>Info: Using configured environment 'production'
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Info: Caching catalog for cu1.eshore.cn
</span><span class='line'>Info: Applying configuration version '1461205750'
</span><span class='line'>Notice: Applied catalog in 0.02 seconds
</span><span class='line'>
</span><span class='line'>[root@cu2 puppet]# puppet cert list -all
</span><span class='line'>+ "cu1.eshore.cn" (SHA256) F9:48:1D:85:A7:44:78:71:AA:44:02:3F:98:20:DB:20:B1:DA:10:EC:3A:6A:AE:85:D4:37:EC:9E:20:AB:84:AA
</span><span class='line'>+ "cu2.eshore.cn" (SHA256) BA:C4:C9:CC:92:6E:45:2E:B1:7F:BC:15:49:0A:2C:BB:5F:C6:B0:73:EB:6C:21:EA:C8:A6:DD:2D:FE:DF:67:70 (alt names: "DNS:puppet", "DNS:cu2.eshore.cn")
</span><span class='line'>+ "cu3.eshore.cn" (SHA256) BA:00:57:50:1D:91:40:0D:7D:E4:C5:99:6F:3F:77:D6:E8:C4:71:5B:8D:8C:AB:FA:D0:D4:5C:36:5D:AB:A7:1B
</span><span class='line'>+ "cu4.eshore.cn" (SHA256) 96:64:4A:73:EC:D7:A6:0D:73:37:82:33:2D:0D:B3:BF:A6:A8:6B:9B:D4:05:D0:2C:46:3B:E2:22:6E:43:39:91
</span><span class='line'>+ "cu5.eshore.cn" (SHA256) 54:48:34:BF:C9:60:8C:4C:D2:9D:C9:A3:52:2E:EB:29:AC:2E:84:2E:9E:34:F1:A3:30:83:46:0E:BF:A9:5D:9A
</span></code></pre></td></tr></table></div></figure>


<p>autosign 除了使用 autosign.conf 配置，还可以使用 shell/命令 来进行适配，具体查看官网文档： <a href="https://docs.puppet.com/puppet/4.4/reference/ssl_autosign.html">https://docs.puppet.com/puppet/4.4/reference/ssl_autosign.html</a></p>

<p>agent执行同步命令每次都要指定server很麻烦，可以修改 puppet.conf 配置，每次执行是从配置文件读取：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 plugins]# vi /etc/puppetlabs/puppet/puppet.conf 
</span><span class='line'>...
</span><span class='line'>[agent]
</span><span class='line'>server = cu2.eshore.cn
</span><span class='line'>certname = cu2.eshore.cn  # 主机名不确定情况下，可以通过这个来指定当前机器的主机名！！每台机器根据主机单独设置！
</span></code></pre></td></tr></table></div></figure>


<h1>命令合集</h1>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>puppet agent --server cu2.eshore.cn --test
</span><span class='line'>
</span><span class='line'>puppet cert list -all
</span><span class='line'>
</span><span class='line'>puppet node clean cu1.ds.ctyun 
</span><span class='line'>puppet cert clean cu2.ds.ctyun
</span><span class='line'>puppet certificate_request destroy cu1.ds.ctyun
</span><span class='line'>
</span><span class='line'>puppet cert sign cu1.eshore.cn
</span><span class='line'>puppet cert sign --all
</span><span class='line'>
</span><span class='line'>puppet config print certname
</span><span class='line'>puppet master --configprint ssldir
</span><span class='line'>puppet agent --configprint ssldir
</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/15/alluxio-quickstart2/">Alluxio入门大全2</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-15T00:41:12+08:00" pubdate data-updated="true">Fri 2016-04-15 00:41</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>alluxio就是原来的tachyon。老大是华人，文档自然就有福利，把en改成cn就可以查看中文版的文档了。</p>

<p><a href="http://alluxio.org/documentation/master/cn/Architecture.html">http://alluxio.org/documentation/master/cn/Architecture.html</a></p>

<p>注意：docker暂时不能部署alluxio： <strong>mount: permission denied</strong></p>

<p>首先介绍alluxio的编译，然后进行本地和集群两种方式的部署，同时介绍HDFS底层存储系统配置和一些常用命令行的使用，最后通过代码和spark读写Alluxio数据，以及升级到V1.1查看系统的Metrics指标来了解存储系统使用情况。</p>

<p>回头看：Alluxio启动时会挂载一个Mem内存盘，其实可以把内存盘路径指定到 /dev/shm 。其他操作就很简单了，也不需要root权限。</p>

<h1>编译</h1>

<ul>
<li><a href="http://alluxio.org/documentation/master/en/Building-Alluxio-Master-Branch.html">http://alluxio.org/documentation/master/en/Building-Alluxio-Master-Branch.html</a></li>
<li><a href="http://alluxio.org/documentation/master/en/Running-Alluxio-Locally.html">http://alluxio.org/documentation/master/en/Running-Alluxio-Locally.html</a></li>
<li><a href="http://alluxio.org/documentation/master/en/Running-Alluxio-on-a-Cluster.html">http://alluxio.org/documentation/master/en/Running-Alluxio-on-a-Cluster.html</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 下载官网打包的bin.tar.gz。不推荐去github下v1.0.1，编译时findbug检查server有两个bug
</span><span class='line'>http://alluxio.org/downloads/files/1.0.1/alluxio-1.0.1-bin.tar.gz
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 ~]$ cd ~/sources/alluxio-1.0.1/
</span><span class='line'>[hadoop@cu2 alluxio-1.0.1]$ export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>[hadoop@cu2 alluxio-1.0.1]$ mvn clean package assembly:single -Phadoop-2.6 -Dhadoop.version=2.6.3 -Pyarn,spark -Dmaven.test.skip=true -Dmaven.javadoc.skip=true
</span></code></pre></td></tr></table></div></figure>


<p>编译成功后会生成 assembly/target/alluxio-1.0.1.tar.gz 文件。部署的时刻直接用编译好的 tar.gz 就行了，内容比较简洁和清晰。</p>

<p>还有一个问题，不要加Profile <strong>compileJsp</strong> ，编译没问题但是部署后访问网页抛 ClassNotFound 异常。</p>

<p>windows alluxio-1.1-snapshot 编译需要注意下。打包 assembly 的时刻换行符没有格式化，还有 mvn 编译时需要用到 test 项目(改成skipTests)。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git diff assembly/src/main/assembly/alluxio-dist.xml
</span><span class='line'>diff --git a/assembly/src/main/assembly/alluxio-dist.xml b/assembly/src/main/assembly/alluxio-dist.xml
</span><span class='line'>index 14ecd19..06ddd51 100644
</span><span class='line'>--- a/assembly/src/main/assembly/alluxio-dist.xml
</span><span class='line'>+++ b/assembly/src/main/assembly/alluxio-dist.xml
</span><span class='line'>@@ -11,6 +11,7 @@
</span><span class='line'>       &lt;outputDirectory&gt;/bin&lt;/outputDirectory&gt;
</span><span class='line'>       &lt;fileMode&gt;0755&lt;/fileMode&gt;
</span><span class='line'>       &lt;directoryMode&gt;0755&lt;/directoryMode&gt;
</span><span class='line'>+      &lt;lineEnding&gt;unix&lt;/lineEnding&gt;
</span><span class='line'>     &lt;/fileSet&gt;
</span><span class='line'>     &lt;fileSet&gt;
</span><span class='line'>       &lt;directory&gt;${basedir}/../conf&lt;/directory&gt;
</span><span class='line'>@@ -19,6 +20,7 @@
</span><span class='line'>     &lt;fileSet&gt;
</span><span class='line'>       &lt;directory&gt;${basedir}/../libexec&lt;/directory&gt;
</span><span class='line'>       &lt;outputDirectory&gt;/libexec&lt;/outputDirectory&gt;
</span><span class='line'>+      &lt;lineEnding&gt;unix&lt;/lineEnding&gt;
</span><span class='line'>     &lt;/fileSet&gt;
</span><span class='line'>     &lt;fileSet&gt;
</span><span class='line'>       &lt;directory&gt;${basedir}/..&lt;/directory&gt;
</span><span class='line'>
</span><span class='line'>E:\git\alluxio&gt;set MAVEN_OPTS="-Xmx2g"
</span><span class='line'>E:\git\alluxio&gt;mvn clean package assembly:single -Phadoop-2.6 -Dhadoop.version=2.6.3 -Pyarn,spark -DskipTests -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<h1>Local部署配置</h1>

<p><a href="http://alluxio.org/documentation/master/cn/Running-Alluxio-Locally.html">http://alluxio.org/documentation/master/cn/Running-Alluxio-Locally.html</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ tar zxf alluxio-1.0.1.tar.gz  
</span><span class='line'>[hadoop@hadoop-master2 ~]$ cd alluxio-1.0.1/conf/
</span><span class='line'>[hadoop@hadoop-master2 conf]$ cp alluxio-env.sh.template alluxio-env.sh
</span><span class='line'>[hadoop@hadoop-master2 conf]$ vi alluxio-env.sh
</span><span class='line'>...
</span><span class='line'>JAVA_HOME=/opt/jdk1.7.0_60
</span><span class='line'>ALLUXIO_UNDERFS_ADDRESS=/home/hadoop/tmp
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 conf]$ cd ..
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio format
</span><span class='line'>Connecting to localhost as hadoop...
</span><span class='line'>Formatting Alluxio Worker @ hadoop-master2
</span><span class='line'>Connection to localhost closed.
</span><span class='line'>Formatting Alluxio Master @ localhost
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ 
</span><span class='line'>
</span><span class='line'># 把hadoop用户加入sudo
</span><span class='line'>[root@hadoop-master2 ~]# visudo 
</span><span class='line'>...
</span><span class='line'>hadoop        ALL=(ALL)       NOPASSWD: ALL
</span><span class='line'>
</span><span class='line'># 机器原来部署过hadoop，localhost已经可以无密钥登录。
</span><span class='line'>
</span><span class='line'># 启动
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio-start.sh local
</span><span class='line'>Killed 0 processes on hadoop-master2
</span><span class='line'>Killed 0 processes on hadoop-master2
</span><span class='line'>Connecting to localhost as hadoop...
</span><span class='line'>Killed 0 processes on hadoop-master2
</span><span class='line'>Connection to localhost closed.
</span><span class='line'>Formatting RamFS: /mnt/ramdisk (1gb)
</span><span class='line'>Starting master @ localhost. Logging to /home/hadoop/alluxio-1.0.1/logs
</span><span class='line'>Starting worker @ hadoop-master2. Logging to /home/hadoop/alluxio-1.0.1/logs
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ 
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ jps
</span><span class='line'>3780 AlluxioMaster
</span><span class='line'>3845 Jps
</span><span class='line'>3807 AlluxioWorker
</span><span class='line'>
</span><span class='line'># localhost:19999 通过web页查看集群状态
</span><span class='line'>
</span><span class='line'># 关闭
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio-stop.sh all
</span><span class='line'>Killed 1 processes on hadoop-master2
</span><span class='line'>Killed 1 processes on hadoop-master2
</span><span class='line'>Connecting to localhost as hadoop...
</span><span class='line'>Killed 0 processes on hadoop-master2
</span><span class='line'>Connection to localhost closed.</span></code></pre></td></tr></table></div></figure>


<p>这里完全安装官网的步骤来弄，正式环境的时刻可以用 root 来 mount 内存盘。下面集群部署再介绍。</p>

<p><img src="/images/blogs/alluxio-local.png" alt="" /></p>

<h1>集群部署</h1>

<ul>
<li><a href="http://alluxio.org/documentation/master/cn/Running-Alluxio-on-a-Cluster.html">http://alluxio.org/documentation/master/cn/Running-Alluxio-on-a-Cluster.html</a></li>
<li>HA <a href="http://alluxio.org/documentation/master/en/Running-Alluxio-Fault-Tolerant.html">http://alluxio.org/documentation/master/en/Running-Alluxio-Fault-Tolerant.html</a></li>
</ul>


<p>步骤和Local类似。把程序部署到workers节点，所有workers节点都 mount 内存盘，然后调用 start.sh 。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># master 和 workers 的无密钥登录。部署过apache-hadoop的肯定都已经弄过了
</span><span class='line'>
</span><span class='line'># 修改配置
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ vi conf/workers 
</span><span class='line'>bigdata1
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ vi conf/alluxio-env.sh
</span><span class='line'>ALLUXIO_MASTER_ADDRESS=hadoop-master2
</span><span class='line'>
</span><span class='line'># 部署程序
</span><span class='line'># bin/alluxio copyDir &lt;dirname&gt; 慎用，会把logs目录也同步过去的，
</span><span class='line'># 当然可以修改alluxio的脚本，反正要知道脚本的作用
</span><span class='line'>[hadoop@hadoop-master2 ~]$ rsync -az alluxio-1.0.1 bigdata1:~/ --exclude=logs --exclude=/*/src --exclude=underfs --exclude=journal
</span><span class='line'>
</span><span class='line'># 使用root用户挂载(workers)节点的内存盘
</span><span class='line'># 当然还有最简单的方式，直接把 ALLUXIO_RAM_FOLDER=/dev/shm 指定到系统的tmpfs，系统的tmpfs其实也主要用的是内存。
</span><span class='line'># 变量 ALLUXIO_WORKER_MEMORY_SIZE=512MB 修改内存盘的大小，小于 /dev/shm 的空间大小。
</span><span class='line'>[root@hadoop-master2 ~]# cd /home/hadoop/alluxio-1.0.1
</span><span class='line'>[root@hadoop-master2 alluxio-1.0.1]# bin/alluxio-mount.sh Mount workers
</span><span class='line'>Connecting to bigdata1 as root...
</span><span class='line'>Warning: Permanently added 'bigdata1,192.168.191.133' (RSA) to the list of known hosts.
</span><span class='line'>Formatting RamFS: /mnt/ramdisk (1gb)
</span><span class='line'>Connection to bigdata1 closed.
</span><span class='line'>
</span><span class='line'># worker节点确认
</span><span class='line'>[hadoop@bigdata1 ~]$ mount
</span><span class='line'>/dev/mapper/VolGroup-lv_root on / type ext4 (rw)
</span><span class='line'>proc on /proc type proc (rw)
</span><span class='line'>sysfs on /sys type sysfs (rw)
</span><span class='line'>devpts on /dev/pts type devpts (rw,gid=5,mode=620)
</span><span class='line'>tmpfs on /dev/shm type tmpfs (rw,rootcontext="system_u:object_r:tmpfs_t:s0")
</span><span class='line'>/dev/sda1 on /boot type ext4 (rw)
</span><span class='line'>none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
</span><span class='line'>ramfs on /mnt/ramdisk type ramfs (rw,size=1gb)
</span><span class='line'>
</span><span class='line'># 格式化：主要是清理/创建JOURNAL目录，清理workers本地缓存(tiered-storage)目录数据
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio format
</span><span class='line'>Connecting to bigdata1 as hadoop...
</span><span class='line'>Formatting Alluxio Worker @ bigdata1
</span><span class='line'>Connection to bigdata1 closed.
</span><span class='line'>Formatting Alluxio Master @ localhost
</span><span class='line'>
</span><span class='line'># 启动
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio-start.sh all NoMount
</span><span class='line'>Killed 1 processes on hadoop-master2
</span><span class='line'>Killed 1 processes on hadoop-master2
</span><span class='line'>Connecting to bigdata1 as hadoop...
</span><span class='line'>Killed 0 processes on bigdata1
</span><span class='line'>Connection to bigdata1 closed.
</span><span class='line'>Starting master @ localhost. Logging to /home/hadoop/alluxio-1.0.1/logs
</span><span class='line'>Connecting to bigdata1 as hadoop...
</span><span class='line'>Starting worker @ bigdata1. Logging to /home/hadoop/alluxio-1.0.1/logs
</span><span class='line'>Connection to bigdata1 closed.
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ jps
</span><span class='line'>5164 AlluxioMaster
</span><span class='line'>5219 Jps
</span><span class='line'>
</span><span class='line'>[hadoop@bigdata1 alluxio-1.0.1]$ jps
</span><span class='line'>1849 Jps
</span><span class='line'>1829 AlluxioWorker
</span></code></pre></td></tr></table></div></figure>


<p>通过网页查看，如果 <strong>Running Workers</strong> 为 <strong>0</strong> ，到workers节点 alluxio-1.0.1/logs 下面去看日志然后定位问题。防火墙没开放？还是其他配置不正确，如hosts等等。</p>

<h1>命令行HelloWorld</h1>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs copyFromLocal conf/alluxio-env.sh /
</span><span class='line'>Copied conf/alluxio-env.sh to /
</span><span class='line'>
</span><span class='line'># worker节点查看内容（当前只有这一个文件啊，查看方便），block-id可以通过网页或者 fs fileInfo查看
</span><span class='line'>[hadoop@bigdata1 alluxio-1.0.1]$ tail -1 /mnt/ramdisk/alluxioworker/117440512 
</span><span class='line'>export ALLUXIO_WORKER_JAVA_OPTS="${ALLUXIO_JAVA_OPTS}"
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 在master机器上调用 persist ，在worker节点没找到对应的数据。竟然直接存储在执行命令的节点了，囧！！！
</span><span class='line'># alluxio.client.file.FileSystemUtils#persistFile
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs persist /alluxio-env.sh
</span><span class='line'>persisted file /alluxio-env.sh with size 5493
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ ll /home/hadoop/tmp/
</span><span class='line'>total 28
</span><span class='line'>-rwxrwxrwx  1 hadoop hadoop 5493 Apr 15 03:33 alluxio-env.sh
</span><span class='line'>
</span><span class='line'>[hadoop@bigdata1 alluxio-1.0.1]$ bin/alluxio fs persist /alluxio-env.sh
</span><span class='line'>/alluxio-env.sh is already persisted
</span><span class='line'>[hadoop@bigdata1 alluxio-1.0.1]$ ll /home/hadoop/tmp
</span><span class='line'>总用量 0</span></code></pre></td></tr></table></div></figure>


<p>在master调用 persist 后，再在worker节点调用 persist 竟然提示 <strong>already persisted</strong> 了。如果在分布式的情况下，本地磁盘 <strong>不适合</strong> 用于做 underfs ！！官网也是说 <strong>单节点</strong> <strong>本地文件系统</strong>。</p>

<blockquote><p>Alluxio提供了通用接口以简化插入不同的底层存储系统。目前我们支持Amazon S3，OpenStack Swift，Apache HDFS，GlusterFS以及单节点本地文件系统</p></blockquote>

<h1>使用HDFS作为底层存储</h1>

<p><a href="http://alluxio.org/documentation/master/en/Configuring-Alluxio-with-HDFS.html">http://alluxio.org/documentation/master/en/Configuring-Alluxio-with-HDFS.html</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ vi conf/alluxio-env.sh
</span><span class='line'>...
</span><span class='line'>JAVA_HOME=/opt/jdk1.7.0_60
</span><span class='line'>HADOOP_HOME=/home/hadoop/hadoop-2.6.3
</span><span class='line'>
</span><span class='line'># source $HADOOP_HOME/libexec/hadoop-config.sh
</span><span class='line'>JAVA_LIBRARY_PATH="$HADOOP_HOME/lib/native"
</span><span class='line'>ALLUXIO_JAVA_OPTS="$ALLUXIO_JAVA_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH"
</span><span class='line'>LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_LIBRARY_PATH
</span><span class='line'>
</span><span class='line'>ALLUXIO_CLASSPATH=$HADOOP_HOME/etc/hadoop:$ALLUXIO_CLASSPATH
</span><span class='line'>ALLUXIO_UNDERFS_ADDRESS=hdfs:///alluxio                       # 配置一个alluxio子路径比较好管理
</span><span class='line'>ALLUXIO_MASTER_ADDRESS=hadoop-master2
</span><span class='line'>
</span><span class='line'># 清理/创建元数据目录和workers节点本地缓冲存储的数据
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio format
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio-start.sh master
</span><span class='line'>
</span><span class='line'># master启动正常后，启动workers节点
</span><span class='line'># 上面已经用root mount了内存盘了，没有的用root执行 bin/alluxio-mount.sh Mount workers
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio-start.sh workers NoMount</span></code></pre></td></tr></table></div></figure>


<ul>
<li>使用</li>
</ul>


<p><a href="http://alluxio.org/documentation/master/en/Command-Line-Interface.html">http://alluxio.org/documentation/master/en/Command-Line-Interface.html</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs copyFromLocal  ~/hadoop-2.6.3/README.txt /
</span><span class='line'>Copied /home/hadoop/hadoop-2.6.3/README.txt to /
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs ls /
</span><span class='line'>1366.00B  04-15-2016 09:30:45:829  In Memory      /README.txt
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs location /README.txt
</span><span class='line'>/README.txt with file id 33554431 is on nodes: 
</span><span class='line'>bigdata1
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs persist /README.txt
</span><span class='line'>/README.txt is already persisted
</span><span class='line'>
</span><span class='line'># 默认文件只写到 Cache ，可以修改配置来进行修改
</span><span class='line'># alluxio.client.WriteType
</span><span class='line'>[hadoop@hadoop-master2 alluxio]$ export ALLUXIO_JAVA_OPTS="-Dalluxio.user.file.writetype.default=CACHE_THROUGH"
</span><span class='line'>[hadoop@hadoop-master2 alluxio]$ bin/alluxio fs copyFromLocal ~/hadoop-2.6.3/README.txt /                      
</span><span class='line'>Copied /home/hadoop/hadoop-2.6.3/README.txt to /
</span><span class='line'>[hadoop@hadoop-master2 alluxio]$ bin/alluxio fs fileInfo /README.txt                                           
</span><span class='line'>FileInfo{fileId=452984831, name=README.txt, path=/README.txt, ufsPath=hdfs:///alluxio/README.txt, length=1366, blockSizeBytes=536870912, creationTimeMs=1460765370996, completed=true, folder=false, pinned=false, cacheable=true, persisted=true, blockIds=[436207616], inMemoryPercentage=100, lastModificationTimesMs=1460765372423, ttl=-1, userName=, groupName=, permission=0, persistenceState=PERSISTED, mountPoint=false}
</span><span class='line'>Containing the following blocks: 
</span><span class='line'>BlockInfo{id=436207616, length=1366, locations=[BlockLocation{workerId=1, address=WorkerNetAddress{host=bigdata1, rpcPort=29998, dataPort=29999, webPort=30000}, tierAlias=MEM}]}
</span><span class='line'>
</span><span class='line'># Creates a 0 byte file. The file will be written to the under file system. 
</span><span class='line'># For example, touch can be used to create a file signifying the compeletion of analysis on a directory.
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs touch /1234.txt    
</span><span class='line'>/1234.txt has been created
</span><span class='line'>
</span><span class='line'># 已经persist的文件，重命名后，hdfs上面的文件也立即改变了
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs mv /1234.txt /4321.txt
</span><span class='line'>Renamed /1234.txt to /4321.txt
</span><span class='line'>
</span><span class='line'># 空文件没有分配实际的存储，只有元数据
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs location /4321.txt    
</span><span class='line'>/4321.txt with file id 67108863 is on nodes: 
</span><span class='line'>
</span><span class='line'># free掉memory，然后删掉underfs目录下的文件
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs free /
</span><span class='line'>/ was successfully freed from memory.
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.3]$ bin/hdfs dfs -rmr /alluxio/*
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs ls /  
</span><span class='line'>1366.00B  04-15-2016 09:30:45:829  Not In Memory  /README.txt
</span><span class='line'>0.00B     04-15-2016 09:37:48:971  In Memory      /4321.txt
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs tail /README.txt
</span><span class='line'>File does not exist: /alluxio/README.txt
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1893)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1834)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1814)
</span><span class='line'>
</span><span class='line'># 按照文件名把 README.txt 放到 underfs 目录下面
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.3]$ bin/hdfs dfs -put *.txt /alluxio/ 
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs tail /README.txt
</span><span class='line'>...
</span><span class='line'>software:
</span><span class='line'>  Hadoop Core uses the SSL libraries from the Jetty project written 
</span><span class='line'>by mortbay.org.
</span><span class='line'>
</span><span class='line'># 数据载入内存
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs load /
</span><span class='line'>/README.txt loaded
</span><span class='line'>/ loaded
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs ls /  
</span><span class='line'>1366.00B  04-15-2016 09:30:45:829  In Memory      /README.txt
</span><span class='line'>0.00B     04-15-2016 09:37:48:971  In Memory      /4321.txt
</span><span class='line'>
</span><span class='line'># 载入underfs的目录结构
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio loadufs / hdfs:///alluxio 
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs ls /
</span><span class='line'>1366.00B  04-15-2016 09:30:45:829  In Memory      /README.txt
</span><span class='line'>0.00B     04-15-2016 09:37:48:971  In Memory      /4321.txt
</span><span class='line'>15.07KB   04-15-2016 10:12:33:176  Not In Memory  /LICENSE.txt
</span><span class='line'>101.00B   04-15-2016 10:12:33:190  Not In Memory  /NOTICE.txt
</span><span class='line'>
</span><span class='line'># 通过 fileInfo 查看信息； fileId, ufsPath, 和分区blocks信息
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs fileInfo /README.txt
</span><span class='line'>
</span><span class='line'># 通配符要这么写，也是醉鸟
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.1.0-SNAPSHOT]$ bin/alluxio fs rm /\\*
</span><span class='line'>/4321.txt has been removed
</span><span class='line'>/LICENSE.txt has been removed
</span><span class='line'>/NOTICE.txt has been removed
</span><span class='line'>/README.md has been removed
</span><span class='line'>/README.txt has been removed
</span><span class='line'>
</span><span class='line'># alluxio系统中没有的文件，但是underfs包括的文件，读取一遍后元数据会载入alluxio
</span><span class='line'>[hadoop@hadoop-master2 ~]$ alluxio fs ls /
</span><span class='line'>1366.00B  04-16-2016 08:09:30:996  In Memory      /README.txt
</span><span class='line'>[hadoop@hadoop-master2 ~]$ alluxio fs cat /LICENSE.txt
</span><span class='line'>[hadoop@hadoop-master2 ~]$ alluxio fs ls /
</span><span class='line'>1366.00B  04-16-2016 08:09:30:996  In Memory      /README.txt
</span><span class='line'>15.07KB   04-16-2016 08:26:22:495  Not In Memory  /LICENSE.txt</span></code></pre></td></tr></table></div></figure>


<p>文件结构大概搞明白了，从 underfs 加载目录结构(loadufs)，文件载入alluxio内存(fs load)，alluxio文件持久化(fs persist)都有对应的命令。
理解 <a href="http://alluxio.org/documentation/master/en/Unified-and-Transparent-Namespace.html">mount</a> 和linux的mount类似，把 underfs 当做一个硬盘设备去理解。</p>

<p>但是好像没有修改文件的API，难道不支持修改？？暂时好像没有(2016-4-15 23:06:20 v1.1)</p>

<blockquote><p><a href="http://alluxio.org/documentation/master/en/Key-Value-System-API.html">http://alluxio.org/documentation/master/en/Key-Value-System-API.html</a>
Like files in Alluxio filesystem, the semantics of key-value system are also write-once</p></blockquote>

<h1>FileSystem API</h1>

<ul>
<li><a href="http://docs.scala-lang.org/tutorials/scala-with-maven.html">http://docs.scala-lang.org/tutorials/scala-with-maven.html</a></li>
<li><a href="http://alluxio.org/documentation/master/en/File-System-API.html">http://alluxio.org/documentation/master/en/File-System-API.html</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># scala
</span><span class='line'>object App {
</span><span class='line'>
</span><span class='line'>  def using[A &lt;: {def close() : Unit}, B](resource: A)(f: A =&gt; B): B =
</span><span class='line'>    try { f(resource) } finally { resource.close() }
</span><span class='line'>
</span><span class='line'>  def main(args: Array[String]) {
</span><span class='line'>    // @see alluxio.Configuration.Configuration(boolean)
</span><span class='line'>    System.setProperty(Constants.MASTER_HOSTNAME, "192.168.191.132")
</span><span class='line'>    System.setProperty("HADOOP_USER_NAME", "hadoop")
</span><span class='line'>
</span><span class='line'>    val fs = FileSystem.Factory.get();
</span><span class='line'>    val path = new AlluxioURI("/README.md");
</span><span class='line'>    using(fs.createFile(path, CreateFileOptions.defaults().setWriteType(WriteType.THROUGH))){ out =&gt;
</span><span class='line'>      val content =
</span><span class='line'>"""FileSystem API Write.
</span><span class='line'>   -------------------------
</span><span class='line'>   Hello World!
</span><span class='line'>"""
</span><span class='line'>      out.write(content.getBytes)
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    using(fs.openFile(path)) { in =&gt;
</span><span class='line'>      val buffer = new Array[Byte](1024)
</span><span class='line'>      val size = in.read(buffer)
</span><span class='line'>      System.out.println(new String(buffer, 0, size))
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'># THROUGH 仅写入到underfs
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ bin/alluxio fs ls /README.md
</span><span class='line'>115.00B   04-15-2016 20:36:57:345  Not In Memory  /README.md
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ ~/hadoop-2.6.3/bin/hadoop fs -cat /alluxio/README.md
</span><span class='line'>FileSystem API Write.
</span><span class='line'>   -------------------------
</span><span class='line'>   Hello World!
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.0.1]$ </span></code></pre></td></tr></table></div></figure>


<p>程序在win10系统运行，需要把 core-site.xml 加到 src/main/resources 下面（前面配置为了省事直接写 <strong>hdfs:///alluxio</strong>, 不加载配置的话程序不知道namenode）</p>

<p>如果<strong>把WriteType设置为 CACHE_THROUGH ，写 underfs 的同时也会写本地缓存</strong>。提交成功后，文件的状态为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 alluxio-1.1.0-SNAPSHOT]$ bin/alluxio fs ls /README.md
</span><span class='line'>115.00B   04-15-2016 23:48:33:749  In Memory      /README.md
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.1.0-SNAPSHOT]$ bin/alluxio fs fileInfo /README.md
</span><span class='line'>FileInfo{fileId=318767103, name=README.md, path=/README.md, ufsPath=hdfs:///alluxio/README.md, length=115, blockSizeBytes=536870912, creationTimeMs=1460735313749, completed=true, folder=false, pinned=false, cacheable=true, persisted=true, blockIds=[301989888], inMemoryPercentage=100, lastModificationTimesMs=1460735315749, ttl=-1, userName=, groupName=, permission=0, persistenceState=PERSISTED, mountPoint=false}
</span><span class='line'>Containing the following blocks: 
</span><span class='line'>BlockInfo{id=301989888, length=115, locations=[BlockLocation{workerId=1, address=WorkerNetAddress{host=bigdata1, rpcPort=29998, dataPort=29999, webPort=30000}, tierAlias=MEM}]}
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.1.0-SNAPSHOT]$ ~/hadoop-2.6.3/bin/hadoop fs -ls /alluxio/ 
</span><span class='line'>Found 4 items
</span><span class='line'>-rw-r--r--   1 hadoop supergroup      15429 2016-04-15 09:57 /alluxio/LICENSE.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        101 2016-04-15 09:57 /alluxio/NOTICE.txt
</span><span class='line'>-rwxrwxrwx   1 hadoop supergroup        115 2016-04-15 23:48 /alluxio/README.md</span></code></pre></td></tr></table></div></figure>


<h1>大数据程序中使用Alluxio</h1>

<p>hadoop2通过 <code>org.apache.hadoop.fs.FileSystem</code> services获取绑定的对象，所以<strong>不需要</strong>在core-site.xml里面配置 <strong>fs.alluxio.impl</strong> 和 <strong>fs.alluxio-ft.impl</strong></p>

<ul>
<li><a href="http://alluxio.org/documentation/master/en/Running-Spark-on-Alluxio.html">http://alluxio.org/documentation/master/en/Running-Spark-on-Alluxio.html</a></li>
<li><a href="http://alluxio.org/documentation/master/en/Running-Hadoop-MapReduce-on-Alluxio.html">http://alluxio.org/documentation/master/en/Running-Hadoop-MapReduce-on-Alluxio.html</a></li>
</ul>


<p>其实都是通过 <strong>Hadoop FileSystem API</strong> 来访问Alluxio的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># .bash_profile加环境变量
</span><span class='line'>[hadoop@hadoop-master2 ~]$ vi ~/.bash_profile 
</span><span class='line'>...
</span><span class='line'>HADOOP_HOME=~/hadoop
</span><span class='line'>SPARK_HOME=~/spark
</span><span class='line'>ALLUXIO_HOME=~/alluxio
</span><span class='line'>
</span><span class='line'>PATH=$HADOOP_HOME/bin:$SPARK_HOME/bin:$ALLUXIO_HOME/bin:$MAVEN_HOME/bin:$ANT_HOME/bin:$PATH
</span><span class='line'># 这里没有 export HADOOP_HOME SPARK_HOME 
</span><span class='line'># 因为在hadoop/spark的启动脚本也定义了这些变量。如果export，也需要把软链接同步到slaves节点
</span><span class='line'>export PATH ANT_HOME MAVEN_HOME
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 ~]$ ln -s hadoop-2.6.3 hadoop
</span><span class='line'>[hadoop@hadoop-master2 ~]$ ln -s alluxio-1.1.0-SNAPSHOT alluxio
</span><span class='line'>[hadoop@hadoop-master2 ~]$ ln -s spark-1.6.0-bin-2.6.3 spark
</span><span class='line'>[hadoop@hadoop-master2 ~]$ . .bash_profile 
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 ~]$ export SPARK_CLASSPATH=\
</span><span class='line'>&gt; ~/alluxio/core/client/target/alluxio-core-client-1.1.0-SNAPSHOT-jar-with-dependencies.jar 
</span><span class='line'>[hadoop@hadoop-master2 ~]$ 
</span><span class='line'>[hadoop@hadoop-master2 ~]$ spark-shell --master local
</span><span class='line'>scala&gt; val file=sc.textFile("alluxio://hadoop-master2:19998/README.txt")
</span><span class='line'>file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at &lt;console&gt;:27
</span><span class='line'>
</span><span class='line'>scala&gt; file.count()
</span><span class='line'>res0: Long = 31
</span><span class='line'>
</span><span class='line'>scala&gt; file.take(2)
</span><span class='line'>res1: Array[String] = Array(For the latest information about Hadoop, please visit our website at:, "")
</span><span class='line'>
</span><span class='line'># wordcount
</span><span class='line'>scala&gt; val op = file.flatMap(_.split(" ")).map((_,1)).reduceByKey(_ + _)
</span><span class='line'># word sort asc
</span><span class='line'>scala&gt; op.sortByKey().take(10)
</span><span class='line'># count sort desc
</span><span class='line'>scala&gt; op.map(kv =&gt; (kv._2, kv._1)).sortByKey(false).map(kv =&gt; (kv._2, kv._1)).take(10)
</span><span class='line'>
</span><span class='line'>scala&gt; op.map(kv =&gt; (kv._2, kv._1)).sortByKey(false).map(kv =&gt; (kv._2, kv._1)).saveAsTextFile("alluxio://hadoop-master2:19998/output/")
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 ~]$ alluxio fs cat /output/*
</span><span class='line'>(,18)
</span><span class='line'>(the,8)
</span><span class='line'>(and,6)
</span><span class='line'>(of,5)
</span><span class='line'>(The,4)
</span><span class='line'>(this,3)
</span><span class='line'>(encryption,3)
</span><span class='line'>(for,3)
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>如果运行在集群，在slave的节点也需要与主节点一样的目录结构。 或者按照<a href="http://spark.apache.org/docs/latest/programming-guide.html#using-the-shell">官网的教程</a>操作。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># spark_classpath 会被带到 task 的启动环境变量里面
</span><span class='line'>[hadoop@hadoop-master2 ~]$ rsync -az alluxio bigdata1:~/
</span><span class='line'>[hadoop@hadoop-master2 ~]$ export SPARK_CLASSPATH=\
</span><span class='line'>&gt; ~/alluxio/core/client/target/alluxio-core-client-1.1.0-SNAPSHOT-jar-with-dependencies.jar
</span><span class='line'> 
</span><span class='line'>[hadoop@hadoop-master2 ~]$ spark-shell --master spark://hadoop-master2:7077
</span><span class='line'>scala&gt; val file=sc.textFile("alluxio://hadoop-master2:19998/README.txt")
</span><span class='line'>file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at &lt;console&gt;:27
</span><span class='line'>
</span><span class='line'>scala&gt; val op = file.flatMap(_.split(" ")).map((_,1)).reduceByKey(_ + _)
</span><span class='line'>op: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:29
</span><span class='line'>
</span><span class='line'>scala&gt; op.map(kv =&gt; (kv._2, kv._1)).sortByKey(false).map(kv =&gt; (kv._2, kv._1)).saveAsTextFile("alluxio://hadoop-master2:19998/output2/")
</span></code></pre></td></tr></table></div></figure>


<p><img src="/images/blogs/alluxio-spark-word-count.png" alt="" /></p>

<h1>Metrics</h1>

<p><a href="http://www.alluxio.org/documentation/master/cn/Metrics-System.html">http://www.alluxio.org/documentation/master/cn/Metrics-System.html</a></p>

<p>v1.0.1有对应的api，可以通过 <a href="http://hadoop-master2:19999/metrics/json/">http://hadoop-master2:19999/metrics/json/</a> 查看。当前master主干分支v1.1.0可以在网页上面查看这些指标。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 拷贝配置
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.1.0-SNAPSHOT]$ cd conf
</span><span class='line'>[hadoop@hadoop-master2 conf]$ cp ~/alluxio-1.0.1/conf/alluxio-env.sh ./
</span><span class='line'>[hadoop@hadoop-master2 conf]$ cp ~/alluxio-1.0.1/conf/log4j.properties ./
</span><span class='line'>[hadoop@hadoop-master2 conf]$ cp ~/alluxio-1.0.1/conf/workers ./ 
</span><span class='line'>
</span><span class='line'># 启动master（使用原来的元数据）
</span><span class='line'># 共享元数据，在 alluxio-env.sh 修改环境变量 ALLUXIO_JAVA_OPTS 
</span><span class='line'># 添加 -Dalluxio.master.journal.folder=${ALLUXIO_JOURNAL_FOLDER} / ALLUXIO_JOURNAL_FOLDER=/home/hadoop/journal
</span><span class='line'>[hadoop@hadoop-master2 alluxio-1.1.0-SNAPSHOT]$ bin/alluxio-start.sh master
</span><span class='line'>Starting master @ hadoop-master2. Logging to /home/hadoop/alluxio-1.1.0-SNAPSHOT/logs</span></code></pre></td></tr></table></div></figure>


<p>v1.1.0 页面多了 Metrics 页签：</p>

<p><img src="/images/blogs/alluxio-metrics.png" alt="" /></p>

<h1>其他文档</h1>

<ul>
<li>配置alluxio-default.properties <a href="http://alluxio.org/documentation/master/en/Configuration-Settings.html">http://alluxio.org/documentation/master/en/Configuration-Settings.html</a></li>
<li>分层本地缓存 <a href="http://alluxio.org/documentation/master/en/Tiered-Storage-on-Alluxio.html">http://alluxio.org/documentation/master/en/Tiered-Storage-on-Alluxio.html</a></li>
<li><a href="https://dzone.com/articles/Accelerate-In-Memory-Processing-with-Spark-from-Hours-to-Seconds-With-Tachyon">https://dzone.com/articles/Accelerate-In-Memory-Processing-with-Spark-from-Hours-to-Seconds-With-Tachyon</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/13/hiveserver2-ui-and-upgrade-hive2-dot-0-0/">Hiveserver2 Ui and Upgrade hive2.0.0</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-13T12:03:43+08:00" pubdate data-updated="true">Wed 2016-04-13 12:03</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>升级hive的标准动作：</p>

<ul>
<li>更新metadata，就是执行sql语句。更新前先备份原来的库！！</li>
<li>调整依赖，我这里是升级spark，编译参考<a href="/blog/2016/03/28/hive-on-spark/">spark-without-hive</a></li>
<li>修改参数(hive/spark/hadoop)来适应新版本</li>
<li>hiveserver2 ui：启动hiveserver2服务，访问10002端口即可。<a href="https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2#SettingUpHiveServer2-WebUIforHiveServer2">UI配置</a></li>
</ul>


<p>环境说明：</p>

<ul>
<li>centos5</li>
<li>hadoop-2.6.3</li>
<li>spark-1.6.0-without-hive</li>
<li>hive-2.0.0</li>
</ul>


<h2>操作详情</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
<span class='line-number'>192</span>
<span class='line-number'>193</span>
<span class='line-number'>194</span>
<span class='line-number'>195</span>
<span class='line-number'>196</span>
<span class='line-number'>197</span>
<span class='line-number'>198</span>
<span class='line-number'>199</span>
<span class='line-number'>200</span>
<span class='line-number'>201</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 备份
</span><span class='line'>[hadoop@file1 tools]$ mysqldump -uroot -p hive &gt;hive1.2.1-20160413.backup.sql
</span><span class='line'>
</span><span class='line'># 准备好程序后的目录结构
</span><span class='line'>[hadoop@file1 ~]$ ll
</span><span class='line'>总计 20
</span><span class='line'>drwxrwxr-x 3 hadoop hadoop 4096 04-13 11:59 collect
</span><span class='line'>drwx------ 3 hadoop hadoop 4096 04-07 16:43 dfs
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop   18 04-11 10:09 hadoop -&gt; tools/hadoop-2.6.3
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop   40 04-13 10:26 hive -&gt; /home/hadoop/tools/apache-hive-2.0.0-bin
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop   42 04-13 10:52 spark -&gt; tools/spark-1.6.0-bin-hadoop2-without-hive
</span><span class='line'>drwxrwxr-x 6 hadoop hadoop 4096 04-13 12:10 tmp
</span><span class='line'>drwxrwxr-x 9 hadoop hadoop 4096 04-13 11:48 tools
</span><span class='line'>[hadoop@file1 tools]$ ll
</span><span class='line'>总计 84
</span><span class='line'>drwxrwxr-x  8 hadoop hadoop  4096 04-08 09:25 apache-hive-1.2.1-bin
</span><span class='line'>drwxrwxr-x  8 hadoop hadoop  4096 04-13 10:16 apache-hive-2.0.0-bin
</span><span class='line'>drwxr-xr-x 11 hadoop hadoop  4096 04-07 16:34 hadoop-2.6.3
</span><span class='line'>-rw-rw-r--  1 hadoop hadoop 46879 04-13 10:11 hive1.2.1-20160413.backup.sql
</span><span class='line'>drwxrwxr-x  2 hadoop hadoop  4096 03-31 15:28 mysql
</span><span class='line'>lrwxrwxrwx  1 hadoop hadoop    36 04-13 10:17 spark -&gt; spark-1.6.0-bin-hadoop2-without-hive
</span><span class='line'>drwxrwxr-x 11 hadoop hadoop  4096 04-07 18:23 spark-1.3.1-bin-hadoop2.6.3-without-hive
</span><span class='line'>drwxrwxr-x 11 hadoop hadoop  4096 03-28 11:15 spark-1.6.0-bin-hadoop2-without-hive
</span><span class='line'>drwxr-xr-x 11 hadoop hadoop  4096 03-31 16:14 zookeeper-3.4.6
</span><span class='line'>
</span><span class='line'># 环境变量我直接加载的是link软链接的，我这直接修改软链就行了。根据情况调整。
</span><span class='line'># apache-hive-2.0.0-bin同级目录建立spark软链接，或者再hive-env.sh中指定SPARK_HOME的位置
</span><span class='line'>
</span><span class='line'># hive-1.2.1并没有txn的表，所有要单独执行下hive-txn-schema-2.0.0.mysql.sql，
</span><span class='line'># 然后再更新（后面的Duplicate column的错没问题的）
</span><span class='line'>[hadoop@file1 tools]$ cd apache-hive-2.0.0-bin/scripts/metastore/upgrade/mysql/
</span><span class='line'>[hadoop@file1 mysql]$ mysql -uroot -p
</span><span class='line'>Enter password: 
</span><span class='line'>Welcome to the MySQL monitor.  Commands end with ; or \g.
</span><span class='line'>Your MySQL connection id is 10765
</span><span class='line'>Server version: 5.5.48 MySQL Community Server (GPL)
</span><span class='line'>
</span><span class='line'>Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.
</span><span class='line'>
</span><span class='line'>Oracle is a registered trademark of Oracle Corporation and/or its
</span><span class='line'>affiliates. Other names may be trademarks of their respective
</span><span class='line'>owners.
</span><span class='line'>
</span><span class='line'>Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
</span><span class='line'>
</span><span class='line'>mysql&gt; use hive;
</span><span class='line'>Reading table information for completion of table and column names
</span><span class='line'>You can turn off this feature to get a quicker startup with -A
</span><span class='line'>
</span><span class='line'>Database changed
</span><span class='line'>mysql&gt; source hive-txn-schema-2.0.0.mysql.sql
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.04 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.03 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.04 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.03 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>
</span><span class='line'>mysql&gt; source upgrade-1.2.0-to-2.0.0.mysql.sql
</span><span class='line'>+------------------------------------------------+
</span><span class='line'>|                                                |
</span><span class='line'>+------------------------------------------------+
</span><span class='line'>| Upgrading MetaStore schema from 1.2.0 to 2.0.0 |
</span><span class='line'>+------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>+---------------------------------------------------------------------------------------------------------------+
</span><span class='line'>|                                                                                                               |
</span><span class='line'>+---------------------------------------------------------------------------------------------------------------+
</span><span class='line'>| &lt; HIVE-7018 Remove Table and Partition tables column LINK_TARGET_ID from Mysql for other DBs do not have it &gt; |
</span><span class='line'>+---------------------------------------------------------------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected, 1 warning (0.03 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>
</span><span class='line'>+---------------------------------+
</span><span class='line'>| Completed remove LINK_TARGET_ID |
</span><span class='line'>+---------------------------------+
</span><span class='line'>| Completed remove LINK_TARGET_ID |
</span><span class='line'>+---------------------------------+
</span><span class='line'>1 row in set (0.02 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.02 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 31 rows affected (0.01 sec)
</span><span class='line'>Records: 31  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.05 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.02 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.03 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.00 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'CQ_HIGHEST_TXN_ID'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'CQ_META_INFO'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'CQ_HADOOP_JOB_ID'
</span><span class='line'>ERROR 1050 (42S01): Table 'COMPLETED_COMPACTIONS' already exists
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'TXN_AGENT_INFO'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'TXN_HEARTBEAT_COUNT'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'HL_HEARTBEAT_COUNT'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'TXN_META_INFO'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'HL_AGENT_INFO'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'HL_BLOCKEDBY_EXT_ID'
</span><span class='line'>ERROR 1060 (42S21): Duplicate column name 'HL_BLOCKEDBY_INT_ID'
</span><span class='line'>ERROR 1050 (42S01): Table 'AUX_TABLE' already exists
</span><span class='line'>Query OK, 1 row affected (0.01 sec)
</span><span class='line'>Rows matched: 1  Changed: 1  Warnings: 0
</span><span class='line'>
</span><span class='line'>+---------------------------------------------------------+
</span><span class='line'>|                                                         |
</span><span class='line'>+---------------------------------------------------------+
</span><span class='line'>| Finished upgrading MetaStore schema from 1.2.0 to 2.0.0 |
</span><span class='line'>+---------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'># 拷贝hive原来的配置和依赖jar
</span><span class='line'>
</span><span class='line'>[hadoop@file1 mysql]$ cd ~/tools/apache-hive-2.0.0-bin/conf/
</span><span class='line'>[hadoop@file1 conf]$ cp ~/tools/apache-hive-1.2.1-bin/conf/hive-site.xml ./
</span><span class='line'>[hadoop@file1 conf]$ cp ~/tools/apache-hive-1.2.1-bin/conf/spark-defaults.conf ./
</span><span class='line'>[hadoop@file1 conf]$ cp ~/tools/apache-hive-1.2.1-bin/conf/hive-env.sh ./
</span><span class='line'>
</span><span class='line'># 用到spark需要加大PermSize
</span><span class='line'>[hadoop@file1 hive]$ vi conf/hive-env.sh
</span><span class='line'>export HADOOP_USER_CLASSPATH_FIRST=true
</span><span class='line'>export HADOOP_OPTS="$HADOOP_OPTS -XX:MaxPermSize=256m"
</span><span class='line'>
</span><span class='line'>[hadoop@file1 conf]$ cd ../lib/
</span><span class='line'>[hadoop@file1 lib]$ cp ~/tools/apache-hive-1.2.1-bin/lib/mysql-connector-java-5.1.34.jar ./
</span><span class='line'>
</span><span class='line'># centos5需要删除下面两个jar，centos6没必要删
</span><span class='line'>[hadoop@file1 apache-hive-2.0.0-bin]$ rm lib/hive-jdbc-2.0.0-standalone.jar 
</span><span class='line'>[hadoop@file1 apache-hive-2.0.0-bin]$ rm lib/snappy-java-1.0.5.jar 
</span><span class='line'>
</span><span class='line'># spark-1.6.0更新
</span><span class='line'>
</span><span class='line'># http://spark.apache.org/docs/latest/hadoop-provided.html
</span><span class='line'># http://stackoverflow.com/questions/30906412/noclassdeffounderror-com-apache-hadoop-fs-fsdatainputstream-when-execute-spark-s
</span><span class='line'>[hadoop@file1 apache-hive-2.0.0-bin]$ cd ~/tools/spark-1.6.0-bin-hadoop2-without-hive/conf/
</span><span class='line'>[hadoop@file1 conf]$ cp spark-env.sh.template spark-env.sh
</span><span class='line'>[hadoop@file1 conf]$ vi spark-env.sh
</span><span class='line'>HADOOP_HOME=/home/hadoop/hadoop
</span><span class='line'>SPARK_DIST_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath`
</span><span class='line'>
</span><span class='line'>[hadoop@file1 ~]$ cp ~/tools/spark-1.6.0-bin-hadoop2-without-hive/lib/spark-1.6.0-yarn-shuffle.jar ~/tools/hadoop-2.6.3/share/hadoop/yarn/
</span><span class='line'>[hadoop@file1 ~]$ rm ~/tools/hadoop-2.6.3/share/hadoop/yarn/spark-1.3.1-yarn-shuffle.jar 
</span><span class='line'>
</span><span class='line'>[hadoop@file1 ~]$ rsync -vaz --delete ~/tools/hadoop-2.6.3/share file2:~/tools/hadoop-2.6.3/ 
</span><span class='line'>[hadoop@file1 ~]$ rsync -vaz --delete ~/tools/hadoop-2.6.3/share file3:~/tools/hadoop-2.6.3/ 
</span><span class='line'>
</span><span class='line'>[hadoop@file1 ~]$ hdfs dfs -put ~/tools/spark-1.6.0-bin-hadoop2-without-hive/lib/spark-assembly-1.6.0-hadoop2.6.3.jar /spark/
</span><span class='line'>
</span><span class='line'>[hadoop@file1 apache-hive-2.0.0-bin]$ vi conf/spark-defaults.conf 
</span><span class='line'>spark.yarn.jar    hdfs:///spark/spark-assembly-1.6.0-hadoop2.6.3.jar
</span><span class='line'>
</span><span class='line'># 重启yarn（如果你用hiveserver2，先往下看，后面还会修改配置重启的）
</span><span class='line'>
</span><span class='line'>[hadoop@file1 apache-hive-2.0.0-bin]$ cd ~/tools/hadoop-2.6.3/
</span><span class='line'>[hadoop@file1 hadoop-2.6.3]$ sbin/stop-yarn.sh 
</span><span class='line'>[hadoop@file1 hadoop-2.6.3]$ sbin/start-yarn.sh 
</span></code></pre></td></tr></table></div></figure>


<p>更新到这里，执行hive命令是ok了的。但是hiveserver还有问题。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 启动hiveserver2
</span><span class='line'>[hadoop@file1 hive]$ nohup bin/hiveserver2 &
</span><span class='line'>
</span><span class='line'># 启动spark historyserver
</span><span class='line'>[hadoop@file1 spark]$ cat start-historyserver.sh 
</span><span class='line'>source $HADOOP_HOME/libexec/hadoop-config.sh
</span><span class='line'>sbin/start-history-server.sh hdfs:///spark-eventlogs
</span><span class='line'>
</span><span class='line'>[hadoop@file1 hive]$ bin/beeline -u jdbc:hive2://file1:10000/ -n hadoop -p hadoop
</span><span class='line'>which: no hbase in (/home/hadoop/hadoop/bin:/home/hadoop/hive/bin:/opt/jdk1.7.0_60/bin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin:/home/hadoop/tools/hadoop-2.6.3/bin:/home/hadoop/tools/hadoop-2.6.3:/home/hadoop/tools/apache-hive-1.2.1-bin:/home/hadoop/bin)
</span><span class='line'>ls: /home/hadoop/hive/lib/hive-jdbc-*-standalone.jar: 没有那个文件或目录
</span><span class='line'>SLF4J: Class path contains multiple SLF4J bindings.
</span><span class='line'>SLF4J: Found binding in [jar:file:/home/hadoop/tools/apache-hive-2.0.0-bin/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
</span><span class='line'>SLF4J: Found binding in [jar:file:/home/hadoop/tools/hadoop-2.6.3/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
</span><span class='line'>SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
</span><span class='line'>SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
</span><span class='line'>Connecting to jdbc:hive2://file1:10000/
</span><span class='line'>Error: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: hadoop is not allowed to impersonate hadoop (state=,code=0)
</span><span class='line'>Beeline version 2.0.0 by Apache Hive
</span><span class='line'>beeline&gt; </span></code></pre></td></tr></table></div></figure>


<p>Beeline连接hiveserver2失败，模拟的hadoop用户授权失败。需要修改hadoop的参数。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># https://community.hortonworks.com/questions/4905/error-while-running-hive-queries-from-zeppelin.html
</span><span class='line'># http://stackoverflow.com/questions/25073792/error-e0902-exception-occured-user-root-is-not-allowed-to-impersonate-root
</span><span class='line'># core-site.xml添加，并重启集群hdfs & yarn
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@file1 hadoop-2.6.3]$ sbin/stop-all.sh
</span><span class='line'>[hadoop@file1 hadoop-2.6.3]$ sbin/start-all.sh 
</span><span class='line'>
</span><span class='line'>[hadoop@file1 hive]$ bin/beeline -u jdbc:hive2://file1:10000 -n hadoop -p hadoop
</span><span class='line'>...
</span><span class='line'>0: jdbc:hive2://file1:10000/&gt; set hive.execution.engine=spark;
</span><span class='line'>No rows affected (0.019 seconds)
</span><span class='line'>0: jdbc:hive2://file1:10000/&gt; select count(*) from t_info where edate=20160413;
</span><span class='line'>INFO  : Compiling command(queryId=hadoop_20160413114039_f930d3e7-af83-4b12-a536-404a4e20eeea): select count(*) from t_info where edate=20160413
</span><span class='line'>INFO  : Semantic Analysis Completed
</span><span class='line'>INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:c0, type:bigint, comment:null)], properties:null)
</span><span class='line'>INFO  : Completed compiling command(queryId=hadoop_20160413114039_f930d3e7-af83-4b12-a536-404a4e20eeea); Time taken: 0.523 seconds
</span><span class='line'>INFO  : Executing command(queryId=hadoop_20160413114039_f930d3e7-af83-4b12-a536-404a4e20eeea): select count(*) from t_info where edate=20160413
</span><span class='line'>INFO  : Query ID = hadoop_20160413114039_f930d3e7-af83-4b12-a536-404a4e20eeea
</span><span class='line'>INFO  : Total jobs = 1
</span><span class='line'>INFO  : Launching Job 1 out of 1
</span><span class='line'>INFO  : Starting task [Stage-1:MAPRED] in serial mode
</span><span class='line'>
</span><span class='line'>INFO  : 
</span><span class='line'>Query Hive on Spark job[0] stages:
</span><span class='line'>INFO  : 0
</span><span class='line'>INFO  : 1
</span><span class='line'>INFO  : 
</span><span class='line'>Status: Running (Hive on Spark job[0])
</span><span class='line'>INFO  : Job Progress Format
</span><span class='line'>CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
</span><span class='line'>INFO  : 2016-04-13 11:41:20,519 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:23,577 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:26,817 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:29,858 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:32,903 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:35,942 Stage-0_0: 0(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:37,969 Stage-0_0: 0(+9)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:38,981 Stage-0_0: 1(+8)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:39,994 Stage-0_0: 3(+7)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:43,030 Stage-0_0: 3(+7)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:45,056 Stage-0_0: 5(+5)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:46,072 Stage-0_0: 6(+4)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:47,085 Stage-0_0: 8(+2)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:48,096 Stage-0_0: 9(+1)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:51,125 Stage-0_0: 9(+1)/10     Stage-1_0: 0/1
</span><span class='line'>INFO  : 2016-04-13 11:41:52,134 Stage-0_0: 10/10 Finished       Stage-1_0: 1/1 Finished
</span><span class='line'>INFO  : Status: Finished successfully in 64.78 seconds
</span><span class='line'>INFO  : Completed executing command(queryId=hadoop_20160413114039_f930d3e7-af83-4b12-a536-404a4e20eeea); Time taken: 71.767 seconds
</span><span class='line'>INFO  : OK
</span><span class='line'>+-----------+--+
</span><span class='line'>|    c0     |
</span><span class='line'>+-----------+--+
</span><span class='line'>| 89867722  |
</span><span class='line'>+-----------+--+
</span><span class='line'>1 row selected (72.45 seconds)
</span></code></pre></td></tr></table></div></figure>


<p>本来升级是想看看UI长什么样子，有点失望，功能太少了。只能看当前执行的SQL和session，历史记录不能查看。期待新版本UI更强大。</p>

<p>升级后beeline上下键切换历史的也不起作用了，hive-2.0.0没也啥吸引的功能（<strong>hive2准备淘汰mr了</strong>），觉得不爽可以直接替换 软链 退回hive1.2.1-spark1.3.1（实践后没问题，spark.yarn.jar记得改）</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/11/spark-on-yarn-memory-allocate/">Spark-on-yarn内存分配</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-11T19:44:51+08:00" pubdate data-updated="true">Mon 2016-04-11 19:44</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>上次写了一篇关于配置参数是如何影响mapreduce的实际调度的<a href="/blog/2016/03/17/hadoop-memory-opts-and-args/">参考</a>：</p>

<ul>
<li>opts（yarn.app.mapreduce.am.command-opts、mapreduce.map.java.opts、mapreduce.reduce.java.opts）是实际运行程序是内存参数。</li>
<li>memory（yarn.app.mapreduce.am.resource.mb、mapreduce.map.memory.mb、mapreduce.reduce.memory.mb）是用于ResourceManager计算集群资源使用和调度。</li>
</ul>


<p>了解参数区别，就没有再深究task内存的问题了。</p>

<h2>新问题-内存分配</h2>

<p>这次又遇到内存问题：spark使用yarn-client的方式运行时，spark有memoryOverhead的设置，但是加了额外的内存后，再经过集群调度内存浪费严重，对于本来就小内存的集群来说完全无法接受。</p>

<ul>
<li>am默认是512加上384 overhead，也就是896m。但是调度后am分配内存资源为1024。</li>
<li>executor默认是1024加上384，等于1408M。单调度后executor分配内存资源为2048。</li>
</ul>


<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-0.png" alt="" /></p>

<p>从appmaster的日志可以看出来请求的内存大小是1408：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-1.png" alt="" /></p>

<p><strong>一个executor就浪费了500M，本来可以跑4个executor的但现在只能执行3个！</strong></p>

<p>关于内存参数的具体含义查看官网： <a href="http://spark.apache.org/docs/latest/running-on-yarn.html">spark-on-yarn</a> 和 <a href="http://hadoop.apache.org/docs/r2.6.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a></p>

<table>
<thead>
<tr>
<th></th>
<th style="text-align:center;"> <em>参数</em>                                  </th>
<th></th>
<th style="text-align:left;"> <em>值</em></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.am.memory                    </td>
<td></td>
<td style="text-align:left;"> 512m</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.driver.memory                     </td>
<td></td>
<td style="text-align:left;"> 1g</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.executor.memoryOverhead      </td>
<td></td>
<td style="text-align:left;"> executorMemory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.driver.memoryOverhead        </td>
<td></td>
<td style="text-align:left;"> driverMemory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.am.memoryOverhead            </td>
<td></td>
<td style="text-align:left;"> AM memory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.nodemanager.resource.memory-mb     </td>
<td></td>
<td style="text-align:left;"> 8192</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.scheduler.minimum-allocation-mb    </td>
<td></td>
<td style="text-align:left;"> 1024</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.scheduler.maximum-allocation-mb    </td>
<td></td>
<td style="text-align:left;"> 8192</td>
</tr>
</tbody>
</table>


<p>分配的内存看着像是 <strong>最小分配内存</strong> 的整数倍。把 <code>yarn.scheduler.minimum-allocation-mb</code> 修改为512，重启yarn再运行，executor的分配的内存果真减少到1536(<strong>512*3</strong>)。</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-3.png" alt="" /></p>

<p>同时 <a href="http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html">http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html</a> 这篇文章也讲 <strong>在YARN中，Container申请的内存大小必须为yarn.scheduler.minimum-allocation-mb的整数倍</strong> 。我们不去猜，调试下调度代码，看看究竟是什么情况。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ sbin/yarn-daemon.sh stop resourcemanager 
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop]$ grep "minimum-allocation-mb" -1 yarn-site.xml 
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;&lt;value&gt;512&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ export YARN_RESOURCEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8000"
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ sbin/yarn-daemon.sh start resourcemanager </span></code></pre></td></tr></table></div></figure>


<p>本地eclipse在 <code>CapacityScheduler#allocate</code> 打断点，然后跑任务：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hive&gt; set hive.execution.engine=spark;
</span><span class='line'>hive&gt; select count(*) from t_ods_access_log2 where month=201512;</span></code></pre></td></tr></table></div></figure>


<p>AppMaster内存分配：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-appmaster.png" alt="" /></p>

<p>Executor内存分配：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-executor.png" alt="" /></p>

<p>request进到allocate后，最终调用 <code>DefaultResourceCalculator.normalize</code> 重新计算了一遍请求需要的资源，把内存调整了。默认的DefaultResourceCalculator可以通过 capacity-scheduler.xml 的 <code>yarn.scheduler.capacity.resource-calculator</code> 来修改。</p>

<p>具体代码调度过程如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public Allocation allocate(ApplicationAttemptId applicationAttemptId,
</span><span class='line'>      List&lt;ResourceRequest&gt; ask, List&lt;ContainerId&gt; release, 
</span><span class='line'>      List&lt;String&gt; blacklistAdditions, List&lt;String&gt; blacklistRemovals) {
</span><span class='line'>    ...
</span><span class='line'>    // Sanity check
</span><span class='line'>    SchedulerUtils.normalizeRequests(
</span><span class='line'>        ask, getResourceCalculator(), getClusterResource(),
</span><span class='line'>        getMinimumResourceCapability(), maximumAllocation);
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>  public static void normalizeRequest(
</span><span class='line'>      ResourceRequest ask, 
</span><span class='line'>      ResourceCalculator resourceCalculator, 
</span><span class='line'>      Resource clusterResource,
</span><span class='line'>      Resource minimumResource,
</span><span class='line'>      Resource maximumResource,
</span><span class='line'>      Resource incrementResource) {
</span><span class='line'>    Resource normalized = 
</span><span class='line'>        Resources.normalize(
</span><span class='line'>            resourceCalculator, ask.getCapability(), minimumResource,
</span><span class='line'>            maximumResource, incrementResource);
</span><span class='line'>    ask.setCapability(normalized);
</span><span class='line'>  }   
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>  public static Resource normalize(
</span><span class='line'>      ResourceCalculator calculator, Resource lhs, Resource min,
</span><span class='line'>      Resource max, Resource increment) {
</span><span class='line'>    return calculator.normalize(lhs, min, max, increment);
</span><span class='line'>  }
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>  public Resource normalize(Resource r, Resource minimumResource,
</span><span class='line'>      Resource maximumResource, Resource stepFactor) {
</span><span class='line'>    int normalizedMemory = Math.min(
</span><span class='line'>        roundUp(
</span><span class='line'>            Math.max(r.getMemory(), minimumResource.getMemory()),
</span><span class='line'>            stepFactor.getMemory()),
</span><span class='line'>            maximumResource.getMemory());
</span><span class='line'>    return Resources.createResource(normalizedMemory);
</span><span class='line'>  }
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>  public static int roundUp(int a, int b) {
</span><span class='line'>    return divideAndCeil(a, b) * b;
</span><span class='line'>  }
</span><span class='line'>  </span></code></pre></td></tr></table></div></figure>


<p></p>

<h2>小结</h2>

<p>今天又重新认识一个yarn参数 <code>yarn.scheduler.minimum-allocation-mb</code> ，不仅仅是最小分配的内存，同时分配的资源也是minimum-allocation-mb的整数倍，还告诉我们 <code>yarn.nodemanager.resource.memory-mb</code> 也最好是minimum-allocation-mb的整数倍。</p>

<p>间接的学习了新的参数，可以通过 <code>yarn.scheduler.capacity.resource-calculator</code> 参数 来修改 CapacityScheduler 调度器的资源计算类。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/08/snappy-centos5-on-hive-on-spark/">Hive-on-spark Snappy on Centos5</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-08T22:27:06+08:00" pubdate data-updated="true">Fri 2016-04-08 22:27</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>hive的assembly包就是一个坑货！既然是一个单独的可运行的jar放到lib包下面干嘛呢！！纯属记录工作过程总的经历，想找干货的飘过吧！！</p>

<p><br/></p>

<p>上周支撑部门其他项目的hadoop项目，由于 <strong>hive mr</strong> 比较慢，想用spark试一试看能不能优化。但是系统使用Centos5，我们项目使用的是Centos6。按部就班的编译呗，hive-on-saprk启用SNAPPY的必要条件：</p>

<ul>
<li>hadoop使用snappy需要native的支持，首先当然是Centos5上编译hadoop。(现在看来可以不必要，但每次hdfs命令都提示我native的错误就很不爽)</li>
<li>hive增加spark。</li>
</ul>


<p>各程序版本信息：</p>

<ul>
<li>hadoop-2.6.3</li>
<li>hive-1.2.1</li>
<li>spark-1.3.1</li>
<li>centos5.4</li>
</ul>


<h2>编译hadoop-snappy</h2>

<ul>
<li>centos5手动</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost snappy-1.1.3]# ./autogen.sh 
</span><span class='line'>Remember to add `AC_PROG_LIBTOOL' to `configure.ac'.
</span><span class='line'>You should update your `aclocal.m4' by running aclocal.
</span><span class='line'>libtoolize: `config.guess' exists: use `--force' to overwrite
</span><span class='line'>libtoolize: `config.sub' exists: use `--force' to overwrite
</span><span class='line'>libtoolize: `ltmain.sh' exists: use `--force' to overwrite
</span><span class='line'>Makefile.am:4: Libtool library used but `LIBTOOL' is undefined
</span><span class='line'>Makefile.am:4: 
</span><span class='line'>Makefile.am:4: The usual way to define `LIBTOOL' is to add `AC_PROG_LIBTOOL'
</span><span class='line'>Makefile.am:4: to `configure.ac' and run `aclocal' and `autoconf' again.
</span><span class='line'>Makefile.am:20: `dist_doc_DATA' is used but `docdir' is undefined</span></code></pre></td></tr></table></div></figure>


<p>在centos5上面手动编译搞不定，不是专业写C的，这些问题就是天书啊(查了很多资料，试了很多方法都没通)！！ <strong>Snappy可以在centos6上面编译，编译好以后再centos5上面也能用，编译hadoop-snappy也是ok的</strong> 。</p>

<ul>
<li>centos5-rpm</li>
</ul>


<p>这里直接用rpm安装snappy。觉得创建虚拟机麻烦的话，也可以用docker。docker不同版本的centos下载： <a href="https://github.com/CentOS/sig-cloud-instance-images/">https://github.com/CentOS/sig-cloud-instance-images/</a> 。然后docker共享host主机的文件： <code>docker run -ti -v /home/hadoop:/home/hadoop -v /opt:/opt -v /data:/data centos:centos5 /bin/bash</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@8fb11f6b3ced ~]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 5.11 (Final)
</span><span class='line'>
</span><span class='line'>https://www.rpmfind.net/linux/rpm2html/search.php?query=snappy
</span><span class='line'>https://www.rpmfind.net/linux/rpm2html/search.php?query=snappy-devel
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# rpm -ivh snappy-1.0.5-1.el5.x86_64.rpm 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# rpm -ivh snappy-devel-1.0.5-1.el5.x86_64.rpm                                                                                  
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# rpm -ql snappy-devel snappy
</span><span class='line'>/usr/include/snappy-c.h
</span><span class='line'>/usr/include/snappy-sinksource.h
</span><span class='line'>/usr/include/snappy-stubs-public.h
</span><span class='line'>/usr/include/snappy.h
</span><span class='line'>/usr/lib64/libsnappy.so
</span><span class='line'>/usr/share/doc/snappy-devel-1.0.5
</span><span class='line'>/usr/share/doc/snappy-devel-1.0.5/format_description.txt
</span><span class='line'>/usr/lib64/libsnappy.so.1
</span><span class='line'>/usr/lib64/libsnappy.so.1.1.3
</span><span class='line'>/usr/share/doc/snappy-1.0.5
</span><span class='line'>/usr/share/doc/snappy-1.0.5/AUTHORS
</span><span class='line'>/usr/share/doc/snappy-1.0.5/COPYING
</span><span class='line'>/usr/share/doc/snappy-1.0.5/ChangeLog
</span><span class='line'>/usr/share/doc/snappy-1.0.5/NEWS
</span><span class='line'>/usr/share/doc/snappy-1.0.5/README
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# export JAVA_HOME=/opt/jdk1.7.0_17
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# export MAVEN_HOME=/opt/apache-maven-3.3.9
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# export PATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PATH
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]#  
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# yum install which gcc gcc-c++ zlib-devel make -y
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# cd protobuf-2.5.0
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# ./configure 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# make && make install
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# which protoc
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# yum install cmake openssl openssl-devel -y
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# cd hadoop-2.6.3-src/
</span><span class='line'># bundle.snappy和snappy.lib一起使用，可以把系统的snappy.so文件拷贝到lib/native下面（方便拷贝）
</span><span class='line'># &lt;http://grepcode.com/file/repo1.maven.org/maven2/org.apache.hadoop/hadoop-project-dist/2.6.0/META-INF/maven/org.apache.hadoop/hadoop-project-dist/pom.xml&gt;
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# mvn clean package -Dmaven.javadoc.skip=true -DskipTests -Drequire.snappy=true -Dbundle.snappy=true -Dsnappy.lib=/usr/lib64 -Pdist,native
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# ll hadoop-dist/target/hadoop-2.6.3/lib/native/
</span><span class='line'>total 3808
</span><span class='line'>-rw-r--r-- 1 root root 1036552 Apr 12 09:35 libhadoop.a
</span><span class='line'>-rw-r--r-- 1 root root 1212600 Apr 12 09:36 libhadooppipes.a
</span><span class='line'>lrwxrwxrwx 1 root root      18 Apr 12 09:35 libhadoop.so -&gt; libhadoop.so.1.0.0
</span><span class='line'>-rwxr-xr-x 1 root root  613267 Apr 12 09:35 libhadoop.so.1.0.0
</span><span class='line'>-rw-r--r-- 1 root root  401836 Apr 12 09:36 libhadooputils.a
</span><span class='line'>-rw-r--r-- 1 root root  364026 Apr 12 09:35 libhdfs.a
</span><span class='line'>lrwxrwxrwx 1 root root      16 Apr 12 09:35 libhdfs.so -&gt; libhdfs.so.0.0.0
</span><span class='line'>-rwxr-xr-x 1 root root  229672 Apr 12 09:35 libhdfs.so.0.0.0
</span><span class='line'>lrwxrwxrwx 1 root root      18 Apr 12 09:35 libsnappy.so -&gt; libsnappy.so.1.1.3
</span><span class='line'>lrwxrwxrwx 1 root root      18 Apr 12 09:35 libsnappy.so.1 -&gt; libsnappy.so.1.1.3
</span><span class='line'>-rwxr-xr-x 1 root root   21568 Apr 12 09:35 libsnappy.so.1.1.3
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# cd hadoop-dist/target/hadoop-2.6.3/
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3]# bin/hadoop checknative -a
</span><span class='line'>16/04/12 09:38:29 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
</span><span class='line'>16/04/12 09:38:29 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
</span><span class='line'>Native library checking:
</span><span class='line'>hadoop:  true /data/bigdata/sources/hadoop-2.6.3-src/hadoop-dist/target/hadoop-2.6.3/lib/native/libhadoop.so.1.0.0
</span><span class='line'>zlib:    true /lib64/libz.so.1
</span><span class='line'>snappy:  true /data/bigdata/sources/hadoop-2.6.3-src/hadoop-dist/target/hadoop-2.6.3/lib/native/libsnappy.so.1
</span><span class='line'>lz4:     true revision:99
</span><span class='line'>bzip2:   false 
</span><span class='line'>openssl: false org.apache.hadoop.crypto.OpensslCipher.initIDs()V
</span><span class='line'>16/04/12 09:38:29 INFO util.ExitUtil: Exiting with status 1</span></code></pre></td></tr></table></div></figure>


<p>把native下面的打tar包，然后替换生产的。一切都是正常的。接下来坑爹的是spark-snappy，具体的说应该是hive-assmably坑！！</p>

<h2>hive-on-spark snappy</h2>

<p>spark官网也没讲使用snappy需要做什么额外的配置（默认spark.io.compression.codec默认为snappy）。部署后设置 <code>hive.execution.engine=spark</code> 执行spark查询，立马就报错了 <strong> Caused by: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.0.5-libsn
appyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9&#8217; not found (required by /tmp/snappy-1.0.5-libsnappyjava.so)</strong> 从错误堆栈看与hadoop-native-snappy没关系，而是一个snappy-java的包。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 ~]$ strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX
</span><span class='line'>GLIBCXX_3.4
</span><span class='line'>GLIBCXX_3.4.1
</span><span class='line'>GLIBCXX_3.4.2
</span><span class='line'>GLIBCXX_3.4.3
</span><span class='line'>GLIBCXX_3.4.4
</span><span class='line'>GLIBCXX_3.4.5
</span><span class='line'>GLIBCXX_3.4.6
</span><span class='line'>GLIBCXX_3.4.7
</span><span class='line'>GLIBCXX_3.4.8
</span><span class='line'>GLIBCXX_FORCE_NEW</span></code></pre></td></tr></table></div></figure>


<p>确实缺少GLIBCXX_3.4.9，最新版本的centos5.11也是一样输出的。</p>

<p>spark的配置为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark.yarn.jar    hdfs:///spark/spark-assembly-1.3.1-hadoop2.6.3.jar
</span><span class='line'>
</span><span class='line'>spark.master  yarn-client
</span><span class='line'>
</span><span class='line'>spark.dynamicAllocation.enabled    true
</span><span class='line'>spark.shuffle.service.enabled      true
</span><span class='line'>spark.dynamicAllocation.minExecutors    2 
</span><span class='line'>spark.dynamicAllocation.maxExecutors    18
</span><span class='line'>
</span><span class='line'>spark.driver.maxResultSize   0
</span><span class='line'>spark.master=yarn-client
</span><span class='line'>spark.driver.memory=5g
</span><span class='line'>spark.eventLog.enabled  true
</span><span class='line'>spark.eventLog.compress  true
</span><span class='line'>spark.eventLog.dir    hdfs:///spark-eventlogs
</span><span class='line'>spark.yarn.historyServer.address file1:18080
</span><span class='line'>
</span><span class='line'>spark.serializer        org.apache.spark.serializer.KryoSerializer
</span><span class='line'>spark.kryoserializer.buffer.max    512m</span></code></pre></td></tr></table></div></figure>


<p>报错的具体信息：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>- 16/04/12 20:20:08 INFO storage.BlockManagerMaster: Registered BlockManager
</span><span class='line'>- java.lang.reflect.InvocationTargetException
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>-        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>-        at java.lang.reflect.Method.invoke(Method.java:606)
</span><span class='line'>-        at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:322)
</span><span class='line'>-        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
</span><span class='line'>-        at org.xerial.snappy.Snappy.&lt;clinit&gt;(Snappy.java:48)
</span><span class='line'>-        at org.apache.spark.io.SnappyCompressionCodec.&lt;init&gt;(CompressionCodec.scala:150)
</span><span class='line'>-        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
</span><span class='line'>-        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
</span><span class='line'>-        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
</span><span class='line'>-        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
</span><span class='line'>-        at org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:68)
</span><span class='line'>-        at org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:60)
</span><span class='line'>-        at org.apache.spark.scheduler.EventLoggingListener.&lt;init&gt;(EventLoggingListener.scala:67)
</span><span class='line'>-        at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:400)
</span><span class='line'>-        at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:61)
</span><span class='line'>-        at org.apache.hive.spark.client.RemoteDriver.&lt;init&gt;(RemoteDriver.java:169)
</span><span class='line'>-        at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>-        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>-        at java.lang.reflect.Method.invoke(Method.java:606)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
</span><span class='line'>- Caused by: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.0.5-libsnappyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by /tmp/snappy-1.0.5-libs
</span><span class='line'>-        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
</span><span class='line'>-        at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1965)
</span><span class='line'>-        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1890)
</span><span class='line'>-        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1851)
</span><span class='line'>-        at java.lang.Runtime.load0(Runtime.java:795)
</span><span class='line'>-        at java.lang.System.load(System.java:1062)
</span><span class='line'>-        at org.xerial.snappy.SnappyNativeLoader.load(SnappyNativeLoader.java:39)
</span><span class='line'>-        ... 28 more</span></code></pre></td></tr></table></div></figure>


<p>spark用到了snappy-java来处理snappy的解压缩。用jinfo获取SparkSubmit进程的classpath，用这个classpath跑helloworld确实是报错的，但是单独用hadoop-common下面的 snappy-java-1.0.4.1.jar 是没问题的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 snappy-java-test]$ cat Hello.java 
</span><span class='line'>import org.xerial.snappy.Snappy;
</span><span class='line'>
</span><span class='line'>public class Hello { 
</span><span class='line'>public static void main(String[] args) throws Exception {
</span><span class='line'>String input = "Hello snappy-java!";
</span><span class='line'>
</span><span class='line'>byte[] compressed = Snappy.compress(input.getBytes("utf-8"));
</span><span class='line'>byte[] uncompressed = Snappy.uncompress(compressed);
</span><span class='line'>
</span><span class='line'>String result = new String(uncompressed, "utf-8");
</span><span class='line'>System.out.println(result);
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[hadoop@file1 snappy-java-test]$ java -cp .:/home/hadoop/tools/hadoop-2.6.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar Hello
</span><span class='line'>Hello snappy-java!</span></code></pre></td></tr></table></div></figure>


<p>而而而，classpath中就只有hadoop-common和hadoop-mapreduce下面有snappy-java包，并且都是1.0.4.1，那TMD的使用SparkSubmit-classpath加载Snappy是哪个jar里面的呢？</p>

<p>调整后的helloworld为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 snappy-java-test]$ cat Hello.java 
</span><span class='line'>import org.xerial.snappy.Snappy;
</span><span class='line'>
</span><span class='line'>public class Hello { 
</span><span class='line'>public static void main(String[] args) throws Exception {
</span><span class='line'>String input = "Hello snappy-java!";
</span><span class='line'>
</span><span class='line'>System.out.println(Snappy.class.getProtectionDomain());
</span><span class='line'>byte[] compressed = Snappy.compress(input.getBytes("utf-8"));
</span><span class='line'>byte[] uncompressed = Snappy.uncompress(compressed);
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>String result = new String(uncompressed, "utf-8");
</span><span class='line'>System.out.println(result);
</span><span class='line'>}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>添加getProtectionDomain查看加载类的jar。再编译跑一次，这次终于找到真凶了！！hive-assembly，assembly包还放在lib下面就tmd的是一个坑货！！hive-exec的guava已经坑了很多人了，这次换hive-jdbc了！！(我这里的环境是centos5，centos6是没有这个问题的！！)</p>

<p><img src="/images/blogs/hive-on-spark-centos5-snappy-hive-jdbc.png" alt="" /></p>

<p>如果指定使用hadoop编译依赖的snappy.so.1.1.3动态链接库会出现版本不兼容的问题。还是干掉hive-jdbc-standalone吧。。。囧</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 查看源码SnappyLoader#loadSnappySystemProperties，可以通过配置指定使用系统动态链接库
</span><span class='line'>[hadoop@file1 snappy-java-test]$ cat org-xerial-snappy.properties 
</span><span class='line'>org.xerial.snappy.use.systemlib=true
</span><span class='line'>[hadoop@file1 snappy-java-test]$ ln -s /home/hadoop/tools/hadoop-2.6.3/lib/native/libsnappy.so libsnappyjava.so
</span><span class='line'>[hadoop@file1 snappy-java-test]$ ll
</span><span class='line'>总计 1240
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop     854 04-08 10:11 Hello.class
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop     408 04-08 10:11 Hello.java
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop      55 04-12 19:37 libsnappyjava.so -&gt; /home/hadoop/tools/hadoop-2.6.3/lib/native/libsnappy.so
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop      37 04-12 19:15 org-xerial-snappy.properties
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop 1251514 2014-04-29 snappy-java-1.0.5.jar
</span><span class='line'>[hadoop@file1 snappy-java-test]$ java -cp .:snappy-java-1.0.5.jar -Djava.library.path=. Hello
</span><span class='line'>ProtectionDomain  (file:/home/hadoop/snappy-java-test/snappy-java-1.0.5.jar &lt;no signer certificates&gt;)
</span><span class='line'> sun.misc.Launcher$AppClassLoader@333cb1eb
</span><span class='line'> &lt;no principals&gt;
</span><span class='line'> java.security.Permissions@7377711 (
</span><span class='line'> ("java.io.FilePermission" "/home/hadoop/snappy-java-test/snappy-java-1.0.5.jar" "read")
</span><span class='line'> ("java.lang.RuntimePermission" "exitVM")
</span><span class='line'>)
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Exception in thread "main" java.lang.UnsatisfiedLinkError: org.xerial.snappy.SnappyNative.maxCompressedLength(I)I
</span><span class='line'>        at org.xerial.snappy.SnappyNative.maxCompressedLength(Native Method)
</span><span class='line'>        at org.xerial.snappy.Snappy.maxCompressedLength(Snappy.java:320)
</span><span class='line'>        at org.xerial.snappy.Snappy.rawCompress(Snappy.java:333)
</span><span class='line'>        at org.xerial.snappy.Snappy.compress(Snappy.java:92)
</span><span class='line'>        at Hello.main(Hello.java:8)
</span><span class='line'>      </span></code></pre></td></tr></table></div></figure>


<p>删掉jdbc-standalone后，hive-on-spark就ok了。如果你无法下手删除 hive-jdbc-1.2.1-standalone.jar ，那就把 <code>spark.io.compression.codec</code> 改成 <code>lz4</code> 等压缩也是可以的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 ~]$ hive
</span><span class='line'>
</span><span class='line'>Logging initialized using configuration in file:/home/hadoop/tools/apache-hive-1.2.1-bin/conf/hive-log4j.properties
</span><span class='line'>hive&gt; set hive.execution.engine=spark;
</span><span class='line'>hive&gt; select count(*) from t_info where edate=20160411;
</span><span class='line'>Query ID = hadoop_20160412205338_2c95c5fd-af50-42ba-8681-e154e4b74cb1
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>In order to change the average load for a reducer (in bytes):
</span><span class='line'>  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
</span><span class='line'>In order to limit the maximum number of reducers:
</span><span class='line'>  set hive.exec.reducers.max=&lt;number&gt;
</span><span class='line'>In order to set a constant number of reducers:
</span><span class='line'>  set mapreduce.job.reduces=&lt;number&gt;
</span><span class='line'>Starting Spark Job = 69afc030-fa1f-4fdf-81ef-12bdca411a4f
</span><span class='line'>
</span><span class='line'>Query Hive on Spark job[0] stages:
</span><span class='line'>0
</span><span class='line'>1
</span><span class='line'>
</span><span class='line'>Status: Running (Hive on Spark job[0])
</span><span class='line'>Job Progress Format
</span><span class='line'>CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
</span><span class='line'>2016-04-12 20:54:11,367 Stage-0_0: 0(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:14,421 Stage-0_0: 0(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:17,457 Stage-0_0: 0(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:19,486 Stage-0_0: 2(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:20,497 Stage-0_0: 3(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:21,509 Stage-0_0: 5(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:22,520 Stage-0_0: 6(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:23,532 Stage-0_0: 7(+2)/234    Stage-1_0: 0/1</span></code></pre></td></tr></table></div></figure>


<h2>小结</h2>

<p>第一，hive的assembly的包太tmd的坑了。第二，以后找java具体加载那个类，可以通过 class.getProtectionDomain 来获取了。第三，又多尝试一个环境部署hadoop。呵呵</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/6">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/4">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2016/04/23/hadoop-guide-catalog/">[整理] Hadoop入门</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/08/19/j2ee-maven-resources-compress/">Maven压缩js/css功能实践</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/08/17/redis-batch-operate/">Redis批量操作</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/28/redis-optimise/">Redis使用优化</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/19/xss-blocked-by-naxsi/">使用 Naxsi 处理 XSS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/14/codis-guide/">Codis简单使用</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/06/28/flume-kafka-elasticsearch-for-analyse/">使用 Flume+kafka+elasticsearch 处理数据</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/06/17/ganglia-install-on-centos-with-puppet/">使用Puppet安装配置Ganglia</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/06/15/elasticsearch-startguide/">Elasticsearch Startguide</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/alluxio/'>alluxio</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/elasticsearch/'>elasticsearch</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/flume/'>flume</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (5) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (42) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/nginx/'>nginx</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/puppet/'>puppet</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (36) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (144)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'winseliu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
