
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="由于hive-0.12.0的FileSystem使用不当导致内存溢出问题，最终考虑升级hive。升级的过程没想象中的那么可怕，步骤很简单：对源数据库执行升级脚本，拷贝原hive-0.12.0的配置和jar，然后把添加jar重启hiverserver2即可。记录了升级到0.13，添加tez， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/14">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
  <li><a href="https://yunpan.cn/cuYhpFBPgQYgT" >Books[5aee]</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/06/21/upgrade-hive/">Upgrade Hive: 0.12.0 to 0.13.1</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-06-21T02:34:59+08:00" pubdate data-updated="true">Sat 2014-06-21 02:34</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>由于hive-0.12.0的FileSystem使用不当导致内存溢出问题，最终考虑升级hive。升级的过程没想象中的那么可怕，步骤很简单：对源数据库执行升级脚本，拷贝原hive-0.12.0的配置和jar，然后把添加jar重启hiverserver2即可。记录了升级到0.13，添加tez，调试hive。</p>

<h2>修改环境变量</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>HIVE_HOME=/home/hadoop/apache-hive-0.13.1-bin
</span><span class='line'>PATH=$JAVA_HOME/bin:$HIVE_HOME/bin:$PATH</span></code></pre></td></tr></table></div></figure>


<p>如果要使用hwi，需要自己下载原来编译生成war。（默认的bin.tar.gz里面不包括）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/hive/hwi
</span><span class='line'>$ mvn package war:war</span></code></pre></td></tr></table></div></figure>


<p>配置的时刻注意下<code>hive.hwi.war.file</code>是相对于<strong>HIVE_HOME</strong>的位置<code>lib/hive-hwi-0.13.1.war</code>。同时需要把<code>$JDK/lib/tools.jar</code>加入到classpath。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/home/eshore/jdk1.7.0_60/lib/tools.jar
</span><span class='line'>
</span><span class='line'>$CD/bin/hive --service hwi</span></code></pre></td></tr></table></div></figure>


<h2>升级metadata</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@ismp0 ~]$ cd apache-hive-0.13.1-bin/scripts/metastore/upgrade/mysql/
</span><span class='line'>
</span><span class='line'>[hadoop@ismp0 mysql]$ mysql -uXXX -hXXX -pXXX
</span><span class='line'>mysql&gt; use hive
</span><span class='line'>Reading table information for completion of table and column names
</span><span class='line'>You can turn off this feature to get a quicker startup with -A
</span><span class='line'>
</span><span class='line'>Database changed
</span><span class='line'>mysql&gt; source upgrade-0.12.0-to-0.13.0.mysql.sql
</span><span class='line'>+--------------------------------------------------+
</span><span class='line'>|                                                  |
</span><span class='line'>+--------------------------------------------------+
</span><span class='line'>| Upgrading MetaStore schema from 0.12.0 to 0.13.0 |
</span><span class='line'>+--------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>+-----------------------------------------------------------------------+
</span><span class='line'>|                                                                       |
</span><span class='line'>+-----------------------------------------------------------------------+
</span><span class='line'>| &lt; HIVE-5700 enforce single date format for partition column storage &gt; |
</span><span class='line'>+-----------------------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.22 sec)
</span><span class='line'>Rows matched: 0  Changed: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>+--------------------------------------------+
</span><span class='line'>|                                            |
</span><span class='line'>+--------------------------------------------+
</span><span class='line'>| &lt; HIVE-6386: Add owner filed to database &gt; |
</span><span class='line'>+--------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.33 sec)
</span><span class='line'>Records: 1  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.16 sec)
</span><span class='line'>Records: 1  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>+---------------------------------------------------------------------------------------------+
</span><span class='line'>|                                                                                             |
</span><span class='line'>+---------------------------------------------------------------------------------------------+
</span><span class='line'>| &lt;HIVE-6458 Add schema upgrade scripts for metastore changes related to permanent functions&gt; |
</span><span class='line'>+---------------------------------------------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>+----------------------------------------------------------------------------------+
</span><span class='line'>|                                                                                  |
</span><span class='line'>+----------------------------------------------------------------------------------+
</span><span class='line'>| &lt;HIVE-6757 Remove deprecated parquet classes from outside of org.apache package&gt; |
</span><span class='line'>+----------------------------------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.04 sec)
</span><span class='line'>Rows matched: 0  Changed: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>Rows matched: 0  Changed: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>Rows matched: 0  Changed: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.07 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.12 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.07 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.05 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.15 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.05 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.07 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.05 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.07 sec)
</span><span class='line'>Rows matched: 1  Changed: 1  Warnings: 0
</span><span class='line'>
</span><span class='line'>+-----------------------------------------------------------+
</span><span class='line'>|                                                           |
</span><span class='line'>+-----------------------------------------------------------+
</span><span class='line'>| Finished upgrading MetaStore schema from 0.12.0 to 0.13.0 |
</span><span class='line'>+-----------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>mysql&gt; 
</span><span class='line'>mysql&gt; 
</span><span class='line'>mysql&gt; exit
</span><span class='line'>Bye
</span><span class='line'>
</span><span class='line'>[hadoop@ismp0 ~]$ vi .bash_profile
</span><span class='line'>[hadoop@ismp0 ~]$ source .bash_profile
</span><span class='line'>[hadoop@ismp0 ~]$ cd apache-hive-0.13.1-bin
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ cd conf/
</span><span class='line'>[hadoop@ismp0 conf]$ cp ~/hive-0.12.0/conf/hive-site.xml ./
</span><span class='line'>[hadoop@ismp0 conf]$ cd ..
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ cp ~/hive-0.12.0/lib/mysql-connector-java-5.1.21-bin.jar lib/
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ hive
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ hive
</span><span class='line'>
</span><span class='line'>hive&gt;  select count(*) from t_ods_idc_isp_log2 where day=20140624;
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>Number of reduce tasks determined at compile time: 1
</span><span class='line'>In order to change the average load for a reducer (in bytes):
</span><span class='line'>  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
</span><span class='line'>In order to limit the maximum number of reducers:
</span><span class='line'>  set hive.exec.reducers.max=&lt;number&gt;
</span><span class='line'>In order to set a constant number of reducers:
</span><span class='line'>  set mapreduce.job.reduces=&lt;number&gt;
</span><span class='line'>Starting Job = job_1403006477300_3403, Tracking URL = http://umcc97-79:8088/proxy/application_1403006477300_3403/
</span><span class='line'>Kill Command = /home/hadoop/hadoop-2.2.0/bin/hadoop job  -kill job_1403006477300_3403
</span><span class='line'>Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
</span><span class='line'>2014-06-24 17:19:07,618 Stage-1 map = 0%,  reduce = 0%
</span><span class='line'>2014-06-24 17:19:15,283 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.37 sec
</span><span class='line'>2014-06-24 17:19:16,360 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.49 sec
</span><span class='line'>2014-06-24 17:19:22,749 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.99 sec
</span><span class='line'>MapReduce Total cumulative CPU time: 7 seconds 990 msec
</span><span class='line'>Ended Job = job_1403006477300_3403
</span><span class='line'>MapReduce Jobs Launched: 
</span><span class='line'>Job 0: Map: 2  Reduce: 1   Cumulative CPU: 7.99 sec   HDFS Read: 19785618 HDFS Write: 6 SUCCESS
</span><span class='line'>Total MapReduce CPU Time Spent: 7 seconds 990 msec
</span><span class='line'>OK
</span><span class='line'>77625
</span><span class='line'>Time taken: 36.387 seconds, Fetched: 1 row(s)
</span><span class='line'>hive&gt; 
</span><span class='line'>
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ nohup bin/hiveserver2 &
</span><span class='line'>
</span><span class='line'>$# 测试hive-jdbc
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ bin/beeline 
</span><span class='line'>Beeline version 0.13.1 by Apache Hive
</span><span class='line'>beeline&gt; !connect jdbc:hive2://10.18.97.22:10000/
</span><span class='line'>scan complete in 7ms
</span><span class='line'>Connecting to jdbc:hive2://10.18.97.22:10000/
</span><span class='line'>Enter username for jdbc:hive2://10.18.97.22:10000/: hadoop
</span><span class='line'>Enter password for jdbc:hive2://10.18.97.22:10000/: 
</span><span class='line'>Connected to: Apache Hive (version 0.13.1)
</span><span class='line'>Driver: Hive JDBC (version 0.13.1)
</span><span class='line'>Transaction isolation: TRANSACTION_REPEATABLE_READ
</span><span class='line'>0: jdbc:hive2://10.18.97.22:10000/&gt; show tables;
</span><span class='line'>+-------------------------+
</span><span class='line'>|        tab_name         |
</span><span class='line'>+-------------------------+
</span><span class='line'>...
</span><span class='line'>| test_123                |
</span><span class='line'>+-------------------------+
</span><span class='line'>10 rows selected (2.547 seconds)
</span><span class='line'>0: jdbc:hive2://10.18.97.22:10000/&gt;  select count(*) from t_ods_idc_isp_log2 where day=20140624;
</span><span class='line'>+--------+
</span><span class='line'>|  _c0   |
</span><span class='line'>+--------+
</span><span class='line'>| 77625  |
</span><span class='line'>+--------+
</span><span class='line'>1 row selected (37.463 seconds)
</span><span class='line'>0: jdbc:hive2://10.18.97.22:10000/&gt; 
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>上一篇tez的安装使用中由于hive的缘故进行了回退，现在升级到hive-0.13后，也在hive上试下tez的功能：</p>

<ul>
<li>本地添加tez依赖，设置环境变量</li>
<li>MR添加tez依赖，添加tez-site.xml</li>
<li>切换到tez的engine</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$# 已上传到HDFS
</span><span class='line'>$ hadoop fs -mkdir /apps
</span><span class='line'>$ hadoop fs -put tez-0.4.0-incubating /apps/
</span><span class='line'>$ hadoop fs -ls /apps
</span><span class='line'>Found 1 items
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2014-09-09 16:19 /apps/tez-0.4.0-incubating
</span><span class='line'>
</span><span class='line'>$ cat etc/hadoop/tez-site.xml 
</span><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- Put site-specific property overrides in this file. --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;tez.lib.uris&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;${fs.default.name}/apps/tez-0.4.0-incubating,${fs.default.name}/apps/tez-0.4.0-incubating/lib/&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>
</span><span class='line'>$ export HADOOP_CLASSPATH=${TEZ_HOME}/*:${TEZ_HOME}/lib/*:$HADOOP_CLASSPATH
</span><span class='line'>$ apache-hive-0.13.1-bin/bin/hive
</span><span class='line'>hive&gt; set hive.execution.engine=tez;
</span><span class='line'>hive&gt; select count(*) from t_ods_idc_isp_log2 ;
</span><span class='line'>Time taken: 24.926 seconds, Fetched: 1 row(s)
</span><span class='line'>
</span><span class='line'>hive&gt; set hive.execution.engine=mr;                              
</span><span class='line'>hive&gt; select count(*) from t_ods_idc_isp_log2 where day=20140720;
</span><span class='line'>Time taken: 40.585 seconds, Fetched: 1 row(s)
</span><span class='line'>
</span><span class='line'>// 添加TEZ的jar到CLASSPATH
</span><span class='line'>$# @hive-env.sh
</span><span class='line'> # export TEZ_HOME=/home/hadoop/tez-0.4.0-incubating
</span><span class='line'> # export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$TEZ_HOME/*:$TEZ_HOME/lib/*
</span><span class='line'>$ last_hour=2014090915
</span><span class='line'>$ hive --hiveconf hive.execution.engine=tez -e "select houseId, count(*) 
</span><span class='line'>from 
</span><span class='line'>(
</span><span class='line'>select houseId
</span><span class='line'>from t_house_monitor2
</span><span class='line'>where hour=$last_hour
</span><span class='line'>group by from_unixtime(cast(accesstime as bigint), 'yyyyMMdd'),houseId,IP,port,domain,serviceType,illegalType,currentState,usr,icpError,regerror,regDomain,use_type,real_useType
</span><span class='line'>) hs
</span><span class='line'>group by houseId"</span></code></pre></td></tr></table></div></figure>


<p>简单从时间上看，还是有效果的。</p>

<p><img src="http://file.bmob.cn/M00/04/A2/wKhkA1PSPSeAb1wWAAER_4gjIug339.png" alt="" /></p>

<h2>调试Hive</h2>

<p>也很简单，hive脚本已经默认集成了这个功能，设置下DEBUG环境变量即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ less apache-hive-0.13.1-bin/bin/ext/debug.sh
</span><span class='line'>[hadoop@master1 bin]$ less hive
</span><span class='line'>
</span><span class='line'>$# 脚本最终会把调试的参数` -agentlib:jdwp=transport=dt_socket,server=y,address=8000,suspend=y`加入到HADOOP_CLIENT_OPTS中，最后合并到HADOOP_OPTS传递给java程序。
</span><span class='line'>
</span><span class='line'>[hadoop@master1 bin]$ DEBUG=true hive
</span><span class='line'>Listening for transport dt_socket at address: 8000</span></code></pre></td></tr></table></div></figure>


<p>然后通过eclipse的远程调试即可一步步的查看整个过程。下面断点处为记录解析功能：</p>

<p><img src="http://file.bmob.cn/M00/0A/D4/wKhkA1QEASyAM9VEAAHQS7gZJlo672.png" alt="" /></p>

<h2>编译源码导入eclipse</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git clone https://github.com/apache/hive.git
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC /cygdrive/e/git/hive
</span><span class='line'>$ git checkout branch-0.13
</span><span class='line'>
</span><span class='line'>E:\git\hive&gt;mvn clean package eclipse:eclipse -DskipTests -Dmaven.test.skip=true -Phadoop-2</span></code></pre></td></tr></table></div></figure>


<h2>注意点</h2>

<ul>
<li>除了分区，hive表数据路径下不能包括其他文件夹</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hive&gt; create database test location '/user/hive/warehouse_temp/' ;
</span><span class='line'>
</span><span class='line'>hive&gt; create table t_ods_ddos as select * from default.t_ods_ddos limit 0;
</span><span class='line'>
</span><span class='line'>hive&gt; select * from t_ods_ddos;
</span><span class='line'>OK
</span><span class='line'>Time taken: 0.176 seconds
</span><span class='line'>
</span><span class='line'>[hadoop@umcc97-44 ~]$ hadoop fs -mkdir /user/hive/warehouse_temp/t_ods_ddos/abc
</span><span class='line'>
</span><span class='line'>hive&gt; select * from t_ods_ddos;
</span><span class='line'>OK
</span><span class='line'>Failed with exception java.io.IOException:java.io.IOException: Not a file: hdfs://umcc97-44:9000/user/hive/warehouse_temp/t_ods_ddos/abc
</span><span class='line'>Time taken: 0.167 seconds</span></code></pre></td></tr></table></div></figure>



</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/06/18/hadoop-tez-firststep/">Tez编译及使用</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-06-18T04:22:58+08:00" pubdate data-updated="true">Wed 2014-06-18 04:22</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>初步了解</h2>

<p>hadoop2自带的mapreduce任务中间只能传递一次，也即一个任务只能聚合一次。而tez项目是对原有yarn架构的一个拓展，使用DAG（无环有向图）实现MRR的任务框架。</p>

<p><img src="http://farm6.staticflickr.com/5571/14256993179_4990fc86d5_o.png" alt="" /></p>

<p>上图中，左边的MR任务完成一个步骤后，需要进行<strong>数据存储</strong>后再执行另一个任务来进行第二个“reduce”； 而tez则可以在reduce后继续执行reduce，减少了中间过程的IO以及mapreduce的启动时间。</p>

<h2>环境整合</h2>

<ul>
<li><a href="http://tez.incubator.apache.org/install.html">Install/Deploy</a></li>
<li>hadoop-2.2.0（umcc97-44：hdfs， umcc97-79：yarn）</li>
<li>windows下使用Cygwin编译</li>
</ul>


<h3>下载编译tez</h3>

<p>首先下载<a href="http://apache.fayea.com/apache-mirror/incubator/tez/tez-0.4.0-incubating/">tez-0.4.0-incubating.tar.gz</a>，同时还需要<a href="http://code.google.com/p/protobuf">protoc</a>的程序支持（编译<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html">hadoop源码</a>也需要这个的）。
解压后，使用mvn编译。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big
</span><span class='line'>$ tar zxvf tez-0.4.0-incubating.tar.gz
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big
</span><span class='line'>$ cd tez-0.4.0-incubating/
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating
</span><span class='line'>$ mvn install -DskipTests -Dmaven.javadoc.skip
</span><span class='line'>...
</span><span class='line'>[INFO] Reactor Summary:
</span><span class='line'>[INFO]
</span><span class='line'>[INFO] tez ............................................... SUCCESS [1.518s]
</span><span class='line'>[INFO] tez-api ........................................... SUCCESS [8.890s]
</span><span class='line'>[INFO] tez-common ........................................ SUCCESS [0.725s]
</span><span class='line'>[INFO] tez-runtime-internals ............................. SUCCESS [2.529s]
</span><span class='line'>[INFO] tez-runtime-library ............................... SUCCESS [5.100s]
</span><span class='line'>[INFO] tez-mapreduce ..................................... SUCCESS [3.666s]
</span><span class='line'>[INFO] tez-mapreduce-examples ............................ SUCCESS [2.692s]
</span><span class='line'>[INFO] tez-dag ........................................... SUCCESS [13.943s]
</span><span class='line'>[INFO] tez-tests ......................................... SUCCESS [1.691s]
</span><span class='line'>[INFO] tez-dist .......................................... SUCCESS [14.370s]
</span><span class='line'>[INFO] Tez ............................................... SUCCESS [0.245s]
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] BUILD SUCCESS
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Total time: 55.791s
</span><span class='line'>[INFO] Finished at: Tue Jun 17 17:33:45 CST 2014
</span><span class='line'>[INFO] Final Memory: 35M/151M
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span></code></pre></td></tr></table></div></figure>


<h3>上传tez程序的jars到HDFS</h3>

<p>为了简单我直接把tez放到开发环境的集群上面去测试了。放到本地环境应该也类似。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating
</span><span class='line'>$ cd tez-dist/
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist
</span><span class='line'>$ cd target/
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist/target
</span><span class='line'>$ export HADOOP_USER_NAME=hadoop
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist/target
</span><span class='line'>$  hadoop dfs -put tez-0.4.0-incubating/tez-0.4.0-incubating/ hdfs://umcc97-44:9000/apps/
</span><span class='line'>DEPRECATED: Use of this script to execute hdfs command is deprecated.
</span><span class='line'>Instead use the hdfs command for it.
</span></code></pre></td></tr></table></div></figure>


<h3>配置集群环境</h3>

<p>首先看下原来集群的classpath路径，由于已经包括了etc/hadoop目录，所以这里我直接把<code>tez-site.xml</code>放到该目录下。把所有的tez-lib上传到share目录下，并添加到HADOOP_CLASSPATH。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  [hadoop@umcc97-79 hadoop]$ hadoop classpath
</span><span class='line'>  /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.2.0/contrib/capacity-scheduler/*.jar
</span><span class='line'>  
</span><span class='line'>  # 用于map/reduce
</span><span class='line'>  [hadoop@umcc97-79 hadoop]$ cat tez-site.xml 
</span><span class='line'>  &lt;?xml version="1.0"?&gt;
</span><span class='line'>  &lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;!-- Put site-specific property overrides in this file. --&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;configuration&gt;
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>      &lt;name&gt;tez.lib.uris&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;${fs.default.name}/apps/tez-0.4.0-incubating,${fs.default.name}/apps/tez-0.4.0-incubating/lib/&lt;/value&gt;
</span><span class='line'>    &lt;/property&gt;
</span><span class='line'>  &lt;/configuration&gt;
</span><span class='line'>  [hadoop@umcc97-79 hadoop]$ 
</span><span class='line'>  
</span><span class='line'>  [hadoop@umcc97-79 hadoop]$ cd ~/hadoop-2.2.0/share/hadoop/tez/
</span><span class='line'>  [hadoop@umcc97-79 tez]$ ll
</span><span class='line'>  total 9616
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  303139 Jun 17 17:33 avro-1.7.4.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop   41123 Jun 17 17:33 commons-cli-1.2.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  610259 Jun 17 17:33 commons-collections4-4.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop 1648200 Jun 17 17:33 guava-11.0.2.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  710492 Jun 17 17:33 guice-3.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  656365 Jun 17 17:33 hadoop-mapreduce-client-common-2.2.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop 1455001 Jun 17 17:33 hadoop-mapreduce-client-core-2.2.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop   21537 Jun 17 17:33 hadoop-mapreduce-client-shuffle-2.2.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop   81743 Jun 17 17:33 jettison-1.3.4.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  533455 Jun 17 17:33 protobuf-java-2.5.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  995968 Jun 17 17:33 snappy-java-1.0.4.1.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  749917 Jun 17 17:33 tez-api-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop   34049 Jun 17 17:33 tez-common-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  970987 Jun 17 17:33 tez-dag-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  246409 Jun 17 17:33 tez-mapreduce-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  199934 Jun 17 17:33 tez-mapreduce-examples-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  114692 Jun 17 17:33 tez-runtime-internals-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  352177 Jun 17 17:33 tez-runtime-library-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop    6845 Jun 17 17:33 tez-tests-0.4.0-incubating.jar
</span><span class='line'>  [hadoop@umcc97-79 tez]$ 
</span><span class='line'>  
</span><span class='line'>  # 用于client任务提交
</span><span class='line'>  [hadoop@umcc97-79 hadoop]$ grep HADOOP_CLASSPATH hadoop-env.sh
</span><span class='line'>  export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tez/*:${HADOOP_HOME}/share/hadoop/tez/lib/*:$HADOOP_CLASSPATH
</span><span class='line'>  
</span><span class='line'>  [hadoop@umcc97-79 hadoop]$ sed -n 19,23p mapred-site.xml
</span><span class='line'>  &lt;configuration&gt;
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;yarn-tez&lt;/value&gt;
</span><span class='line'>    &lt;/property&gt;
</span></code></pre></td></tr></table></div></figure>


<h3>同步，重启yarn</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for h in `cat hadoop-2.2.0/etc/hadoop/slaves ` ; do rsync -vaz --exclude=logs --exclude=pid --exclude=tmp  hadoop-2.2.0 $h:~/ ; done
</span><span class='line'>
</span><span class='line'>rsync -vaz --exclude=logs --exclude=pid --exclude=tmp  hadoop-2.2.0 umcc97-44:~/</span></code></pre></td></tr></table></div></figure>


<h3>测试效果</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  [hadoop@umcc97-79 ~]$ hadoop classpath
</span><span class='line'>  /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.2.0/share/hadoop/tez/*:/home/hadoop/hadoop-2.2.0/share/hadoop/tez/lib/*:/home/hadoop/hadoop-2.2.0/contrib/capacity-scheduler/*.jar
</span><span class='line'>  [hadoop@umcc97-79 ~]$ cd hadoop-2.2.0/share/hadoop/mapreduce/
</span><span class='line'>  [hadoop@umcc97-79 mapreduce]$ hadoop jar hadoop-mapreduce-client-jobclient-2.2.0-tests.jar sleep -mt 1 -rt 1 -m 1 -r 1
</span><span class='line'>  
</span><span class='line'>    hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount  /hello/in /hello/out
</span><span class='line'>    hadoop fs -put hadoop-2.2.0/logs/yarn-hadoop-resourcemanager-umcc97-79.* /hello/in
</span><span class='line'>    hadoop fs -rmr /hello/out
</span><span class='line'>    hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount  /hello/in /hello/out
</span></code></pre></td></tr></table></div></figure>


<h3>回滚，使用时临时修改环境变量即可</h3>

<p>使用了tez后，使用hive-0.12.0不能运行了。由于其他同事需要用hive，得把配置全部修改回去【<a href="/blog/2014/06/21/upgrade-hive/">hive-0.13中使用tez</a>】。</p>

<p>其实在<strong>提交任务</strong>时指定配置参数即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-79 ~]$ export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tez/*:${HADOOP_HOME}/share/hadoop/tez/lib/*:$HADOOP_CLASSPATH
</span><span class='line'>[hadoop@umcc97-79 ~]$ hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount -Dmapreduce.framework.name=yarn-tez  /hello/in /hello/out</span></code></pre></td></tr></table></div></figure>


<p>org.apache.tez.mapreduce.examples.OrderedWordCount不仅计算出了结果，同时按个数大小进行了排序。</p>

<p>问题： tez的任务的history还不知道怎么弄的，启动historyserver没作用。</p>

<p>0.6版本已经有ui了。</p>

<h3>持续更新</h3>

<p>本来想编译好tez-0.6就往hive-0.13上面放，没想到遇到钉子了！！hive-0.13不支持！！</p>

<p>在编译tez并想集成到hive，先下载hive的源码，看看pom.xml中使用的是到底是什么版本的tez，再编译tez不迟！！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>apache-hive-1.1.0-src.tar.gz/pom.xml
</span><span class='line'>    &lt;tez.version&gt;0.5.2&lt;/tez.version&gt;</span></code></pre></td></tr></table></div></figure>


<p>tez-0.6在hadoop-2.2基础上编译：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\local\opt\bigdata\apache-tez-0.6.0-src&gt;mvn  package -Dhadoop.version=2.2.0 -DskipTests -Dmaven.javadoc.skip=true -DskipATS
</span><span class='line'>
</span><span class='line'>tez-dist/pom.xml
</span><span class='line'>&lt;profile&gt;
</span><span class='line'>      &lt;id&gt;hadoop26&lt;/id&gt;
</span><span class='line'>      &lt;activation&gt;
</span><span class='line'>        &lt;activeByDefault&gt;false&lt;/activeByDefault&gt;
</span><span class='line'>      &lt;/activation&gt;</span></code></pre></td></tr></table></div></figure>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/04/22/remote-debug-hadoop2/">远程调试hadoop2以及错误处理方法</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-04-22T06:47:48+08:00" pubdate data-updated="true">Tue 2014-04-22 06:47</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>了解程序运行过程，除了一行行代码的扫射源代码。更快捷的方式是运行调试源码，通过F6/F7来一步步的带领我们熟悉程序。针对特定细节具体数据，打个断点调试则是水到渠成的方式。</p>

<h2>Java远程调试</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> * JDK 1.3 or earlier -Xnoagent -Djava.compiler=NONE -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=6006
</span><span class='line'> * JDK 1.4(linux ok) -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=6006
</span><span class='line'> * newer JDK(win7 & jdk7) -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=6006</span></code></pre></td></tr></table></div></figure>


<h2>同一操作系统任务提交</h2>

<p>windows提交到windows，linux提交到linux，可以直接通过命令行添加参数调试wordcount任务：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\local\dotfile&gt;hdfs dfs -rmr /out # native-lib放在非path路径下，cmd脚本中有对其进行处理
</span><span class='line'>
</span><span class='line'>E:\local\dotfile&gt;hadoop org.apache.hadoop.examples.WordCount  "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090 -Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native -Dmapreduce.reduce.java.opts=-Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native"  /in /out</span></code></pre></td></tr></table></div></figure>


<p><strong>suspend设置为y，会等待客户端连接再运行</strong>。在eclipse中在WordCount$TokenizerMapper#map打个断点，然后再使用<code>Remote Java Application</code>就可以调试程序了。</p>

<h2>Hadoop集群环境下调试任务</h2>

<p>hadoop有很多的程序，同样有对应的环境变量选项来进行设置！</p>

<ul>
<li>主程序-调试Job提交

<ul>
<li><code>set HADOOP_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"</code></li>
<li>可以在配置文件中进行设置。需要注意可能会覆盖已经设置的该参数的值。</li>
</ul>
</li>
<li>Nodemanager调试

<ul>
<li><code>set HADOOP_NODEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8092"</code></li>
<li>(linux下需要定义在文件中)<code>YARN_NODEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8092"</code></li>
</ul>
</li>
<li>ResourceManager调试

<ul>
<li>HADOOP_RESOURCEMANAGER_OPTS</li>
<li><code>export YARN_RESOURCEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8091"</code></li>
</ul>
</li>
</ul>


<p>Linux上的设置略有不同，通过SSH再调用的进程(如NodeManager)需要把其OPTS写到命令行脚本文件中！！
linux需要远程调试NodeManager的话，需要写到etc/hadoop/yarn-env.sh文件中！不然，nodemanger不生效（通过ssh去执行的）！</p>

<h3>其他调试技巧</h3>

<p>调试测试集群环境，比本地windows开发环境复杂点。毕竟本地windows的就一个主一个从。而把<strong>任务放到分布式集群</strong>上时，例如调试分布式缓存的！
那么就需要一些小技巧来获取任务运行所在的机器！下面的步骤中有具体操作命令。</p>

<h3>任务配置及运行</h3>

<p>eclipse下windows提交job到linux的补丁，查阅<a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655">[MAPREDUCE-5655]</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 配置
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapred.remote.os&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;Linux&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.job.jar&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;dta-analyser-all.jar&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;-Xmx1024m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapred.task.timeout&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;1800000&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'># 代码，map/reduce数都设置为1 
</span><span class='line'>job.setNumReduceTasks(1);
</span><span class='line'>job.getConfiguration().setInt(MRJobConfig.NUM_MAPS, 1);
</span></code></pre></td></tr></table></div></figure>


<p></p>

<ul>
<li>调试的时刻把超时时间设置的久一点，否则：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> Got exception: java.net.SocketTimeoutException: Call From winseliu/127.0.0.1 to winse.com:2850 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch :</span></code></pre></td></tr></table></div></figure>


<ul>
<li>调试main方法参数设置</li>
</ul>


<p>调试main（转瞬即逝的把suspend设置为true！），map的调试选项的语句写在配置文件里面</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export HADOOP_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8073"
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ sh -x bin/hadoop org.apache.hadoop.examples.WordCount /in /out </span></code></pre></td></tr></table></div></figure>


<h3>遍历所有子节点，查找节点运行map程序的信息</h3>

<p>map调试的端口配置为18090，根据这个选项来查找程序运行的机器。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 ~]$ for h in `cat hadoop-2.2.0/etc/hadoop/slaves` ; do ssh $h 'ps aux|grep java | grep 18090'; echo $h;  done
</span><span class='line'>hadoop    8667  0.0  0.0  63888  1268 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
</span><span class='line'>umcc97-142
</span><span class='line'>hadoop   12686  0.0  0.0  63868  1260 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
</span><span class='line'>umcc97-143
</span><span class='line'>hadoop   23516  0.0  0.0  63856  1108 ?        Ss   18:11   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1605/container_1397006359464_1605_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 57576 attempt_1397006359464_1605_m_000000_0 2 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002/stderr 
</span><span class='line'>hadoop   23522  0.0  0.0 605136 15728 ?        Sl   18:11   0:00 /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1605/container_1397006359464_1605_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 57576 attempt_1397006359464_1605_m_000000_0 2
</span><span class='line'>hadoop   23665  0.0  0.0  63856  1264 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
</span><span class='line'>umcc97-144</span></code></pre></td></tr></table></div></figure>


<p>仅打印运行map的节点名称</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 ~]$ for h in `cat hadoop-2.2.0/etc/hadoop/slaves` ; do ssh $h 'if ps aux|grep -v grep | grep java | grep 18090 | grep -v bash 2&gt;&1 1&gt;/dev/null ; then echo `hostname`; fi'; done
</span><span class='line'>umcc97-142
</span><span class='line'>[hadoop@umcc97-44 ~]$ </span></code></pre></td></tr></table></div></figure>


<p>后面的操作就和普通的java程序调试步骤一样了。不再赘述。</p>

<h2>任务运行过程中的数据</h2>

<h4>辅助运行的两个bash程序</h4>

<p>运行的第一个程序（000001）为AppMaster，第二程序（000002）才是我们提交job的map任务。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-143 ~]$ cd hadoop-2.2.0/tmp/nm-local-dir/nmPrivate
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ ls -Rl
</span><span class='line'>.:
</span><span class='line'>total 12
</span><span class='line'>drwxrwxr-x 4 hadoop hadoop 4096 Apr 21 18:34 application_1397006359464_1606
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop    6 Apr 21 18:34 container_1397006359464_1606_01_000001.pid
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop    6 Apr 21 18:34 container_1397006359464_1606_01_000002.pid
</span><span class='line'>
</span><span class='line'>./application_1397006359464_1606:
</span><span class='line'>total 8
</span><span class='line'>drwxrwxr-x 2 hadoop hadoop 4096 Apr 21 18:34 container_1397006359464_1606_01_000001
</span><span class='line'>drwxrwxr-x 2 hadoop hadoop 4096 Apr 21 18:34 container_1397006359464_1606_01_000002
</span><span class='line'>
</span><span class='line'>./application_1397006359464_1606/container_1397006359464_1606_01_000001:
</span><span class='line'>total 8
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop   95 Apr 21 18:34 container_1397006359464_1606_01_000001.tokens
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop 3121 Apr 21 18:34 launch_container.sh
</span><span class='line'>
</span><span class='line'>./application_1397006359464_1606/container_1397006359464_1606_01_000002:
</span><span class='line'>total 8
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop  129 Apr 21 18:34 container_1397006359464_1606_01_000002.tokens
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop 3532 Apr 21 18:34 launch_container.sh
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ 
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ jps
</span><span class='line'>4692 NodeManager
</span><span class='line'>4173 DataNode
</span><span class='line'>13497 YarnChild
</span><span class='line'>7538 HRegionServer
</span><span class='line'>13376 MRAppMaster
</span><span class='line'>13574 Jps
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ cat *.pid
</span><span class='line'>13366
</span><span class='line'>13491
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ ps aux | grep 13366
</span><span class='line'>hadoop   13366  0.0  0.0  63868  1088 ?        Ss   18:34   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001/stderr 
</span><span class='line'>hadoop   13594  0.0  0.0  61204   760 pts/2    S+   18:36   0:00 grep 13366
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ ps aux | grep 13491
</span><span class='line'>hadoop   13491  0.0  0.0  63868  1100 ?        Ss   18:34   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/container_1397006359464_1606_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 52046 attempt_1397006359464_1606_m_000000_0 2 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002/stderr 
</span><span class='line'>hadoop   13599  0.0  0.0  61204   760 pts/2    S+   18:37   0:00 grep 13491
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ </span></code></pre></td></tr></table></div></figure>


<h4>程序运行本地缓存数据</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-143 container_1397006359464_1606_01_000002]$ ls -l
</span><span class='line'>total 28
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop  129 Apr 21 18:34 container_tokens
</span><span class='line'>-rwx------ 1 hadoop hadoop  516 Apr 21 18:34 default_container_executor.sh
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop   65 Apr 21 18:34 filter.io -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/filecache/10/filter.io
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop  120 Apr 21 18:34 job.jar -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/filecache/10/job.jar
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop  120 Apr 21 18:34 job.xml -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/filecache/13/job.xml
</span><span class='line'>-rwx------ 1 hadoop hadoop 3532 Apr 21 18:34 launch_container.sh
</span><span class='line'>drwx--x--- 2 hadoop hadoop 4096 Apr 21 18:34 tmp
</span><span class='line'>[hadoop@umcc97-143 container_1397006359464_1606_01_000002]$ </span></code></pre></td></tr></table></div></figure>


<h2>处理问题方法</h2>

<ul>
<li>打印DEBUG日志：<code>export HADOOP_ROOT_LOGGER=DEBUG,console</code>

<ul>
<li>日志文件放置在nodemanager节点的logs/userlogs目录下。</li>
</ul>
</li>
<li>打印DEBUG日志也搞不定时，可以在源码里面sysout信息然后把<strong>class覆盖</strong>，来进行定位配置的问题。</li>
<li>如果不清楚shell的执行过程，可以通过<code>sh -x [CMD]</code>，或者在脚本文件的操作前加上<code>set -x</code>。相当于windows-batch的<code>echo on</code>功能。</li>
</ul>


<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/975271/remote-debugging-a-java-application">remote debugger opts</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/04/22/hadoop-category/">Hadoop2学习过程/资源</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-04-22T06:37:20+08:00" pubdate data-updated="true">Tue 2014-04-22 06:37</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>接触集群，起始为毕业论文觉得做一个SSH的内容管理系统觉得无趣，选择了Hadoop相关的选题。尽管做的很烂，但是当时做出来一个东西还是挺开心的。中间断了近一年，但是在鼎象做游戏的时刻部署系统/维护查日志，对linux熟悉了不少。在科韵开始做插件开发，神马都的看源码。而后，真正的做了一个hadoop的项目，相比2年前，对编程和学习的方式都有了提升。hadoop，kettle这些都是很牛掰的很火热软件，但是中文资料相对较少，变化也很快。慢慢的觉得自己看代码和英文的资源都过得去。</p>

<p>再碌碌无为的过了一年，而今又接触hadoop，时代变了，但是基本的技术还是相同的。从hadoop1升级到hadoop2，尽管变化很大，熟悉的过程中再次遇到很多的问题。觉得无能无力，原来的日子好似白过了！记录是一种美德，不仅是走过路过的足迹，亦是摘树后人乘凉的盛举。</p>

<p>要弄hadoop，首先得把对english的偏见放下！如果还是baidu查找你遇到的问题，那或许你会多走很多的弯路！</p>

<h2>书籍</h2>

<p>开始还是推荐下中文资料：</p>

<ul>
<li>[Hadoop权威指南/Hadoop The Definitive Guide] 大师写的书，值的膜拜</li>
<li>[hadoop实战/Hadoop in Action] 相对来说是也是一本不错的</li>
</ul>


<h2>网页资源</h2>

<ol>
<li>Linux部署Hadoop

<ul>
<li><a href="http://developer.yahoo.com/hadoop/tutorial/">Hadoop Tutorial - YDN</a></li>
<li><a href="http://shiyanjun.cn/archives/561.html">Hadoop-2.2.0集群安装配置实践</a></li>
<li><a href="http://www.cnblogs.com/i80386/p/3548132.html">hadoop2.2.0 单机伪分布式（含64位hadoop编译）及 eclipse hadoop开发环境搭建</a></li>
</ul>
</li>
<li>Windows部署Hadoop

<ul>
<li><a href="http://yangshangchuan.iteye.com/blog/1839812">配置Cygwin支持无密码SSH登陆</a></li>
<li><a href="http://wiki.apache.org/hadoop/Hadoop2OnWindows">Hadoop2OnWindows - Hadoop Wiki</a></li>
<li><a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655">[MAPREDUCE-5655] job submit from windows to a linux hadoop cluster fails due to wrong classpath</a></li>
<li><a href="http://stackoverflow.com/questions/11821378/what-does-bashno-job-control-in-this-shell-mean">linux - what does &ldquo;bash:no job control in this shell” mean? - Stack Overflow</a></li>
<li><a href="http://stackoverflow.com/questions/18630019/running-apache-hadoop-2-1-0-on-windows">Running Apache Hadoop 2.1.0 on Windows - Stack Overflow</a></li>
<li><a href="http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval">Error &lsquo;LINK : fatal error LNK1123: failure during conversion to COFF: file invalid or corrupt&rsquo;</a></li>
<li><a href="http://blog.csdn.net/xzz_hust/article/details/9450289">VS2010 error: LINK : fatal error LNK1123: failure during conversion to COFF: file invalid or corrupt</a></li>
<li><a href="https://issues.apache.org/jira/browse/HADOOP-10144">[HADOOP-10144] Error on build in windows 7 box</a></li>
<li><a href="http://www.srccodes.com/p/article/38/build-install-configure-run-apache-hadoop-2.2.0-microsoft-windows-os#comment-1150229068">Build, Install, Configure and Run Apache Hadoop 2.2.0 in Microsoft Windows OS</a></li>
<li><a href="http://blog.csdn.net/bamuta/article/details/13506843">hadoop2.2.0遇到NativeLibraries错误的解决过程</a></li>
<li><a href="http://blog.csdn.net/bamuta/article/details/13506893">hadoop2.2.0遇到64位操作系统平台报错，重新编译hadoop</a></li>
<li><a href="http://stackoverflow.com/questions/19943766/hadoop-unable-to-load-native-hadoop-library-for-your-platform-error-on-centos">Hadoop &ldquo;Unable to load native-hadoop library for your platform&rdquo; error on CentOS</a></li>
</ul>
</li>
<li>eclipse直接访问HDFS/提交任务

<ul>
<li><a href="http://zy19982004.iteye.com/blog/2031172">Hadoop学习三十二：Win7下无法提交MapReduce Job到集群环境</a></li>
<li><a href="http://blog.csdn.net/fansy1990/article/details/22896249">Eclipse调用hadoop2运行MR程序</a></li>
</ul>
</li>
<li>cygwin部署hadoop

<ul>
<li><a href="http://blog.csdn.net/needle2/article/details/5416571">cygwin sshd 安装配置</a></li>
<li><a href="http://www.ipv6bbs.cn/thread-209-1-1.html">如何确定自己是否已接入IPv6网络及故障分析（提问必看）</a></li>
<li><a href="http://www.ipv6bbs.cn/thread-151-1-1.html">在IPv4网络下接入IPv6网络的方法（隧道与第三方软件）</a></li>
</ul>
</li>
<li>hdfs脚本文件系统</li>
<li>编译源码

<ul>
<li><a href="http://blog.163.com/universsky@126/blog/static/112760232201362743156735/">maven工程pom添加log4j依赖 Missing artifact javax.jms:jms:jar:1.1:compile 为pom.xml添加dependency，编译报错缺少jar包，但是本地库中有这个包</a></li>
<li><a href="http://www.cnblogs.com/sysuys/p/3492791.html">找不到org.mortbay.component.AbstractLifeCycle的类文件</a></li>
<li><a href="https://issues.apache.org/jira/browse/HADOOP-10110">[HADOOP-10110]hadoop-auth has a build break due to missing dependency</a></li>
<li><a href="http://blog.csdn.net/superye1983/article/details/16884097">Hadoop CDH5 手动安装伪分布式模式 </a></li>
</ul>
</li>
<li>远程调试

<ul>
<li><a href="http://stackoverflow.com/questions/975271/remote-debugging-a-java-application">Remote debugging a Java application</a></li>
<li><a href="http://liugang594.iteye.com/blog/154710">Eclipse调试框架的学习与理解</a></li>
<li><a href="http://duming115.iteye.com/blog/791218">Java的远程debug</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/opensource/os-eclipse-javadebug/index.html">使用 Eclipse 远程调试 Java 应用程序</a></li>
</ul>
</li>
<li>与正式环境有关

<ul>
<li><a href="http://cn.mzcart.com/2012/04/24.html">putty – 使用putty命令行参数</a></li>
<li><a href="http://www.ctohome.com/FuWuQi/0b/301.html">Windows下使用ssh代理来访问国外的youtube和twitter/fackbook等网站</a></li>
<li><a href="http://hejianhuacn.iteye.com/blog/1972033">使用SecureCRT的SSH端口转发，使用PLSQL访问内网数据库</a></li>
<li><a href="http://qn-lf.iteye.com/blog/859662">用ssh端口转发功能访问远程服务器</a></li>
<li><a href="http://blog.csdn.net/linuxoostudy/article/details/7097418">SecureCRT设置SSH端口转发详解</a></li>
</ul>
</li>
<li>开发参考的资源/代码访问集群

<ul>
<li><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/">Hadoop 新 MapReduce 框架 Yarn 详解</a></li>
<li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-2-0-jobhistory-log/">Hadoop 2.0中作业日志收集原理以及配置方法</a></li>
<li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-logs-placement/">Hadoop日志到底存在哪里？</a></li>
<li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-distributedcache-details/">Hadoop DistributedCache详解</a></li>
<li><a href="http://hpuxtbjvip0.blog.163.com/blog/static/3674131320132794940734/">Hadoop DistributedCache使用及原理</a></li>
</ul>
</li>
</ol>


<h2>工具</h2>

<ul>
<li>lrzsz</li>
<li>SecureCRT</li>
<li>WinSCP</li>
<li>w3m # 最后使用SSH代理访问取代！</li>
</ul>


<h2>思维</h2>

<ul>
<li>化整为零</li>
</ul>


<h2>技巧</h2>

<ul>
<li>查看jar列表：<code>jar tvf JAR</code></li>
<li>ssh-copy-id</li>
<li>rsync</li>
<li>find</li>
<li>ls -Sl</li>
<li>while/for</li>
<li><p><code>cat unknown.txt | cut -b 62- | sort | uniq</code></p></li>
<li><p>持续的更新 -</p></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-04-21T16:27:11+08:00" pubdate data-updated="true">Mon 2014-04-21 16:27</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Windows作为开发屌丝必备，在windows上如何跑集群方便开发调试，以及怎么把eclipse写好的任务mapreduce提交到测试的集群(linux)上面去跑？这些都是需要直面并解决的问题。</p>

<p>本文主要记录在windows上hadoop集群的环境准备，以及eclipse调试功能等。</p>

<ol>
<li>windows伪分布式部署

<ul>
<li>cmd</li>
<li>cygwin shell</li>
</ul>
</li>
<li>windows-eclipse提交任务到linux集群</li>
<li>导入源码到eclipse</li>
</ol>


<p>这篇文章并非按照操作的时间顺序来进行编写。而是，如果再安装第二遍的话，自己应该如何去操作来组织下文。</p>

<h2>一、Windows伪分布式部署</h2>

<p>尽管一直用windows，但是对windows自带的cmd命令很是不屑！想在cygwin下部署，现在想来，最终用的是windows的java！在cygwin下不就是把路径转换后再传给java执行吗！</p>

<p>所以，如果把cygwin环境搭建好了的话，其实已经把windows的环境也搭建好了！同样hadoop的windows环境配置好了，cygwin环境也同样配置好了。但是，在cygwin下面提交mapreduce任务会有各种&#8221;凌乱&#8221;的问题！</p>

<p>先说说在windows环境搭建的步骤，然后再讲cygwin下运行。</p>

<ol>
<li>需要用到的软件环境</li>
<li>编译windows环境变量配置</li>
<li>编译hadoop-common源代码生成本地依赖库</li>
<li>伪分布式配置</li>
<li>windows下运行</li>
<li>cygwin下运行</li>
</ol>


<h3>1.1 需要用到的软件环境</h3>

<ul>
<li>Win7-x86</li>
<li>hadoop-2.2.0.tar.gz</li>
<li>git</li>
<li>cygwin (源码编译时需要执行sh命令)</li>
<li>visual studio 2010（如果与.net framework4有关的问题请查阅： <a href="http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval">[*]</a> <a href="http://stackoverflow.com/questions/12267158/failure-during-conversion-to-coff-file-invalid-or-corrupt">[*]</a> <a href="http://social.msdn.microsoft.com/Forums/vstudio/en-US/eb4a7699-0f3c-4701-9790-199787f1b359/vs-2010-error-lnk1123-failure-during-conversion-to-coff-file-invalid-or-corrupt?forum=vcgeneral">[*]</a>）</li>
<li>protoc(protoc-2.5.0-win32.zip)(<strong>解压，然后把路径加入到PATH</strong>)</li>
</ul>


<p>搭建环境之前，<strong>建议您看看<a href="http://wiki.apache.org/hadoop/Hadoop2OnWindows">wiki-Hadoop2OnWindows</a></strong>。最终有用的步骤都在上面了！不过在自己瞎折腾的过程中也弄了不少东西，记录下来！</p>

<h3>1.2 编译windows环境变量配置</h3>

<table>
<thead>
<tr>
<th style="text-align:left;"> 变量              </th>
<th style="text-align:left;"> windows</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> Platform          </td>
<td style="text-align:left;"> Win32</td>
</tr>
<tr>
<td style="text-align:left;"> ANT_HOME          </td>
<td style="text-align:left;"> D:\local\usr\apache-ant-1.9.0</td>
</tr>
<tr>
<td style="text-align:left;"> MAVEN_HOME        </td>
<td style="text-align:left;"> D:\local\usr\apache-maven-3.0.4</td>
</tr>
<tr>
<td style="text-align:left;"> JAVA_HOME         </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02</td>
</tr>
<tr>
<td style="text-align:left;"> PATH              </td>
<td style="text-align:left;"> C:\cygwin\bin;C:\protoc;D:\local\usr\apache-maven-3.0.4\bin;D:\local\usr\apache-ant-1.9.0/bin;D:\Java\jdk1.7.0_02\bin;%PATH%</td>
</tr>
</tbody>
</table>


<p><del>编译时，在打开的命令行加入cygwin的路径即可。</del>
在maven编译最后需要用到sh的shell命令，需要把<code>c:\cygwin\bin</code>目录加入到path环境变量。
这里先不配置hadoop的环境变量，因为我只需要用到编译后的本地库而已！！</p>

<h3>1.3 编译源代码生成本地依赖库(dll, exe)</h3>

<p>hadoop2.2.0操作本地文件针对平台的进行了处理。也就是只要在windows运行集群，不管怎么样，你都得先把winutils.exe、hadoop.dll编译出来，用来处理对本地文件赋权、软链接等（类似Linux-Shell的功能）。否则会看到下面的错误：</p>

<ul>
<li>命令执行出错，少了winutils.exe</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>14/04/14 20:07:58 ERROR util.Shell: Failed to locate the winutils binary in the hadoop binary path
</span><span class='line'>java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
</span><span class='line'>  at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
</span><span class='line'>  at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
</span><span class='line'>  at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:293)
</span><span class='line'>
</span><span class='line'>14/04/17 21:22:32 INFO service.AbstractService: Service org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerServic
</span><span class='line'>e failed in state INITED; cause: java.lang.NullPointerException
</span><span class='line'>java.lang.NullPointerException
</span><span class='line'>      at java.lang.ProcessBuilder.start(ProcessBuilder.java:1010)
</span><span class='line'>      at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
</span><span class='line'>      at org.apache.hadoop.util.Shell.run(Shell.java:379)
</span><span class='line'>      at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
</span><span class='line'>      at org.apache.hadoop.util.Shell.execCommand(Shell.java:678)
</span><span class='line'>      at org.apache.hadoop.util.Shell.execCommand(Shell.java:661)
</span><span class='line'>      at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:639)
</span><span class='line'>      at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:435)</span></code></pre></td></tr></table></div></figure>


<ul>
<li>少了hadoop.dll的本地库文件</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>14/04/17 21:30:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-ja
</span><span class='line'>va classes where applicable
</span><span class='line'>14/04/17 21:30:29 FATAL datanode.DataNode: Exception in secureMain
</span><span class='line'>java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
</span><span class='line'>      at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
</span><span class='line'>      at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:435)
</span><span class='line'>      at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)
</span><span class='line'>      at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)
</span><span class='line'>      at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)
</span><span class='line'>      at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:147)
</span><span class='line'>      at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:1698)</span></code></pre></td></tr></table></div></figure>


<h4>下载源码进行编译</h4>

<p>下面需要用到visual studio修改项目配置信息（或者直接修改sln文件也行），然后再使用maven进行编译。</p>

<p>这里仅编译hadoop-common项目，最后把生成winutils.exe/hadoop.dll放到hadoop程序bin目录下。</p>

<p>第一步 下载源码</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/*  https://github.com/apache/hadoop-common.git  */
</span><span class='line'>
</span><span class='line'>Administrator@WINSELIU /e/git/hadoop-common (master)
</span><span class='line'>$ git checkout branch-2.2.0
</span><span class='line'>Checking out files: 100% (5536/5536), done.
</span><span class='line'>Branch branch-2.2.0 set up to track remote branch branch-2.2.0 from origin.
</span><span class='line'>Switched to a new branch 'branch-2.2.0'</span></code></pre></td></tr></table></div></figure>


<p>第二步 应用补丁patch-native-win32</p>

<p>jira: <a href="https://issues.apache.org/jira/browse/HADOOP-9922">https://issues.apache.org/jira/browse/HADOOP-9922</a>   <br/>
patch: <a href="https://issues.apache.org/jira/secure/attachment/12600760/HADOOP-9922.patch">https://issues.apache.org/jira/secure/attachment/12600760/HADOOP-9922.patch</a></p>

<p>native.sln-patch有点问题，下面通过vs修改，使用Visual Studio修改native的活动平台</p>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFebmAcNJsAATafRp0jDs763.png" alt="" /></p>

<p>第三步 在<code>Visual Studio 命令提示(2010)</code>命令行进行Maven编译(仅需编译hadoop-common)</p>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFeeGAfZgdAAWIDc_jnSM492.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\git\hadoop-common\hadoop-common-project\hadoop-common&gt;mvn package -Pdist,native-win -DskipTests -Dtar -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'>/*  native files  */
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/git/hadoop-common/hadoop-common-project/hadoop-common
</span><span class='line'>$ ls -1 target/bin/
</span><span class='line'>hadoop.dll
</span><span class='line'>hadoop.exp
</span><span class='line'>hadoop.lib
</span><span class='line'>hadoop.pdb
</span><span class='line'>libwinutils.lib
</span><span class='line'>winutils.exe
</span><span class='line'>winutils.pdb
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/git/hadoop-common/hadoop-common-project/hadoop-common
</span><span class='line'>$ cp target/bin/* ~/hadoop/bin/</span></code></pre></td></tr></table></div></figure>


<p>windows的本地库的路径就是PATH环境变量。所以<strong>windows下最好还是把dll放到bin目录下，同时把<code>HADOOP_HOME/bin</code>加入到环境变量中！！</strong>
修改PATH环境变量。</p>

<p>可以把dll放到自定义的位置，但是同样最好把该路径加入到PATH环境变量。java默认会到PATH路径下找动态链接库dll。</p>

<h3>1.4 修改hadoop配置，部署伪分布式环境</h3>

<p>可以直接把linux伪分布式的配置cp过来用。然后修改namenode/datanode/yarn文件的存储路径就可以了。
这里有个坑，<code>hdfs-default.xml</code>中的路径前面都加了<code>file://</code>前缀！所以hdfs配置中涉及到路径的，这里都得进行了修改。</p>

<p><strong>Notepad++的Ctrl+D是一个好功能啊</strong></p>

<table>
<thead>
<tr>
<th style="text-align:left;"> 属性                                    </th>
<th style="text-align:left;"> 值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> <strong>slaves</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> localhost</td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> <strong>core-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> fs.defaultFS                            </td>
<td style="text-align:left;"> hdfs://localhost:9000</td>
</tr>
<tr>
<td style="text-align:left;"> io.file.buffer.size                     </td>
<td style="text-align:left;"> 10240</td>
</tr>
<tr>
<td style="text-align:left;"> hadoop.tmp.dir                          </td>
<td style="text-align:left;"> file:///e:/tmp/hadoop</td>
</tr>
<tr>
<td style="text-align:left;"> <strong>hdfs-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> dfs.replication                         </td>
<td style="text-align:left;"> 1</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.namenode.secondary.http-address     </td>
<td style="text-align:left;"> localhost:9001 #设置为空可以禁用</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.namenode.name.dir                   </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/name</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.datanode.data.dir                   </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/data</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.namenode.checkpoint.dir             </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/namesecondary</td>
</tr>
<tr>
<td style="text-align:left;"> <del>dfs.namenode.shared.edits.dir</del>       </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/shared/edits</td>
</tr>
<tr>
<td style="text-align:left;"> <strong>mapred-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> mapreduce.framework.name                </td>
<td style="text-align:left;"> yarn</td>
</tr>
<tr>
<td style="text-align:left;"> mapreduce.jobhistory.address            </td>
<td style="text-align:left;"> localhost:10020</td>
</tr>
<tr>
<td style="text-align:left;"> mapreduce.jobhistory.webapp.address     </td>
<td style="text-align:left;"> localhost:19888</td>
</tr>
<tr>
<td style="text-align:left;"> <strong>yarn-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> yarn.nodemanager.aux-services           </td>
<td style="text-align:left;"> mapreduce_shuffle</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.nodemanager.aux-services.mapreduce_shuffle.class  </td>
<td style="text-align:left;"> org.apache.hadoop.mapred.ShuffleHandler</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.address            </td>
<td style="text-align:left;"> localhost:8032</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.scheduler.address  </td>
<td style="text-align:left;"> localhost:8030</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.resource-tracker.address  </td>
<td style="text-align:left;"> localhost:8031</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.admin.address      </td>
<td style="text-align:left;"> localhost:8033</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.webapp.address     </td>
<td style="text-align:left;"> localhost:8088</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.application.classpath              </td>
<td style="text-align:left;"> %HADOOP_CONF_DIR%, %HADOOP_COMMON_HOME%/share/hadoop/common/<em>, %HADOOP_COMMON_HOME%/share/hadoop/common/lib/</em>, %HADOOP_HDFS_HOME%/share/hadoop/hdfs/<em>, %HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/</em>, %HADOOP_YARN_HOME%/share/hadoop/yarn/<em>, %HADOOP_YARN_HOME%/share/hadoop/yarn/lib/</em></td>
</tr>
</tbody>
</table>


<p>注意点：</p>

<ul>
<li>yarn.application.classpath必须定义！尽管程序中有判断不同平台的默认值不同，但是在yarn-default.xml中已经有值了！

<ul>
<li>yarn.application.classpath对启动程序没影响，但是在运行mapreduce时影响巨大破坏力极强！</li>
</ul>
</li>
<li>自定library的路径是个坑！！

<ul>
<li>在windows下，执行java程序java.library.path默认到PATH路径找。这也是需要定义环境变量HADOOP_HOME，以及把bin加入到PATH的原因吧！</li>
</ul>
</li>
</ul>


<hr />

<h3>1.5 Windows直接运行cmd启动</h3>

<p>如果是用windows的cmd的话，到这里已经基本ok了！<strong>格式化namenode</strong>（<code>hadoop namenode -format</code>），启动就ok了！
<del>发现自己其实很傻×，固执的要用cygwin启动运行！用windows的cmd启动，然后用cygwin的终端查看数据不就行了！两不耽误！</del></p>

<p>cmd命令<strong>默认</strong>是去bin目录下找hadoop.dll的，同时hadoop命令会把bin加入到java.library.path路径下。再次强调/推荐：直接把hadoop.dll放到bin路径。
设置环境变量，启动文件系统：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/* **设置环境变量** */
</span><span class='line'>HADOOP_HOME=E:\local\libs\big\hadoop-2.2.0 
</span><span class='line'>PATH=%HADOOP_HOME%\bin;%PATH%
</span><span class='line'>
</span><span class='line'>/* 格式化namenode */
</span><span class='line'>hadoop namenode -format
</span><span class='line'>
</span><span class='line'>/* 操作HDFS */
</span><span class='line'>set HADOOP_ROOT_LOGGER=DEBUG,console
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;sbin\start-dfs.cmd
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;hdfs dfs -put README.txt /   # 很弱，fs简化操作都不兼容！
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;hdfs dfs -ls /
</span><span class='line'>Found 1 items
</span><span class='line'>-rw-r--r--   1 Administrator supergroup       1366 2014-04-22 22:20 /README.txt</span></code></pre></td></tr></table></div></figure>


<p>JAVA_HOME的路径中最好不要有空格！否则测试下面的方式进行处理：</p>

<blockquote><p>instead e.g. c:\Progra~1\Java&#8230; instead of c:\Program Files\Java.&hellip;</p></blockquote>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFehCATjV7AAVxPp-3G94526.png" alt="" /></p>

<p>好处也是明显的，直接是windows执行，可以使用jdk自带的工具查看运行情况。</p>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFejOAbQmkAAZxcxSbXv0535.png" alt="" /></p>

<p>?疑问： log日志都写在hadoop.log文件中了？反正我是没看到hadoop.log的文件！</p>

<p>HDFS操作文件OK，如果按照上面步骤或者<a href="http://wiki.apache.org/hadoop/Hadoop2OnWindows">官网的wiki</a>操作，则运行mapreduce也是不会出问题的!!</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;sbin\start-yarn.cmd
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;hadoop org.apache.hadoop.examples.WordCount /README.txt /out
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;hdfs dfs -ls /out
</span><span class='line'>Found 2 items
</span><span class='line'>-rw-r--r--   1 Administrator supergroup          0 2014-04-22 22:22 /out/_SUCCESS
</span><span class='line'>-rw-r--r--   1 Administrator supergroup       1306 2014-04-22 22:22 /out/part-r-00000</span></code></pre></td></tr></table></div></figure>


<p>如果你使用上面的hadoop命令执行不了命令，请把hadoop.cmd的换行（下载下来后是unix的）转成windows的换行！</p>

<h4>问题原因分析</h4>

<p>如果你运行mapreduce失败，不外乎三种情况：没有定义HADOOP_HOME系统环境变量，hadoop.dll没有放在PATH路径下，以及yarn.application.classpath没有设置。这三个问题导致。如果你不幸碰到了，那我们如何来确认问题呢？</p>

<p>下面一步步的来解读这个处理过程。在运行mapreduce时报错，可以使用远程调试方式来确认发生的具体位置。（如果你还没有弄好本地开发环境，请先看[三、导入源码到eclipse]）</p>

<p>第一步 调试NodeManager，从根源下手</p>

<p>由于windows的hadoop的程序都是<strong>直接</strong>运行的，不像linux还要ssh再登陆然后在启动。所以这里直接设置HADOOP_NODEMANAGER_OPTS就可以了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set HADOOP_NODEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8092"
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0\sbin\start-yarn.cmd
</span><span class='line'>starting yarn daemons
</span><span class='line'>
</span><span class='line'>E:\&gt;hadoop org.apache.hadoop.examples.WordCount /in /out</span></code></pre></td></tr></table></div></figure>


<p>运行任务之前，在ContainerLaunch#call#171行打个断点（可以查看执行的java命令脚本内容，#254writeLaunchEnv写入cmd文件）。同时可以去到<code>nm-local-dir/nmPrivate</code>目录下查看任务的本地临时文件。application_XXX/containter_XXX/launch_container.cmd文件是MRAppMaster/YarnChild/YarnChild的启动脚本。</p>

<ul>
<li><p>调试。备份生成的脚本文件，开启死循环拷贝模式，把缓存留下来慢慢看</p>

<pre><code class="``">  while true ; do cp -rf nm-local-dir/ backup/ ; sleep 0.5; done
</code></pre>

<p>  <img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFe0aAKlntAAg2A_A0xS0493.png" alt="" /></p></li>
<li><p>查看缓存文件</p>

<ul>
<li>真正启动Mapreduce(yarnchild)的脚本文件launch_container.cmd</li>
<li><p>查看系统日志，确定错误</p>

<p>  <img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFd1GAMcbxAAIU6umDgu4394.png" alt="" /></p></li>
<li><p>classpath路径</p>

<p>  <img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFdsqAVo5pAAlRz9SLM3s104.png" alt="" /></p></li>
<li><p>Job任务类型。第三个参数！</p>

<p>  <img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFd4qAJKJAAAr2lBOh9yU947.png" alt="" /></p></li>
</ul>
</li>
</ul>


<p>这里可以查看脚本，确认HADOOP的相关目录是否正确！以及查看classpath的MANIFEST.MF查看依赖的jar是否完整！也可以通过任务的名称了解相关信息。</p>

<ul>
<li>路径问题，不影响大局（可以不关注/不修改）</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFd8uAR1y0AAcQFk5KVVo655.png" alt="" /></p>

<ul>
<li>调试map/reduce</li>
</ul>


<p>调试程序mapreduce比较好办了，毕竟代码都是自己写的好弄。可以使用mrunit。</p>

<p>map和reduce的进程都是动态的，既不能通过命令行的OPTS参数指定。如果要调试map/reduce需要在opts中传递给它们。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop org.apache.hadoop.examples.WordCount  "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090" /in /out</span></code></pre></td></tr></table></div></figure>


<ul>
<li>library问题</li>
</ul>


<p>如果因为library的问题报access$0的错，提交任务都不成功，可以把自定义的dll路径加入java.library.path尝试一下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop org.apache.hadoop.examples.WordCount "-Dmapreduce.map.java.opts= -Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native" "-Dmapreduce.reduce.java.opts=-Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native"  /in /out</span></code></pre></td></tr></table></div></figure>


<hr />

<h3>1.6 cygwin下运行</h3>

<p>要在cygwin下面把hadoop弄起来，你要把cygwin与java的路径区分，理清楚路径，配置工作就成功一半咯！既然用的还是windows的java程序。配置文件也是最终提供给java执行的，所以配置都不需要修改。</p>

<p>要在cygwin中运行hadoop，仅仅搞定脚本就ok了！在执行java命令之前，把cygwin的路径转换为windows。</p>

<ul>
<li>修改了hadoop-env.sh的内容：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export JAVA_HOME=/cygdrive/d/Java/jdk1.7.0_02 #本来已经在环境变量中定义了，但是执行后台批处理的时刻不会调用环境变量的配置！
</span><span class='line'>export HADOOP_HEAPSIZE=512
</span><span class='line'>export HADOOP_PID_DIR=${HADOOP_PID_DIR:-${HADOOP_LOG_DIR}}</span></code></pre></td></tr></table></div></figure>


<p>cygwin也就是linux的默认加载native的路径是libs/native！！拷一份过去把！！或者配置JAVA_LIBRARY_PATH，参见下面的修改Shell脚本部分。</p>

<p>cygwin自带的工具有个优势：运行脚本和java命令都不出现乱码。（或许把SecureCRT改成GBK编码也行）</p>

<ul>
<li>修改shell脚本命令</li>
</ul>


<p>由于java在windows和linux在识别文件路径上也有差异。如/data传给java，在windows会加上当前路径的盘符(e.g. E)，那写入数据目录就为<code>e:/data</code>。</p>

<p>同时，不同操作系统的classpath的组织方式也不同。(1)需要对classpath已经文件夹的路径进行转换，才能在cygwin下正常的运行java程序。
所以，只要在执行java命令之前对路径和classpath进行转换即可。(2)还需要对getconf返回值的换行符进行处理。涉及到下列的文件：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>libexec/hadoop-config.sh
</span><span class='line'>bin/hadoop
</span><span class='line'>bin/hdfs
</span><span class='line'>bin/mapred
</span><span class='line'>bin/yarn
</span><span class='line'>sbin/start-dfs.sh
</span><span class='line'>sbin/stop-dfs.sh</span></code></pre></td></tr></table></div></figure>


<p>重点修改两个问题如下：</p>

<ul>
<li>配置</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/* hadoop-config.sh */
</span><span class='line'>
</span><span class='line'># 定义时注意，处理cygwin路径时只处理了以/cygdrive开头的路径！ 
</span><span class='line'>export JAVA_LIBRARY_PATH=/cygdrive/e/local/libs/big/hadoop-2.2.0/bin</span></code></pre></td></tr></table></div></figure>


<p>由于windows配置时，把hadoop.dll的动态链接库放到bin目录下，而linux（cygwin）的sh脚本默认是去lib/native下面，所以需要定义一下链接库的查找路径。</p>

<ul>
<li>脚本</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/* hadoop-config.sh */
</span><span class='line'>
</span><span class='line'>/* 在调用java命令前，调用该方法 */
</span><span class='line'>function Cygwin_Patch_PathConvert() {
</span><span class='line'>
</span><span class='line'>  cygwin=false
</span><span class='line'>  case "`uname`" in
</span><span class='line'>  CYGWIN*) cygwin=true;;
</span><span class='line'>  esac
</span><span class='line'>
</span><span class='line'>  # cygwin path translation
</span><span class='line'>  if $cygwin; then
</span><span class='line'>      CLASSPATH=`cygpath -p -w "$CLASSPATH"`
</span><span class='line'>      # ssh过来执行命令是不从.bash_profile获取参数！
</span><span class='line'>      if [ "X$HADOOP_HOME" != "X" ]; then
</span><span class='line'>          HADOOP_HOME=`cygpath -w "$HADOOP_HOME"`
</span><span class='line'>      fi
</span><span class='line'>      HADOOP_LOG_DIR=`cygpath -w "$HADOOP_LOG_DIR"`
</span><span class='line'>      if [ "X$TOOL_PATH" != "X" ]; then
</span><span class='line'>          TOOL_PATH=`cygpath -p -w "$TOOL_PATH"`
</span><span class='line'>      fi
</span><span class='line'>      
</span><span class='line'>HADOOP_COMMON_HOME=`cygpath -w "$HADOOP_COMMON_HOME"`
</span><span class='line'>JAVA_HOME=`cygpath -w "$JAVA_HOME"`
</span><span class='line'>HADOOP_YARN_HOME=`cygpath -w "$HADOOP_YARN_HOME"`
</span><span class='line'>HADOOP_HDFS_HOME=`cygpath -w "$HADOOP_HDFS_HOME"`
</span><span class='line'>HADOOP_CONF_DIR=`cygpath -w "$HADOOP_CONF_DIR"`
</span><span class='line'>
</span><span class='line'># HOME
</span><span class='line'>      
</span><span class='line'>      # 把带/cygdrive/[abc]形式的路径转换为windows路径
</span><span class='line'>      HADOOP_OPTS=`echo $HADOOP_OPTS | awk -F" " '{for(i=1;i&lt;=NF;i++)print $i}' | awk -F"=" ' {if($2~/^\/cygdrive\/[a|b|c|d|e]/){print $1;system("cygpath -w -p " $2 )}else{ print $0 }; print ""}' | awk 'BEGIN{opt="";last=""}{if($0~/^$/){ opt=opt " "; last="" }else{ if(last!=""){ opt=opt "="} opt=opt $0; last=$0; }; }END{ print opt }' `
</span><span class='line'>      
</span><span class='line'>      YARN_OPTS=`echo $YARN_OPTS | awk -F" " '{for(i=1;i&lt;=NF;i++)print $i}' | awk -F"=" ' {if($2~/^\/cygdrive\/[a|b|c|d|e]/){print $1;system("cygpath -w -p " $2 )}else{ print $0 }; print ""}' | awk 'BEGIN{opt="";last=""}{if($0~/^$/){ opt=opt " "; last="" }else{ if(last!=""){ opt=opt "="} opt=opt $0; last=$0; }; }END{ print opt }' `
</span><span class='line'>
</span><span class='line'>      JAVA_LIBRARY_PATH=`cygpath -p -w "$JAVA_LIBRARY_PATH"`
</span><span class='line'>      
</span><span class='line'>  fi
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>/* 系统的换行符不同，需要转换 */
</span><span class='line'>SECONDARY_NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -secondarynamenodes 2&gt;/dev/null | sed 's/^M//g' )</span></code></pre></td></tr></table></div></figure>


<p>在解析OPTS时执行cygpath转换的时刻，也需要加上-p的参数！OPTS中有java.library.path的环境变量！</p>

<ul>
<li>HDFS文件系统测试</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/hadoop namenode -format
</span><span class='line'>sbin/start-dfs.sh
</span><span class='line'>ps</span></code></pre></td></tr></table></div></figure>


<p>jps没有作用了；或者也可以通过任务管理器/<strong>ProcessExplorer</strong>查看java.exe，命令行列还可以查看具体的执行命令，对应的什么服务。</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> 映像名称     </th>
<th style="text-align:left;"> 用户名         </th>
<th style="text-align:left;"> 命令行</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_namenode -Xmx512m &hellip; org.apache.hadoop.hdfs.server.namenode.NameNode</td>
</tr>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_datanode -Xmx512m &hellip; org.apache.hadoop.hdfs.server.datanode.DataNode</td>
</tr>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_secondarynamenode &hellip; org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode</td>
</tr>
</tbody>
</table>


<p>修改了hadoop的脚本，启动环境（cygwin下启动和windows启动都可以），就可以操作HDFS了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~
</span><span class='line'>$ hadoop/bin/hadoop fs -put job.xml /
</span><span class='line'>14/04/22 23:53:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~
</span><span class='line'>$ export JAVA_LIBRARY_PATH=/cygdrive/e/local/libs/big/hadoop-2.2.0/bin
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~
</span><span class='line'>$ hadoop/bin/hadoop fs -ls /
</span><span class='line'>Found 4 items
</span><span class='line'>-rw-r--r--   1 Administrator supergroup       1366 2014-04-22 22:20 /README.txt
</span><span class='line'>-rw-r--r--   1 Administrator supergroup      66539 2014-04-22 23:53 /job.xml
</span><span class='line'>drwxr-xr-x   - Administrator supergroup          0 2014-04-22 23:34 /out
</span><span class='line'>drwx------   - Administrator supergroup          0 2014-04-22 22:21 /tmp</span></code></pre></td></tr></table></div></figure>


<p>如果执行权限问题，可以使用设置HADOOP_USER_NAME的方式处理：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ export HADOOP_USER_NAME=Administrator
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ bin/hadoop fs -rmr /out</span></code></pre></td></tr></table></div></figure>


<h4>MapReduce任务测试</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sbin/start-yarn.sh
</span><span class='line'>ps</span></code></pre></td></tr></table></div></figure>


<p>yarn资源框架启动后，任务管理又会添加两个java的程序：</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> 映像名称     </th>
<th style="text-align:left;"> 用户名         </th>
<th style="text-align:left;"> 命令行</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_resourcemanager &hellip;  org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</td>
</tr>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_nodemanager &hellip; org.apache.hadoop.yarn.server.nodemanager.NodeManager</td>
</tr>
</tbody>
</table>


<h4>提交任务，执行任务处理</h4>

<p>在cygwin环境下，hdfs和yarn都启动成功了，并且能传文件到HDFS中。但是由于cygwin环境最终还是使用windows的java程序集群执行任务！</p>

<p>（可考虑[2.2 Eclipse提交MapReduce]）</p>

<ul>
<li>已处理问题一： cygwin下启动nodemanager，路径没转换</li>
</ul>


<p>由于在cygwin下面启动，大部分的环境变量都是从cygwin带过来的！解析conf中的变量时会使用nodemanager中对应变量的值，如HADOOP_MAPRED_HOME等。</p>

<p>在cygwin使用start-yarn.sh调用java启动程序之前需要转换路径为windows下的路径。在上面的操作已经进行了处理。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 在临时目录下生成了launch_container.cmd文件，用于执行命令，而里面环境变量的值有些cygwin环境下的！
</span><span class='line'>
</span><span class='line'># 设置端口调试nodemanager
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ grep "8092" etc/hadoop/*
</span><span class='line'>etc/hadoop/yarn-env.sh:export YARN_NODEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8092"
</span><span class='line'>
</span><span class='line'>## windows下这个方法大有文章，会把客户端传递的CLASSPATH写入jar的MANIFEST.MF中！
</span><span class='line'>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv()</span></code></pre></td></tr></table></div></figure>


<ul>
<li>已处理问题二：执行mapreduce任务时，缺少环境变量（使用Process Explorer工具查看）</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 设置远程调试map
</span><span class='line'>hadoop org.apache.hadoop.examples.WordCount  "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090" /job.xml /out
</span><span class='line'>
</span><span class='line'># mapred-site.xml设置超时时间
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapred.task.timeout&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;1800000&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'># 结束任务
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ bin/hadoop job -kill job_1398407971082_0003</span></code></pre></td></tr></table></div></figure>


<ul>
<li>取不到HADOOP_HOME环境变量，查找winutils.exe时报错！

<ul>
<li>在hadoop-env.sh中增加定义HADOOP_HOME！</li>
</ul>
</li>
<li>library路径问题，解析动态链接库hadoop.dll失败！

<ul>
<li>增加-D参数吧！</li>
</ul>
</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop org.apache.hadoop.examples.WordCount "-Dmapreduce.map.java.opts= -Djava.library.path=E:\local\libs\big\hadoop-2.2.0\bin" "-Dmapreduce.reduce.java.opts=-Djava.library.path=E:\local\libs\big\hadoop-2.2.0\bin"  /job.xml /out</span></code></pre></td></tr></table></div></figure>


<p>windows泽腾啊。</p>

<ul>
<li>问题二：直接提交任务到linux集群，环境变量不匹配</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ bin/hadoop  fs -ls hdfs://192.168.1.104:9000/
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ export HADOOP_USER_NAME=hadoop
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$  bin/hadoop  org.apache.hadoop.examples.WordCount   -fs hdfs://192.168.1.104:9000 -jt 192.168.1.104 /in /out</span></code></pre></td></tr></table></div></figure>


<p>由于本地是windows的java执行任务提交到集群，所以使用了<code>%JAVA_HOME%</code>，以及windows下的CLASSPATH！执行任务时，同时把nodemanager节点的临时目录备份下来再慢慢查看：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@slave temp]$ while true ; do cp -rf nm-local-dir/ backup/ ; sleep 0.1; done
</span><span class='line'>[hadoop@slave temp]$ find . -name "*.sh"</span></code></pre></td></tr></table></div></figure>


<p>修复该问题，可以参考[2.2 Eclipse提交MapReduce]。</p>

<h3>参考</h3>

<hr />

<h2>二、Windows下使用eclipse连接linux集群</h2>

<h3>2.1 java代码操作HDFS</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>public class HelloHdfs {
</span><span class='line'>
</span><span class='line'>  public static boolean FINISH_CLEAN = true;
</span><span class='line'>
</span><span class='line'>  public static void main(String[] args) throws IOException {
</span><span class='line'>      System.setProperty("HADOOP_USER_NAME", "hadoop"); // 设置用户，否则会有读取权限的问题
</span><span class='line'>      
</span><span class='line'>      FileSystem fs = FileSystem.get(new Configuration());
</span><span class='line'>
</span><span class='line'>      fs.mkdirs(new Path("/java/folder"));
</span><span class='line'>      OutputStream os = fs.create(new Path("/java/folder/hello.txt"));
</span><span class='line'>      Writer w = new BufferedWriter(new OutputStreamWriter(os, "UTF-8"));
</span><span class='line'>      w.write("hello hadoop!");
</span><span class='line'>      w.flush();
</span><span class='line'>      w.close();
</span><span class='line'>      os.close();
</span><span class='line'>
</span><span class='line'>      FSDataInputStream is = fs.open(new Path("/java/folder/hello.txt"));
</span><span class='line'>      BufferedReader br = new BufferedReader(new InputStreamReader(is, "UTF-8"));
</span><span class='line'>      System.out.println(br.readLine());
</span><span class='line'>      br.close();
</span><span class='line'>      is.close();
</span><span class='line'>
</span><span class='line'>      // IOUtils.copyBytes(in, out, 4096, true);
</span><span class='line'>
</span><span class='line'>      if (FINISH_CLEAN)
</span><span class='line'>          fs.delete(new Path("/java"), true);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>对于访问linux集群的hdfs，只要编译通过，对集群HDFS文件系统的CRUD基本没有不会遇到什么问题。写代码过程中遇到过下面两个问题：</p>

<ul>
<li><p>如果你也引入了hive的包，可能会抛不能重写final方法的错误！由于hive中也就了proto的代码（final），调整下顺序先加载proto的包就可以了！</p>

<pre><code class="``">  log4j:WARN Please initialize the log4j system properly.
  Exception in thread "main" java.lang.VerifyError: class org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
      at java.lang.ClassLoader.defineClass1(Native Method)
      at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
</code></pre></li>
<li><p>Permission denied: user=Administrator, access=WRITE, inode=&ldquo;/&rdquo;:hadoop:supergroup:drwxr-xr-x
  这个问题的处理方式有很多。</p>

<ul>
<li>hadoop fs -chmod 777 /</li>
<li>在hdfs的配置文件中，将dfs.permissions修改为False</li>
<li>System.setProperty(&ldquo;user.name&rdquo;, &ldquo;hduser&rdquo;)/System.setProperty(&ldquo;HADOOP_USER_NAME&rdquo;, &ldquo;hduser&rdquo;)/configuration.set(&ldquo;hadoop.job.ugi&rdquo;, &ldquo;hduser&rdquo;);</li>
</ul>
</li>
</ul>


<h3>2.2 Eclipse提交MapReduce</h3>

<ul>
<li><p>需要设置HADOOP_HOME/hadoop.home.dir的环境变量，即在该目录下面有bin\winutils.exe的文件。否则会报错：</p>

<pre><code class="``">  14/04/14 20:07:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  14/04/14 20:07:58 ERROR util.Shell: Failed to locate the winutils binary in the hadoop binary path
  java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
      at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
      at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
      at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:293)
</code></pre></li>
<li><p>任务端(map/reduce)执行命令的classpath变量在客户端Client拼装的！</p>

<p>  浏览官网的<a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655" title="Remote job submit from windows to a linux hadoop cluster fails due to wrong classpath">jira</a>，然后下载并应用<a href="https://issues.apache.org/jira/secure/attachment/12616981/MRApps.patch">MRApps.patch</a>和<a href="https://issues.apache.org/jira/secure/attachment/12616982/YARNRunner.patch">YARNRunner.patch</a>两个补丁。</p>

<p>  其实就是修改Apps#addToEnvironment(Map&lt;String, String>, String, String)来拼装特定操作系统的classpath。以及JAVA_HOME等一些环境变量的值（<code>$JAVA_HOME</code> or <code>%JAVA_HOME%</code>）</p>

<p>  使用<code>patch -p1 &lt; PATCH</code>进行修复。如果patch文件不在项目根路径，可以删除补丁内容前面文件夹路径，直接与源文件放一起然后应用patch就行了。当然你根据修改的内容手动修改也是OK的。</p></li>
</ul>


<p>如果仅仅是作为客户端client提交任务时使用。如仅在eclipse中运行main提交任务，那么就没有必要打包！直接放到需要项目源码中即可。</p>

<pre><code>* 把应用了补丁的YARNRunner和MRApps加入到项目中
* 然后再configuration中加入`config.set("mapred.remote.os", "Linux")`
* 把mapreduce的任务打包为jar，然后`job.setJar("helloyarn.jar")`
* 最后`Run As -&gt; Java Application`运行提交
</code></pre>

<p>如果很多项目使用，可以打包出来，然后把它添加到classpath中，同时添加加入自定义的xml配置。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>lib-ext&gt;jar tvf window-client-mapreduce-patch.jar
</span><span class='line'>  25 Wed Apr 16 11:21:26 CST 2014 META-INF/MANIFEST.MF
</span><span class='line'> 26684 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapred/YARNRunner.class
</span><span class='line'> 24397 Tue Apr 15 10:32:28 CST 2014 org/apache/hadoop/mapred/YARNRunner.java
</span><span class='line'>  1406 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps$1.class
</span><span class='line'>  2450 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps$TaskAttemptStateUI.class
</span><span class='line'> 19887 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps.class
</span><span class='line'> 18879 Tue Apr 15 11:42:42 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps.java</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFe3eATYhNAAifW_Xk8HI644.png" alt="" /></p>

<h3>参考：</h3>

<ul>
<li><a href="http://zy19982004.iteye.com/blog/2031172">Hadoop学习三十二：Win7下无法提交MapReduce Job到集群环境</a></li>
<li><a href="http://blog.csdn.net/fansy1990/article/details/22896249">Eclipse调用hadoop2运行MR程序</a></li>
<li><a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655">jira-Remote job submit from windows to a linux hadoop cluster fails due to wrong classpath</a></li>
</ul>


<hr />

<h2>三、导入源码到eclipse</h2>

<h3>环境</h3>

<p>参考前面的【window伪分布式部署】</p>

<h3>打开Visual Studio的命令行工具</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>启动\所有程序\Microsoft Visual Studio 2010\Visual Studio Tools\Visual Studio 命令提示(2010)</span></code></pre></td></tr></table></div></figure>


<h3>获取源码，检查2.2.0的分支</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone git@github.com:apache/hadoop-common.git
</span><span class='line'>git checkout branch-2.2.0</span></code></pre></td></tr></table></div></figure>


<p>也可以下载src的源码包，但是如果想修改点东西的话，clone源码应该是最佳的选择了。</p>

<ul>
<li>前面说的win32的patch，如果记得打上哦！参见[1.3 编译源代码生成本地依赖库(dll, exe)]</li>
<li>编译hadoop-auth项目的时刻报错，需要在pom中添加jetty-util的依赖，参考<a href="http://www.cnblogs.com/sysuys/p/3492791.html">找不到org.mortbay.component.AbstractLifeCycle的类文件</a>。</li>
</ul>


<h3>编译生成打包</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set PATH=c:\cygwin\bin;%PATH%
</span><span class='line'>mvn package -Pdist,native-win -DskipTests -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<p>最好加上skipTests条件，不然编译等待时间不是一般的长！！</p>

<h3>导入eclipse</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn eclipse:eclipse</span></code></pre></td></tr></table></div></figure>


<p>然后使用eclipse导入已经存在的工程(existing projects into workspace)，导入后存在两个问题：</p>

<ol>
<li>stream工程的conf源码包找不到。修改为在.project文件中引用，然后把conf引用加入到.classpath。</li>
<li>common下的test代码报错。把<code>target/generated-test-sources/java</code>文件夹的也作为源码包即可。</li>
</ol>


<p><img src="http://file.bmob.cn/M00/0B/29/wKhkA1QFeTmAStwwAAuCI-ODX3Q346.png" alt="" /></p>

<p>eclipse的maven插件你得安装了（要用到M2_REPO路径），同时引用正确conf\settings.xml的Maven配置路径。</p>

<p>注意： 不要使用eclipse导入已经存在的maven方式！eclipse的m2e有些属性和插件还不支持，导入后会报很多错！而使用<code>mvn eclipse:eclipse</code>的方式是把依赖的jar加入到<code>.classpath</code>。</p>

<h3>参考</h3>

<ul>
<li><a href="http://www.cnblogs.com/zhengcong/p/3592490.html">使用Maven将Hadoop2.2.0源码编译成Eclipse项目</a></li>
</ul>


<hr />

<h2>四、胡乱噗噗</h2>

<h3>查看Debug日志</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 ~]$ export HADOOP_ROOT_LOGGER=DEBUG,console
</span><span class='line'>[hadoop@umcc97-44 ~]$ hadoop fs -ls /</span></code></pre></td></tr></table></div></figure>


<h3>java加载动态链接库的环境变量java.library.path</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>D:\local\cygwin\Administrator\test&gt;java LoadLib
</span><span class='line'>
</span><span class='line'>D:\local\cygwin\Administrator\test&gt;java -Djava.library.path=. LoadLib
</span><span class='line'>Exception in thread "main" java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
</span><span class='line'>        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860)
</span><span class='line'>        at java.lang.Runtime.loadLibrary0(Runtime.java:845)
</span><span class='line'>        at java.lang.System.loadLibrary(System.java:1084)
</span><span class='line'>        at LoadLib.main(LoadLib.java:3)
</span><span class='line'>
</span><span class='line'>D:\local\cygwin\Administrator\test&gt;java -Djava.library.path=".;%PATH%" LoadLib
</span></code></pre></td></tr></table></div></figure>


<p>没有定义的时刻，会去PATH路径下找。一旦定义了java.library.path只会在给定的路径下查找！</p>

<h3>hadoop的本地native-library的位置</h3>

<p>文件具体放什么位置，随便运行一个命令，通过debug的日志就可以看到默认Library的路径。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ export HADOOP_ROOT_LOGGER=DEBUG,console
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ bin/hadoop fs -ls /
</span><span class='line'>
</span><span class='line'>14/04/18 09:48:39 DEBUG util.NativeCodeLoader: java.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native
</span><span class='line'>14/04/18 09:48:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span></code></pre></td></tr></table></div></figure>


<h3>cygwin下运行java程序，路径问题</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ ls
</span><span class='line'>ENV.class  ENV.java  w3m-0.5.2
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ java ENV
</span><span class='line'>Windows 7
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ jar cvf test.jar *.class
</span><span class='line'>已添加清单
</span><span class='line'>正在添加: ENV.class(输入 = 475) (输出 = 307)(压缩了 35%)
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ ls -l
</span><span class='line'>总用量 18
</span><span class='line'>-rwxr-xr-x  1 Administrator None 475 四月  4 15:00 ENV.class
</span><span class='line'>-rw-r--r--  1 Administrator None 117 四月  4 15:00 ENV.java
</span><span class='line'>-rwxr-xr-x  1 Administrator None 758 四月 19 12:03 test.jar
</span><span class='line'>drwxr-xr-x+ 1 Administrator None   0 四月 12 20:31 w3m-0.5.2
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ java -cp test.jar ENV
</span><span class='line'>Windows 7
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ java -cp /home/Administrator/test/test.jar ENV
</span><span class='line'>错误: 找不到或无法加载主类 ENV
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ set -x
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ java -cp `cygpath -w /home/Administrator/test/test.jar` ENV
</span><span class='line'>++ cygpath -w /home/Administrator/test/test.jar
</span><span class='line'>+ java -cp 'D:\local\cygwin\Administrator\test\test.jar' ENV
</span><span class='line'>Windows 7</span></code></pre></td></tr></table></div></figure>


<h3>[cygwin]ssh单独用户权限问题</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~
</span><span class='line'>$ hadoop/bin/hadoop  fs -put .bash_profile /bash.info
</span><span class='line'>put: Permission denied: user=Administrator, access=WRITE, inode="/":cyg_server:supergroup:drwxr-xr-x</span></code></pre></td></tr></table></div></figure>


<ul>
<li>设置环境变量<code>HADOOP_USER_NAME=hadoop</code></li>
<li>可以使用dfs.permissions属性设置为false。</li>
<li>给位置chown/chmod赋权: <code>hadoop fs -chmod 777 /</code></li>
<li>也可以使用ssh-host-config的<code>Should privilege separation be used? (yes/no) no</code>设置为<strong>no</strong>。使用当前用户进行管理。
  <code>
  Administrator@winseliu /var
  $ chown Administrator:None empty/
  Administrator@winseliu ~
  $ /usr/sbin/sshd.exe # 启动，也可以弄个脚本到启动项，开机启动
  Administrator@winseliu ~/hadoop
  $ ps | grep ssh
       4384       1    4384       4384  ?        500 02:41:21 /usr/sbin/sshd
 </code></li>
</ul>


<h3>Visual Studio处理winutils工程</h3>

<ul>
<li><p>winutils的32位编译
  .net framework4, vs2010, 属性修改设置
  <a href="http://stackoverflow.com/questions/12267158/failure-during-conversion-to-coff-file-invalid-or-corrupt">http://stackoverflow.com/questions/12267158/failure-during-conversion-to-coff-file-invalid-or-corrupt</a>
  <a href="http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval">http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval</a>
  <a href="http://social.msdn.microsoft.com/Forums/vstudio/en-US/eb4a7699-0f3c-4701-9790-199787f1b359/vs-2010-error-lnk1123-failure-during-conversion-to-coff-file-invalid-or-corrupt?forum=vcgeneral">http://social.msdn.microsoft.com/Forums/vstudio/en-US/eb4a7699-0f3c-4701-9790-199787f1b359/vs-2010-error-lnk1123-failure-during-conversion-to-coff-file-invalid-or-corrupt?forum=vcgeneral</a></p>

<p>  <a href="http://hi.baidu.com/dreamthief/item/aa690d1494e2caca38cb306d">http://hi.baidu.com/dreamthief/item/aa690d1494e2caca38cb306d</a></p>

<p>  在cygwin安装的时刻也看过这篇，用64位环境maven的是可以编译的
  <a href="http://www.srccodes.com/p/article/38/build-install-configure-run-apache-hadoop-2.2.0-microsoft-windows-os">http://www.srccodes.com/p/article/38/build-install-configure-run-apache-hadoop-2.2.0-microsoft-windows-os</a></p>

<p>  <a href="http://stackoverflow.com/questions/18630019/running-apache-hadoop-2-1-0-on-windows">http://stackoverflow.com/questions/18630019/running-apache-hadoop-2-1-0-on-windows</a></p></li>
</ul>


<h3>[cygwin]ipv6的问题，改成ipv4后不能登陆！</h3>

<p>可能是新版本的openssh的bug！！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu /cygdrive/h/documents
</span><span class='line'>$ ssh -o AddressFamily=inet localhost -v
</span><span class='line'>OpenSSH_6.5, OpenSSL 1.0.1g 7 Apr 2014
</span><span class='line'>debug1: Reading configuration data /etc/ssh_config
</span><span class='line'>debug1: Connecting to localhost [127.0.0.1] port 22.
</span><span class='line'>debug1: Connection established.
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_rsa type 1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_rsa-cert type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_dsa type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_dsa-cert type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_ecdsa type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_ecdsa-cert type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_ed25519 type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_ed25519-cert type -1
</span><span class='line'>debug1: Enabling compatibility mode for protocol 2.0
</span><span class='line'>debug1: Local version string SSH-2.0-OpenSSH_6.5
</span><span class='line'>ssh_exchange_identification: read: Connection reset by peer
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~
</span><span class='line'>$ ping localhost
</span><span class='line'>
</span><span class='line'>正在 Ping winseliu [::1] 具有 32 字节的数据:
</span><span class='line'>来自 ::1 的回复: 时间&lt;1ms
</span><span class='line'>来自 ::1 的回复: 时间&lt;1ms
</span></code></pre></td></tr></table></div></figure>


<p>还不能在hosts文件中加！如，指定localhost为127.0.0.1后，得到结果为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ ssh localhost
</span><span class='line'>ssh_exchange_identification: read: Connection reset by peer</span></code></pre></td></tr></table></div></figure>

</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/15">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/13">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2016/01/07/hadoop-install-and-upgrade-3-ha/">Hadoop安装与升级-(3)HA配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/">在windows开发测试mapreduce几种方式</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/19/hole/">坑</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/12/tez-ui-config-and-run/">配置TEZ-UI</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/07/hadoop-install-and-upgrade-4-ha-upgrade/">Hadoop安装与升级-(4)HA升级</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/07/hadoop-install-and-upgrade-3-ha/">Hadoop安装与升级-(3)HA配置</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/07/hadoop-install-and-upgrade-2-hadoop-upgrade/">Hadoop安装与升级-(2)2.2升级到2.6</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/07/hadoop-install-and-upgrade-1-install-in-docker/">Hadoop安装与升级-Docker中安装(1)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/12/13/postgresql-start-guide/">Postgresql入门</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (38) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (29) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (106)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'winseliu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
