
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="HDFS提供的CP是单线程的，对于大数据量的拷贝操作希望能并行的复制。Hadoop Tools提供了DistCp工具，通过调用MapRed来实现并行的拷贝。 先来了解下hdfs cp的功能： 1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Usage: &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/12">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
  <li><a href="https://yunpan.cn/cuYhpFBPgQYgT" >Books[5aee]</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/03/13/hadoop-distcp/">Hadoop Distcp</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-03-13T20:38:23+08:00" pubdate data-updated="true">Fri 2015-03-13 20:38</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>HDFS提供的CP是单线程的，对于大数据量的拷贝操作希望能并行的复制。Hadoop Tools提供了DistCp工具，通过调用MapRed来实现并行的拷贝。</p>

<h2>先来了解下hdfs cp的功能：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage: hdfs dfs -cp [-f] [-p | -p[topax]] URI [URI ...] &lt;dest&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-not-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -mkdir /cp-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-not-exists2/
</span><span class='line'>cp: `/cp-not-exists2/': No such file or directory
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -ls -R /
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 19:55 /cp
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 19:55 /cp/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 19:54 /cp/README.txt
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-exists
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-exists/cp
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-exists/cp/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-exists/cp/README.txt
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-not-exists
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-not-exists/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-not-exists/README.txt</span></code></pre></td></tr></table></div></figure>


<h2>DistCp(distributed copy)分布式拷贝简单使用方式：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ bin/hadoop distcp /cp /cp-distcp</span></code></pre></td></tr></table></div></figure>


<p>用到分布式一般就说明规模不少，且数据量大，操作时间长。DistCp提供了一些参数来控制程序：</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> DistCpOptionSwitch选项    </th>
<th style="text-align:center;"> 命令行参数                      </th>
<th style="text-align:left;"> 描述                                        </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> LOG_PATH                  </td>
<td style="text-align:center;"> <code>-log &lt;logdir&gt;               </code> </td>
<td style="text-align:left;"> map结果输出的目录。默认为<code>JobStagingDir/_logs</code>，在DistCp#configureOutputFormat把该路径设置给CopyOutputFormat#setOutputPath。</td>
</tr>
<tr>
<td style="text-align:left;"> SOURCE_FILE_LISTING       </td>
<td style="text-align:center;"> <code>-f &lt;urilist_uri&gt;            </code> </td>
<td style="text-align:left;"> 需要拷贝的source-path&hellip;从改文件获取。</td>
</tr>
<tr>
<td style="text-align:left;"> MAX_MAPS                  </td>
<td style="text-align:center;"> <code>-m &lt;num_maps&gt;               </code> </td>
<td style="text-align:left;"> 默认20个，创建job时通过<code>JobContext.NUM_MAPS</code>添加到配置。</td>
</tr>
<tr>
<td style="text-align:left;"> ATOMIC_COMMIT             </td>
<td style="text-align:center;"> <code>-atomic                     </code> </td>
<td style="text-align:left;"> 原子操作。要么全部拷贝成功，那么失败。与<code>SYNC_FOLDERS</code> &amp; <code>DELETE_MISSING</code>选项不兼容。</td>
</tr>
<tr>
<td style="text-align:left;"> WORK_PATH                 </td>
<td style="text-align:center;"> <code>-tmp &lt;tmp_dir&gt;              </code> </td>
<td style="text-align:left;"> 与atomic一起使用，中间过程存储数据目录。成功后在CopyCommitter一次性移动到target-path下。</td>
</tr>
<tr>
<td style="text-align:left;"> SYNC_FOLDERS              </td>
<td style="text-align:center;"> <code>-update                     </code> </td>
<td style="text-align:left;"> 新建或更新文件。当文件大小和blockSize（以及crc）一样忽略。</td>
</tr>
<tr>
<td style="text-align:left;"> DELETE_MISSING            </td>
<td style="text-align:center;"> <code>-delete                     </code> </td>
<td style="text-align:left;"> 针对target-path目录，清理source-paths目录下没有的文件。常和<code>SYNC_FOLDERS</code>选项一起使用。</td>
</tr>
<tr>
<td style="text-align:left;"> BLOCKING                  </td>
<td style="text-align:center;"> <code>-async                      </code> </td>
<td style="text-align:left;"> 异步运行。其实就是job提交后，不打印日志了没有调用<code>job.waitForCompletion(true)</code>罢了。</td>
</tr>
<tr>
<td style="text-align:left;"> BANDWIDTH                 </td>
<td style="text-align:center;"> <code>-bandwidth num(M)           </code> </td>
<td style="text-align:left;"> 获取数据的最大速度。结合ThrottledInputStream来进行控制，在RetriableFileCopyCommand中初始化。</td>
</tr>
<tr>
<td style="text-align:left;"> COPY_STRATEGY             </td>
<td style="text-align:center;"> <code>-strategy dynamic/uniformsize</code> </td>
<td style="text-align:left;"> 复制的时刻分组策略，即每个Map到底处理写什么数据。后面会讲到，分为静态和动态。</td>
</tr>
</tbody>
</table>


<p>还有新增的两个属性skipcrccheck（SKIP_CRC），append（APPEND）。保留Preserve 属性和ssl选项由于暂时没用到，这里不表，以后用到再补充。</p>

<h2>DistCp的源码</h2>

<p>放在<code>hadoop-2.6.0-src\hadoop-tools\hadoop-distcp</code>目录下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn eclipse:eclipse </span></code></pre></td></tr></table></div></figure>


<p>网络没问题的话，一般都能成功生成.classpath和.project两个Eclipse需要的项目文件。然后把项目导入eclipse即可。包括4个目录。</p>

<p>还是先说说整个distcp的实现流程，看看distcp怎么跑的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ export HADOOP_CLIENT_OPTS="-Dhadoop.root.logger=debug,console -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8071"
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop distcp /cp /cp-distcp
</span><span class='line'>Listening for transport dt_socket at address: 8071</span></code></pre></td></tr></table></div></figure>


<p>运行eclipse远程调试，连接服务器的8071端口，在DistCp的run方法打个断点，就可以调试了解其运行方式。修改log4j为debug，可以查看更详细的日志，了解执行的流程。</p>

<p>服务器的jdk版本和本地eclipse的jdk版本最好一致，这样调试的时刻比较顺畅。</p>

<h3>Driver</h3>

<p>首先进到DistCp(Driver)的main方法，DistCp继承Configured实现了Tool接口，</p>

<p>第一步解析参数</p>

<ol>
<li>使用<code>ToolRunner.run</code>运行会调用GenericOptionsParser解析<code>-D</code>的属性到Configuration实例；</li>
<li>进到run方法后，通过<code>OptionsParser.parse</code>来解析配置为DistCpOptions实例；功能比较单一，主要涉及到DistCpOptionSwitch和DistCpOptions两个类。</li>
</ol>


<p>第二步准备MapRed的Job实例</p>

<ol>
<li>创建metaFolderPath（后面的 待拷贝文件seq文件存取的位置：StagingDir/_distcp[RAND]），对应<code>CONF_LABEL_META_FOLDER</code>属性；</li>
<li>创建Job，设置名称、InputFormat(UniformSizeInputFormat|DynamicInputFormat)、Map类CopyMapper、Map个数（默认20个）、Reduce个数（0个）、OutputKey|ValueClass、MAP_SPECULATIVE（使用RetriableCommand代替）、CopyOutputFormat</li>
<li>把命令行的配置写入Configuration。</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>metaFolderPath /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp-1344594636</span></code></pre></td></tr></table></div></figure>


<p>此处有话题，设置InputFormat时通过<code>DistCpUtils#getStrategy</code>获取，代码中并没有<code>strategy.impl</code>的键加入到configuration。why？此处也是我们可以学习的，这个设置项在distcp-default.xml配置文件中，这种方式可以实现代码的解耦。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public static Class&lt;? extends InputFormat&gt; getStrategy(Configuration conf,
</span><span class='line'>                                                                 DistCpOptions options) {
</span><span class='line'>    String confLabel = "distcp." +
</span><span class='line'>        options.getCopyStrategy().toLowerCase(Locale.getDefault()) + ".strategy.impl";
</span><span class='line'>    return conf.getClass(confLabel, UniformSizeInputFormat.class, InputFormat.class);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>// 配置
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>        &lt;name&gt;distcp.dynamic.strategy.impl&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;org.apache.hadoop.tools.mapred.lib.DynamicInputFormat&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;Implementation of dynamic input format&lt;/description&gt;
</span><span class='line'>    &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>        &lt;name&gt;distcp.static.strategy.impl&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;org.apache.hadoop.tools.mapred.UniformSizeInputFormat&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;Implementation of static input format&lt;/description&gt;
</span><span class='line'>    &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>配置CopyOutputFormat时，设置了三个路径：</p>

<ul>
<li>WorkingDirectory（中间临时存储目录，atomic选项时为tmp路径，否则为target-path路径）；</li>
<li>CommitDirectory（文件拷贝最终目录，即target-path）；</li>
<li>OutputPath（map write记录输出路径）。</li>
</ul>


<p>关于命令行选项有一个疑问，用eclipse查看<code>Call Hierachy</code>调用关系的时刻，并没有发现调用<code>DistCpOptions#getXXX</code>的方法，那么是通过什么方式把这些配置项设置到Configuration的呢？ 在DistCpOptionSwitch的枚举类中定义了每个选项的confLabel，在<code>DistCpOptions#appendToConf</code>方法中一起把这些属性填充到Configuration中。 [统一配置] ！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public void appendToConf(Configuration conf) {
</span><span class='line'>    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.ATOMIC_COMMIT,
</span><span class='line'>        String.valueOf(atomicCommit));
</span><span class='line'>    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.IGNORE_FAILURES,
</span><span class='line'>        String.valueOf(ignoreFailures));
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>第三步整理需要拷贝的文件列表</p>

<p>这个真tmd的独到，提前把要做的事情规划好。需要拷贝的列表数据最终写入<code>[metaFolder]/fileList.seq</code>（key：与source-path的相对路径，value：该文件的CopyListingFileStatus），对应<code>CONF_LABEL_LISTING_FILE_PATH</code>，也就是map的输入（在自定义的InputFormat中处理）。</p>

<p>涉及CopyList的三个实现FileBasedCopyListing（-f）、GlobbedCopyListing、SimpleCopyListing。最终都调用SimpleCopyListing把文件和空目录列表写入到fileList.seq；最后校验否有重复的文件名，如果存在会抛出DuplicateFileException。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/tmp/hadoop-yarn/staging/hadoop/.staging/_distcp179796572/fileList.seq</span></code></pre></td></tr></table></div></figure>


<p>同时计算需要拷贝的个数和大小（Byte），对应<code>CONF_LABEL_TOTAL_BYTES_TO_BE_COPIED</code>和<code>CONF_LABEL_TOTAL_NUMBER_OF_RECORDS</code>。</p>

<p>第四步提交任务，等待等待无尽的等待。</p>

<p>也可以设置async选项，提交成功后直接完成Driver。</p>

<h3>Mapper</h3>

<p>首先，setup从Configuration中获取配置属性：sync(update)/忽略错误(i)/校验码/overWrite/workPath/finalPath</p>

<p>然后，从CONF_LABEL_LISTING_FILE_PATH路径获取准备好的sourcepath->CopyListingFileStatus键值对作为map的输入。</p>

<p>其实CopyListingFileStatus这个对象真正用到的就是原始Path的路径，真心不知道搞这么多属性干嘛！获取原始路径后又重新实例CopyListingFileStatus为sourceCurrStatus。</p>

<ul>
<li>如果源路径为文件夹，调用createTargetDirsWithRetry（RetriableDirectoryCreateCommand）创建路径，COPY计数加1，return。</li>
<li>如果源路径为文件，但是checkUpdate（文件大小和块大小一致）为skip，SKIP计数加1，BYTESSKIPPED计数加上sourceCurrStatus的长度，把改条记录写入map输出，return。</li>
<li>如果源路径为文件，且检查后不是skip则调用copyFileWithRetry（RetriableFileCopyCommand）拷贝文件，BYTESEXPECTED计数加上sourceCurrStatus的长度，BYTESCOPIED计数加上拷贝文件的大小，COPY计数加1，再return。</li>
<li>如果配置有保留文件/文件夹属性，对目标进行属性修改。</li>
</ul>


<p>从CopyListing获取数据，调用FileSystem-IO接口进行数据的拷贝（在原有IO的基础上封装了ThrottledInputStream来进行限流处理）。于此同时会涉及到source路径是文件夹但是target不是文件夹等的检查；更新还是覆盖；文件属性的保留和Map计数值的更新操作。</p>

<h3>InputFormat</h3>

<p>自定义了InputFormat来UniformSizeInputFormat进行拆分构造FileSplit，对CONF_LABEL_LISTING_FILE_PATH文件的每个键值的文件大小平均分成Map num
个数小块，根据键值的位置构造Map num个FileSplit对象。执行map时，RecordReader根据FileSplit来获取键值对，然后传递给map。</p>

<p>新版本的增加了DynamicInputFormat，实现能者多难的功能。先通过实际的日志，看看运行效果：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ export HADOOP_CLIENT_OPTS="-Dhadoop.root.logger=debug,console -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8071"
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop distcp "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090" -strategy dynamic -m 2 /cp /cp-distcp-dynamic
</span><span class='line'>
</span><span class='line'># 创建的chunk
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        198 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00000
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        224 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00001
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        220 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00002
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>
</span><span class='line'># 分配后的chunk
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        220 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00002
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        198 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/task_1426773672048_0006_m_000000
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        224 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/task_1426773672048_0006_m_000001
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>
</span><span class='line'># map获取后
</span><span class='line'>[hadoop@hadoop-master2 ~]$  ssh -g -L 8090:hadoop-slaver1:8090 hadoop-slaver1
</span><span class='line'># 每拷贝完一个chunk/最后map结束，会把上一个跑完的chunk文件删除
</span><span class='line'># job跑完后，临时目录的数据就被清楚了
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>ls: `/tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446': No such file or directory</span></code></pre></td></tr></table></div></figure>


<p>由于设置的map num为2，还有一个chunk没有分配出去，等到真正执行的时刻再进行分配。体现了策略的动态性。这个<strong>chunkm_000000分配给map0(其他类似)</strong>，其他没有分配出去的chunk让给map去<strong>抢</strong>。</p>

<p>首先InputFormat创建FileSplit，在此过程中把原来的<code>CONF_LABEL_LISTING_FILE_PATH</code>中的需要处理的文件根据个数等份成chunk。（具体实现看源码，其中numEntriesPerChunk计算一个chunk几个文件比较复杂点）</p>

<p>chunk中的也是sourcepath->CopyListingFileStatus键值对，以seq格式的存储文件中。<code>DynamicInputChunk#acquire(TaskAttemptContext)</code>读取数据的时刻比较有意思，在Driver阶段分配的chunk处理完后，就会动态的取处理余下的chunk，能者多劳。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public static DynamicInputChunk acquire(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
</span><span class='line'>    if (!areInvariantsInitialized())
</span><span class='line'>        initializeChunkInvariants(taskAttemptContext.getConfiguration());
</span><span class='line'>
</span><span class='line'>    String taskId = taskAttemptContext.getTaskAttemptID().getTaskID().toString();
</span><span class='line'>    Path acquiredFilePath = new Path(chunkRootPath, taskId);
</span><span class='line'>
</span><span class='line'>    if (fs.exists(acquiredFilePath)) {
</span><span class='line'>      LOG.info("Acquiring pre-assigned chunk: " + acquiredFilePath);
</span><span class='line'>      return new DynamicInputChunk(acquiredFilePath, taskAttemptContext);
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    for (FileStatus chunkFile : getListOfChunkFiles()) {
</span><span class='line'>      if (fs.rename(chunkFile.getPath(), acquiredFilePath)) {
</span><span class='line'>        LOG.info(taskId + " acquired " + chunkFile.getPath());
</span><span class='line'>        return new DynamicInputChunk(acquiredFilePath, taskAttemptContext);
</span><span class='line'>      }
</span><span class='line'>      else
</span><span class='line'>        LOG.warn(taskId + " could not acquire " + chunkFile.getPath());
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    return null;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<h3>OutputFormat &amp; Committer</h3>

<p>自定义的CopyOutputFormat包括了working/commit/output路径的get/set方法，同时指定了自定义的OutputCommitter：CopyCommitter。</p>

<p>正常情况为app-master调用CopyCommitter#commitJob处理善后的事情：保留文件属性的情况下更新文件的属性，atomic情况下把working转到commit路径，delete情况下删除target目录多余的文件。最后清理临时目录。</p>

<p>看完DistCp然后再去看DistCpV1，尽管说功能上类似，但是要和新版本对上仍然要去看distcp的代码。好的代码就是这样吧，让人很自然轻松的理解，而不必反复来回的折腾，甚至于为了免得来回折腾而记住该代码块。（类太大，方法太长，变量定义和使用的位置相隔很远！一个变量作用域太长赋值变更次数太多）</p>

<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/FileSystemShell.html#cp">FileSystemShell cp</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistCp.html">DistCp官方文档</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/03/09/windows-build-hadoop-2-dot-6/">Windows Build hadoop-2.6</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-03-09T12:01:55+08:00" pubdate data-updated="true">Mon 2015-03-09 12:01</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>环境</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>C:\Users\winse&gt;java -version
</span><span class='line'>java version "1.7.0_02"
</span><span class='line'>Java(TM) SE Runtime Environment (build 1.7.0_02-b13)
</span><span class='line'>Java HotSpot(TM) Client VM (build 22.0-b10, mixed mode, sharing)
</span><span class='line'>
</span><span class='line'>C:\Users\winse&gt;protoc --version
</span><span class='line'>libprotoc 2.5.0
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ cygcheck -c cygwin
</span><span class='line'>Cygwin Package Information
</span><span class='line'>Package              Version        Status
</span><span class='line'>cygwin               1.7.33-1       OK</span></code></pre></td></tr></table></div></figure>


<h2>具体步骤</h2>

<p>在windows下，hadoop-2.6还不能直接编译java-x86的dll。需要自己处理/打patch<a href="https://issues.apache.org/jira/browse/HADOOP-9922">HADOOP-9922</a>，但是官网jira-patch给出来的和2.6.0-src对不上。自己动手丰衣足食，把x64的全部改成Win32即可，附编译成功的patch<a href="http://yunpan.cn/cJaZzSu6DIibg">下载hadoop-2.6.0-common-native-win32-diff.patch（提取码：08fd）</a>。</p>

<ul>
<li>用visual studio2010的x86命令行进入：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Visual Studio 命令提示(2010)
</span><span class='line'>
</span><span class='line'>Setting environment for using Microsoft Visual Studio 2010 x86 tools.</span></code></pre></td></tr></table></div></figure>


<ul>
<li>切换到hadoop源码目录，打补丁和编译。同时protobuf目录和cygwin\bin目录加入PATH：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd hadoop-2.6.0-src
</span><span class='line'>cd hadoop-common-project\hadoop-common
</span><span class='line'>patch -p0 &lt; hadoop-2.6.0-common-native-win32-diff.patch
</span><span class='line'>
</span><span class='line'>set PATH=%PATH%;E:\local\home\Administrator\bin;c:\cygwin\bin
</span><span class='line'>
</span><span class='line'>mvn package -Pdist,native-win -DskipTests -Dtar -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<ul>
<li>编译完成后，直接把<code>hadoop-common\target\bin</code>目录下的内容拷贝到程序的bin目录下。</li>
</ul>


<p>在windows下，执行java程序java.library.path默认到PATH路径找。这也是需要定义<strong>环境变量HADOOP_HOME</strong>，以及把<code>%HADOOP_HOME%\bin</code>加入到PATH的原因！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>HADOOP_HOME=E:\local\libs\big\hadoop-2.2.0 
</span><span class='line'>PATH=%HADOOP_HOME%\bin;%PATH%</span></code></pre></td></tr></table></div></figure>


<ul>
<li>配置坑：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0
</span><span class='line'>$ find . -name "*-default.xml" | xargs -I{} grep "hadoop.tmp.dir" {}
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/local&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/system&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/staging&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/temp&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/history/recoverystore&lt;/value&gt;
</span><span class='line'>  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/io/local&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/s3&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/s3a&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/name&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/data&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/namesecondary&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn/system/rmstore&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/nm-local-dir&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn-nm-recovery&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn/timeline&lt;/value&gt;</span></code></pre></td></tr></table></div></figure>


<p>就dfs的在前面加了<code>file://</code>前缀！</p>

<p>所以，在windows下如果你只配置hadoop.tmp.dir（<code>file:///e:/tmp/hadoop</code>）的话还得同时配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/name&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/data&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/namesecondary&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>接下来格式化，启动都和同时一样。</p>

<h2>其他</h2>

<p>调试，下载maven源码等</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set HADOOP_NAMENODE_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"
</span><span class='line'>
</span><span class='line'>mvn dependency:resolve -Dclassifier=sources
</span><span class='line'>
</span><span class='line'>mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs 
</span><span class='line'>
</span><span class='line'>mvn dependency:sources 
</span><span class='line'>mvn dependency:resolve -Dclassifier=javadoc
</span><span class='line'>
</span><span class='line'>/* 操作HDFS */
</span><span class='line'>set HADOOP_ROOT_LOGGER=DEBUG,console</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/03/08/vmware-build-hadoop2-dot-6/">VMware-Centos6 Build hadoop-2.6</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2015-03-08T08:22:14+08:00" pubdate data-updated="true">Sun 2015-03-08 08:22</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>每次编译hadoop（-common）都是惊心动魄，没一次顺顺当当的！由于作者的偷懒(vmware共享windows目录)，引发的又一起血案~~~</p>

<p>同时，有时生产环境不是自己能选择的，需要适应各种环境来编译相应的hadoop，此时在已有的linux开发环境使用docker搭建各种linux及其方便的事情。这里在centos6上搭建docker-centos5实例来编译hadoop。</p>

<h2>环境说明</h2>

<ul>
<li>操作系统</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# uname -a
</span><span class='line'>Linux localhost.localdomain 2.6.32-431.el6.x86_64 #1 SMP Fri Nov 22 03:15:09 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
</span><span class='line'>[root@localhost ~]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.5 (Final)</span></code></pre></td></tr></table></div></figure>


<ul>
<li>使用VMware的<strong>Shared Folders</strong>建立了maven和hadoop-2.6.0-src到宿主机器的映射：(不要直接在源码映射的目录下编译，先拷贝到linux的硬盘下！！)</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# ll -a hadoop-2.6.0-src maven
</span><span class='line'>lrwxrwxrwx. 1 root root 26 Mar  7 22:47 hadoop-2.6.0-src -&gt; /mnt/hgfs/hadoop-2.6.0-src
</span><span class='line'>lrwxrwxrwx. 1 root root 15 Mar  7 22:47 maven -&gt; /mnt/hgfs/maven</span></code></pre></td></tr></table></div></figure>


<h2>具体操作</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 安装maven，jdk
</span><span class='line'>cat apache-maven-3.2.3-bin.tar.gz | ssh root@192.168.154.130 "cat - | tar zxv "
</span><span class='line'>
</span><span class='line'>tar zxvf jdk-7u60-linux-x64.gz -C ~/
</span><span class='line'>vi .bash_profile 
</span><span class='line'>
</span><span class='line'># 开发环境
</span><span class='line'>yum install gcc glibc-headers gcc-c++ zlib-devel
</span><span class='line'>yum install openssl-devel
</span><span class='line'>
</span><span class='line'># 安装protobuf
</span><span class='line'>tar zxvf protobuf-2.5.0.tar.gz 
</span><span class='line'>cd protobuf-2.5.0
</span><span class='line'>./configure 
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>## 编译hadoop-common
</span><span class='line'># 从映射文件中拷贝hadoop-common到linux文件系统，然后在编译hadoop-common
</span><span class='line'>cd hadoop-2.6.0-src/hadoop-common-project/hadoop-common/
</span><span class='line'>cd ..
</span><span class='line'>cp -r  hadoop-common ~/  #Q:为啥要拷贝一份，【遇到的问题】中有进行解析
</span><span class='line'>cd ~/hadoop-common
</span><span class='line'>mvn install
</span><span class='line'>mvn -X clean package -Pdist,native -Dmaven.test.skip=true -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'>## 编译全部，耗时比较久，可以先去吃个饭^v^
</span><span class='line'>cp -r /mnt/hgfs/hadoop-2.6.0-src ~/
</span><span class='line'>mvn package -Pdist,native -DskipTests -Dmaven.javadoc.skip=true #Q:这里为啥不能用maven.test.skip?</span></code></pre></td></tr></table></div></figure>


<p>$$TAG centos5 20160402</p>

<ul>
<li>docker build hadoop-2.6.3(比自己搞个虚拟机更快)</li>
</ul>


<p>实际生产需要使用centos5，这里在centos5编译。其他下载<a href="https://github.com/CentOS/sig-cloud-instance-images">Centos</a>特定版本，步骤是一样的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 ~]$ cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.6 (Final)
</span><span class='line'>
</span><span class='line'>[root@cu2 shm]# unzip sig-cloud-instance-images-centos-5.zip 
</span><span class='line'>[root@cu2 shm]# cd sig-cloud-instance-images-c8d1a81b0516bca0f20434be8d0fac4f7d58a04a/docker/
</span><span class='line'>[root@cu2 docker]# cat centos-5-20150304_1234-docker.tar.xz | docker import - centos:centos5
</span><span class='line'>[root@cu2 ~]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>centos              centos5             a3f6a632c5ec        27 seconds ago      284.1 MB
</span><span class='line'>
</span><span class='line'># 把本机原有资源利用起来，如：maven/repo/jdk/hadoop等
</span><span class='line'>[root@cu2 ~]# docker run -ti -v /home/hadoop:/home/hadoop -v /opt:/opt -v /data:/data centos:centos5 /bin/bash
</span><span class='line'>
</span><span class='line'>export JAVA_HOME=/opt/jdk1.7.0_17
</span><span class='line'>export MAVEN_HOME=/opt/apache-maven-3.3.9
</span><span class='line'>export PATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PATH
</span><span class='line'>
</span><span class='line'>yum install lrzsz zlib-devel make which gcc gcc-c++ cmake openssl openssl-devel -y
</span><span class='line'>
</span><span class='line'>cd protobuf-2.5.0
</span><span class='line'>./configure 
</span><span class='line'>make && make install
</span><span class='line'>which protoc
</span><span class='line'>
</span><span class='line'>cd hadoop-2.6.3-src/
</span><span class='line'>mvn clean package -Dmaven.javadoc.skip=true -DskipTests -Pdist,native 
</span><span class='line'>
</span><span class='line'>cd hadoop-dist/target/hadoop-2.6.3/lib/native/
</span><span class='line'>cd ..
</span><span class='line'>tar zcvf native-hadoop2.6.3-centos5.tar.gz native
</span><span class='line'>
</span><span class='line'>----
</span><span class='line'>
</span><span class='line'>在centos5编译snappy-1.1.3死都过不去，**Makefile.am:4: Libtool library used but `LIBTOOL' is undefined** 
</span><span class='line'>网上资料都差了，最后直接用centos6编译好的snappy可以。哎，有的用就好。
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# mvn package -Dmaven.javadoc.skip=true -DskipTests -Pdist,native  -Drequire.snappy=true  -Dsnappy.prefix=/home/hadoop/snappy
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# cd hadoop-dist/target/hadoop-2.6.3/
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3]# pwd
</span><span class='line'>/home/hadoop/sources/hadoop-2.6.3-src/hadoop-dist/target/hadoop-2.6.3
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3]# cd lib/native/
</span><span class='line'>[root@8fb11f6b3ced native]# tar zxvf /home/hadoop/snappy/snappy-libs.tar.gz 
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced native]# cd /home/hadoop/sources/hadoop-2.6.3-src/hadoop-dist/target/hadoop-2.6.3
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3]# bin/hadoop checknative -a
</span><span class='line'>
</span><span class='line'># 打包到正式环境
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3]# cd lib/
</span><span class='line'>[root@8fb11f6b3ced lib]# tar zcvf native-hadoop2.6.3-centos5-with-snappy.tar.gz native</span></code></pre></td></tr></table></div></figure>


<p>$$END TAG centos5 20160402</p>

<h2>遇到的问题</h2>

<ul>
<li><p>第一个问题肯定是没有<strong>c</strong>的编译环境，安装gcc即可。</p></li>
<li><p><code>configure: error: C++ preprocessor "/lib/cpp" fails sanity check</code>，安装c++。</p></li>
</ul>


<p>-> <a href="http://www.cnblogs.com/niocai/archive/2011/11/04/2236458.html">configure: error: C++ preprocessor &ldquo;/lib/cpp&rdquo; fails sanity check</a></p>

<ul>
<li><code>Unknown lifecycle phase "c"</code>，点击错误提示最后的链接查看解决方法，即执行<code>mvn install</code>。</li>
</ul>


<p>-> <a href="http://blog.csdn.net/kamemo/article/details/6523992">执行第一maven用例出错：Unknown lifecycle phase &ldquo;complile&rdquo;.</a>
-> <a href="https://cwiki.apache.org/confluence/display/MAVEN/LifecyclePhaseNotFoundException">LifecyclePhaseNotFoundException</a></p>

<ul>
<li><code>CMake Error at /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:108 (message): Could NOT find ZLIB (missing: ZLIB_INCLUDE_DIR)</code>， 缺少zlib-devel。</li>
</ul>


<p>-> <a href="http://ask.csdn.net/questions/62307">Cmake时报错：Could NOT find ImageMagick</a></p>

<ul>
<li><code>cmake_symlink_library: System Error: Operation not supported</code>， 共享的windows目录下不能创建linux的软链接。</li>
</ul>


<p>-> <a href="http://bbs.chinaunix.net/forum.php?mod=viewthread&amp;tid=3595245&amp;fromuid=26971268">参见9楼回复</a></p>

<blockquote><p>创建链接不成功，要确认当前帐户下是否有权限在编译的目录中有创建链接的权限</p>

<p>比如，你如果是在一个WINDOWS机器上的共享目录中编译，就没法创建链接，就会失败。把源码复制到本地的目录中再编译就不会有这问题。</p></blockquote>

<ul>
<li>全部编译时仅能用skipTests，不能maven.test.skip。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>main:
</span><span class='line'>     [echo] Running test_libhdfs_threaded
</span><span class='line'>     [exec] nmdCreate: NativeMiniDfsCluster#Builder#Builder error:
</span><span class='line'>     [exec] java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/MiniDFSCluster$Builder
</span><span class='line'>     [exec] Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdfs.MiniDFSCluster$Builder
</span><span class='line'>     [exec]     at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
</span><span class='line'>     [exec]     at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
</span><span class='line'>     [exec]     at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>     [exec]     at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
</span><span class='line'>     [exec]     at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
</span><span class='line'>     [exec]     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
</span><span class='line'>     [exec]     at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</span><span class='line'>     [exec] TEST_ERROR: failed on /root/hadoop-2.6.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c:326 (errno: 2): got NULL from tlhCluster</span></code></pre></td></tr></table></div></figure>


<ul>
<li><code>Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the</code>，安装openssl-devel。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>main:
</span><span class='line'>    [mkdir] Created dir: /root/hadoop-2.6.0-src/hadoop-tools/hadoop-pipes/target/native
</span><span class='line'>     [exec] -- The C compiler identification is GNU 4.4.7
</span><span class='line'>     [exec] -- The CXX compiler identification is GNU 4.4.7
</span><span class='line'>     [exec] -- Check for working C compiler: /usr/bin/cc
</span><span class='line'>     [exec] -- Check for working C compiler: /usr/bin/cc -- works
</span><span class='line'>     [exec] -- Detecting C compiler ABI info
</span><span class='line'>     [exec] -- Detecting C compiler ABI info - done
</span><span class='line'>     [exec] -- Check for working CXX compiler: /usr/bin/c++
</span><span class='line'>     [exec] -- Check for working CXX compiler: /usr/bin/c++ -- works
</span><span class='line'>     [exec] -- Detecting CXX compiler ABI info
</span><span class='line'>     [exec] -- Detecting CXX compiler ABI info - done
</span><span class='line'>     [exec] -- Configuring incomplete, errors occurred!
</span><span class='line'>     [exec] See also "/root/hadoop-2.6.0-src/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeOutput.log".
</span><span class='line'>     [exec] CMake Error at /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:108 (message):
</span><span class='line'>     [exec]   Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the
</span><span class='line'>     [exec]   system variable OPENSSL_ROOT_DIR (missing: OPENSSL_LIBRARIES
</span><span class='line'>     [exec]   OPENSSL_INCLUDE_DIR)
</span><span class='line'>     [exec] Call Stack (most recent call first):
</span><span class='line'>     [exec]   /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:315 (_FPHSA_FAILURE_MESSAGE)
</span><span class='line'>     [exec]   /usr/share/cmake/Modules/FindOpenSSL.cmake:313 (find_package_handle_standard_args)
</span><span class='line'>     [exec]   CMakeLists.txt:20 (find_package)
</span><span class='line'>     [exec] 
</span><span class='line'>     [exec] </span></code></pre></td></tr></table></div></figure>


<h2>成功</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[INFO] Executed tasks
</span><span class='line'>[INFO] 
</span><span class='line'>[INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---
</span><span class='line'>[INFO] Skipping javadoc generation
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Reactor Summary:
</span><span class='line'>[INFO] 
</span><span class='line'>[INFO] Apache Hadoop Main ................................. SUCCESS [ 43.005 s]
</span><span class='line'>[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 25.511 s]
</span><span class='line'>[INFO] Apache Hadoop Annotations .......................... SUCCESS [ 21.177 s]
</span><span class='line'>[INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 11.728 s]
</span><span class='line'>[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 51.274 s]
</span><span class='line'>[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 35.625 s]
</span><span class='line'>[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 21.936 s]
</span><span class='line'>[INFO] Apache Hadoop Auth ................................. SUCCESS [ 24.665 s]
</span><span class='line'>[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 17.058 s]
</span><span class='line'>[INFO] Apache Hadoop Common ............................... SUCCESS [06:07 min]
</span><span class='line'>[INFO] Apache Hadoop NFS .................................. SUCCESS [ 41.279 s]
</span><span class='line'>[INFO] Apache Hadoop KMS .................................. SUCCESS [ 59.186 s]
</span><span class='line'>[INFO] Apache Hadoop Common Project ....................... SUCCESS [  7.216 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS ................................. SUCCESS [04:29 min]
</span><span class='line'>[INFO] Apache Hadoop HttpFS ............................... SUCCESS [ 52.883 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.972 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 24.901 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  7.486 s]
</span><span class='line'>[INFO] hadoop-yarn ........................................ SUCCESS [  7.466 s]
</span><span class='line'>[INFO] hadoop-yarn-api .................................... SUCCESS [ 32.970 s]
</span><span class='line'>[INFO] hadoop-yarn-common ................................. SUCCESS [ 25.549 s]
</span><span class='line'>[INFO] hadoop-yarn-server ................................. SUCCESS [  6.709 s]
</span><span class='line'>[INFO] hadoop-yarn-server-common .......................... SUCCESS [ 25.292 s]
</span><span class='line'>[INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 29.555 s]
</span><span class='line'>[INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [ 12.800 s]
</span><span class='line'>[INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 14.025 s]
</span><span class='line'>[INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 21.121 s]
</span><span class='line'>[INFO] hadoop-yarn-server-tests ........................... SUCCESS [ 24.019 s]
</span><span class='line'>[INFO] hadoop-yarn-client ................................. SUCCESS [ 18.949 s]
</span><span class='line'>[INFO] hadoop-yarn-applications ........................... SUCCESS [  7.586 s]
</span><span class='line'>[INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  8.428 s]
</span><span class='line'>[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [ 12.671 s]
</span><span class='line'>[INFO] hadoop-yarn-site ................................... SUCCESS [  7.518 s]
</span><span class='line'>[INFO] hadoop-yarn-registry ............................... SUCCESS [ 18.518 s]
</span><span class='line'>[INFO] hadoop-yarn-project ................................ SUCCESS [ 38.781 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client ............................ SUCCESS [ 13.133 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 23.772 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 22.815 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [ 16.810 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.404 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [ 18.157 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 14.637 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  9.190 s]
</span><span class='line'>[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  9.037 s]
</span><span class='line'>[INFO] hadoop-mapreduce ................................... SUCCESS [ 59.280 s]
</span><span class='line'>[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 26.724 s]
</span><span class='line'>[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 31.503 s]
</span><span class='line'>[INFO] Apache Hadoop Archives ............................. SUCCESS [ 19.867 s]
</span><span class='line'>[INFO] Apache Hadoop Rumen ................................ SUCCESS [ 27.401 s]
</span><span class='line'>[INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 20.102 s]
</span><span class='line'>[INFO] Apache Hadoop Data Join ............................ SUCCESS [ 20.382 s]
</span><span class='line'>[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [ 12.207 s]
</span><span class='line'>[INFO] Apache Hadoop Extras ............................... SUCCESS [ 24.069 s]
</span><span class='line'>[INFO] Apache Hadoop Pipes ................................ SUCCESS [ 31.975 s]
</span><span class='line'>[INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 32.225 s]
</span><span class='line'>[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [02:45 min]
</span><span class='line'>[INFO] Apache Hadoop Client ............................... SUCCESS [01:38 min]
</span><span class='line'>[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 15.450 s]
</span><span class='line'>[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 46.489 s]
</span><span class='line'>[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [01:31 min]
</span><span class='line'>[INFO] Apache Hadoop Tools ................................ SUCCESS [  7.603 s]
</span><span class='line'>[INFO] Apache Hadoop Distribution ......................... SUCCESS [ 32.967 s]
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] BUILD SUCCESS
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Total time: 39:30 min
</span><span class='line'>[INFO] Finished at: 2015-03-08T10:55:47+08:00
</span><span class='line'>[INFO] Final Memory: 102M/340M
</span><span class='line'>[INFO] ------------------------------------------------------------------------</span></code></pre></td></tr></table></div></figure>


<p>把src编译出来的native下面的文件拷贝到hadoop集群程序目录下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 lib]$ scp -r root@172.17.42.1:~/hadoop-2.6.0-src/hadoop-dist/target/hadoop-2.6.0/lib/native ./
</span><span class='line'>[hadoop@hadoop-master1 lib]$ cd native/
</span><span class='line'>[hadoop@hadoop-master1 native]$ ll
</span><span class='line'>total 4356
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop 1119518 Mar  8 03:11 libhadoop.a
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop 1486964 Mar  8 03:11 libhadooppipes.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop      18 Mar  3 21:08 libhadoop.so -&gt; libhadoop.so.1.0.0
</span><span class='line'>-rwxr-xr-x. 1 hadoop hadoop  671237 Mar  8 03:11 libhadoop.so.1.0.0
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop  581944 Mar  8 03:11 libhadooputils.a
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop  359490 Mar  8 03:11 libhdfs.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop      16 Mar  3 21:08 libhdfs.so -&gt; libhdfs.so.0.0.0
</span><span class='line'>-rwxr-xr-x. 1 hadoop hadoop  228451 Mar  8 03:11 libhdfs.so.0.0.0</span></code></pre></td></tr></table></div></figure>


<p>添加编译的native包前后对比：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.0]$ hadoop fs -ls /
</span><span class='line'>15/03/08 03:09:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Found 3 items
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-06 16:49 /README.txt
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-06 16:54 /tmp
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-06 16:54 /user
</span><span class='line'>
</span><span class='line'># 编译好后，警告提示没有了
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.0]$ hadoop fs -ls /
</span><span class='line'>Found 3 items
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-06 16:49 /README.txt
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-06 16:54 /tmp
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-06 16:54 /user</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/03/07/vmware-sharefolder/">VMware共享目录</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-03-07T22:25:52+08:00" pubdate data-updated="true">Sat 2015-03-07 22:25</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>VMware提供了与主机共享目录的功能，可以在虚拟机访问宿主机器的文件。</p>

<ol>
<li>选择映射目录
 选择[Edit virtual machine settings]，在弹出的对话框中选择[Options]页签，选择[Shared Folders]，点击右边的[Add]按钮添加需要映射(maven)的本地目录。</li>
<li>安装VMware Tools

<ul>
<li>启动linux虚拟机，选择[VM]菜单，再选择[Install VMware Tools&hellip;]菜单。下载完成后，会自动通过cdrom加载到虚拟机。</li>
<li>登录linux虚拟机，执行以下命令：</li>
</ul>
</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /mnt
</span><span class='line'>mkdir cdrom
</span><span class='line'>mount /dev/cdrom cdrom
</span><span class='line'>cd cdrom/
</span><span class='line'>mkdir ~/vmware
</span><span class='line'>tar zxvf VMwareTools-9.2.0-799703.tar.gz -C ~/vmware
</span><span class='line'>
</span><span class='line'>cd ~/vmware
</span><span class='line'>cd vmware-tools-distrib/
</span><span class='line'>./vmware-install.pl 
</span><span class='line'>reboot
</span><span class='line'>
</span><span class='line'>cd /mnt/hgfs/maven</span></code></pre></td></tr></table></div></figure>


<p>当前的maven目录是映射到宿主的机器目录。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost maven]# ll -a
</span><span class='line'>total 3
</span><span class='line'>drwxrwxrwx. 1 root root    0 Dec 28  2012 .
</span><span class='line'>dr-xr-xr-x. 1 root root 4192 Mar  7 22:41 ..
</span><span class='line'>drwxrwxrwx. 1 root root    0 Dec 28  2012 .m2</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/19/starting-2015-spring-festival/">开年2015</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-02-19T01:42:27+08:00" pubdate data-updated="true">Thu 2015-02-19 01:42</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>红包把整个2015春节捣腾的。。。</p>

<p>到了凌晨，春晚谢幕，寒气开始袭人，陆陆续续大家都开始休息，除夕的红包大战也告一段落。</p>

<p>静下来，方能更好的反思总结。</p>

<p>2014年过的还算顺当，任务上不仅仅是一些繁琐的应付事的工作。提供了足够的空间余地，可以做做自己喜欢的事情，捣腾一阵后，有时间可以给自己思考。</p>

<p>工作4年，到新的环境已然不是新人，时间的积淀和历练让我们更成熟，更有资本的同时，承担更多的责任。在项目组中，可能需要做一些表率。我觉得也是最尴尬的时间，没有傲立群雄的能耐，也不是自甘堕落的无能者，后有追兵前有猛狼。真实的感觉到85后尴尬一代的现实！</p>

<p>工作一段时间后，越来越容易被身边的事情干扰，时不时就会被这样那样的事情打断头绪！在工作之余，学习的时间越来越少，被电视剧和游戏霸占，觉得很是不应该！对于好胜心强的自己来说，接下来还是应该一心一意的去做一件事情！这样自己才能提升的明显，不至于自己觉得碌碌无为！</p>

<p>2014也帮师兄做了些事情，遇到了一些不一样的人和事情，改变了自己原来而一些看法（或者说被现实打败了）。原来总认为别人做了类似的东西，咋用就好了，原来的自己不屑一顾这些东西。还要自己去再造轮子，不希望也觉得浪费时间精力。</p>

<p>其实已有的东西，自己实现一遍后，才是你自己的东西！！不要觉得是在做无用功，当你改进或者添加新的功能时，你会发现自己写的自己实践过的才是自己的，才能驾轻就熟！</p>

<p>好基友都结婚的勾搭的，知名而不认命，什么时刻能踏上点呢！？</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/13">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/11">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2016/04/23/hadoop-guide-catalog/">[整理] Hadoop入门</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/06/17/ganglia-install-on-centos-with-puppet/">使用Puppet安装配置Ganglia</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/05/08/rrc-apache-spark-source-inside-shell/">[读读书]Apache Spark源码剖析-Shell</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/05/07/rrc-apache-spark-source-inside-preface/">[读读书]Apache Spark源码剖析-序</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/05/05/hdfs-heterogeneous-storage/">Hdfs异构存储实操</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/05/05/puppetboard-install/">Puppetboard Install</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/05/03/hiera-and-facts/">Hiera and Facts</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/04/28/mcollective-plugins/">MCollective Plugins</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/04/28/mcollective-quick-start/">MCollective安装配置</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/alluxio/'>alluxio</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (5) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (42) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/puppet/'>puppet</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (33) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (136)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'winseliu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
