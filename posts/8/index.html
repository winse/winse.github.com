
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="hive的assembly包就是一个坑货！既然是一个单独的可运行的jar放到lib包下面干嘛呢！！纯属记录工作过程总的经历，想找干货的飘过吧！！ 上周支撑部门其他项目的hadoop项目，由于 hive mr 比较慢，想用spark试一试看能不能优化。但是系统使用Centos5， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/8">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
  <li><a href="https://yunpan.cn/cuYhpFBPgQYgT" >Books[5aee]</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/08/snappy-centos5-on-hive-on-spark/">Hive-on-spark Snappy on Centos5</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-08T22:27:06+08:00" pubdate data-updated="true">Fri 2016-04-08 22:27</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>hive的assembly包就是一个坑货！既然是一个单独的可运行的jar放到lib包下面干嘛呢！！纯属记录工作过程总的经历，想找干货的飘过吧！！</p>

<p><br/></p>

<p>上周支撑部门其他项目的hadoop项目，由于 <strong>hive mr</strong> 比较慢，想用spark试一试看能不能优化。但是系统使用Centos5，我们项目使用的是Centos6。按部就班的编译呗，hive-on-saprk启用SNAPPY的必要条件：</p>

<ul>
<li>hadoop使用snappy需要native的支持，首先当然是Centos5上编译hadoop。(现在看来可以不必要，但每次hdfs命令都提示我native的错误就很不爽)</li>
<li>hive增加spark。</li>
</ul>


<p>各程序版本信息：</p>

<ul>
<li>hadoop-2.6.3</li>
<li>hive-1.2.1</li>
<li>spark-1.3.1</li>
<li>centos5.4</li>
</ul>


<h2>编译hadoop-snappy</h2>

<ul>
<li>centos5手动</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost snappy-1.1.3]# ./autogen.sh 
</span><span class='line'>Remember to add `AC_PROG_LIBTOOL' to `configure.ac'.
</span><span class='line'>You should update your `aclocal.m4' by running aclocal.
</span><span class='line'>libtoolize: `config.guess' exists: use `--force' to overwrite
</span><span class='line'>libtoolize: `config.sub' exists: use `--force' to overwrite
</span><span class='line'>libtoolize: `ltmain.sh' exists: use `--force' to overwrite
</span><span class='line'>Makefile.am:4: Libtool library used but `LIBTOOL' is undefined
</span><span class='line'>Makefile.am:4: 
</span><span class='line'>Makefile.am:4: The usual way to define `LIBTOOL' is to add `AC_PROG_LIBTOOL'
</span><span class='line'>Makefile.am:4: to `configure.ac' and run `aclocal' and `autoconf' again.
</span><span class='line'>Makefile.am:20: `dist_doc_DATA' is used but `docdir' is undefined</span></code></pre></td></tr></table></div></figure>


<p>在centos5上面手动编译搞不定，不是专业写C的，这些问题就是天书啊(查了很多资料，试了很多方法都没通)！！ <strong>Snappy可以在centos6上面编译，编译好以后再centos5上面也能用，编译hadoop-snappy也是ok的</strong> 。</p>

<ul>
<li>centos5-rpm</li>
</ul>


<p>这里直接用rpm安装snappy。觉得创建虚拟机麻烦的话，也可以用docker。docker不同版本的centos下载： <a href="https://github.com/CentOS/sig-cloud-instance-images/">https://github.com/CentOS/sig-cloud-instance-images/</a> 。然后docker共享host主机的文件： <code>docker run -ti -v /home/hadoop:/home/hadoop -v /opt:/opt -v /data:/data centos:centos5 /bin/bash</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@8fb11f6b3ced ~]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 5.11 (Final)
</span><span class='line'>
</span><span class='line'>https://www.rpmfind.net/linux/rpm2html/search.php?query=snappy
</span><span class='line'>https://www.rpmfind.net/linux/rpm2html/search.php?query=snappy-devel
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# rpm -ivh snappy-1.0.5-1.el5.x86_64.rpm 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# rpm -ivh snappy-devel-1.0.5-1.el5.x86_64.rpm                                                                                  
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# rpm -ql snappy-devel snappy
</span><span class='line'>/usr/include/snappy-c.h
</span><span class='line'>/usr/include/snappy-sinksource.h
</span><span class='line'>/usr/include/snappy-stubs-public.h
</span><span class='line'>/usr/include/snappy.h
</span><span class='line'>/usr/lib64/libsnappy.so
</span><span class='line'>/usr/share/doc/snappy-devel-1.0.5
</span><span class='line'>/usr/share/doc/snappy-devel-1.0.5/format_description.txt
</span><span class='line'>/usr/lib64/libsnappy.so.1
</span><span class='line'>/usr/lib64/libsnappy.so.1.1.3
</span><span class='line'>/usr/share/doc/snappy-1.0.5
</span><span class='line'>/usr/share/doc/snappy-1.0.5/AUTHORS
</span><span class='line'>/usr/share/doc/snappy-1.0.5/COPYING
</span><span class='line'>/usr/share/doc/snappy-1.0.5/ChangeLog
</span><span class='line'>/usr/share/doc/snappy-1.0.5/NEWS
</span><span class='line'>/usr/share/doc/snappy-1.0.5/README
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# export JAVA_HOME=/opt/jdk1.7.0_17
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# export MAVEN_HOME=/opt/apache-maven-3.3.9
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# export PATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PATH
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]#  
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# yum install which gcc gcc-c++ zlib-devel make -y
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# cd protobuf-2.5.0
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# ./configure 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# make && make install
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# which protoc
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# 
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# yum install cmake openssl openssl-devel -y
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# cd hadoop-2.6.3-src/
</span><span class='line'># bundle.snappy和snappy.lib一起使用，可以把系统的snappy.so文件拷贝到lib/native下面（方便拷贝）
</span><span class='line'># &lt;http://grepcode.com/file/repo1.maven.org/maven2/org.apache.hadoop/hadoop-project-dist/2.6.0/META-INF/maven/org.apache.hadoop/hadoop-project-dist/pom.xml&gt;
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# mvn clean package -Dmaven.javadoc.skip=true -DskipTests -Drequire.snappy=true -Dbundle.snappy=true -Dsnappy.lib=/usr/lib64 -Pdist,native
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# ll hadoop-dist/target/hadoop-2.6.3/lib/native/
</span><span class='line'>total 3808
</span><span class='line'>-rw-r--r-- 1 root root 1036552 Apr 12 09:35 libhadoop.a
</span><span class='line'>-rw-r--r-- 1 root root 1212600 Apr 12 09:36 libhadooppipes.a
</span><span class='line'>lrwxrwxrwx 1 root root      18 Apr 12 09:35 libhadoop.so -&gt; libhadoop.so.1.0.0
</span><span class='line'>-rwxr-xr-x 1 root root  613267 Apr 12 09:35 libhadoop.so.1.0.0
</span><span class='line'>-rw-r--r-- 1 root root  401836 Apr 12 09:36 libhadooputils.a
</span><span class='line'>-rw-r--r-- 1 root root  364026 Apr 12 09:35 libhdfs.a
</span><span class='line'>lrwxrwxrwx 1 root root      16 Apr 12 09:35 libhdfs.so -&gt; libhdfs.so.0.0.0
</span><span class='line'>-rwxr-xr-x 1 root root  229672 Apr 12 09:35 libhdfs.so.0.0.0
</span><span class='line'>lrwxrwxrwx 1 root root      18 Apr 12 09:35 libsnappy.so -&gt; libsnappy.so.1.1.3
</span><span class='line'>lrwxrwxrwx 1 root root      18 Apr 12 09:35 libsnappy.so.1 -&gt; libsnappy.so.1.1.3
</span><span class='line'>-rwxr-xr-x 1 root root   21568 Apr 12 09:35 libsnappy.so.1.1.3
</span><span class='line'>
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3-src]# cd hadoop-dist/target/hadoop-2.6.3/
</span><span class='line'>[root@8fb11f6b3ced hadoop-2.6.3]# bin/hadoop checknative -a
</span><span class='line'>16/04/12 09:38:29 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
</span><span class='line'>16/04/12 09:38:29 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
</span><span class='line'>Native library checking:
</span><span class='line'>hadoop:  true /data/bigdata/sources/hadoop-2.6.3-src/hadoop-dist/target/hadoop-2.6.3/lib/native/libhadoop.so.1.0.0
</span><span class='line'>zlib:    true /lib64/libz.so.1
</span><span class='line'>snappy:  true /data/bigdata/sources/hadoop-2.6.3-src/hadoop-dist/target/hadoop-2.6.3/lib/native/libsnappy.so.1
</span><span class='line'>lz4:     true revision:99
</span><span class='line'>bzip2:   false 
</span><span class='line'>openssl: false org.apache.hadoop.crypto.OpensslCipher.initIDs()V
</span><span class='line'>16/04/12 09:38:29 INFO util.ExitUtil: Exiting with status 1</span></code></pre></td></tr></table></div></figure>


<p>把native下面的打tar包，然后替换生产的。一切都是正常的。接下来坑爹的是spark-snappy，具体的说应该是hive-assmably坑！！</p>

<h2>hive-on-spark snappy</h2>

<p>spark官网也没讲使用snappy需要做什么额外的配置（默认spark.io.compression.codec默认为snappy）。部署后设置 <code>hive.execution.engine=spark</code> 执行spark查询，立马就报错了 <strong> Caused by: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.0.5-libsn
appyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9&#8217; not found (required by /tmp/snappy-1.0.5-libsnappyjava.so)</strong> 从错误堆栈看与hadoop-native-snappy没关系，而是一个snappy-java的包。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 ~]$ strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX
</span><span class='line'>GLIBCXX_3.4
</span><span class='line'>GLIBCXX_3.4.1
</span><span class='line'>GLIBCXX_3.4.2
</span><span class='line'>GLIBCXX_3.4.3
</span><span class='line'>GLIBCXX_3.4.4
</span><span class='line'>GLIBCXX_3.4.5
</span><span class='line'>GLIBCXX_3.4.6
</span><span class='line'>GLIBCXX_3.4.7
</span><span class='line'>GLIBCXX_3.4.8
</span><span class='line'>GLIBCXX_FORCE_NEW</span></code></pre></td></tr></table></div></figure>


<p>确实缺少GLIBCXX_3.4.9，最新版本的centos5.11也是一样输出的。</p>

<p>spark的配置为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark.yarn.jar    hdfs:///spark/spark-assembly-1.3.1-hadoop2.6.3.jar
</span><span class='line'>
</span><span class='line'>spark.master  yarn-client
</span><span class='line'>
</span><span class='line'>spark.dynamicAllocation.enabled    true
</span><span class='line'>spark.shuffle.service.enabled      true
</span><span class='line'>spark.dynamicAllocation.minExecutors    2 
</span><span class='line'>spark.dynamicAllocation.maxExecutors    18
</span><span class='line'>
</span><span class='line'>spark.driver.maxResultSize   0
</span><span class='line'>spark.master=yarn-client
</span><span class='line'>spark.driver.memory=5g
</span><span class='line'>spark.eventLog.enabled  true
</span><span class='line'>spark.eventLog.compress  true
</span><span class='line'>spark.eventLog.dir    hdfs:///spark-eventlogs
</span><span class='line'>spark.yarn.historyServer.address file1:18080
</span><span class='line'>
</span><span class='line'>spark.serializer        org.apache.spark.serializer.KryoSerializer
</span><span class='line'>spark.kryoserializer.buffer.max    512m</span></code></pre></td></tr></table></div></figure>


<p>报错的具体信息：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>- 16/04/12 20:20:08 INFO storage.BlockManagerMaster: Registered BlockManager
</span><span class='line'>- java.lang.reflect.InvocationTargetException
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>-        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>-        at java.lang.reflect.Method.invoke(Method.java:606)
</span><span class='line'>-        at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:322)
</span><span class='line'>-        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
</span><span class='line'>-        at org.xerial.snappy.Snappy.&lt;clinit&gt;(Snappy.java:48)
</span><span class='line'>-        at org.apache.spark.io.SnappyCompressionCodec.&lt;init&gt;(CompressionCodec.scala:150)
</span><span class='line'>-        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
</span><span class='line'>-        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
</span><span class='line'>-        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
</span><span class='line'>-        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
</span><span class='line'>-        at org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:68)
</span><span class='line'>-        at org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:60)
</span><span class='line'>-        at org.apache.spark.scheduler.EventLoggingListener.&lt;init&gt;(EventLoggingListener.scala:67)
</span><span class='line'>-        at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:400)
</span><span class='line'>-        at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:61)
</span><span class='line'>-        at org.apache.hive.spark.client.RemoteDriver.&lt;init&gt;(RemoteDriver.java:169)
</span><span class='line'>-        at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>-        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>-        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>-        at java.lang.reflect.Method.invoke(Method.java:606)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
</span><span class='line'>-        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
</span><span class='line'>- Caused by: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.0.5-libsnappyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by /tmp/snappy-1.0.5-libs
</span><span class='line'>-        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
</span><span class='line'>-        at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1965)
</span><span class='line'>-        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1890)
</span><span class='line'>-        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1851)
</span><span class='line'>-        at java.lang.Runtime.load0(Runtime.java:795)
</span><span class='line'>-        at java.lang.System.load(System.java:1062)
</span><span class='line'>-        at org.xerial.snappy.SnappyNativeLoader.load(SnappyNativeLoader.java:39)
</span><span class='line'>-        ... 28 more</span></code></pre></td></tr></table></div></figure>


<p>spark用到了snappy-java来处理snappy的解压缩。用jinfo获取SparkSubmit进程的classpath，用这个classpath跑helloworld确实是报错的，但是单独用hadoop-common下面的 snappy-java-1.0.4.1.jar 是没问题的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 snappy-java-test]$ cat Hello.java 
</span><span class='line'>import org.xerial.snappy.Snappy;
</span><span class='line'>
</span><span class='line'>public class Hello { 
</span><span class='line'>public static void main(String[] args) throws Exception {
</span><span class='line'>String input = "Hello snappy-java!";
</span><span class='line'>
</span><span class='line'>byte[] compressed = Snappy.compress(input.getBytes("utf-8"));
</span><span class='line'>byte[] uncompressed = Snappy.uncompress(compressed);
</span><span class='line'>
</span><span class='line'>String result = new String(uncompressed, "utf-8");
</span><span class='line'>System.out.println(result);
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[hadoop@file1 snappy-java-test]$ java -cp .:/home/hadoop/tools/hadoop-2.6.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar Hello
</span><span class='line'>Hello snappy-java!</span></code></pre></td></tr></table></div></figure>


<p>而而而，classpath中就只有hadoop-common和hadoop-mapreduce下面有snappy-java包，并且都是1.0.4.1，那TMD的使用SparkSubmit-classpath加载Snappy是哪个jar里面的呢？</p>

<p>调整后的helloworld为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 snappy-java-test]$ cat Hello.java 
</span><span class='line'>import org.xerial.snappy.Snappy;
</span><span class='line'>
</span><span class='line'>public class Hello { 
</span><span class='line'>public static void main(String[] args) throws Exception {
</span><span class='line'>String input = "Hello snappy-java!";
</span><span class='line'>
</span><span class='line'>System.out.println(Snappy.class.getProtectionDomain());
</span><span class='line'>byte[] compressed = Snappy.compress(input.getBytes("utf-8"));
</span><span class='line'>byte[] uncompressed = Snappy.uncompress(compressed);
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>String result = new String(uncompressed, "utf-8");
</span><span class='line'>System.out.println(result);
</span><span class='line'>}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>添加getProtectionDomain查看加载类的jar。再编译跑一次，这次终于找到真凶了！！hive-assembly，assembly包还放在lib下面就tmd的是一个坑货！！hive-exec的guava已经坑了很多人了，这次换hive-jdbc了！！(我这里的环境是centos5，centos6是没有这个问题的！！)</p>

<p><img src="/images/blogs/hive-on-spark-centos5-snappy-hive-jdbc.png" alt="" /></p>

<p>如果指定使用hadoop编译依赖的snappy.so.1.1.3动态链接库会出现版本不兼容的问题。还是干掉hive-jdbc-standalone吧。。。囧</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 查看源码SnappyLoader#loadSnappySystemProperties，可以通过配置指定使用系统动态链接库
</span><span class='line'>[hadoop@file1 snappy-java-test]$ cat org-xerial-snappy.properties 
</span><span class='line'>org.xerial.snappy.use.systemlib=true
</span><span class='line'>[hadoop@file1 snappy-java-test]$ ln -s /home/hadoop/tools/hadoop-2.6.3/lib/native/libsnappy.so libsnappyjava.so
</span><span class='line'>[hadoop@file1 snappy-java-test]$ ll
</span><span class='line'>总计 1240
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop     854 04-08 10:11 Hello.class
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop     408 04-08 10:11 Hello.java
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop      55 04-12 19:37 libsnappyjava.so -&gt; /home/hadoop/tools/hadoop-2.6.3/lib/native/libsnappy.so
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop      37 04-12 19:15 org-xerial-snappy.properties
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop 1251514 2014-04-29 snappy-java-1.0.5.jar
</span><span class='line'>[hadoop@file1 snappy-java-test]$ java -cp .:snappy-java-1.0.5.jar -Djava.library.path=. Hello
</span><span class='line'>ProtectionDomain  (file:/home/hadoop/snappy-java-test/snappy-java-1.0.5.jar &lt;no signer certificates&gt;)
</span><span class='line'> sun.misc.Launcher$AppClassLoader@333cb1eb
</span><span class='line'> &lt;no principals&gt;
</span><span class='line'> java.security.Permissions@7377711 (
</span><span class='line'> ("java.io.FilePermission" "/home/hadoop/snappy-java-test/snappy-java-1.0.5.jar" "read")
</span><span class='line'> ("java.lang.RuntimePermission" "exitVM")
</span><span class='line'>)
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Exception in thread "main" java.lang.UnsatisfiedLinkError: org.xerial.snappy.SnappyNative.maxCompressedLength(I)I
</span><span class='line'>        at org.xerial.snappy.SnappyNative.maxCompressedLength(Native Method)
</span><span class='line'>        at org.xerial.snappy.Snappy.maxCompressedLength(Snappy.java:320)
</span><span class='line'>        at org.xerial.snappy.Snappy.rawCompress(Snappy.java:333)
</span><span class='line'>        at org.xerial.snappy.Snappy.compress(Snappy.java:92)
</span><span class='line'>        at Hello.main(Hello.java:8)
</span><span class='line'>      </span></code></pre></td></tr></table></div></figure>


<p>删掉jdbc-standalone后，hive-on-spark就ok了。如果你无法下手删除 hive-jdbc-1.2.1-standalone.jar ，那就把 <code>spark.io.compression.codec</code> 改成 <code>lz4</code> 等压缩也是可以的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@file1 ~]$ hive
</span><span class='line'>
</span><span class='line'>Logging initialized using configuration in file:/home/hadoop/tools/apache-hive-1.2.1-bin/conf/hive-log4j.properties
</span><span class='line'>hive&gt; set hive.execution.engine=spark;
</span><span class='line'>hive&gt; select count(*) from t_info where edate=20160411;
</span><span class='line'>Query ID = hadoop_20160412205338_2c95c5fd-af50-42ba-8681-e154e4b74cb1
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>In order to change the average load for a reducer (in bytes):
</span><span class='line'>  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
</span><span class='line'>In order to limit the maximum number of reducers:
</span><span class='line'>  set hive.exec.reducers.max=&lt;number&gt;
</span><span class='line'>In order to set a constant number of reducers:
</span><span class='line'>  set mapreduce.job.reduces=&lt;number&gt;
</span><span class='line'>Starting Spark Job = 69afc030-fa1f-4fdf-81ef-12bdca411a4f
</span><span class='line'>
</span><span class='line'>Query Hive on Spark job[0] stages:
</span><span class='line'>0
</span><span class='line'>1
</span><span class='line'>
</span><span class='line'>Status: Running (Hive on Spark job[0])
</span><span class='line'>Job Progress Format
</span><span class='line'>CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
</span><span class='line'>2016-04-12 20:54:11,367 Stage-0_0: 0(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:14,421 Stage-0_0: 0(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:17,457 Stage-0_0: 0(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:19,486 Stage-0_0: 2(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:20,497 Stage-0_0: 3(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:21,509 Stage-0_0: 5(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:22,520 Stage-0_0: 6(+2)/234    Stage-1_0: 0/1
</span><span class='line'>2016-04-12 20:54:23,532 Stage-0_0: 7(+2)/234    Stage-1_0: 0/1</span></code></pre></td></tr></table></div></figure>


<h2>小结</h2>

<p>第一，hive的assembly的包太tmd的坑了。第二，以后找java具体加载那个类，可以通过 class.getProtectionDomain 来获取了。第三，又多尝试一个环境部署hadoop。呵呵</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/08/puppet-install/">puppet4.4.1入门安装</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2016-04-08T19:49:32+08:00" pubdate data-updated="true">Fri 2016-04-08 19:49</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>网上资料比较多比较老，基本操作可以借鉴。安装Puppet最简单的方式就是用yum来安装(操作系统centos6），由于天朝的特殊环境最好建立本地仓库。本文记录我自己安装过程的过程，先介绍本地仓库创建，然后介绍Puppet环境的搭建。</p>

<p>操作系统：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 ~]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.5 (Final)</span></code></pre></td></tr></table></div></figure>


<h2>更新</h2>

<p>2016-4-28 15:42:32 - rpm强制安装puppetserver。依赖jdk8有点麻烦，自己安装jdk7就好了。
2016-5-3 09:39:40  - 更新puppetserver性能的部分，运行在Jetty之上不需要再折腾passenger了。见文章最后。</p>

<h2>本地仓库搭建</h2>

<p>Puppet4所有依赖都进行统一打包，其实通过rpm就能直接安装。为了体现下高大山、并且Puppet内部的项目之间是有依赖的。这里先使用createrepo创建本地库。</p>

<p>createrepo其实就是用来创建目录下rpm文件的索引数据(repodata)。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 ~]# yum install createrepo
</span><span class='line'>
</span><span class='line'># 下载系统对应的puppet-pc1的包: https://yum.puppetlabs.com/el/6/PC1/x86_64/ 全部最新版本
</span><span class='line'>[root@hadoop-master2 repo]# ls -1
</span><span class='line'>puppet-agent-1.4.1-1.el6.x86_64.rpm
</span><span class='line'>puppet-dashboard-1.2.23-0.1rc3.el6.noarch.rpm
</span><span class='line'>puppetdb-4.0.0-1.el6.noarch.rpm
</span><span class='line'>puppetdb-termini-3.2.4-1.el6.noarch.rpm
</span><span class='line'>puppetdb-terminus-3-1.el6.noarch.rpm
</span><span class='line'>puppetserver-2.3.1-1.el6.noarch.rpm
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 repo]# createrepo .
</span><span class='line'>Spawning worker 0 with 6 pkgs
</span><span class='line'>Workers Finished
</span><span class='line'>Gathering worker results
</span><span class='line'>
</span><span class='line'>Saving Primary metadata
</span><span class='line'>Saving file lists metadata
</span><span class='line'>Saving other metadata
</span><span class='line'>Generating sqlite DBs
</span><span class='line'>Sqlite DBs complete
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 puppetlabs]# cat /etc/yum.repos.d/puppet-local.repo 
</span><span class='line'>[puppet-local]
</span><span class='line'>name=Puppet Local
</span><span class='line'>baseurl=file:///opt/puppetlabs/repo
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span></code></pre></td></tr></table></div></figure>


<p>查看local下的rpm包：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 repo]# yum clean all
</span><span class='line'>Loaded plugins: fastestmirror, security
</span><span class='line'>Cleaning repos: base epel extras pgdg94 puppet-local updates
</span><span class='line'>Cleaning up Everything
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 repo]# yum list all | grep "puppet-local"
</span><span class='line'>puppet-agent.x86_64                         1.4.1-1.el6                  @puppet-local
</span><span class='line'>puppet-dashboard.noarch                     1.2.23-0.1rc3.el6            @puppet-local
</span><span class='line'>puppetdb.noarch                             4.0.0-1.el6                  @puppet-local
</span><span class='line'>puppetdb-termini.noarch                     3.2.4-1.el6                  @puppet-local
</span><span class='line'>puppetserver.noarch                         2.3.1-1.el6                  @puppet-local
</span><span class='line'>puppetdb-terminus.noarch                    3-1.el6                      puppet-local
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 repo]# yum search puppet</span></code></pre></td></tr></table></div></figure>


<p>网上资料还有安装 <code>yum-priorities</code> 来设置repo优先级的。我这里没有包冲突问题所以并没有安装这个。</p>

<h2>单机安装</h2>

<p>安装前翻一翻官网的文档： <a href="https://docs.puppet.com/puppetserver/latest/install_from_packages.html">https://docs.puppet.com/puppetserver/latest/install_from_packages.html</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 先看看 puppet-agent 和 puppetserver 的依赖
</span><span class='line'>[root@hadoop-master2 repo]# yum deplist puppet-agent
</span><span class='line'>Loaded plugins: fastestmirror, security
</span><span class='line'>Loading mirror speeds from cached hostfile
</span><span class='line'> * base: mirrors.aliyun.com
</span><span class='line'> * epel: ftp.cuhk.edu.hk
</span><span class='line'> * extras: mirrors.aliyun.com
</span><span class='line'> * updates: mirrors.aliyun.com
</span><span class='line'>Finding dependencies: 
</span><span class='line'>package: puppet-agent.x86_64 1.4.1-1.el6
</span><span class='line'>  dependency: tar
</span><span class='line'>   provider: tar.x86_64 2:1.23-13.el6
</span><span class='line'>  dependency: /bin/sh
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6_7.1
</span><span class='line'>  dependency: readline
</span><span class='line'>   provider: readline.i686 6.0-4.el6
</span><span class='line'>   provider: readline.x86_64 6.0-4.el6
</span><span class='line'>  dependency: util-linux
</span><span class='line'>   provider: util-linux-ng.i686 2.17.2-12.18.el6
</span><span class='line'>   provider: util-linux-ng.x86_64 2.17.2-12.18.el6
</span><span class='line'>  dependency: chkconfig
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-5.el6
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-5.el6_7.2
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 repo]# yum deplist puppetserver
</span><span class='line'>Loaded plugins: fastestmirror, security
</span><span class='line'>Loading mirror speeds from cached hostfile
</span><span class='line'> * base: mirrors.aliyun.com
</span><span class='line'> * epel: ftp.cuhk.edu.hk
</span><span class='line'> * extras: mirrors.aliyun.com
</span><span class='line'> * updates: mirrors.aliyun.com
</span><span class='line'>Finding dependencies: 
</span><span class='line'>package: puppetserver.noarch 2.3.1-1.el6
</span><span class='line'>  dependency: /bin/bash
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6_7.1
</span><span class='line'>  dependency: java-1.8.0-openjdk-headless
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.45-35.b13.el6
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.51-0.b16.el6_6
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.51-1.b16.el6_7
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.51-3.b16.el6_7
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.65-0.b17.el6_7
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.71-1.b15.el6_7
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1:1.8.0.77-0.b03.el6_7
</span><span class='line'>  dependency: puppet-agent &gt;= 1.4.0
</span><span class='line'>   provider: puppet-agent.x86_64 1.4.1-1.el6
</span><span class='line'>  dependency: net-tools
</span><span class='line'>   provider: net-tools.x86_64 1.60-110.el6_2
</span><span class='line'>  dependency: /usr/bin/env
</span><span class='line'>   provider: coreutils.x86_64 8.4-37.el6
</span><span class='line'>   provider: coreutils.x86_64 8.4-37.el6_7.3
</span><span class='line'>  dependency: /bin/sh
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6_7.1
</span><span class='line'>  dependency: chkconfig
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-5.el6
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-5.el6_7.2
</span><span class='line'>
</span><span class='line'># 安装
</span><span class='line'>[root@hadoop-master2 repo]# yum install puppetserver
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 repo]# netstat -anp | grep 4526
</span><span class='line'>tcp        0      0 :::8140                     :::*                        LISTEN      4526/java           
</span><span class='line'>
</span><span class='line'># 安装好后，查看各版本软件版本信息
</span><span class='line'>[root@hadoop-master2 repo]# puppet -V
</span><span class='line'>4.4.1
</span><span class='line'>[root@hadoop-master2 repo]# facter -v
</span><span class='line'>3.1.5 (commit b5c2cf9b2ac290cb17fcadea19b467a39e17c1fd)
</span><span class='line'>[root@hadoop-master2 repo]# puppetserver -v
</span><span class='line'>puppetserver version: 2.3.1</span></code></pre></td></tr></table></div></figure>


<p>puppetserver依赖puppet-agent，而puppet-agent是一个all-in-one的assembly的包。所以服务端安装puppetserver就行了。客户端仅安装puppet-agent即可。</p>

<p>Puppet4的目录进行比较大的调整，程序路径为 <code>/opt/puppetlabs</code> ，配置路径为 <code>/etc/puppetlabs</code> 。如果你看的是puppet3资料，对照查看官网 <a href="https://docs.puppet.com/puppet/4.4/reference/whered_it_go.html">Where Did Everything Go in Puppet 4.x?</a> 了解各程序的目录位置。</p>

<p>如果你单独安装了jdk(依赖的是jdk8也是挺烦的)，也可以使用rpm强制安装puppetserver，然后指定java程序的路径：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bash-4.1# yum deplist puppetserver
</span><span class='line'>Loaded plugins: fastestmirror, priorities
</span><span class='line'>Loading mirror speeds from cached hostfile
</span><span class='line'> * centos-local: 172.17.42.1:8888
</span><span class='line'>Finding dependencies: 
</span><span class='line'>package: puppetserver.noarch 2.3.1-1.el6
</span><span class='line'>  dependency: /bin/bash
</span><span class='line'>   provider: bash.x86_64 4.1.2-29.el6
</span><span class='line'>  dependency: java-1.8.0-openjdk-headless
</span><span class='line'>   provider: java-1.8.0-openjdk-headless.x86_64 1.8.0.20-3.b26.el6
</span><span class='line'>  dependency: puppet-agent &gt;= 1.4.0
</span><span class='line'>   provider: puppet-agent.x86_64 1.4.1-1.el6
</span><span class='line'>  dependency: net-tools
</span><span class='line'>   provider: net-tools.x86_64 1.60-110.el6_2
</span><span class='line'>  dependency: /usr/bin/env
</span><span class='line'>   provider: coreutils.x86_64 8.4-37.el6
</span><span class='line'>  dependency: /bin/sh
</span><span class='line'>   provider: bash.x86_64 4.1.2-29.el6
</span><span class='line'>  dependency: chkconfig
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-2.el6_4.1
</span><span class='line'>
</span><span class='line'>bash-4.1# rpm -ivh http://172.17.42.1:8888/centos6/puppet/puppetserver-2.3.1-1.el6.noarch.rpm --nodeps --force
</span><span class='line'>Retrieving http://172.17.42.1:8888/centos6/puppet/puppetserver-2.3.1-1.el6.noarch.rpm
</span><span class='line'>warning: /var/tmp/rpm-tmp.7CAtn8: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY
</span><span class='line'>Preparing...                ########################################### [100%]
</span><span class='line'>usermod: no changes
</span><span class='line'>   1:puppetserver           ########################################### [100%]
</span><span class='line'>usermod: no changes
</span><span class='line'>bash-4.1# chkconfig --list | grep puppetserver
</span><span class='line'>puppetserver    0:off   1:off   2:on    3:on    4:on    5:on    6:off
</span><span class='line'>
</span><span class='line'>bash-4.1# cat /etc/sysconfig/puppetserver 
</span><span class='line'>...
</span><span class='line'>JAVA_BIN="/opt/jdk1.7.0_60/bin/java"
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>bash-4.1# netstat -a
</span><span class='line'>Active Internet connections (servers and established)
</span><span class='line'>Proto Recv-Q Send-Q Local Address               Foreign Address             State      
</span><span class='line'>tcp        0      0 *:8140                      *:*                         LISTEN      
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<h2>单机版HelloWorld</h2>

<p>单机模式不需要认证，当做学习调试环境挺好的：方便简单。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 manifests]# vi helloworld.pp
</span><span class='line'>notify { 'greeting':
</span><span class='line'>  message =&gt; 'Hello, world!'
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 manifests]# puppet apply helloworld.pp 
</span><span class='line'>Notice: Compiled catalog for hadoop-master2.localdomain in environment production in 0.03 seconds
</span><span class='line'>Notice: Hello, world!
</span><span class='line'>Notice: /Stage[main]/Main/Notify[greeting]/message: defined 'message' as 'Hello, world!'
</span><span class='line'>Notice: Applied catalog in 0.04 seconds
</span><span class='line'>
</span><span class='line'># 可以用resource根据当前环境生成配置
</span><span class='line'>[root@hadoop-master2 manifests]# puppet resource user hadoop
</span><span class='line'>user { 'hadoop':
</span><span class='line'>  ensure           =&gt; 'present',
</span><span class='line'>  gid              =&gt; '500',
</span><span class='line'>  home             =&gt; '/home/hadoop',
</span><span class='line'>  password         =&gt; 'XXXXXX',
</span><span class='line'>  password_max_age =&gt; '99999',
</span><span class='line'>  password_min_age =&gt; '0',
</span><span class='line'>  shell            =&gt; '/bin/bash',
</span><span class='line'>  uid              =&gt; '500',
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'># 状态变更
</span><span class='line'>[root@hadoop-master2 puppetlabs]# bin/puppet resource service puppet ensure=running enable=false
</span><span class='line'>Notice: /Service[puppet]/enable: enable changed 'true' to 'false'
</span><span class='line'>service { 'puppet':
</span><span class='line'>  ensure =&gt; 'running',
</span><span class='line'>  enable =&gt; 'false',
</span><span class='line'>}
</span><span class='line'>[root@hadoop-master2 puppetlabs]# chkconfig --list | grep puppet
</span><span class='line'>puppet          0:off   1:off   2:off   3:off   4:off   5:off   6:off
</span><span class='line'>puppetserver    0:off   1:off   2:on    3:on    4:on    5:on    6:off
</span></code></pre></td></tr></table></div></figure>


<h2>CS模式配置</h2>

<p>这里完全模拟生产环境情况(内网)，首先搭建两个本地仓库：centos，puppet。puppet依赖RPM根据具体情况下载即可，我这里用的是centos6.5。</p>

<p>搭建私有仓库：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>增加 java-1.8.0-openjdk-headless 和 tzdata-java-2014g(iso带的2013g不适配)
</span><span class='line'>[root@hadoop-master2 repo]# ll
</span><span class='line'>total 142344
</span><span class='line'>-rw-r--r-- 1 root root 33135156 Apr  9 21:47 java-1.8.0-openjdk-headless-1.8.0.51-3.b16.el6_7.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root 26740012 Apr  9 11:29 puppet-agent-1.4.1-1.el6.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root  4509000 Apr  9 11:29 puppet-dashboard-1.2.23-0.1rc3.el6.noarch.rpm
</span><span class='line'>-rw-r--r-- 1 root root 21866876 Apr  9 11:29 puppetdb-4.0.0-1.el6.noarch.rpm
</span><span class='line'>-rw-r--r-- 1 root root    25516 Apr  9 11:29 puppetdb-termini-3.2.4-1.el6.noarch.rpm
</span><span class='line'>-rw-r--r-- 1 root root     3676 Apr  9 11:29 puppetdb-terminus-3-1.el6.noarch.rpm
</span><span class='line'>-rw-r--r-- 1 root root 33412844 Apr  9 11:29 puppetserver-2.3.1-1.el6.noarch.rpm
</span><span class='line'>drwxr-xr-x 2 root root     4096 Apr  9 22:56 repodata
</span><span class='line'>-rw-r--r-- 1 root root   181196 Sep 17  2014 tzdata-java-2014g-1.el6.noarch.rpm
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 ~]# mount -t iso9660 -o loop CentOS-6.5-x86_64-bin-DVD1.iso /mnt/cdrom
</span><span class='line'># httpd 我的系统已经安装了
</span><span class='line'>[root@hadoop-master2 ~]# cd /var/www/html/
</span><span class='line'>[root@hadoop-master2 html]# ll
</span><span class='line'>total 820
</span><span class='line'>lrwxrwxrwx  1 root root     10 Apr  9 21:54 centos6_5 -&gt; /mnt/cdrom
</span><span class='line'>lrwxrwxrwx  1 root root     20 Mar 30 17:11 puppet -&gt; /opt/puppetlabs/repo</span></code></pre></td></tr></table></div></figure>


<p>启动docker实例，参考 <a href="http://www.winseliu.com/blog/2014/09/30/docker-ssh-on-centos/">docker的安装</a>。由于centos和puppet中有包冲突，需要安装 <code>yum-priorities</code> 。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 repo]# docker run -i -t centos:centos6 /bin/bash
</span><span class='line'>bash-4.1# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.5 (Final)
</span><span class='line'>
</span><span class='line'>bash-4.1# yum install yum-plugin-priorities-1.1.30-30.el6.noarch.rpm 
</span><span class='line'>
</span><span class='line'># 把默认的repo清理掉，添加puppet和centos
</span><span class='line'>bash-4.1# cat /etc/yum.repos.d/puppet-local.repo 
</span><span class='line'>[puppet-local]
</span><span class='line'>name=Puppet Local
</span><span class='line'>baseurl=http://172.17.42.1/puppet
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span><span class='line'>priority=1
</span><span class='line'>bash-4.1# cat /etc/yum.repos.d/centos-local.repo 
</span><span class='line'>[centos-local]
</span><span class='line'>name=Centos Local
</span><span class='line'>baseurl=http://172.17.42.1/centos6_5
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span><span class='line'>priority=2
</span><span class='line'>
</span><span class='line'>bash-4.1# yum install puppetserver
</span><span class='line'>
</span><span class='line'># 加载环境变量
</span><span class='line'>bash-4.1# source /etc/profile.d/puppet-agent.sh
</span><span class='line'># 查看puppet各程序版本
</span><span class='line'>bash-4.1# puppet -V
</span><span class='line'>4.4.1
</span><span class='line'>bash-4.1# puppetserver -v
</span><span class='line'>puppetserver version: 2.3.1
</span><span class='line'>bash-4.1# facter -v
</span><span class='line'>3.1.5 (commit b5c2cf9b2ac290cb17fcadea19b467a39e17c1fd)</span></code></pre></td></tr></table></div></figure>


<p>Agent安装：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bash-4.1# cat /etc/yum.repos.d/puppet-local.repo 
</span><span class='line'>[puppet-local]
</span><span class='line'>name=Puppet Local
</span><span class='line'>baseurl=http://172.17.42.1/puppet
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span><span class='line'>
</span><span class='line'>[centos-local]
</span><span class='line'>name=Centos Local
</span><span class='line'>baseurl=http://172.17.42.1/centos6_5
</span><span class='line'>failovermethod=priority
</span><span class='line'>enabled=1
</span><span class='line'>gpgcheck=0
</span><span class='line'>
</span><span class='line'>bash-4.1# yum install puppet-agent -y</span></code></pre></td></tr></table></div></figure>


<p>配置：</p>

<ul>
<li>添加hosts</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bash-4.1# cat /etc/hosts
</span><span class='line'>172.17.0.4 puppet
</span><span class='line'>172.17.0.5 agent1
</span><span class='line'>172.17.0.6 agent2</span></code></pre></td></tr></table></div></figure>


<ul>
<li>master自测</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bash-4.1# puppet agent -t
</span><span class='line'>Info: Using configured environment 'production'
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Info: Caching catalog for 3e4b2ba27563.localdomain
</span><span class='line'>Info: Applying configuration version '1460222292'
</span><span class='line'>Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml
</span><span class='line'>Notice: Applied catalog in 0.01 seconds</span></code></pre></td></tr></table></div></figure>


<ul>
<li>agent连接服务器</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bash-4.1# puppet agent -t
</span><span class='line'>Info: Creating a new SSL key for 5a56be361905.localdomain
</span><span class='line'>Info: Caching certificate for ca
</span><span class='line'>Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
</span><span class='line'>Info: Creating a new SSL certificate request for 5a56be361905.localdomain
</span><span class='line'>Info: Certificate Request fingerprint (SHA256): 58:1A:2E:28:D3:D7:C5:7B:E3:1A:C2:0F:70:D0:46:C0:34:39:7F:EC:98:65:B1:09:96:D3:4B:A7:4B:32:A6:C6
</span><span class='line'>Info: Caching certificate for ca
</span><span class='line'>Exiting; no certificate found and waitforcert is disabled
</span><span class='line'>
</span><span class='line'># master查看/认证
</span><span class='line'>bash-4.1# puppet cert list
</span><span class='line'>  "5a56be361905.localdomain" (SHA256) 58:1A:2E:28:D3:D7:C5:7B:E3:1A:C2:0F:70:D0:46:C0:34:39:7F:EC:98:65:B1:09:96:D3:4B:A7:4B:32:A6:C6
</span><span class='line'>  "6516b8d0538b.localdomain" (SHA256) F7:49:CC:93:EA:5D:D9:A2:90:33:01:A9:74:86:97:0C:20:0C:EB:24:3A:13:85:64:5C:32:A8:D7:36:91:3C:77
</span><span class='line'>bash-4.1# puppet cert sign --all 
</span><span class='line'>Notice: Signed certificate request for 6516b8d0538b.localdomain
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest 6516b8d0538b.localdomain at '/etc/puppetlabs/puppet/ssl/ca/requests/6516b8d0538b.localdomain.pem'
</span><span class='line'>Notice: Signed certificate request for 5a56be361905.localdomain
</span><span class='line'>Notice: Removing file Puppet::SSL::CertificateRequest 5a56be361905.localdomain at '/etc/puppetlabs/puppet/ssl/ca/requests/5a56be361905.localdomain.pem'
</span><span class='line'>
</span><span class='line'># agent再连
</span><span class='line'>bash-4.1# puppet agent -t
</span><span class='line'>Info: Caching certificate for 5a56be361905.localdomain
</span><span class='line'>Info: Caching certificate_revocation_list for ca
</span><span class='line'>Info: Caching certificate for 5a56be361905.localdomain
</span><span class='line'>Info: Using configured environment 'production'
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Info: Caching catalog for 5a56be361905.localdomain
</span><span class='line'>Info: Applying configuration version '1460222614'
</span><span class='line'>Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml
</span><span class='line'>Notice: Applied catalog in 0.02 seconds</span></code></pre></td></tr></table></div></figure>


<p>相比puppet那么多配置项，安装还是相对简单的。安装写到这些也差不多了，接下来要研究下监控和puppet的配置。</p>

<p>安装过程中也遇到一些问题，主要都是DNS导致。一开始 <strong>直接用hosts</strong> 来配置是最简便的，把server的ip指定为puppet域名。</p>

<p>再来个Hello：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># master
</span><span class='line'>bash-4.1# cd /etc/puppetlabs/code/environments/production/
</span><span class='line'>bash-4.1# ls
</span><span class='line'>environment.conf  hieradata  manifests  modules
</span><span class='line'>bash-4.1# cd manifests/
</span><span class='line'>bash-4.1# cat helloworld.pp 
</span><span class='line'>notify { 'Hello World' : 
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'># agent
</span><span class='line'>bash-4.1# puppet agent -t
</span><span class='line'>Info: Using configured environment 'production'
</span><span class='line'>Info: Retrieving pluginfacts
</span><span class='line'>Info: Retrieving plugin
</span><span class='line'>Info: Caching catalog for 5a56be361905.localdomain
</span><span class='line'>Info: Applying configuration version '1460223248'
</span><span class='line'>Notice: Hello World
</span><span class='line'>Notice: /Stage[main]/Main/Notify[Hello World]/message: defined 'message' as 'Hello World'
</span><span class='line'>Notice: Applied catalog in 0.02 seconds
</span><span class='line'>bash-4.1# </span></code></pre></td></tr></table></div></figure>


<h2>最后说说PuppetServer性能</h2>

<ul>
<li><a href="https://docs.puppet.com/puppetserver/latest/">https://docs.puppet.com/puppetserver/latest/</a></li>
<li><a href="https://docs.puppet.com/puppetserver/latest/puppetserver_vs_passenger.html">puppetserver_vs_passenger</a></li>
<li>master与ca分离 <a href="https://docs.puppet.com/puppetserver/latest/external_ca_configuration.html">https://docs.puppet.com/puppetserver/latest/external_ca_configuration.html</a></li>
<li><a href="https://docs.puppet.com/puppetserver/latest/ssl_server_certificate_change_and_virtual_ips.html">https://docs.puppet.com/puppetserver/latest/ssl_server_certificate_change_and_virtual_ips.html</a></li>
</ul>


<p>晚上很多资料都是旧的，一般都是 puppetmaster + apache/nginx + passenger 。新版本使用puppetserver后，服务运行在JVM之上（ Puppet Server is hosted by a Jetty web server ），性能比原来ruby的方式更好（<a href="https://docs.puppet.com/puppetserver/latest/puppetserver_vs_passenger.html">反正官网是这么说的</a>）。所以没必要折腾其他ruby的东西了。</p>

<p><strong>题外话</strong>：搭上JVM（java）的车，对于大家都好^_^，现在大数据HADOOP都是基于java的，spark的scala也是运行在JVM之上。</p>

<blockquote><p>Because Puppet Server runs on the JVM, it takes a bit longer than the Apache/Passenger stack to start and get ready to accept HTTP connections.</p>

<p>Overall, Puppet Server performance is significantly better than a Puppet master running on the Apache/Passenger stack, but the initial startup is definitely slower.</p></blockquote>

<h2>参考</h2>

<ul>
<li><a href="https://docs.puppet.com/puppet/4.4/reference/">https://docs.puppet.com/puppet/4.4/reference/</a></li>
<li><a href="https://docs.puppetlabs.com/puppet/latest/reference/install_pre.html">https://docs.puppetlabs.com/puppet/latest/reference/install_pre.html</a></li>
<li><a href="https://docs.puppet.com/puppetserver/latest/install_from_packages.html">https://docs.puppet.com/puppetserver/latest/install_from_packages.html</a></li>
<li><a href="https://docs.puppet.com/puppet/4.4/reference/whered_it_go.html">https://docs.puppet.com/puppet/4.4/reference/whered_it_go.html</a></li>
<li><a href="https://github.com/puppetlabs/puppet-specifications/blob/master/file_paths.md">https://github.com/puppetlabs/puppet-specifications/blob/master/file_paths.md</a></li>
<li><a href="https://docs.puppet.com/puppet/4.4/reference/install_linux.html#installing-release-packages-on-yum-based-systems">https://docs.puppet.com/puppet/4.4/reference/install_linux.html#installing-release-packages-on-yum-based-systems</a></li>
<li><a href="https://yum.puppetlabs.com/el/6/PC1/x86_64/">https://yum.puppetlabs.com/el/6/PC1/x86_64/</a></li>
<li><a href="https://docs.puppetlabs.com/puppet/latest/reference/type.html#file">https://docs.puppetlabs.com/puppet/latest/reference/type.html#file</a></li>
<li><a href="https://docs.puppetlabs.com/puppet/latest/reference/config_important_settings.html">https://docs.puppetlabs.com/puppet/latest/reference/config_important_settings.html</a></li>
<li><a href="https://docs.puppetlabs.com/puppetdb/4.0/install_from_packages.html">https://docs.puppetlabs.com/puppetdb/4.0/install_from_packages.html</a></li>
<li><a href="https://docs.puppetlabs.com/puppet/4.4/reference/config_file_auth.html">https://docs.puppetlabs.com/puppet/4.4/reference/config_file_auth.html</a></li>
<li><p><a href="https://docs.puppetlabs.com/puppet/4.4/reference/config_file_autosign.html">https://docs.puppetlabs.com/puppet/4.4/reference/config_file_autosign.html</a></p></li>
<li><p>yum配置与各种使用 <a href="http://www.cnblogs.com/mchina/archive/2013/01/04/2842275.html">http://www.cnblogs.com/mchina/archive/2013/01/04/2842275.html</a></p></li>
<li><a href="http://kisspuppet.com/2014/01/26/puppet_create_repo/">http://kisspuppet.com/2014/01/26/puppet_create_repo/</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/08/dbcp-parameters/">DBCP参数在Hive JDBC上的实践</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2016-04-08T19:48:01+08:00" pubdate data-updated="true">Fri 2016-04-08 19:48</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>查询程序一开始只是简单使用dbcp来做连接的限制。在实践的过程中遇到各种问题，本文记录DBCP的参数优化提高程序健壮性的两次过程。</p>

<p>最开始的DBCP的配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;bean id="hiveDataSource" class="org.apache.commons.dbcp.BasicDataSource"
</span><span class='line'>  destroy-method="close" 
</span><span class='line'>  p:driverClassName="${hiveDriverClassName}"
</span><span class='line'>  p:url="${hiveUrl}" 
</span><span class='line'>  p:username="${hiveUsername}" 
</span><span class='line'>  p:password="${hivePassword}"
</span><span class='line'>  p:maxIdle="${hiveMaxIdle}" 
</span><span class='line'>  p:maxWait="${hiveMaxWait}" 
</span><span class='line'>  p:maxActive="${hiveMaxActive}" /&gt;
</span><span class='line'>
</span><span class='line'>&lt;bean id="hiveTemplate" class="org.springframework.jdbc.core.JdbcTemplate"&gt;
</span><span class='line'>  &lt;property name="dataSource"&gt;
</span><span class='line'>      &lt;ref bean="hiveDataSource" /&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>&lt;/bean&gt;</span></code></pre></td></tr></table></div></figure>


<p>第一个遇到的问题，就是每次hiveserver2重启后，这个查询程序也得重启。在实际使用过程中非常的麻烦！！</p>

<h4>重启问题（连接断开后不能重连）</h4>

<p>首先给出学习的链接 <a href="http://elf8848.iteye.com/blog/1931778">http://elf8848.iteye.com/blog/1931778</a> 巨详细，同时问题的场景都一模一样啊！！</p>

<p>添加三个参数：</p>

<ul>
<li>testOnBorrow = &ldquo;true&rdquo;       借出连接时不要测试，否则很影响性能</li>
<li>testWhileIdle = &ldquo;true&rdquo;       指明连接是否被空闲连接回收器(如果有)进行检验.如果检测失败,则连接将被从池中去除.</li>
<li>validationQuery = &ldquo;show databases&rdquo; 验证连接是否可用，使用的SQL语句</li>
</ul>


<p>解释：</p>

<p>testWhileIdle = &ldquo;true&rdquo; 表示每 {timeBetweenEvictionRunsMillis} (默认-1，不执行)秒，取出 {numTestsPerEvictionRun} (默认值3)条连接，使用 {validationQuery} 进行测试 ，测试不成功就销毁连接。销毁连接后，连接数量就少了，如果小于minIdle数量，就新建连接。</p>

<p>testOnBorrow = &ldquo;true&rdquo; 它的默认值是true，如果测试失败会drop掉然后再borrow。false表示每次从连接池中取出连接时，不需要执行 {validationQuery} 中的SQL进行测试。若配置为true,对性能有非常大的影响，性能会下降7-10倍。所在一定要配置为false.</p>

<p>调整参数后hiveserver2重启，查询再连会先报错然后再连。在每次取连接的时刻使用 <code>show databases</code> 测试，如果失败则从pool中删掉这个连接，重新再取，实现了重连的效果。这里不用 <code>select 1</code> hive里面执行很慢， 同时testWhileIdle并没有生效，因为没有配置timeBetweenEvictionRunsMillis参数。</p>

<p>调整后的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;bean id="hiveDataSource" class="org.apache.commons.dbcp.BasicDataSource"
</span><span class='line'>destroy-method="close" 
</span><span class='line'>p:driverClassName="${hiveDriverClassName}"
</span><span class='line'>p:url="${hiveUrl}" 
</span><span class='line'>p:username="${hiveUsername}" 
</span><span class='line'>p:password="${hivePassword}"
</span><span class='line'>p:testOnBorrow="${hiveTestOnBorrow}"
</span><span class='line'>p:testWhileIdle="${hiveTestWhileIdle}" 
</span><span class='line'>p:validationQuery="${hiveValidationQuery}"
</span><span class='line'>p:maxIdle="${hiveMaxIdle}" 
</span><span class='line'>p:maxWait="${hiveMaxWait}" 
</span><span class='line'>p:maxActive="${hiveMaxActive}" 
</span><span class='line'>/&gt;</span></code></pre></td></tr></table></div></figure>


<p>问题又来了，由于测试切换tez和spark才配置了上面的重连。但是切换到spark后，启动的spark会一直保持(连接创建的session不会主动关闭)，直到hiveserver2 session超时(默认6h检查一次，7h idle就关闭)。</p>

<p>注意：有个隐忧，hive-on-spark每个连接都创建一个SESSION，这就退化到MR操作了。不能完全利用SPARK的优势！！例如业务中，即查询count、又获取一页数据，这里就是两个单独的spark程序！！N个session就N个 <strong>hive on spark</strong> 啊！！</p>

<h4>第二个问题，服务端session强制关闭</h4>

<p>问题其实和参考中的: <strong>MySQL8小时问题，Mysql服务器默认连接的“wait_timeout”是8小时，也就是说一个connection空闲超过8个小时，Mysql将自动断开该 connection</strong> 一模一样的。在增加 <strong>minEvictableIdleTimeMillis</strong> 和 <strong>timeBetweenEvictionRunsMillis</strong> 设置检查和回收的时间。</p>

<ul>
<li>timeBetweenEvictionRunsMillis = &ldquo;1800000&rdquo;  每30分钟运行一次空闲连接回收器，没必要那么频繁。</li>
<li>minEvictableIdleTimeMillis = &ldquo;3600000&rdquo;  池中的连接空闲1个小时后被回收，如果1个半小时没有操作，这个session就会被客户端关闭。可以通过yarn-8088的scheduler页面查看。</li>
</ul>


<p>设置后的最终效果：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;bean id="hiveDataSource" class="org.apache.commons.dbcp.BasicDataSource"
</span><span class='line'>destroy-method="close" 
</span><span class='line'>p:driverClassName="${hiveDriverClassName}"
</span><span class='line'>p:url="${hiveUrl}" 
</span><span class='line'>p:username="${hiveUsername}" 
</span><span class='line'>p:password="${hivePassword}"
</span><span class='line'>p:testOnBorrow="${hiveTestOnBorrow}"
</span><span class='line'>p:validationQuery="${hiveValidationQuery}"
</span><span class='line'>p:maxWait="${hiveMaxWait}" 
</span><span class='line'>p:maxIdle="${hiveMaxIdle}" 
</span><span class='line'>p:maxActive="${hiveMaxActive}" 
</span><span class='line'>p:testWhileIdle="${hiveTestWhileIdle}" 
</span><span class='line'>p:timeBetweenEvictionRunsMillis="${hiveTimeBetweenEvictionRunsMillis}" 
</span><span class='line'>p:minEvictableIdleTimeMillis="${hiveMinEvictableIdleTimeMillis}" 
</span><span class='line'>p:removeAbandoned="true"
</span><span class='line'>p:logAbandoned="true"
</span><span class='line'>/&gt;</span></code></pre></td></tr></table></div></figure>


<p>很多程序都有很多参数，大部分能通过文档明白，但是一些参数不到实践真的很难真正体会它的含义。参考的文章两次改进我查看了，但是第一次看的时刻根本没去加其他参数，因为对我来说没用，解决当前问题用不到嘛。</p>

<p>hadoop的参数更多，core/hdfs/mapred/yarn需要多用才能发现参数的功能和妙用。<strong>纸上得来终觉浅，绝知此事要躬行</strong> 。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/04/04/rpm-build-your-package/">RPM打包</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-04-04T16:07:21+08:00" pubdate data-updated="true">Mon 2016-04-04 16:07</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>资料</h2>

<ul>
<li><a href="http://www.rpm.org/max-rpm-snapshot/rpmbuild.8.html">http://www.rpm.org/max-rpm-snapshot/rpmbuild.8.html</a></li>
<li><a href="https://fedoraproject.org/wiki/How_to_create_an_RPM_package/zh-cn">https://fedoraproject.org/wiki/How_to_create_an_RPM_package/zh-cn</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/management/package/rpm/part1/index.html">用 RPM 打包软件-打包教程</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/management/package/rpm/part3/index.html">用 RPM 打包软件-高级部分：安装前后控制</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/l-rpm/index.html">RPM 打包技术与典型 SPEC 文件分析-各变量含义</a></li>
<li><a href="http://hlee.iteye.com/blog/343499">http://hlee.iteye.com/blog/343499</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/management/package/rpm/part1/indent-2.spec">案例</a></li>
<li><p><a href="https://github.com/apache/zookeeper/tree/release-3.4.8/src/packages">zookeeper打包案例</a></p></li>
<li><p><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-checkinstall/index.html">http://www.ibm.com/developerworks/cn/linux/l-cn-checkinstall/index.html</a></p></li>
</ul>


<h2>实践</h2>

<ul>
<li>系统配置准备</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 新建一个docker实例，来测试、学习
</span><span class='line'>[root@cu1 ~]# docker run -ti centos:centos6 /bin/bash
</span><span class='line'>
</span><span class='line'>[root@bdc25400cc63 mywget]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.6 (Final)
</span><span class='line'>
</span><span class='line'># 安装编译环境所需的软件
</span><span class='line'>yum install which tree lrzsz tar gcc rpm-build
</span><span class='line'># wget编译的依赖
</span><span class='line'>yum install -y gnutls gnutls-devel</span></code></pre></td></tr></table></div></figure>


<ul>
<li>步骤</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@bdc25400cc63 home]# mkdir mywget 
</span><span class='line'>[root@bdc25400cc63 home]# cd mywget/
</span><span class='line'>[root@bdc25400cc63 mywget]# mkdir BUILD RPMS SOURCES SPECS SRPMS
</span><span class='line'>[root@bdc25400cc63 mywget]# cd SOURCES/
</span><span class='line'>[root@bdc25400cc63 SOURCES]# mv /home/wget-1.17.tar.gz .
</span><span class='line'>[root@bdc25400cc63 SOURCES]# ls
</span><span class='line'>wget-1.17.tar.gz
</span><span class='line'>[root@bdc25400cc63 SOURCES]# cd ..
</span><span class='line'>
</span><span class='line'>[root@bdc25400cc63 mywget]# rpmbuild --showrc
</span><span class='line'>[test@bdc25400cc63 mywget]$ rpm --eval "%{_topdir}"
</span><span class='line'>
</span><span class='line'>[test@bdc25400cc63 mywget]$ grep -i _topdir /usr/lib/rpm/rpmrc /usr/lib/rpm/redhat/rpmrc /usr/lib/rpm/macros /usr/lib/rpm/redhat/macros  | less
</span><span class='line'>/usr/lib/rpm/macros:%_builddir          %{_topdir}/BUILD
</span><span class='line'>/usr/lib/rpm/macros:%_rpmdir            %{_topdir}/RPMS
</span><span class='line'>/usr/lib/rpm/macros:%_sourcedir         %{_topdir}/SOURCES
</span><span class='line'>/usr/lib/rpm/macros:%_specdir           %{_topdir}/SPECS
</span><span class='line'>/usr/lib/rpm/macros:%_srcrpmdir         %{_topdir}/SRPMS
</span><span class='line'>/usr/lib/rpm/macros:%_buildrootdir              %{_topdir}/BUILDROOT
</span><span class='line'>/usr/lib/rpm/macros:%_topdir            %{getenv:HOME}/rpmbuild
</span><span class='line'>
</span><span class='line'>[test@bdc25400cc63 mywget]$ cat ~/.rpmmacros 
</span><span class='line'>%_topdir /home/mywget/rpm
</span><span class='line'>
</span><span class='line'># 2016-5-12 15:28:35
</span><span class='line'># spec里面有define和global，应该是这个导致的！用global应该即可以了？
</span><span class='line'>
</span><span class='line'>[root@bdc25400cc63 mywget]# vi SPECS/wget.spec
</span><span class='line'>  # this is a sample spec file for wget
</span><span class='line'>  
</span><span class='line'>  %define _topdir /home/mywget
</span><span class='line'>  %define name    wget
</span><span class='line'>  %define release 2
</span><span class='line'>  %define version 1.17
</span><span class='line'>  # 定义 _buildrootdir 不起作用，不知道为啥??? 在 .rpmmacros 定义了 %_topdir，root转到 /home/mywget/rpm/BUILDROOT 了。
</span><span class='line'>  
</span><span class='line'>  %define _unpackaged_files_terminate_build 0
</span><span class='line'>  
</span><span class='line'>  Summary:   GNU wget
</span><span class='line'>  License:   GPL
</span><span class='line'>  Name:      %{name}
</span><span class='line'>  Version:   %{version}
</span><span class='line'>  Release:   %{release}
</span><span class='line'>  Source:    %{name}-%{version}.tar.gz
</span><span class='line'>  Prefix:    /usr/local/wget
</span><span class='line'>  Group:     Development/Tools
</span><span class='line'>  
</span><span class='line'>  %description
</span><span class='line'>  The GNU wget program downloads files from the Internet using the command-line.
</span><span class='line'>  
</span><span class='line'>  %prep
</span><span class='line'>  %setup -q
</span><span class='line'>  
</span><span class='line'>  %build
</span><span class='line'>  ./configure
</span><span class='line'>  make
</span><span class='line'>  
</span><span class='line'>  %install
</span><span class='line'>  make install prefix=$RPM_BUILD_ROOT/usr/local/wget # or use DESTDIR=$RPM_BUILD_ROOT
</span><span class='line'>  
</span><span class='line'>  %post
</span><span class='line'>  echo "hello world"
</span><span class='line'>  
</span><span class='line'>  %preun
</span><span class='line'>  echo "bye"
</span><span class='line'>  
</span><span class='line'>  %clean
</span><span class='line'>  rm -rf $RPM_BUILD_ROOT
</span><span class='line'>  
</span><span class='line'>  %files
</span><span class='line'>  %defattr(-, root, root)
</span><span class='line'>  /usr/local/wget/bin/wget
</span><span class='line'>  
</span><span class='line'>[root@bdc25400cc63 mywget]# rpmbuild -vv -bb --clean SPECS/wget.spec 
</span><span class='line'>
</span><span class='line'>[root@bdc25400cc63 mywget]# tree .
</span><span class='line'>.
</span><span class='line'>├── BUILD
</span><span class='line'>├── RPMS
</span><span class='line'>│   └── x86_64
</span><span class='line'>│       ├── wget-1.17-2.x86_64.rpm
</span><span class='line'>│       └── wget-debuginfo-1.17-2.x86_64.rpm
</span><span class='line'>├── SOURCES
</span><span class='line'>│   └── wget-1.17.tar.gz
</span><span class='line'>├── SPECS
</span><span class='line'>│   └── wget.spec
</span><span class='line'>└── SRPMS
</span><span class='line'>
</span><span class='line'>6 directories, 4 files
</span><span class='line'>
</span><span class='line'>[root@bdc25400cc63 mywget]# rpm -qpl RPMS/x86_64/wget-1.17-2.x86_64.rpm  
</span><span class='line'>/usr/local/wget/bin/wget
</span></code></pre></td></tr></table></div></figure>


<p>接下来就可以直接拿到这个包到其他机器上安装了，如果自己建立了本地库，使用createrepo更新下，就可以使用yum安装最新打的包了。</p>

<p>注： <code>%pre</code> , <code>%post</code> 和 <code>%preun</code> , <code>%postun</code> 可以在安装前后执行一些脚本。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ganglia-build]# mkdir BUILD RPMS SOURCES SPECS SRPMS
</span><span class='line'>[root@cu2 ganglia-build]# cd SOURCES/
</span><span class='line'>[root@cu2 SOURCES]# ll
</span><span class='line'>total 1272
</span><span class='line'>-rw-r--r-- 1 root root 1302320 Jan 20 09:35 ganglia-3.7.2.tar.gz
</span><span class='line'>[root@cu2 SOURCES]# cd ..
</span><span class='line'>
</span><span class='line'>[root@cu2 ganglia-build]# ll
</span><span class='line'>total 20
</span><span class='line'>drwxr-xr-x 2 root root 4096 Jun 15 10:25 BUILD
</span><span class='line'>drwxr-xr-x 2 root root 4096 Jun 15 10:25 RPMS
</span><span class='line'>drwxr-xr-x 2 root root 4096 Jun 15 10:25 SOURCES
</span><span class='line'>drwxr-xr-x 2 root root 4096 Jun 15 10:25 SPECS
</span><span class='line'>drwxr-xr-x 2 root root 4096 Jun 15 10:25 SRPMS
</span><span class='line'>
</span><span class='line'>[root@cu2 ganglia-build]# cd SPECS/
</span><span class='line'>[root@cu2 SPECS]# vi gmetad.spec
</span><span class='line'>
</span><span class='line'>[root@cu2 ganglia-build]# rpmbuild --clean -v -ba SPECS/gmetad.spec 
</span><span class='line'>
</span><span class='line'>[root@cu2 ganglia-build]# rpm -qpl RPMS/x86_64/ganglia-3.7.2-1.el6.x86_64.rpm </span></code></pre></td></tr></table></div></figure>


<h2>重新打包已有rpm</h2>

<p>下载源码包，再修改内容，最后使用rpm-build重新打包。</p>

<p>这里以puppetserver为例，使用jdk7即可但官网打包的依赖是jdk8，这里修改依赖然后重新打包：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 rpmbuild]# rpm -ivh puppetserver-2.3.1-1.el6.src.rpm 
</span><span class='line'>warning: puppetserver-2.3.1-1.el6.src.rpm: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY
</span><span class='line'>   1:puppetserver           warning: user mockbuild does not exist - using root
</span><span class='line'>warning: group mockbuild does not exist - using root
</span><span class='line'>########################################### [100%]
</span><span class='line'>warning: user mockbuild does not exist - using root
</span><span class='line'>warning: group mockbuild does not exist - using root
</span><span class='line'>[root@cu2 rpmbuild]# ll
</span><span class='line'>total 32904
</span><span class='line'>-rw-r--r-- 1 root root 33681889 May 10 17:44 puppetserver-2.3.1-1.el6.src.rpm
</span><span class='line'>drwxr-xr-x 2 root root     4096 May 10 17:55 SOURCES
</span><span class='line'>drwxr-xr-x 2 root root     4096 May 10 17:55 SPECS
</span><span class='line'>
</span><span class='line'>#-- 注释掉jdk8的部分
</span><span class='line'>[root@cu2 rpmbuild]# grep -3 jdk SPECS/puppetserver.spec 
</span><span class='line'>
</span><span class='line'># java 1.8.0 is available starting in fedora 20 and el 6
</span><span class='line'>#%if 0%{?fedora} &gt;= 20 || 0%{?rhel} &gt;= 6
</span><span class='line'>#%global open_jdk          java-1.8.0-openjdk-headless
</span><span class='line'>#%else
</span><span class='line'>%global open_jdk          java-1.7.0-openjdk
</span><span class='line'>#%endif
</span><span class='line'>
</span><span class='line'>[root@cu2 rpmbuild]# yum install -y ruby
</span><span class='line'>[root@cu2 rpmbuild]# rpmbuild -v -bb --clean SPECS/puppetserver.spec 
</span><span class='line'>
</span><span class='line'>[root@cu2 rpmbuild]# yum deplist RPMS/noarch/puppetserver-2.3.1-1.el6.noarch.rpm 
</span><span class='line'>Loaded plugins: fastestmirror, priorities
</span><span class='line'>Finding dependencies: 
</span><span class='line'>Loading mirror speeds from cached hostfile
</span><span class='line'> * base: centos.ustc.edu.cn
</span><span class='line'> * centosplus: centos.ustc.edu.cn
</span><span class='line'> * epel: mirror01.idc.hinet.net
</span><span class='line'> * extras: centos.ustc.edu.cn
</span><span class='line'> * updates: centos.ustc.edu.cn
</span><span class='line'>193 packages excluded due to repository priority protections
</span><span class='line'>package: puppetserver.noarch 2.3.1-1.el6
</span><span class='line'>  dependency: chkconfig
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-5.el6
</span><span class='line'>   provider: chkconfig.x86_64 1.3.49.3-5.el6_7.2
</span><span class='line'>  dependency: /bin/bash
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6_7.1
</span><span class='line'>  dependency: java-1.7.0-openjdk
</span><span class='line'>   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.79-2.5.5.4.el6
</span><span class='line'>   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.101-2.6.6.1.el6_7
</span><span class='line'>   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.85-2.6.1.3.el6_6
</span><span class='line'>   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.85-2.6.1.3.el6_7
</span><span class='line'>   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.91-2.6.2.2.el6_7
</span><span class='line'>   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.95-2.6.4.0.el6_7
</span><span class='line'>   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.99-2.6.5.0.el6_7
</span><span class='line'>  dependency: puppet-agent &gt;= 1.4.0
</span><span class='line'>   provider: puppet-agent.x86_64 1.4.1-1.el6
</span><span class='line'>  dependency: net-tools
</span><span class='line'>   provider: net-tools.x86_64 1.60-110.el6_2
</span><span class='line'>  dependency: /usr/bin/env
</span><span class='line'>   provider: coreutils.x86_64 8.4-37.el6
</span><span class='line'>   provider: coreutils.x86_64 8.4-37.el6_7.3
</span><span class='line'>  dependency: /bin/sh
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6
</span><span class='line'>   provider: bash.x86_64 4.1.2-33.el6_7.1
</span><span class='line'>  dependency: config(puppetserver) = 2.3.1-1.el6
</span><span class='line'>   provider: puppetserver.noarch 2.3.1-1.el6 
</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/03/29/parquet-simple-view/">Parquet学习</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-03-29T19:13:53+08:00" pubdate data-updated="true">Tue 2016-03-29 19:13</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>经典文章</h2>

<ul>
<li><a href="http://parquet.apache.org/documentation/latest/">http://parquet.apache.org/documentation/latest/</a></li>
<li><a href="https://blog.twitter.com/2013/dremel-made-simple-with-parquet">https://blog.twitter.com/2013/dremel-made-simple-with-parquet</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL</a></li>
</ul>


<h2>概念</h2>

<ul>
<li>Row Group

<ul>
<li>Column Chunk

<ul>
<li>Page

<ul>
<li>Definition Levels: To support nested records we need to store the level for which the field is null. This is what the definition level is for: from 0 at the root of the schema up to the maximum level for this column. When a field is defined then all its parents are defined too, but <strong>when it is null we need to record the level at which it started being null to be able to reconstruct the record</strong>.</li>
<li>Repetition Levels: To support repeated fields we need to store when new lists are starting in a column of values. This is what repetition level is for: it is the level at which we have to create a new list for the current value. <strong>In other words, the repetition level can be seen as a marker of when to start a new list and at which level</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>FileMetaData</li>
</ul>


<p>The definition and repetition levels are optional, based on the schema definition. If the column is not nested (i.e. the path to the column has length 1), we do not encode the repetition levels (it would always have the value 1). For data that is required, the definition levels are skipped (if encoded, it will always have the value of the max definition level).</p>

<p>For example, in the case where the column is non-nested and required, the data in the page is only the encoded values.</p>

<p><strong>An optimized read setup would be: 1GB row groups, 1GB HDFS block size, 1 HDFS block per HDFS file.</strong></p>

<h2>texfile转parquet</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ cd apache-hive-1.2.1-bin/
</span><span class='line'>[hadoop@hadoop-master2 apache-hive-1.2.1-bin]$ bin/hive
</span><span class='line'>hive&gt; CREATE TABLE `t_ods_access_log2_parquet`(   `houseid` string,    `sourceip` string,    `destinationip` string,    `sourceport` string,    `destinationport` string,    `domain` string,    `url` string,    `accesstime` string,    `logid` string,    `sourceipnum` bigint,    `timedetected` string,    `protocol` string,    `duration` string) ROW FORMAT DELIMITED    FIELDS TERMINATED BY '|'  STORED AS PARQUET LOCATION   '/user/hive/t_ods_access_log2_parquet'</span></code></pre></td></tr></table></div></figure>


<p>关键 <strong>STORED AS PARQUET</strong>。</p>

<p>关于压缩，可以通过mapreduce参数设置（ <code>mapreduce.output.fileoutputformat.compress</code> 和 <code>mapreduce.output.fileoutputformat.compress.codec</code> ），但是推荐使用 <code>parquet.compression</code> 属性来指定。</p>

<p>reader/writer都会从 <code>CodecConfig.getCodec()</code> 获取压缩编码。代码中会从parquet属性和mapreduce获取压缩参数。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>alter table t_ods_access_log2_parquet SET TBLPROPERTIES ('parquet.compression' = 'SNAPPY' );
</span><span class='line'>
</span><span class='line'>create table t_ods_access_log2_parquet_none like t_ods_access_log2_parquet TBLPROPERTIES ('parquet.compression' = 'UNCOMPRESSED' );
</span><span class='line'>create table t_ods_access_log2_parquet_gzip like t_ods_access_log2_parquet TBLPROPERTIES ('parquet.compression' = 'GZIP' );</span></code></pre></td></tr></table></div></figure>


<p>直接使用hive的insert into语句就可以把原来的textfile的文件转成parquet格式。同时也转成gzip和uncompress比较了一下：</p>

<table>
<thead>
<tr>
<th style="text-align:left;">文件格式     </th>
<th style="text-align:left;"> 压缩       </th>
<th style="text-align:left;"> 大小</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">textfile     </td>
<td style="text-align:left;"> snappy     </td>
<td style="text-align:left;"> 4.1G</td>
</tr>
<tr>
<td style="text-align:left;">parquet      </td>
<td style="text-align:left;"> snappy     </td>
<td style="text-align:left;"> 3.6G</td>
</tr>
<tr>
<td style="text-align:left;">parquet      </td>
<td style="text-align:left;"> uncompress </td>
<td style="text-align:left;"> 7.2G</td>
</tr>
<tr>
<td style="text-align:left;">parquet      </td>
<td style="text-align:left;"> gzip       </td>
<td style="text-align:left;"> 2.2G</td>
</tr>
</tbody>
</table>


<p>直接count整个数据表，使用parquet的输入1M不到数据，太环保了！！（文件都是几十M的，一个文件都在一台机器上）。</p>

<table>
<thead>
<tr>
<th style="text-align:left;">文件格式     </th>
<th style="text-align:left;"> 运行引擎       </th>
<th style="text-align:left;"> 大小</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">textfile     </td>
<td style="text-align:left;"> tez            </td>
<td style="text-align:left;"> HDFS_BYTES_READ    4,454,071,542</td>
</tr>
<tr>
<td style="text-align:left;">parquet      </td>
<td style="text-align:left;"> tez            </td>
<td style="text-align:left;"> HDFS_BYTES_READ    415,870</td>
</tr>
<tr>
<td style="text-align:left;">textfile     </td>
<td style="text-align:left;"> sparksql       </td>
<td style="text-align:left;"> Input  4.1 GB</td>
</tr>
<tr>
<td style="text-align:left;">parquet      </td>
<td style="text-align:left;"> sparksql       </td>
<td style="text-align:left;"> Input  384.9 KB</td>
</tr>
</tbody>
</table>


<p>用sparksql跑textfile尽让更快。果然内存大暴力也很牛啊！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
<span class='line-number'>192</span>
<span class='line-number'>193</span>
<span class='line-number'>194</span>
<span class='line-number'>195</span>
<span class='line-number'>196</span>
<span class='line-number'>197</span>
<span class='line-number'>198</span>
<span class='line-number'>199</span>
<span class='line-number'>200</span>
<span class='line-number'>201</span>
<span class='line-number'>202</span>
<span class='line-number'>203</span>
<span class='line-number'>204</span>
<span class='line-number'>205</span>
<span class='line-number'>206</span>
<span class='line-number'>207</span>
<span class='line-number'>208</span>
<span class='line-number'>209</span>
<span class='line-number'>210</span>
<span class='line-number'>211</span>
<span class='line-number'>212</span>
<span class='line-number'>213</span>
<span class='line-number'>214</span>
<span class='line-number'>215</span>
<span class='line-number'>216</span>
<span class='line-number'>217</span>
<span class='line-number'>218</span>
<span class='line-number'>219</span>
<span class='line-number'>220</span>
<span class='line-number'>221</span>
<span class='line-number'>222</span>
<span class='line-number'>223</span>
<span class='line-number'>224</span>
<span class='line-number'>225</span>
<span class='line-number'>226</span>
<span class='line-number'>227</span>
<span class='line-number'>228</span>
<span class='line-number'>229</span>
<span class='line-number'>230</span>
<span class='line-number'>231</span>
<span class='line-number'>232</span>
<span class='line-number'>233</span>
<span class='line-number'>234</span>
<span class='line-number'>235</span>
<span class='line-number'>236</span>
<span class='line-number'>237</span>
<span class='line-number'>238</span>
<span class='line-number'>239</span>
<span class='line-number'>240</span>
<span class='line-number'>241</span>
<span class='line-number'>242</span>
<span class='line-number'>243</span>
<span class='line-number'>244</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hive&gt; insert into t_ods_access_log2_back select houseid,  sourceip,  destinationip,  sourceport,  destinationport,  domain,  url,  accesstime,  logid,  sourceipnum,  timedetected,  protocol,  duration from t_ods_access_log2 where hour=2016032804 ;
</span><span class='line'>Query ID = hadoop_20160329200414_96f1de35-48c5-4b38-977f-05de8554f388
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Status: Running (Executing on YARN cluster with App id application_1458893800770_3955)
</span><span class='line'>
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>Map 1 ..........   SUCCEEDED    152        152        0        0       1       0
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>VERTICES: 01/01  [==========================&gt;&gt;] 100%  ELAPSED TIME: 341.56 s   
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>Loading data to table default.t_ods_access_log2_back
</span><span class='line'>Table default.t_ods_access_log2_back stats: [numFiles=152, numRows=57688987, totalSize=4454071542, rawDataSize=11018516544]
</span><span class='line'>OK
</span><span class='line'>Time taken: 347.997 seconds
</span><span class='line'>hive&gt; insert into t_ods_access_log2_parquet select houseid,  sourceip,  destinationip,  sourceport,  destinationport,  domain,  url,  accesstime,  logid,  sourceipnum,  timedetected,  protocol,  duration from t_ods_access_log2 where hour=2016032804 ;
</span><span class='line'>Query ID = hadoop_20160329212157_57b66595-5dfc-4fc9-9ad1-398e2b8ade6b
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>Tez session was closed. Reopening...
</span><span class='line'>Session re-established.
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Status: Running (Executing on YARN cluster with App id application_1458893800770_3992)
</span><span class='line'>
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>Map 1 ..........   SUCCEEDED    152        152        0        0       0       0
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>VERTICES: 01/01  [==========================&gt;&gt;] 100%  ELAPSED TIME: 237.28 s   
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>Loading data to table default.t_ods_access_log2_parquet
</span><span class='line'>Table default.t_ods_access_log2_parquet stats: [numFiles=0, numRows=1305035789, totalSize=0, rawDataSize=16965465257]
</span><span class='line'>OK
</span><span class='line'>Time taken: 260.515 seconds
</span><span class='line'>hive&gt; select count(*) from t_ods_access_log2_back;
</span><span class='line'>Query ID = hadoop_20160329212644_da8e7997-5bcc-41ab-8b63-f1a5919c5a2f
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Status: Running (Executing on YARN cluster with App id application_1458893800770_3992)
</span><span class='line'>
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>Map 1 ..........   SUCCEEDED    107        107        0        0       0       0
</span><span class='line'>Reducer 2 ......   SUCCEEDED      1          1        0        0       0       0
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>VERTICES: 02/02  [==========================&gt;&gt;] 100%  ELAPSED TIME: 59.01 s    
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>OK
</span><span class='line'>57688987
</span><span class='line'>Time taken: 59.768 seconds, Fetched: 1 row(s)
</span><span class='line'>hive&gt; select count(*) from t_ods_access_log2_parquet;
</span><span class='line'>Query ID = hadoop_20160329212813_2fb8dafa-5c9a-40e8-a904-13e7cf865ec6
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Status: Running (Executing on YARN cluster with App id application_1458893800770_3992)
</span><span class='line'>
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>Map 1 ..........   SUCCEEDED    106        106        0        0       0       0
</span><span class='line'>Reducer 2 ......   SUCCEEDED      1          1        0        0       0       0
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>VERTICES: 02/02  [==========================&gt;&gt;] 100%  ELAPSED TIME: 45.82 s    
</span><span class='line'>--------------------------------------------------------------------------------
</span><span class='line'>OK
</span><span class='line'>57688987
</span><span class='line'>Time taken: 47.275 seconds, Fetched: 1 row(s)
</span><span class='line'>
</span><span class='line'>hive&gt; set hive.execution.engine=spark;
</span><span class='line'>hive&gt; set spark.master=yarn-client;
</span><span class='line'>hive&gt; select count(*) from t_ods_access_log2_back;
</span><span class='line'>Query ID = hadoop_20160329214550_a58d1056-9c91-4bbe-be7d-122ec3efdd8d
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>In order to change the average load for a reducer (in bytes):
</span><span class='line'>  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
</span><span class='line'>In order to limit the maximum number of reducers:
</span><span class='line'>  set hive.exec.reducers.max=&lt;number&gt;
</span><span class='line'>In order to set a constant number of reducers:
</span><span class='line'>  set mapreduce.job.reduces=&lt;number&gt;
</span><span class='line'>Starting Spark Job = 3a03d432-83a4-4d5a-a878-c9e52aa94bed
</span><span class='line'>
</span><span class='line'>Query Hive on Spark job[0] stages:
</span><span class='line'>0
</span><span class='line'>1
</span><span class='line'>
</span><span class='line'>Status: Running (Hive on Spark job[0])
</span><span class='line'>Job Progress Format
</span><span class='line'>CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
</span><span class='line'>2016-03-29 21:46:26,523 Stage-0_0: 0(+114)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:27,535 Stage-0_0: 0(+115)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:30,563 Stage-0_0: 0(+115)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:33,582 Stage-0_0: 0(+115)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:36,606 Stage-0_0: 0(+115)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:39,624 Stage-0_0: 0(+115)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:41,637 Stage-0_0: 0(+118)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:42,644 Stage-0_0: 4(+115)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:43,651 Stage-0_0: 110(+41)/152 Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:44,658 Stage-0_0: 124(+28)/152 Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:45,665 Stage-0_0: 128(+24)/152 Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:46,671 Stage-0_0: 138(+14)/152 Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:47,677 Stage-0_0: 142(+10)/152 Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:48,684 Stage-0_0: 144(+8)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:49,691 Stage-0_0: 147(+5)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:50,698 Stage-0_0: 148(+4)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:51,705 Stage-0_0: 149(+3)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:52,712 Stage-0_0: 150(+2)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:55,731 Stage-0_0: 151(+1)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:46:58,750 Stage-0_0: 151(+1)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:47:01,769 Stage-0_0: 151(+1)/152  Stage-1_0: 0/1
</span><span class='line'>2016-03-29 21:47:02,776 Stage-0_0: 152/152 Finished     Stage-1_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:47:05,793 Stage-0_0: 152/152 Finished     Stage-1_0: 1/1 Finished
</span><span class='line'>Status: Finished successfully in 70.33 seconds
</span><span class='line'>OK
</span><span class='line'>57688987
</span><span class='line'>Time taken: 75.211 seconds, Fetched: 1 row(s)
</span><span class='line'>hive&gt; select count(*) from t_ods_access_log2_back;
</span><span class='line'>Query ID = hadoop_20160329214723_9663eaf7-7014-46b1-b2ca-811ba64fc55c
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>In order to change the average load for a reducer (in bytes):
</span><span class='line'>  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
</span><span class='line'>In order to limit the maximum number of reducers:
</span><span class='line'>  set hive.exec.reducers.max=&lt;number&gt;
</span><span class='line'>In order to set a constant number of reducers:
</span><span class='line'>  set mapreduce.job.reduces=&lt;number&gt;
</span><span class='line'>Starting Spark Job = f2dbcd55-b23c-4eb3-9439-8f1c825fbac3
</span><span class='line'>
</span><span class='line'>Query Hive on Spark job[1] stages:
</span><span class='line'>2
</span><span class='line'>3
</span><span class='line'>
</span><span class='line'>Status: Running (Hive on Spark job[1])
</span><span class='line'>Job Progress Format
</span><span class='line'>CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
</span><span class='line'>2016-03-29 21:47:24,449 Stage-2_0: 0(+122)/152  Stage-3_0: 0/1
</span><span class='line'>2016-03-29 21:47:25,455 Stage-2_0: 96(+56)/152  Stage-3_0: 0/1
</span><span class='line'>2016-03-29 21:47:26,462 Stage-2_0: 123(+29)/152 Stage-3_0: 0/1
</span><span class='line'>2016-03-29 21:47:27,469 Stage-2_0: 128(+24)/152 Stage-3_0: 0/1
</span><span class='line'>2016-03-29 21:47:28,476 Stage-2_0: 132(+20)/152 Stage-3_0: 0/1
</span><span class='line'>2016-03-29 21:47:29,483 Stage-2_0: 137(+15)/152 Stage-3_0: 0/1
</span><span class='line'>2016-03-29 21:47:30,489 Stage-2_0: 145(+7)/152  Stage-3_0: 0/1
</span><span class='line'>2016-03-29 21:47:31,495 Stage-2_0: 146(+6)/152  Stage-3_0: 0/1
</span><span class='line'>2016-03-29 21:47:32,500 Stage-2_0: 150(+2)/152  Stage-3_0: 0/1
</span><span class='line'>2016-03-29 21:47:33,506 Stage-2_0: 152/152 Finished     Stage-3_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:47:36,524 Stage-2_0: 152/152 Finished     Stage-3_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:47:39,540 Stage-2_0: 152/152 Finished     Stage-3_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:47:42,557 Stage-2_0: 152/152 Finished     Stage-3_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:47:45,573 Stage-2_0: 152/152 Finished     Stage-3_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:47:48,589 Stage-2_0: 152/152 Finished     Stage-3_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:47:49,594 Stage-2_0: 152/152 Finished     Stage-3_0: 1/1 Finished
</span><span class='line'>Status: Finished successfully in 26.15 seconds
</span><span class='line'>OK
</span><span class='line'>57688987
</span><span class='line'>Time taken: 26.392 seconds, Fetched: 1 row(s)
</span><span class='line'>hive&gt; select count(*) from t_ods_access_log2_parquet;
</span><span class='line'>Query ID = hadoop_20160329214758_25084e25-fdaf-4ef8-9c1a-2573515caca6
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>In order to change the average load for a reducer (in bytes):
</span><span class='line'>  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
</span><span class='line'>In order to limit the maximum number of reducers:
</span><span class='line'>  set hive.exec.reducers.max=&lt;number&gt;
</span><span class='line'>In order to set a constant number of reducers:
</span><span class='line'>  set mapreduce.job.reduces=&lt;number&gt;
</span><span class='line'>Starting Spark Job = 4360be5c-4188-49c4-a2a7-e5bb80164646
</span><span class='line'>
</span><span class='line'>Query Hive on Spark job[2] stages:
</span><span class='line'>5
</span><span class='line'>4
</span><span class='line'>
</span><span class='line'>Status: Running (Hive on Spark job[2])
</span><span class='line'>Job Progress Format
</span><span class='line'>CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
</span><span class='line'>2016-03-29 21:47:59,472 Stage-4_0: 0(+63)/65    Stage-5_0: 0/1
</span><span class='line'>2016-03-29 21:48:00,478 Stage-4_0: 1(+62)/65    Stage-5_0: 0/1
</span><span class='line'>2016-03-29 21:48:01,486 Stage-4_0: 49(+14)/65   Stage-5_0: 0/1
</span><span class='line'>2016-03-29 21:48:02,492 Stage-4_0: 51(+14)/65   Stage-5_0: 0/1
</span><span class='line'>2016-03-29 21:48:03,498 Stage-4_0: 57(+8)/65    Stage-5_0: 0/1
</span><span class='line'>2016-03-29 21:48:04,505 Stage-4_0: 62(+3)/65    Stage-5_0: 0/1
</span><span class='line'>2016-03-29 21:48:05,511 Stage-4_0: 63(+2)/65    Stage-5_0: 0/1
</span><span class='line'>2016-03-29 21:48:06,518 Stage-4_0: 65/65 Finished       Stage-5_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:48:09,537 Stage-4_0: 65/65 Finished       Stage-5_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:48:12,556 Stage-4_0: 65/65 Finished       Stage-5_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:48:15,574 Stage-4_0: 65/65 Finished       Stage-5_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:48:18,592 Stage-4_0: 65/65 Finished       Stage-5_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:48:21,608 Stage-4_0: 65/65 Finished       Stage-5_0: 1/1 Finished
</span><span class='line'>Status: Finished successfully in 23.14 seconds
</span><span class='line'>OK
</span><span class='line'>57688987
</span><span class='line'>Time taken: 23.376 seconds, Fetched: 1 row(s)
</span><span class='line'>hive&gt; select count(*) from t_ods_access_log2_parquet;
</span><span class='line'>Query ID = hadoop_20160329214826_173311b1-0083-4e11-9a29-fe13f48bb649
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>In order to change the average load for a reducer (in bytes):
</span><span class='line'>  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
</span><span class='line'>In order to limit the maximum number of reducers:
</span><span class='line'>  set hive.exec.reducers.max=&lt;number&gt;
</span><span class='line'>In order to set a constant number of reducers:
</span><span class='line'>  set mapreduce.job.reduces=&lt;number&gt;
</span><span class='line'>Starting Spark Job = c452b02b-c68f-4c68-bc28-cb9748d7dcb2
</span><span class='line'>
</span><span class='line'>Query Hive on Spark job[3] stages:
</span><span class='line'>6
</span><span class='line'>7
</span><span class='line'>
</span><span class='line'>Status: Running (Hive on Spark job[3])
</span><span class='line'>Job Progress Format
</span><span class='line'>CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
</span><span class='line'>2016-03-29 21:48:27,332 Stage-6_0: 3(+60)/65    Stage-7_0: 0/1
</span><span class='line'>2016-03-29 21:48:28,338 Stage-6_0: 53(+10)/65   Stage-7_0: 0/1
</span><span class='line'>2016-03-29 21:48:29,343 Stage-6_0: 60(+3)/65    Stage-7_0: 0/1
</span><span class='line'>2016-03-29 21:48:30,349 Stage-6_0: 61(+4)/65    Stage-7_0: 0/1
</span><span class='line'>2016-03-29 21:48:31,354 Stage-6_0: 63(+2)/65    Stage-7_0: 0/1
</span><span class='line'>2016-03-29 21:48:32,360 Stage-6_0: 65/65 Finished       Stage-7_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:48:35,377 Stage-6_0: 65/65 Finished       Stage-7_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:48:38,393 Stage-6_0: 65/65 Finished       Stage-7_0: 0(+1)/1
</span><span class='line'>2016-03-29 21:48:40,404 Stage-6_0: 65/65 Finished       Stage-7_0: 1/1 Finished
</span><span class='line'>Status: Finished successfully in 14.08 seconds
</span><span class='line'>OK
</span><span class='line'>57688987
</span><span class='line'>Time taken: 14.306 seconds, Fetched: 1 row(s)
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 spark-1.6.0-bin-2.6.3]$ bin/spark-sql --master yarn-client --hiveconf hive.execution.engine=mr 
</span><span class='line'>         &gt; select count(*) from t_ods_access_log2_parquet;
</span><span class='line'>57688987
</span><span class='line'>16/03/29 22:19:51 INFO CliDriver: Time taken: 21.82 seconds, Fetched 1 row(s)
</span><span class='line'>
</span><span class='line'>         &gt; select count(*) from t_ods_access_log2_back;
</span><span class='line'>57688987
</span><span class='line'>16/03/29 22:20:44 INFO CliDriver: Time taken: 6.634 seconds, Fetched 1 row(s)
</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/9">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/7">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2016/04/23/hadoop-guide-catalog/">[整理] Hadoop入门</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/01/19/nginx-https/">Nginx配置https</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/12/14/elasticsearch5-head-plugin-config/">elasticsearch5安装Head插件</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/12/09/spark2-0-kafka0-10-1-partitions-work-incorrent/">Spark2-0 & Kafka0-10-1订阅多个单只读一个分区</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/12/01/jasperreports-brief-summary/">Jasperreports使用小结</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/11/10/play2-development-environment-with-eclipse/">Play2开发环境搭建</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/20/ssh-upgrade-on-centos6/">红帽6升级SSH</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/12/sparksql-view-and-debug-generatecode/">SparkSQL查看调试生成代码</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/12/maven-create-ourself-archetype/">Maven创建自己的Archetype</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/alluxio/'>alluxio</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/bigdata/'>bigdata</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/elasticsearch/'>elasticsearch</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/flume/'>flume</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (5) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (42) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/nginx/'>nginx</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/puppet/'>puppet</a> (10) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (11) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (41) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/zookeeper/'>zookeeper</a> (1) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (155)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'winseliu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
