
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="本来第二篇应该是与 [第1章 初识Spark] 有关，但我们运行helloworld、以及提交任务都是通过脚本 bin/spark-shell ，完全不知道那些脚本是干啥的？而且，在开发环境运行shell来启动应用总觉得怪怪的，这篇先来简单了解脚本的功能、以及Launcher模块。 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com/posts/17">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->


  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43198550-1', 'auto');
  ga('send', 'pageview');

</script>



</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="站内搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/archives/updated.html">Updated</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/05/08/rrc-apache-spark-source-inside-shell/">[读读书]Apache Spark源码剖析-Shell</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2016-05-08T21:41:01+08:00" pubdate data-updated="true">Sun 2016-05-08 21:41</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本来第二篇应该是与 [第1章 初识Spark] 有关，但我们运行helloworld、以及提交任务都是通过脚本 <code>bin/spark-shell</code> ，完全不知道那些脚本是干啥的？而且，在开发环境运行shell来启动应用总觉得怪怪的，这篇先来简单了解脚本的功能、以及Launcher模块。</p>

<p><strong> 其实每个大数据的框架，shell脚本都是通用入口，也是研读源码的第一个突破口 </strong>。掌握脚本功能相当于熟悉了基本的API功能，把 spark/bin 目录下面的脚本理清楚，然后再去写搭建开发环境、编写调试helloworld就事半功倍了。</p>

<p>官网 <strong> Quick Start </strong> 提供的简短例子都是通过 bin/spark-shell 来运行的。Submit页面提供了 bin/spark-submit 提交jar发布任务的方式。 spark-shell，spark-submit 就是两个非常重要的脚本，这里就来看下这两个脚本。</p>

<h2>spark-shell - 对应[3.1 spark-shell]章节</h2>

<p>spark-shell 脚本的内容相对多一些，主要代码如下（其他代码都是为了兼容cygwin弄的，我们这里不关注）：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"
</span><span class='line'>trap onExit INT     # 程序终止(interrupt)信号, 在用户键入INTR字符(通常是Ctrl + C)时触发
</span><span class='line'>
</span><span class='line'>export SPARK_SUBMIT_OPTS
</span><span class='line'>"${SPARK_HOME}"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"</span></code></pre></td></tr></table></div></figure>


<p>最终调用 bin/spark-submit 脚本。其实和我们自己提交 helloworld.jar 命令一样：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ bin/spark-submit \
</span><span class='line'>  --class "HelloWorld" \
</span><span class='line'>  --master local[2] \
</span><span class='line'>  target/scala-2.10/helloworld_2.10-1.0.jar</span></code></pre></td></tr></table></div></figure>


<p>不过通过 bin/spark-shell 提交运行的类是spark自带，没有附加（不需要）额外的jar。这个后面再讲，我们也可以通过这种方式类运行公共位置的jar，可以减少一些不必要的网络带宽。</p>

<h2>spark-submit</h2>

<p>submit脚本更简单。就是把 <strong>org.apache.spark.deploy.SparkSubmit</strong> 和 <strong>输入参数</strong> 全部传递给脚本 bin/spark-class 。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</span></code></pre></td></tr></table></div></figure>


<h2>spark-class</h2>

<p>主要的功能都集中在 bin/spark-class。bin/spark-class脚本最终启动java、调用 <strong>Launcher模块</strong> 。而 <strong>Launcher模块</strong> 解析输入参数并输出 <strong>最终输出Driver启动的命令</strong>，然后shell再通过 <strong>exec</strong> 来运行Driver程序。</p>

<p>要讲清楚 bin/spark-class 相对复杂点：通过脚本传递参数，调用java处理参数，又输出脚本，最后运行脚本才真正运行了Driver。所以这里通过 <strong>脚本</strong> 和 <strong>程序</strong> 来进行说明。</p>

<h4>脚本</h4>

<ul>
<li>先加载环境变量配置文件</li>
<li>再获取 assembly.jar 位置</li>
<li>然后调用 <code>org.apache.spark.launcher.Main</code> ， Main类根据环境变量和传入参数算出真正执行的命令(具体在【程序】部分讲)。</li>
</ul>


<p>下面是核心脚本的内容：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>. "${SPARK_HOME}"/bin/load-spark-env.sh 
</span><span class='line'>  # 把load-spark-env.sh展开
</span><span class='line'>  . "${user_conf_dir}/spark-env.sh"
</span><span class='line'>  
</span><span class='line'>  ASSEMBLY_DIR1="${SPARK_HOME}/assembly/target/scala-2.10"  # 通过ASSEMBLY路径来判断SPARK_SCALA_VERSION，编译打包成tar的不需要这个变量
</span><span class='line'>  export SPARK_SCALA_VERSION="2.10"
</span><span class='line'>
</span><span class='line'>RUNNER="${JAVA_HOME}/bin/java"
</span><span class='line'>
</span><span class='line'>SPARK_ASSEMBLY_JAR=
</span><span class='line'>if [ -f "${SPARK_HOME}/RELEASE" ]; then
</span><span class='line'>  ASSEMBLY_DIR="${SPARK_HOME}/lib"
</span><span class='line'>else
</span><span class='line'>  ASSEMBLY_DIR="${SPARK_HOME}/assembly/target/scala-$SPARK_SCALA_VERSION"
</span><span class='line'>fi
</span><span class='line'>ASSEMBLY_JARS="$(ls -1 "$ASSEMBLY_DIR" | grep "^spark-assembly.*hadoop.*\.jar$" || true)"
</span><span class='line'>SPARK_ASSEMBLY_JAR="${ASSEMBLY_DIR}/${ASSEMBLY_JARS}"
</span><span class='line'>LAUNCH_CLASSPATH="$SPARK_ASSEMBLY_JAR"
</span><span class='line'>
</span><span class='line'>export _SPARK_ASSEMBLY="$SPARK_ASSEMBLY_JAR"
</span><span class='line'>
</span><span class='line'>CMD=()
</span><span class='line'>while IFS= read -d '' -r ARG; do
</span><span class='line'>  CMD+=("$ARG")
</span><span class='line'>done &lt; &lt;("$RUNNER" -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@")
</span><span class='line'>exec "${CMD[@]}"</span></code></pre></td></tr></table></div></figure>


<p>大部分内容都是准备环境变量，就最后几行代码比较复杂。这里设置DEBUG在脚本 <code>while</code> 循环打印每个输出的值看下输出的是什么。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 修改后的效果
</span><span class='line'>CMD=()
</span><span class='line'>while IFS= read -d '' -r ARG; do
</span><span class='line'>  echo "[DEBUG] $ARG"
</span><span class='line'>  CMD+=("$ARG")
</span><span class='line'>done &lt; &lt;(set -x; "$RUNNER" -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@")
</span><span class='line'>echo "${CMD[@]}"
</span><span class='line'>exec "${CMD[@]}"</span></code></pre></td></tr></table></div></figure>


<p>启动 bin/spark-shell（最终会调用 bin/spark-class，上面已经讲过脚本之间的关系），查看输出的调试信息：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ bin/spark-shell 
</span><span class='line'>++ /opt/jdk1.8.0/bin/java -cp /home/hadoop/spark-1.6.0-bin-2.6.3/lib/spark-assembly-1.6.0-hadoop2.6.3-ext-2.1.jar org.apache.spark.launcher.Main org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name 'Spark shell'
</span><span class='line'>[DEBUG] /opt/jdk1.8.0/bin/java
</span><span class='line'>[DEBUG] -cp
</span><span class='line'>[DEBUG] /home/hadoop/spark/lib/mysql-connector-java-5.1.34.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/conf/:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/spark-assembly-1.6.0-hadoop2.6.3-ext-2.1.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-rdbms-3.2.9.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-core-3.2.10.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-api-jdo-3.2.6.jar:/home/hadoop/hadoop/etc/hadoop/
</span><span class='line'>[DEBUG] -Dscala.usejavacp=true
</span><span class='line'>[DEBUG] -Xms512m
</span><span class='line'>[DEBUG] -Xmx512m
</span><span class='line'>[DEBUG] org.apache.spark.deploy.SparkSubmit
</span><span class='line'>[DEBUG] --class
</span><span class='line'>[DEBUG] org.apache.spark.repl.Main
</span><span class='line'>[DEBUG] --name
</span><span class='line'>[DEBUG] Spark shell
</span><span class='line'>[DEBUG] spark-shell
</span><span class='line'>/opt/jdk1.8.0/bin/java -cp /home/hadoop/spark/lib/mysql-connector-java-5.1.34.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/conf/:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/spark-assembly-1.6.0-hadoop2.6.3-ext-2.1.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-rdbms-3.2.9.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-core-3.2.10.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-api-jdo-3.2.6.jar:/home/hadoop/hadoop/etc/hadoop/ -Dscala.usejavacp=true -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name 'Spark shell' spark-shell
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>从上面的调试信息可以看出：</p>

<ul>
<li><code>org.apache.spark.launcher.Main</code> 把传入参数整理后重新输出</li>
<li>脚本把java输出内容保存到 <code>CMD[@]</code> 数组中</li>
<li>最后使用exec来执行。</li>
</ul>


<p>根据上面 bin/spark-class 产生的启动命令可以直接在idea里面运行，效果与直接运行 bin/spark-shell 一样：</p>

<p><img src="/images/blogs/rrc-spark/idea-spark-shell.png" alt="" /></p>

<p><strong>注意：</strong> 这里的 spark-shell 是一个特殊的字符串，代码中会对其进行特殊处理不额外加载jar。类似的字符串还有： pyspark-shell, sparkr-shell, spark-internal（参看SparkSubmit），如果调用类就在SPARK_CLASSPATH可以使用它们减少不必要的网络传输。</p>

<h4>Launcher模块</h4>

<p>发现 shell 和 launcher的java代码 功能逻辑非常类似。比如说获取java程序路径的代码：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>List&lt;String&gt; buildJavaCommand(String extraClassPath) throws IOException {
</span><span class='line'>  ...
</span><span class='line'>  if (javaHome != null) {
</span><span class='line'>      cmd.add(join(File.separator, javaHome, "bin", "java"));
</span><span class='line'>  } else if ((envJavaHome = System.getenv("JAVA_HOME")) != null) {
</span><span class='line'>      cmd.add(join(File.separator, envJavaHome, "bin", "java"));
</span><span class='line'>  } else {
</span><span class='line'>      cmd.add(join(File.separator, System.getProperty("java.home"), "bin", "java"));
</span><span class='line'>  }
</span><span class='line'>  ...
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>在shell脚本里面的处理是：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Find the java binary
</span><span class='line'>if [ -n "${JAVA_HOME}" ]; then
</span><span class='line'>  RUNNER="${JAVA_HOME}/bin/java"
</span><span class='line'>else
</span><span class='line'>  if [ `command -v java` ]; then
</span><span class='line'>  RUNNER="java"
</span><span class='line'>  else
</span><span class='line'>  echo "JAVA_HOME is not set" &gt;&2
</span><span class='line'>  exit 1
</span><span class='line'>  fi
</span><span class='line'>fi</span></code></pre></td></tr></table></div></figure>


<p>对比两者，其实是用脚本更加直观。但是使用java编写一个模块更便于管理和扩展，稍微调整下就能复用代码。比如说要添加windows的cmd脚本、又或者为了兼容多个操作系统/多语言(python，r 等)。所以提取一个公共的 <strong>Launcher模块</strong> 出来其实是个挺不错的选择。同时对于不是很熟悉shell的程序员来说也更方便了解系统运作。</p>

<p><strong>Launcher模块</strong> 按功能可以分为 CommandBuilder 和 SparkLauncher 两个部分。</p>

<ol>
<li><p>CommandBuilder</p></li>
<li><p>SparkSubmitCommandBuilder: 解析用户输入的参数并输出命令给脚本使用</p></li>
<li>SparkClassCommandBuilder: 主要为后台进程产生启动命令（sbin目录下面的脚本）。</li>
</ol>


<p>1.1 公共类</p>

<ul>
<li>Main ： 统一入口</li>
<li>AbstractCommandBuilder : 提供构造命令的公共基类

<ul>
<li>buildJavaCommand

<ul>
<li>buildClassPath

<ul>
<li>SPARK_CLASSPATH</li>
<li>extraClassPath</li>
<li>getConfDir : 等于环境变量 $SPARK_CONF_DIR 或者 $SPARK_HOME/conf 的值</li>
<li>classes

<ul>
<li>SPARK_PREPEND_CLASSES</li>
<li>SPARK_TESTING</li>
</ul>
</li>
<li>findAssembly : 获取 spark-assembly-1.6.0-hadoop2.6.3.jar 的路径，lib 或者 assembly/target/scala-$SPARK_SCALA_VERSION 路径下

<ul>
<li>_SPARK_ASSEMBLY</li>
</ul>
</li>
<li>datanucleus-* : 从 lib / lib_managed/jars 目录下获取</li>
<li>HADOOP_CONF_DIR</li>
<li>YARN_CONF_DIR</li>
<li>SPARK_DIST_CLASSPATH</li>
</ul>
</li>
</ul>
</li>
<li>getEffectiveConfig : 获取 spark-defaults.conf 的内容</li>
</ul>
</li>
</ul>


<p>1.2 SparkSubmitCommandBuilder</p>

<p>主要的类以及参数：</p>

<ul>
<li>SparkSubmitCommandBuilder

<ul>
<li>构造函数调用OptionParser解析参数，解析handle有处理specialClasses！</li>
<li>buildSparkSubmitCommand

<ul>
<li>getEffectiveConfig</li>
<li>extraClassPath : spark.driver.extraClassPath</li>
<li>SPARK_SUBMIT_OPTS</li>
<li>SPARK_JAVA_OPTS</li>
<li>client模式下加载配置

<ul>
<li>spark.driver.memory / SPARK_DRIVER_MEMORY / SPARK_MEM / DEFAULT_MEM(1g)</li>
<li>DRIVER_EXTRA_JAVA_OPTIONS</li>
<li>DRIVER_EXTRA_LIBRARY_PATH</li>
</ul>
</li>
<li>buildSparkSubmitArgs</li>
</ul>
</li>
</ul>
</li>
<li>SparkSubmitOptionParser(子类需要实现handle方法)</li>
<li>SparkSubmitCommandBuilder$OptionParser 命令参数

<ul>
<li><code>bin/spark-submit -h</code> 查看可以<strong>设置的参数</strong></li>
<li>直接查看<a href="http://spark.apache.org/docs/latest/submitting-applications.html">官网文档</a></li>
</ul>
</li>
</ul>


<p>1.3 SparkClassCommandBuilder</p>

<p>主要CommandBuilder的功能上面已经都覆盖了，SparkClassCommandBuilder主要关注命令行可以设置哪些环境变量：</p>

<ul>
<li>org.apache.spark.deploy.master.Master

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_MASTER_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.deploy.worker.Worker

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_WORKER_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.deploy.history.HistoryServer

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_HISTORY_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.executor.CoarseGrainedExecutorBackend

<ul>
<li>SPARK_JAVA_OPTS</li>
<li>SPARK_EXECUTOR_OPTS</li>
<li>SPARK_EXECUTOR_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.executor.MesosExecutorBackend

<ul>
<li>SPARK_EXECUTOR_OPTS</li>
<li>SPARK_EXECUTOR_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.deploy.ExternalShuffleService / org.apache.spark.deploy.mesos.MesosExternalShuffleService

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_SHUFFLE_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.tools.

<ul>
<li>extraClassPath : spark-tools_.*.jar</li>
<li>SPARK_JAVA_OPTS</li>
<li>DEFAULT_MEM(1g)</li>
</ul>
</li>
<li>other

<ul>
<li>SPARK_JAVA_OPTS</li>
<li>SPARK_DRIVER_MEMORY</li>
</ul>
</li>
</ul>


<h4>SparkLauncher</h4>

<p>SparkLauncher提供了在程序中提交任务的方式。通过Driver端的支持获取程序执行动态（通过socket与Driver交互），为实现后端管理应用提供一种可行的方式。</p>

<p>SparkLauncher提交任务其中一部分还是使用spark-submit脚本，绕一圈又回到上面的参数解析生成命令然后exec执行。另外SparkLauncher通过启动 SocketServer(LauncherServer)接收来自Driver(LauncherBackend)任务执行情况的最新状态。</p>

<p><img src="/images/blogs/rrc-spark/spark-launcher.jpg" alt="" /></p>

<p>代码包括：</p>

<ul>
<li>SparkLauncher 主要是startApplication。其他都是解析设置参数，相当于把shell的工作用java重写了一遍</li>
<li>LauncherServer 服务SocketServer类</li>
<li>LauncherServer$ServerConnection 状态处理类</li>
<li>LauncherConnection 通信基类：接收、发送消息</li>
<li>LauncherProtocol 通信协议</li>
<li>ChildProcAppHandle : SparkAppHandle 接收到Driver的状态后，请求分发类</li>
</ul>


<p>具体功能的流转请下载代码 <a href="https://github.com/winse/spark-examples/blob/master/src/main/scala/com/github/winse/spark/HelloWorldLauncher.scala">HelloWorldLauncher.scala</a> ，然后本地调试一步步的追踪学习。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/05/07/rrc-apache-spark-source-inside-preface/">[读读书]Apache Spark源码剖析-序</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2016-05-07T23:58:57+08:00" pubdate data-updated="true">Sat 2016-05-07 23:58</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://dongxicheng.org/mapreduce-nextgen/how-to-read-hadoop-code-effectively/">如何高效的阅读hadoop源代码？</a> 先看看这篇。</p>

<p>今天去广州图书馆办了证，借了几本关于大数据的书。老实说，国家提供的便民基础设施应该发挥她的价值，国家建那么多公共设施，还有很多人在后台让这些服务运作起来。借书是一种最高性价比学习的方式，第一：不能乱写乱画必须做笔记或者背下来，把最有价值的东西汇集；第二：有时间限制，好书逼着我们持续的去读；第三：自然是读到烂书也不用花钱，有价值的书必然也是最多人看的，看到翻的很旧的新书你就借了吧。</p>

<p>其中一个《Apache Spark源码剖析-徐鹏》，大致翻了一下，老实说作者很牛逼啊，从那么多的代码里面挑出和主题相关的，不比鸡蛋里面挑石头容易，跟着作者的思路去读应该不错。打算每天读点代码，同时把看书和看代码也记录下来，每天一小结，同时希望对别人有些参考作用。</p>

<p>Spark更新的很快，书本介绍的是 spark-1.0 ，不过书中介绍的主要是思路，我们这里选择比较新的版本 1.6.0 来读（生产用的是1.6）。</p>

<p><strong> 说到思路，如果你对Redis也感兴趣，强烈推荐读读 《Redis设计与实现-黄建宏》 </strong></p>

<h2>使用环境说明</h2>

<p>和作者不同，我选择直接在windows来读/调试代码，为了读个代码还得整一套linux的开发环境挺累的（原来也试过整linux开发环境后来放弃了），Windows 积累的经验已经可以让我自由调试和看代码了。</p>

<p>吐槽下sbt，很讨厌这玩意又慢还用ivy，我X，大数据不都用 maven 嘛，难道我还得为 spark 整一套完全一样的jar本地缓冲？不过还好 spark-1.6 已经是用 maven 来管理了。</p>

<ul>
<li>win10 + cygwin</li>
<li>jdk8_x64（内存可以调到大于1G）</li>
<li>maven3</li>
<li>scala_2.10</li>
<li>spark_1.6.0</li>
<li>hive_1.2.1</li>
<li>hadoop_2.6.3</li>
<li>JetBrains idea 看代码确实不错</li>
</ul>


<h2>Spark开发环境搭建 - 对应书本的[附录A Spark源码调试]部分</h2>

<h4>配置 idea-scala</h4>

<h6>优化idea启动参数</h6>

<p>安装 <strong>最新版idea</strong> (当前最新版本是15.0.5)。在程序安装的 bin 目录下，有x64配置文件 idea64.exe.vmoptions ，在配置文件开头添加jdk8内存配置：</p>

<pre><code>-server
-Xms1g
-Xmx2g
-XX:MetaspaceSize=256m
-XX:MaxMetaspaceSize=256m
</code></pre>

<p>由于机器 eclipse 原来使用的 jdk_x86，为了兼容，单独编写 idea64.exe 的启动脚本 <strong> idea.bat </strong>：</p>

<pre><code>set JAVA_HOME=D:\Java\jdk1.8.0_40
D:
cd "D:\Program Files\JetBrains\IntelliJ IDEA Community Edition 15.0.5\bin"
start idea64.exe"

exit
</code></pre>

<p><strong> [IDEA的快键配置]：IDEA 适配 Eclipse 的快键集，通过 <code>Settings -&gt; Keymap -&gt; Keymaps</code> 配置。 </strong></p>

<h6>安装scala插件</h6>

<ol>
<li>第一种方式：当然最好就是通过plugins的搜索框就能安装，但在中国这得看运气。</li>
<li><p>第二种方式：首先下载好插件，然后选择从硬盘安装插件。</p></li>
<li><p>从网络安装</p></li>
</ol>


<p>打开 plugins 管理页面：（也可以通过 File -> Settings&hellip; -> Plugins 打开）</p>

<p><img src="/images/blogs/rrc-spark/idea-start-configure.png" alt="" /></p>

<p>弹出的 Plugins 对话框显示了当前已经安装的插件：</p>

<p><img src="/images/blogs/rrc-spark/idea-plugins-list.png" alt="" /></p>

<p>在 Plugins 对话框页面选择 [<strong>Browse repositories&hellip;</strong>] 按钮，再在弹出的对话框中查找 <strong>Scala</strong> 的插件：</p>

<p><img src="/images/blogs/rrc-spark/idea-browse-plugins.png" alt="" /></p>

<p>选择安装 Scala ，当然你也可以同时安装上 SBT 。</p>

<ul>
<li>从硬盘安装</li>
</ul>


<p>运气好就算可以直接从网络安装，但是下载过程其实也挺慢的。</p>

<p>我们还可以先自己下载好插件再安装（或者从其他同学获取、迅雷分分钟下完）。首先需要查看自己 idea 的版本，再在 <a href="https://plugins.jetbrains.com/?idea_ce">https://plugins.jetbrains.com/?idea_ce</a> 查找下载符合自己版本的 <a href="https://plugins.jetbrains.com/plugin/1347?pr=idea_ce">scala 插件</a>，最后通过 [<strong>Install plugin from disk&hellip;</strong>] 安装，然后重启IDEA即可。</p>

<p><img src="/images/blogs/rrc-spark/idea-version.png" alt="" />
<img src="/images/blogs/rrc-spark/download-scala-plugin.png" alt="" />
<img src="/images/blogs/rrc-spark/idea-scala-from-disk.png" alt="" /></p>

<h4>下载 spark 源码，并导入idea</h4>

<ol>
<li>下载源码，检出 1.6.0 版本</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git clone https://github.com/apache/spark.git
</span><span class='line'>$ git checkout v1.6.0</span></code></pre></td></tr></table></div></figure>


<p>如果你只想看 1.6.0 的内容，可以直接在clone命令添加参数指定版本：</p>

<pre><code>$ git clone https://github.com/apache/spark.git -b v1.6.0
</code></pre>

<ol>
<li>导入idea</li>
</ol>


<p>导入之前先要生成arvo的java类(这里直接package编译一下)：</p>

<pre><code>E:\git\spark\external\flume-sink&gt;mvn package -DskipTests
</code></pre>

<p>由于我使用 hadoop-2.6.3 ，并且导入过程中不能修改环境变量，直接修改 pom.xml 里面 hadoop.version 属性的值。</p>

<p><img src="/images/blogs/rrc-spark/spark-hadoop-version.png" alt="" /></p>

<p>启动IDEA，使用 [<strong>Import Project</strong>] 导入源代码; 然后选择 <code>E:/git/spark</code>（刚刚下载的源码位置）; 然后选择导入maven项目; 在 profile 页把必要的都选上（当然也可以后期通过 <code>Maven Projects</code> 面板来修改）:</p>

<p><img src="/images/blogs/rrc-spark/spark-import-profile.png" alt="" /></p>

<p>导入完成后，依赖关系maven已经处理好了，直接就能用了。也可以 Make Projects 再编译一次，并把运行application的make去掉，免得浪费编译时间）。</p>

<p><strong> 注意：mvn idea:idea 其实不咋的，生成的配置不兼容。最好不要用！！ </strong></p>

<ol>
<li>调试/测试</li>
</ol>


<p>在调试运行之前，先了解下并解决 idea maven-provided 的问题：</p>

<p>在idea里面直接运行 src/main/java 下面的类会被当做在生产环境运行，所以idea不会把这些 provided的依赖 加入到运行的classpath。</p>

<ul>
<li><a href="https://youtrack.jetbrains.com/issue/IDEA-54595">https://youtrack.jetbrains.com/issue/IDEA-54595</a></li>
<li><a href="http://stackoverflow.com/questions/30453269/maven-provided-dependency-will-cause-noclassdeffounderror-in-intellij">http://stackoverflow.com/questions/30453269/maven-provided-dependency-will-cause-noclassdeffounderror-in-intellij</a></li>
</ul>


<p><img src="/images/blogs/rrc-spark/idea-maven-provided.png" alt="" /></p>

<p>IDEA运行时是从 <code>examples/spark-examples_2.10.iml</code> 文件中读取classpath的配置，所以我们直接把 <code>spark-examples_2.10.iml</code> 的 <code>scope="PROVIDED"</code> 全部删掉即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 一次全部删掉！
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ find . -name "*.iml"  | xargs -I{} sed -i 's/scope="PROVIDED"//' {}</span></code></pre></td></tr></table></div></figure>


<p>首先右键 [<strong>Run LogQuery</strong>] 运行（由于缺少master的配置会报错的），主要用于生成启动的 <code>LogQuery Configuration</code>：</p>

<p><img src="/images/blogs/rrc-spark/spark-logquery-firststart.png" alt="" /></p>

<p>然后选择上图中下拉选项的 [<strong>Edit Configurations&hellip;</strong>] ，在弹出配置对话框为中为 <code>LogQuery</code> 添加 <strong>VM options</strong> 配置: <code>-Dspark.master=local</code> ，接下来我们就可以打断点，Debug调试了。</p>

<p><img src="/images/blogs/rrc-spark/spark-logquery-config.png" alt="" /></p>

<p>运行结果如下：</p>

<p><img src="/images/blogs/rrc-spark/spark-logquery-result.png" alt="" /></p>

<p>遇到IDEA导入maven依赖有问题的，可以参考下 <a href="http://stackoverflow.com/questions/11454822/import-maven-dependencies-in-intellij-idea">Import Maven dependencies in IntelliJ IDEA</a> 。</p>

<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/05/06/flamegraphs-java-cpu/">Flamegraphs Java Cpu</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-05-06T21:35:03+08:00" pubdate data-updated="true">Fri 2016-05-06 21:35</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在MacTalk的公众号上读到了agentzh关于火焰图介绍(2016年5月6日07:57 动态追踪技术（中） - Dtrace、SystemTap、火焰图)，挺新奇的，并且应该对于查询热线程还是有作用的。</p>

<p>先了解perf和flamegraphs基础知识：</p>

<ul>
<li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-perf1/">https://www.ibm.com/developerworks/cn/linux/l-cn-perf1/</a></li>
<li><a href="http://www.brendangregg.com/perf.html#FlameGraphs">perf Examples</a></li>
<li><a href="http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">CPU Flame Graphs</a></li>
<li><a href="http://techblog.netflix.com/2015/07/java-in-flames.html">Java in Flames</a></li>
<li><a href="http://isuru-perera.blogspot.hk/2015/07/java-cpu-flame-graphs.html">Java CPU Flame Graphs</a></li>
<li>使用方法<a href="https://randomascii.wordpress.com/2013/03/26/summarizing-xperf-cpu-usage-with-flame-graphs/">xperf - windows perf</a></li>
<li>工具<a href="https://github.com/google/UIforETW/blob/master/bin/xperf_to_collapsedstacks.py">UIforETW</a></li>
</ul>


<p>perf好像有点类似java的btrace，不过perf是操作系统层面的。把操作系统当做服务，客户端通过perf来获取/查询系统的信息。</p>

<h2>监控系统</h2>

<p>perf包括在linux 2.6.31代码里面，没装的话redhat可以通过yum来安装/更新：</p>

<ul>
<li>虚拟机</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 ~]# yum install perf
</span><span class='line'>...
</span><span class='line'>Installed:
</span><span class='line'>  perf.x86_64 0:2.6.32-573.26.1.el6  
</span><span class='line'>  
</span><span class='line'>[root@hadoop-master2 ~]# perf stat ls /dev/shm
</span><span class='line'>
</span><span class='line'> Performance counter stats for 'ls /dev/shm':
</span><span class='line'>
</span><span class='line'>          0.697115 task-clock                #    0.613 CPUs utilized          
</span><span class='line'>                 0 context-switches          #    0.000 K/sec                  
</span><span class='line'>                 0 cpu-migrations            #    0.000 K/sec                  
</span><span class='line'>               236 page-faults               #    0.339 M/sec                  
</span><span class='line'>   &lt;not supported&gt; cycles                  
</span><span class='line'>   &lt;not supported&gt; stalled-cycles-frontend 
</span><span class='line'>   &lt;not supported&gt; stalled-cycles-backend  
</span><span class='line'>   &lt;not supported&gt; instructions            
</span><span class='line'>   &lt;not supported&gt; branches                
</span><span class='line'>   &lt;not supported&gt; branch-misses           
</span><span class='line'>
</span><span class='line'>       0.001137015 seconds time elapsed
</span></code></pre></td></tr></table></div></figure>


<p>虚拟机可能有一些event不能用，到真正的实体机上面应该是没问题的（网上有同学验证过）。可以通过 <code>perf list</code> 查看支持的event。</p>

<ul>
<li>实体机指标项：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@dacs ~]# perf stat ls /dev/shm
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'> Performance counter stats for 'ls /dev/shm':
</span><span class='line'>
</span><span class='line'>          1.793297      task-clock (msec)         #    0.677 CPUs utilized          
</span><span class='line'>                 1      context-switches          #    0.558 K/sec                  
</span><span class='line'>                 0      cpu-migrations            #    0.000 K/sec                  
</span><span class='line'>               255      page-faults               #    0.142 M/sec                  
</span><span class='line'>           2765454      cycles                    #    1.542 GHz                     [44.66%]
</span><span class='line'>           1544155      stalled-cycles-frontend   #   55.84% frontend cycles idle    [64.12%]
</span><span class='line'>           1013635      stalled-cycles-backend    #   36.65% backend  cycles idle   
</span><span class='line'>           2692743      instructions              #    0.97  insns per cycle        
</span><span class='line'>                                                  #    0.57  stalled cycles per insn
</span><span class='line'>            603340      branches                  #  336.442 M/sec                  
</span><span class='line'>             12499      branch-misses             #    2.07% of all branches         [98.00%]
</span><span class='line'>
</span><span class='line'>       0.002650313 seconds time elapsed
</span></code></pre></td></tr></table></div></figure>


<p>windows的话直接下载 UIforETW ，运行 UIforETW.exe 就可以用来采样了。把采样产生的etl文件传给xperf_to_collapsedstacks.py，最后用flamegraph.pl画图。</p>

<ul>
<li>perf的常用命令：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># http://www.brendangregg.com/perf.html
</span><span class='line'>perf list
</span><span class='line'>
</span><span class='line'>perf stat ./t1 
</span><span class='line'>perf stat -a -A ls
</span><span class='line'>
</span><span class='line'>perf top
</span><span class='line'> 
</span><span class='line'>perf record – e cpu-clock ./t1 
</span><span class='line'>perf report</span></code></pre></td></tr></table></div></figure>


<p>参考：</p>

<ul>
<li><a href="http://isuru-perera.blogspot.hk/2015/07/java-cpu-flame-graphs.html">http://isuru-perera.blogspot.hk/2015/07/java-cpu-flame-graphs.html</a></li>
<li><a href="https://github.com/coderplay/perfj/releases">https://github.com/coderplay/perfj/releases</a></li>
<li><a href="http://techblog.netflix.com/2015/07/java-in-flames.html">http://techblog.netflix.com/2015/07/java-in-flames.html</a></li>
</ul>


<h4>绘制系统火焰图</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html
</span><span class='line'># https://github.com/brendangregg/FlameGraph
</span><span class='line'># 真实的机器效果还是挺不错的
</span><span class='line'>perf record -F 99 -a -g -- sleep 60
</span><span class='line'>perf script | ~/FlameGraph/stackcollapse-perf.pl &gt;out.perf-folded
</span><span class='line'>~/FlameGraph/flamegraph.pl out.perf-folded &gt;perf.svg
</span><span class='line'>sz perf.svg
</span><span class='line'>
</span><span class='line'># --
</span><span class='line'># perf script | ./stackcollapse-perf.pl &gt; out.perf-folded
</span><span class='line'># grep -v cpu_idle out.perf-folded | ./flamegraph.pl &gt; nonidle.svg
</span><span class='line'># grep ext4 out.perf-folded | ./flamegraph.pl &gt; ext4internals.svg
</span><span class='line'># egrep 'system_call.*sys_(read|write)' out.perf-folded | ./flamegraph.pl &gt; rw.svg
</span></code></pre></td></tr></table></div></figure>


<p>安装的虚拟机中操作没采集到有用的。虚拟机和真实机器两个图</p>

<p>实体机：</p>

<p><img src="/images/blogs/flames/flames-real.png" alt="" /></p>

<p>虚拟机：</p>

<p><img src="/images/blogs/flames/flames-vm.png" alt="" /></p>

<h2>监控java</h2>

<p>首先需要jdk8_u60+，直接下载最新的jdk就好了。应用启动带上参数 -XX:+PreserveFramePointer ：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 ~]# java -version
</span><span class='line'>java version "1.8.0_92"
</span><span class='line'>Java(TM) SE Runtime Environment (build 1.8.0_92-b14)
</span><span class='line'>Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode)
</span><span class='line'>[root@hadoop-master2 ~]# cd /home/hadoop/spark-1.6.0-bin-2.6.3/
</span><span class='line'>[root@hadoop-master2 spark-1.6.0-bin-2.6.3]# export SPARK_SUBMIT_OPTS=-XX:+PreserveFramePointer     
</span><span class='line'>[root@hadoop-master2 spark-1.6.0-bin-2.6.3]# bin/spark-shell --master local   </span></code></pre></td></tr></table></div></figure>


<p>这里java进程使用root启动的，如果是普通用户如hadoop，为了采样需要把hadoop用户加入sudoer，在采样时使用 <code>sudo -u hadoop CMD</code>。</p>

<p><a href="http://techblog.netflix.com/2015/07/java-in-flames.html">http://techblog.netflix.com/2015/07/java-in-flames.html</a></p>

<h4>操作方法一：使用perf-map-agent（推荐指数：AAAAA）</h4>

<p><strong>老版本OLD 实际操作</strong> ：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/jrudolph/perf-map-agent.git
</span><span class='line'>cd perf-map-agent/
</span><span class='line'>export JAVA_HOME=/opt/jdk1.8.0_92
</span><span class='line'>cmake .
</span><span class='line'>make
</span><span class='line'>
</span><span class='line'>perf record -F 99 -g -p 7661 -- sleep 120
</span><span class='line'>bin/create-java-perf-map.sh 7661
</span><span class='line'>
</span><span class='line'>sudo perf script | ~/FlameGraph/stackcollapse-perf.pl &gt;out.perf-folded
</span><span class='line'>cat out.perf-folded | ~/FlameGraph/flamegraph.pl --color=java &gt;perf.svg
</span><span class='line'>sz perf.svg</span></code></pre></td></tr></table></div></figure>


<p><strong>新版本NEW 再实践</strong> ：</p>

<ul>
<li>NOTE: 2017-10-21 项目改名了，挂到更牛逼的一个组织下了：jvm-profiling-tools</li>
<li>NOTE: 2018-03-09 再实践</li>
<li>NOTE: 2019-6-19 再更</li>
<li>NOTE: 2020-03-15 再更</li>
</ul>


<p>参考：</p>

<ul>
<li><a href="https://colobu.com/2016/08/10/Java-Flame-Graphs/">https://colobu.com/2016/08/10/Java-Flame-Graphs/</a></li>
<li><a href="https://medium.com/netflix-techblog/java-in-flames-e763b3d32166">https://medium.com/netflix-techblog/java-in-flames-e763b3d32166</a></li>
<li><a href="http://neoremind.com/2017/09/%E4%BD%BF%E7%94%A8%E7%81%AB%E7%84%B0%E5%9B%BE%E5%81%9A%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/">http://neoremind.com/2017/09/%E4%BD%BF%E7%94%A8%E7%81%AB%E7%84%B0%E5%9B%BE%E5%81%9A%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</a></li>
<li><a href="https://github.com/jvm-profiling-tools/perf-map-agent">https://github.com/jvm-profiling-tools/perf-map-agent</a></li>
</ul>


<p>实际操作：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@dispatch-op-1 bigendian]# yum install git cmake make gcc gcc-c++ perf -y 
</span><span class='line'>[root@dispatch-op-1 bigendian]# git clone https://github.com/jvm-profiling-tools/perf-map-agent.git
</span><span class='line'>[root@dispatch-op-1 perf-map-agent]# cmake .
</span><span class='line'>[root@dispatch-op-1 perf-map-agent]# make 
</span><span class='line'>
</span><span class='line'>[root@dispatch-op-1 perf-map-agent]# mkdir -p /home/bigendian/jpt 
</span><span class='line'>[root@dispatch-op-1 perf-map-agent]# bin/create-links-in /home/bigendian/jpt
</span><span class='line'>
</span><span class='line'>[root@dispatch-op-1 bigendian]# git clone https://github.com/brendangregg/FlameGraph 
</span><span class='line'>
</span><span class='line'>[bigendian@dispatch-op-1 jpt]$ ll
</span><span class='line'>total 0
</span><span class='line'>lrwxrwxrwx 1 root root 51 Mar 15 13:23 perf-java-flames -&gt; /home/bigendian/perf-map-agent/bin/perf-java-flames
</span><span class='line'>lrwxrwxrwx 1 root root 57 Mar 15 13:23 perf-java-record-stack -&gt; /home/bigendian/perf-map-agent/bin/perf-java-record-stack
</span><span class='line'>lrwxrwxrwx 1 root root 57 Mar 15 13:23 perf-java-report-stack -&gt; /home/bigendian/perf-map-agent/bin/perf-java-report-stack
</span><span class='line'>lrwxrwxrwx 1 root root 48 Mar 15 13:23 perf-java-top -&gt; /home/bigendian/perf-map-agent/bin/perf-java-top
</span><span class='line'>
</span><span class='line'>[bigendian@dispatch-op-1 ~]$ export FLAMEGRAPH_DIR=~/FlameGraph/
</span><span class='line'>[bigendian@dispatch-op-1 ~]$ jpt/perf-java-flames 23652
</span><span class='line'>Recording events for 15 seconds (adapt by setting PERF_RECORD_SECONDS)
</span><span class='line'>
</span><span class='line'>JAVA_OPTS+=" -XX:+PreserveFramePointer  "
</span><span class='line'>[bigendian@dispatch-op-1 ~]$ PERF_RECORD_SECONDS=360 jpt/perf-java-flames 25564 
</span></code></pre></td></tr></table></div></figure>


<p>然后把生成svg拷贝到本地看。</p>

<p>注意，x 轴表示抽样数, 如果一个函数在 x 轴占据的宽度越宽, 就表示它被抽到的次数多, 即执行的时间长. 注意, x 轴不代表时间, 而是所有的调用栈合并后, 按字母顺序排列的. 火焰图就是看顶层的哪个函数占据的宽度最大。只要有”平顶”(plateaus)，就表示该函数可能存在性能问题，然后结合具体代码进行分析。</p>

<p>按下 Ctrl + F 会显示一个搜索框，用户可以输入关键词或正则表达式，所有符合条件的函数名会高亮显示.</p>

<p>调用栈不完整: 当调用栈过深时，某些系统只返回前面的一部分（比如前10层）。</p>

<h4>操作方式二：使用perfj采样（不再推荐）</h4>

<p>NOTE：2017-10-21 这个项目好久没更新了，用上面 <strong>perf-map-agent</strong> 吧。</p>

<p>参考：<a href="http://greenteajug.cn/2015/07/02/greenteajug%E6%B4%BB%E5%8A%A8-%E7%AC%AC16%E6%9C%9F-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%88%A9%E5%99%A8-perfj/">性能调优利器——PerfJ</a> ，直接下载release-1.0的版本，解压后给 bin/perfj 加上执行权限。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 测试的时刻可以把-F 99设置大一点
</span><span class='line'># java和perfj的用户得一致！！
</span><span class='line'># https://github.com/coderplay/perfj/wiki/CPU-Flame-Graph
</span><span class='line'>
</span><span class='line'>[root@dacs ~]# export JAVA_HOME=/usr/java/jdk1.8.0_92 
</span><span class='line'>[root@dacs ~]# wget http://blog.minzhou.info/perfj/leveldb-benchmark.jar
</span><span class='line'>[root@dacs ~]# $JAVA_HOME/bin/java -cp leveldb-benchmark.jar -XX:+PreserveFramePointer org.iq80.leveldb.benchmark.DbBenchmark --benchmarks=fillrandom --num=100000000
</span><span class='line'>
</span><span class='line'>[root@dacs ~]# export JAVA_HOME=/usr/java/jdk1.8.0_92 
</span><span class='line'>[root@dacs ~]# perfj-1.0/bin/perfj record -F 999 -g -p `pgrep -f DbBenchmark` 
</span><span class='line'>
</span><span class='line'>perf script | ~/FlameGraph/stackcollapse-perf.pl &gt;out.perf-folded
</span><span class='line'>~/FlameGraph/flamegraph.pl out.perf-folded  --color=java &gt;perf.svg
</span><span class='line'>sz perf.svg
</span></code></pre></td></tr></table></div></figure>


<p>还是挺有意思的。运行效果：</p>

<p><img src="/images/blogs/flames/flames-java-leveldb.png" alt="" /></p>

<p>虚拟机的少了好多信息！一模一样的命令，得出来的东西差好远！！</p>

<p><img src="/images/blogs/flames/flames-java-leveldb-vm.png" alt="" /></p>

<p>另一个 <strong>Context Switch</strong> 案例：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># https://github.com/coderplay/perfj/wiki/Context-Switch-Analysis
</span><span class='line'># 在vmware虚拟机里面运行啥都看不到！实体机也看不到作者的那些栈信息
</span><span class='line'>[root@dacs ~]# wget http://blog.minzhou.info/perfj/leveldb-benchmark.jar
</span><span class='line'>[root@dacs ~]# export JAVA_HOME=/usr/java/jdk1.8.0_92 
</span><span class='line'>[root@dacs ~]# $JAVA_HOME/bin/javac ContextSwitchTest.java 
</span><span class='line'>[root@dacs ~]# $JAVA_HOME/bin/java -XX:+PreserveFramePointer ContextSwitchTest
</span><span class='line'>
</span><span class='line'>[root@dacs ~]# export JAVA_HOME=/usr/java/jdk1.8.0_92 
</span><span class='line'>[root@dacs ~]# perfj-1.0/bin/perfj record  -e sched:sched_switch -F 999 -g -p `pgrep -f ContextSwitchTest` 
</span><span class='line'>[root@dacs ~]# perfj-1.0/bin/perfj report --stdio
</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/05/05/hdfs-heterogeneous-storage/">Hdfs异构存储实操</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-05-05T21:41:39+08:00" pubdate data-updated="true">Thu 2016-05-05 21:41</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>[注意] 查看官方文档一定要和自己使用的环境对应！操作 storagepolicies 不同版本对应的命令不同（2.6.3<->2.7.2）！</p>

<p>我这里测试环境使用的是 2.6.3 <a href="https://hadoop.apache.org/docs/r2.6.3/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Heterogeneous Storage: Archival Storage, SSD &amp; Memory</a></p>

<h2>配置</h2>

<p>直接把内存盘放到 /dev/shm 下，单独挂载一个 tmpfs 的效果也差不多。<a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">r2.7.2 Memory Storage Support in HDFS</a> 2.6.3没有这个文档 概念都适应的。</p>

<p>1 调节系统参数</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>vi /etc/security/limits.conf
</span><span class='line'>
</span><span class='line'>  hadoop           -       nofile          65535
</span><span class='line'>  hadoop           -       nproc           65535
</span><span class='line'>  hadoop           -       memlock         268435456
</span></code></pre></td></tr></table></div></figure>


<p>需要调节memlock的大小，否则启动datanode报错。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2016-05-05 19:22:22,674 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
</span><span class='line'>java.lang.RuntimeException: Cannot start datanode because the configured max locked memory size (dfs.datanode.max.locked.memory) of 134217728 bytes is more than the datanode's available RLIMIT_MEMLOCK ulimit of 65536 bytes.
</span><span class='line'>        at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1067)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.datanode.DataNode.&lt;init&gt;(DataNode.java:417)
</span></code></pre></td></tr></table></div></figure>


<p>2 添加RAM_DISK</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>vi hdfs-site.xml
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;/data/bigdata/hadoop/dfs/data,[RAM_DISK]/dev/shm/dfs/data&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.datanode.max.locked.memory&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;134217728&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  </span></code></pre></td></tr></table></div></figure>


<p>注意内存盘的写法，<code>[RAM_DISK]</code> 必须这些写，不然datanode不知道指定路径的storage的类型(默认是 DISK )。<a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html#Storage_Types_and_Storage_Policies">Storage_Types_and_Storage_Policies</a></p>

<blockquote><p>The default storage type of a datanode storage location will be DISK if it does not have a storage type tagged explicitly.</p></blockquote>

<p>3 同步配置并重启dfs</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# scp /etc/security/limits.conf cu3:/etc/security/
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ rsync -vaz etc cu3:~/hadoop-2.6.3/ 
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3] sbin/stop-dfs.sh
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3] sbin/start-dfs.sh</span></code></pre></td></tr></table></div></figure>


<p>可以去到datanode查看日志，可以看到 /dev/shm/dfs/data 路径 <strong>StorageType</strong> 为 <strong>RAM_DISK</strong> ：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2016-05-05 19:33:39,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /data/bigdata/hadoop/dfs/data/current
</span><span class='line'>2016-05-05 19:33:39,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /data/bigdata/hadoop/dfs/data/current, StorageType: DISK
</span><span class='line'>2016-05-05 19:33:39,863 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /dev/shm/dfs/data/current
</span><span class='line'>2016-05-05 19:33:39,863 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /dev/shm/dfs/data/current, StorageType: RAM_DISK</span></code></pre></td></tr></table></div></figure>


<p>同时查看 内存盘 的路径内容：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ ssh cu3 tree /dev/shm/dfs
</span><span class='line'>/dev/shm/dfs
</span><span class='line'>└── data
</span><span class='line'>    ├── current
</span><span class='line'>    │   ├── BP-1108852639-192.168.0.148-1452322889531
</span><span class='line'>    │   │   ├── current
</span><span class='line'>    │   │   │   ├── finalized
</span><span class='line'>    │   │   │   ├── rbw
</span><span class='line'>    │   │   │   └── VERSION
</span><span class='line'>    │   │   └── tmp
</span><span class='line'>    │   └── VERSION
</span><span class='line'>    └── in_use.lock
</span><span class='line'>
</span><span class='line'>7 directories, 3 files</span></code></pre></td></tr></table></div></figure>


<h2>测试使用</h2>

<p>通过三个例子对比，简单描述下使用。</p>

<p>首先，使用默认的方式(主要用于对比)， <br/>
第二，写文件时添加参数，  <br/>
第三，设置目录的存储类型（目录/文件会继承父目录的存储类型）</p>

<p>1 测试1</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -put README.txt /tmp/
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs fsck /tmp/README.txt -files -blocks -locations
</span><span class='line'>...
</span><span class='line'>/tmp/README.txt 1366 bytes, 1 block(s):  OK
</span><span class='line'>0. BP-1108852639-192.168.0.148-1452322889531:blk_1073752574_11776 len=1366 repl=1 [192.168.0.148:50010]
</span><span class='line'>
</span><span class='line'>[hadoop@cu3 hadoop-2.6.3]$ find /data/bigdata/hadoop/dfs/data/ /dev/shm/dfs/data/ -name "*1073752574*"
</span><span class='line'>/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir41/blk_1073752574_11776.meta
</span><span class='line'>/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir41/blk_1073752574</span></code></pre></td></tr></table></div></figure>


<p>2 写文件时添加 lazy_persist 标识</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 添加 -l 参数，后台代码会加上 LAZY_PERSIST 标识。
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -help put 
</span><span class='line'>-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt; :
</span><span class='line'>  Copy files from the local file system into fs. Copying fails if the file already
</span><span class='line'>  exists, unless the -f flag is given.
</span><span class='line'>  Flags:
</span><span class='line'>                                                                       
</span><span class='line'>  -p  Preserves access and modification times, ownership and the mode. 
</span><span class='line'>  -f  Overwrites the destination if it already exists.                 
</span><span class='line'>  -l  Allow DataNode to lazily persist the file to disk. Forces        
</span><span class='line'>         replication factor of 1. This flag will result in reduced
</span><span class='line'>         durability. Use with care.
</span></code></pre></td></tr></table></div></figure>


<p><img src="/images/blogs/storage-lazy.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># -l 参数会把 replication 强制设置成数字1 ！
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -put -l README.txt /tmp/readme.txt2
</span><span class='line'>
</span><span class='line'># 查看namenode的日志，可以看到文件写入到 RAM_DISK 类型的存储
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ less logs/hadoop-hadoop-namenode-cu2.log 
</span><span class='line'>
</span><span class='line'>  2016-05-05 20:38:36,465 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/readme.txt2._COPYING_. BP-1108852639-192.168.0.148-1452322889531 blk_1073752578_11780{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-dcb2673f-3297-4bd7-af1c-ac0ee3eebaf9:NORMAL:192.168.0.30:50010|RBW]]}
</span><span class='line'>  2016-05-05 20:38:36,592 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 192.168.0.30:50010 is added to blk_1073752578_11780{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[RAM_DISK]DS-bf1ab64f-7eb3-41e0-8466-43287de9893d:NORMAL:192.168.0.30:50010|FINALIZED]]} size 0
</span><span class='line'>  2016-05-05 20:38:36,594 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/readme.txt2._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1388277364_1
</span><span class='line'>
</span><span class='line'># 具体的内容所在位置
</span><span class='line'>[hadoop@cu4 ~]$ tree /dev/shm/dfs/data/
</span><span class='line'>/dev/shm/dfs/data/
</span><span class='line'>├── current
</span><span class='line'>│   ├── BP-1108852639-192.168.0.148-1452322889531
</span><span class='line'>│   │   ├── current
</span><span class='line'>│   │   │   ├── finalized
</span><span class='line'>│   │   │   │   └── subdir0
</span><span class='line'>│   │   │   │       └── subdir42
</span><span class='line'>│   │   │   │           ├── blk_1073752578
</span><span class='line'>│   │   │   │           └── blk_1073752578_11780.meta
</span><span class='line'>│   │   │   ├── rbw
</span><span class='line'>│   │   │   └── VERSION
</span><span class='line'>│   │   └── tmp
</span><span class='line'>│   └── VERSION
</span><span class='line'>└── in_use.lock
</span></code></pre></td></tr></table></div></figure>


<p>3 设置目录的存储类型</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -mkdir /ramdisk
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -setStoragePolicy /ramdisk LAZY_PERSIST 
</span><span class='line'>Set storage policy LAZY_PERSIST on /ramdisk
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -put README.txt /ramdisk
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk
</span><span class='line'>The storage policy of /ramdisk:
</span><span class='line'>BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}
</span><span class='line'>
</span><span class='line'># 不支持通配符
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk/*
</span><span class='line'>getStoragePolicy: File/Directory does not exist: /ramdisk/*
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk/README.txt
</span><span class='line'>The storage policy of /ramdisk/README.txt:
</span><span class='line'>BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># 添加replication参数，再测试多个备份只有一个写ram_disk
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -Ddfs.replication=3 -put README.txt /ramdisk/readme.txt2
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk/readme.txt2
</span><span class='line'>The storage policy of /ramdisk/readme.txt2:
</span><span class='line'>BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.6.3]$ hdfs fsck /ramdisk/readme.txt2 -files -blocks -locations
</span><span class='line'>
</span><span class='line'>  /ramdisk/readme.txt2 1366 bytes, 1 block(s):  OK
</span><span class='line'>  0. BP-1108852639-192.168.0.148-1452322889531:blk_1073752580_11782 len=1366 repl=3 [192.168.0.30:50010, 192.168.0.174:50010, 192.168.0.148:50010]
</span><span class='line'>
</span><span class='line'>[hadoop@cu3 ~]$ find /data/bigdata/hadoop/dfs/data /dev/shm/dfs/data -name "*1073752580*"
</span><span class='line'>/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580
</span><span class='line'>/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580_11782.meta
</span><span class='line'>
</span><span class='line'># 已经把ram_disk的内容持久化到磁盘了("Lazy_Persist")
</span><span class='line'>[hadoop@cu4 ~]$ find /data/bigdata/hadoop/dfs/data /dev/shm/dfs/data -name "*1073752580*"
</span><span class='line'>/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/lazypersist/subdir0/subdir42/blk_1073752580
</span><span class='line'>/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/lazypersist/subdir0/subdir42/blk_1073752580_11782.meta
</span><span class='line'>/dev/shm/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580
</span><span class='line'>/dev/shm/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580_11782.meta
</span><span class='line'>
</span><span class='line'>[hadoop@cu5 ~]$ find /data/bigdata/hadoop/dfs/data /dev/shm/dfs/data -name "*1073752580*"
</span><span class='line'>/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580_11782.meta
</span><span class='line'>/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580
</span></code></pre></td></tr></table></div></figure>


<p>[设想] 对于那些处理完就删除的临时文件，可以把保存的时间设置的久一点 <code>dfs.datanode.lazywriter.interval.sec</code>。这样就不需要写磁盘了。</p>

<p>不要妄想了，反正都会持久化！就是缓冲的效果，其他没有了！！一次性存储并且不需要持久化的还是用alluxio吧。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.LazyWriter#saveNextReplica
</span><span class='line'>  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService#submitLazyPersistTask</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li>挺详细的<a href="http://blog.csdn.net/androidlushangderen/article/details/51105876">HDFS异构存储</a></li>
</ul>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/05/05/puppetboard-install/">Puppetboard Install</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-05-05T10:54:26+08:00" pubdate data-updated="true">Thu 2016-05-05 10:54</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>对于我这样的python小白来说，有网络来安装 puppetboard 还是比较容易的（离线安装依赖处理可能比较麻烦）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># https://fedoraproject.org/wiki/EPEL/zh-cn
</span><span class='line'>[root@cu2 ~]# yum search epel
</span><span class='line'>[root@cu2 ~]# yum install epel-release
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# yum repolist
</span><span class='line'>Loaded plugins: fastestmirror, priorities
</span><span class='line'>Loading mirror speeds from cached hostfile
</span><span class='line'> * base: mirrors.skyshe.cn
</span><span class='line'> * centosplus: mirrors.pubyun.com
</span><span class='line'> * epel: mirror01.idc.hinet.net
</span><span class='line'> * extras: mirrors.skyshe.cn
</span><span class='line'> * updates: mirrors.skyshe.cn
</span><span class='line'>193 packages excluded due to repository priority protections
</span><span class='line'>repo id                                   repo name                                                                   status
</span><span class='line'>base                                      CentOS-6 - Base                                                                  6,575
</span><span class='line'>centosplus                                CentOS-6 - Centosplus                                                             0+76
</span><span class='line'>epel                                      Extra Packages for Enterprise Linux 6 - x86_64                              12,127+117
</span><span class='line'>extras                                    CentOS-6 - Extras                                                                   62
</span><span class='line'>puppet-local                              Puppet Local                                                                         5
</span><span class='line'>updates                                   CentOS-6 - Updates                                                               1,607
</span><span class='line'>repolist: 20,376
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# yum install python-pip -y
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# pip install puppetboard
</span><span class='line'>/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
</span><span class='line'>  InsecurePlatformWarning
</span><span class='line'>You are using pip version 7.1.0, however version 8.1.1 is available.
</span><span class='line'>You should consider upgrading via the 'pip install --upgrade pip' command.
</span><span class='line'>Collecting puppetboard
</span><span class='line'>/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
</span><span class='line'>  InsecurePlatformWarning
</span><span class='line'>  Downloading puppetboard-0.1.3.tar.gz (598kB)
</span><span class='line'>    100% |████████████████████████████████| 602kB 726kB/s 
</span><span class='line'>Collecting Flask&gt;=0.10.1 (from puppetboard)
</span><span class='line'>  Downloading Flask-0.10.1.tar.gz (544kB)
</span><span class='line'>    100% |████████████████████████████████| 544kB 734kB/s 
</span><span class='line'>Collecting Flask-WTF&lt;=0.9.5,&gt;=0.9.4 (from puppetboard)
</span><span class='line'>  Downloading Flask-WTF-0.9.5.tar.gz (245kB)
</span><span class='line'>    100% |████████████████████████████████| 249kB 320kB/s 
</span><span class='line'>Collecting WTForms&lt;2.0 (from puppetboard)
</span><span class='line'>  Downloading WTForms-1.0.5.zip (355kB)
</span><span class='line'>    100% |████████████████████████████████| 356kB 1.3MB/s 
</span><span class='line'>Collecting pypuppetdb&lt;0.3.0,&gt;=0.2.1 (from puppetboard)
</span><span class='line'>  Downloading pypuppetdb-0.2.1.tar.gz
</span><span class='line'>Collecting Werkzeug&gt;=0.7 (from Flask&gt;=0.10.1-&gt;puppetboard)
</span><span class='line'>  Downloading Werkzeug-0.11.9-py2.py3-none-any.whl (306kB)
</span><span class='line'>    100% |████████████████████████████████| 307kB 1.5MB/s 
</span><span class='line'>Collecting Jinja2&gt;=2.4 (from Flask&gt;=0.10.1-&gt;puppetboard)
</span><span class='line'>  Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB)
</span><span class='line'>    100% |████████████████████████████████| 266kB 2.3MB/s 
</span><span class='line'>Collecting itsdangerous&gt;=0.21 (from Flask&gt;=0.10.1-&gt;puppetboard)
</span><span class='line'>  Downloading itsdangerous-0.24.tar.gz (46kB)
</span><span class='line'>    100% |████████████████████████████████| 49kB 7.2MB/s 
</span><span class='line'>Collecting requests&gt;=1.2.3 (from pypuppetdb&lt;0.3.0,&gt;=0.2.1-&gt;puppetboard)
</span><span class='line'>  Downloading requests-2.10.0-py2.py3-none-any.whl (506kB)
</span><span class='line'>    100% |████████████████████████████████| 507kB 920kB/s 
</span><span class='line'>Collecting MarkupSafe (from Jinja2&gt;=2.4-&gt;Flask&gt;=0.10.1-&gt;puppetboard)
</span><span class='line'>  Downloading MarkupSafe-0.23.tar.gz
</span><span class='line'>Installing collected packages: Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask, WTForms, Flask-WTF, requests, pypuppetdb, puppetboard
</span><span class='line'>  Running setup.py install for MarkupSafe
</span><span class='line'>  Running setup.py install for itsdangerous
</span><span class='line'>  Running setup.py install for Flask
</span><span class='line'>  Running setup.py install for WTForms
</span><span class='line'>  Running setup.py install for Flask-WTF
</span><span class='line'>  Running setup.py install for pypuppetdb
</span><span class='line'>  Running setup.py install for puppetboard
</span><span class='line'>Successfully installed Flask-0.10.1 Flask-WTF-0.9.5 Jinja2-2.8 MarkupSafe-0.23 WTForms-1.0.5 Werkzeug-0.11.9 itsdangerous-0.24 puppetboard-0.1.3 pypuppetdb-0.2.1 requests-2.10.0
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# pip show puppetboard
</span><span class='line'>You are using pip version 7.1.0, however version 8.1.1 is available.
</span><span class='line'>You should consider upgrading via the 'pip install --upgrade pip' command.
</span><span class='line'>---
</span><span class='line'>Metadata-Version: 1.0
</span><span class='line'>Name: puppetboard
</span><span class='line'>Version: 0.1.3
</span><span class='line'>Summary: Web frontend for PuppetDB
</span><span class='line'>Home-page: https://github.com/puppet-community/puppetboard
</span><span class='line'>Author: Daniele Sluijters
</span><span class='line'>Author-email: daniele.sluijters+pypi@gmail.com
</span><span class='line'>License: Apache License 2.0
</span><span class='line'>Location: /usr/lib/python2.6/site-packages
</span><span class='line'>Requires: Flask, Flask-WTF, WTForms, pypuppetdb
</span><span class='line'>[root@cu2 ~]# ll /usr/lib/python2.6/site-packages/puppetboard
</span><span class='line'>total 100
</span><span class='line'>-rw-r--r-- 1 root root 31629 May  5 09:12 app.py
</span><span class='line'>-rw-r--r-- 1 root root 30481 May  5 09:12 app.pyc
</span><span class='line'>-rw-r--r-- 1 root root  1206 May  5 09:12 default_settings.py
</span><span class='line'>-rw-r--r-- 1 root root  1477 May  5 09:12 default_settings.pyc
</span><span class='line'>-rw-r--r-- 1 root root  1025 May  5 09:12 forms.py
</span><span class='line'>-rw-r--r-- 1 root root  1982 May  5 09:12 forms.pyc
</span><span class='line'>-rw-r--r-- 1 root root     0 May  5 09:12 __init__.py
</span><span class='line'>-rw-r--r-- 1 root root   143 May  5 09:12 __init__.pyc
</span><span class='line'>drwxr-xr-x 9 root root  4096 May  5 09:12 static
</span><span class='line'>drwxr-xr-x 2 root root  4096 May  5 09:12 templates
</span><span class='line'>-rw-r--r-- 1 root root  2155 May  5 09:12 utils.py
</span><span class='line'>-rw-r--r-- 1 root root  3433 May  5 09:12 utils.pyc
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# pip install uwsgi
</span><span class='line'>You are using pip version 7.1.0, however version 8.1.1 is available.
</span><span class='line'>You should consider upgrading via the 'pip install --upgrade pip' command.
</span><span class='line'>Collecting uwsgi
</span><span class='line'>/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
</span><span class='line'>  InsecurePlatformWarning
</span><span class='line'>  Downloading uwsgi-2.0.12.tar.gz (784kB)
</span><span class='line'>    100% |████████████████████████████████| 786kB 143kB/s 
</span><span class='line'>Installing collected packages: uwsgi
</span><span class='line'>  Running setup.py install for uwsgi
</span><span class='line'>Successfully installed uwsgi-2.0.12
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# mkdir -p /var/www/puppetboard
</span><span class='line'>[root@cu2 ~]# cd /var/www/puppetboard/
</span><span class='line'>[root@cu2 puppetboard]# cp /usr/lib/python2.6/site-packages/puppetboard/default_settings.py ./settings.py
</span><span class='line'># 修改配置 
</span><span class='line'># https://github.com/voxpupuli/puppetboard#settings
</span><span class='line'>PUPPETDB_HOST = 'cu3'
</span><span class='line'>PUPPETDB_PORT = 8080
</span><span class='line'>REPORTS_COUNT = 21
</span><span class='line'>ENABLE_CATALOG = True
</span><span class='line'>
</span><span class='line'>[root@cu2 puppetboard]# vi wsgi.py 
</span><span class='line'>from __future__ import absolute_import
</span><span class='line'>import os
</span><span class='line'>
</span><span class='line'>os.environ['PUPPETDOARD_SETTINGS'] = '/var/www/puppetboard/settings.py'
</span><span class='line'>from puppetboard.app import app as application
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># A 直接用uwsgi-http
</span><span class='line'># http://yongqing.is-programmer.com/posts/43688.html
</span><span class='line'>[root@cu2 puppetboard]# uwsgi --http :9091 --wsgi-file /var/www/puppetboard/wsgi.py 
</span><span class='line'>
</span><span class='line'># 使用 supervisord 管理
</span><span class='line'>[root@cu2 supervisord.d]# cat uwsgi.ini 
</span><span class='line'>[program:puppetboard]
</span><span class='line'>command=uwsgi --http :9091 --wsgi-file /var/www/puppetboard/wsgi.py 
</span><span class='line'>[root@cu2 supervisord.d]# supervisorctl update
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># B nginx + uwsgi-socket
</span><span class='line'># 需要对应到 / ，新增一个9091的server
</span><span class='line'>[root@cu2 puppetboard]# vi /home/hadoop/nginx/conf/nginx.conf
</span><span class='line'>server {
</span><span class='line'>  listen 9091;
</span><span class='line'>
</span><span class='line'>  location /static {
</span><span class='line'>    alias /usr/lib/python2.6/site-packages/puppetboard/static;
</span><span class='line'>  }
</span><span class='line'>  location / {
</span><span class='line'>    include uwsgi_params;
</span><span class='line'>    uwsgi_pass 127.0.0.1:9090;
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@cu2 puppetboard]# uwsgi --socket :9090 --wsgi-file /var/www/puppetboard/wsgi.py 
</span><span class='line'>
</span><span class='line'>[root@cu2 puppetboard]# /home/hadoop/nginx/sbin/nginx -s reload
</span></code></pre></td></tr></table></div></figure>


<p><img src="/images/blogs/puppetboard-install.png" alt="" /></p>

<p>配置SSL访问需要把ssl_verify设置为false。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 2.7.9+网上说好像就没问题
</span><span class='line'># http://stackoverflow.com/questions/29099404/ssl-insecureplatform-error-when-using-requests-package
</span><span class='line'># https://github.com/pypa/pip/issues/2681
</span><span class='line'>[root@cu2 ~]# yum install -y  libffi-devel libffi 
</span><span class='line'>[root@cu2 ~]# pip install 'requests[security]'
</span><span class='line'>
</span><span class='line'># [重要] 两个链接内容一样的：
</span><span class='line'># * https://groups.google.com/forum/#!msg/puppet-users/m7Sakf4bQ7Q/y6uAa0AUsZIJ
</span><span class='line'># * http://grokbase.com/t/gg/puppet-users/1428vjkncr/puppetboard-and-ssl
</span><span class='line'># You have two choices now, set SSL_VERIFY to False and trust that you're
</span><span class='line'># always talking to your actual PuppetDB or copy from the Puppet CA
</span><span class='line'># $vardir/ssl/ca_crt.pem to /etc/puppetboard and set SSL_VERIFY to the path
</span><span class='line'># of ca_crt.pem. In that case the file SSL_VERIFY points to will be used to
</span><span class='line'># verify PuppetDB's server certificate instead of the OS truststore.
</span><span class='line'>[root@cu2 puppetboard]# vi settings.py 
</span><span class='line'>PUPPETDB_HOST = 'cu3.eshore.cn'
</span><span class='line'>PUPPETDB_PORT = 8081
</span><span class='line'>PUPPETDB_SSL_VERIFY = False  # 这里设置为false
</span><span class='line'>PUPPETDB_KEY = '/etc/puppetlabs/puppet/ssl/private_keys/cu2.eshore.cn.pem'
</span><span class='line'>PUPPETDB_CERT = '/etc/puppetlabs/puppet/ssl/ca/signed/cu2.eshore.cn.pem'
</span><span class='line'>
</span><span class='line'># 重启uwsgi-http服务
</span><span class='line'>[root@cu2 ~]# supervisorctl restart puppetboard
</span></code></pre></td></tr></table></div></figure>


<p>如果 puppetboard 和 puppetdb 安装在同一机器，可以使用 puppetdb/ssl 路径下的ssl文件（puppetdb/ssl也是从puppet/ssl拷贝过来的）：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu3 ~]# puppetdb ssl-setup -f
</span><span class='line'>PEM files in /etc/puppetlabs/puppetdb/ssl are missing, we will move them into place for you
</span><span class='line'>Copying files: /etc/puppetlabs/puppet/ssl/certs/ca.pem, /etc/puppetlabs/puppet/ssl/private_keys/cu3.eshore.cn.pem and /etc/puppetlabs/puppet/ssl/certs/cu3.eshore.cn.pem to /etc/puppetlabs/puppetdb/ssl
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[root@cu3 ~]# tree /etc/puppetlabs/puppetdb/ssl/
</span><span class='line'>/etc/puppetlabs/puppetdb/ssl/
</span><span class='line'>├── ca.pem
</span><span class='line'>├── private.pem
</span><span class='line'>└── public.pem
</span></code></pre></td></tr></table></div></figure>


<p>&ndash;END</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/18">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/16">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2019/04/10/try-k8s/">Try K8s</a>
			</li>
		
			<li class="post">
				<a href="/blog/2018/08/25/video-auto-translate/">视频自动翻译</a>
			</li>
		
			<li class="post">
				<a href="/blog/2018/06/09/reasonable-way-to-access-the-internet/">科学上网（续）</a>
			</li>
		
			<li class="post">
				<a href="/blog/2018/01/30/map-started-guide/">Map入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2018/01/20/gitalk-on-octopress/">Gitalk on Octopress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/11/16/sphinx-generate-docs/">使用Sphinx生成/管理文档</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/10/30/windows-run-ubuntu/">Windows Run Ubuntu</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/08/10/java-bytecode-security/">保护/加密JAVA代码</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/07/30/kubeadm-install-kubenetes-on-centos7/">Kubeadm部署kubernetes</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/07/08/casperjs-crawler/">爬虫之CasperJS</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/05/23/spark-on-hive-speculation-shit-bug/">Hive on Spark预测性执行BUG一枚</a>
			</li>
		
			<li class="post">
				<a href="/blog/2017/01/25/develop-environment-prepare/">[整理] 环境准备工具集</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/09/19/163-open-movies-download/">批量下载163-open的视频</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/04/23/hadoop-guide-catalog/">[整理] Hadoop入门</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/04/04/rpm-build-your-package/">RPM打包</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a>
			</li>
		
			<li class="post">
				<a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/11/22/gfw-ladder/">搭梯笔记</a>
			</li>
		
			<li class="post">
				<a href="/blog/2015/08/24/manual-install-supervisor/">Supervisor安装配置</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/10/16/spark-build-and-configuration/">编译/搭建Spark环境</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">查找逐步定位Java程序OOM的异常实践</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/02/23/quickly-open-program-in-windows/">[Windows运行]快速打开程序</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2020/05/11/redmine-on-arm-pi/">在树莓派上部署redmine</a>
      </li>
    
      <li class="post">
        <a href="/blog/2020/04/12/appium-android-auto-test/">appium-Android自动化测试</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/07/26/android-linux-via-termux/">Android Linux via Termux</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/04/10/try-bk-dot-tencent-dot-com/">Try bk.tencent.com</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/04/10/try-k8s/">Try K8s</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/10/20/jcef-build-on-win64/">编译JCEF - Win64</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/08/25/video-auto-translate/">视频自动翻译</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/06/20/k2-reburn/">斐讯K2刷机记录</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/alluxio/'>alluxio</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/bigdata/'>bigdata</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (6) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/deprecated/'>deprecated</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/devops/'>devops</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (15) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/elasticsearch/'>elasticsearch</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/es/'>es</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/flume/'>flume</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (5) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (44) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (8) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hole/'>hole</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (13) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (8) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jenkins/'>jenkins</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/k8s/'>k8s</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kafka/'>kafka</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/kubeadm/'>kubeadm</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/logstash/'>logstash</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/map/'>map</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/nginx/'>nginx</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/puppet/'>puppet</a> (11) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (28) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (12) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/staf/'>staf</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tachyon/'>tachyon</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (68) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/vagrant/'>vagrant</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/zookeeper/'>zookeeper</a> (1) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (216)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
<!--
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.6.3</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.96.0</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-1.2.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.7.0</a>
    </li>
  </ul>
&#8211;>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2021 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

<script>

var time=location.pathname.substring(6).substring(0,11);
var eName=location.pathname.substring(17);
var gitalk = new Gitalk({
  clientID: 'c14f68eac6330d15d984',
  clientSecret: '73b7c1fffa98e299ff0cdd332821201933858e6e',
  repo: 'winse.github.com',
  owner: 'winse',
  admin: ['winse'],
  id: eName,
  labels: ['Gitalk', time],
  body: "http://winseliu.com" + location.pathname,
  createIssueManually: true,
  
  // facebook-like distraction free mode
  distractionFreeMode: false
})

gitalk.render('gitalk-container')

</script>



<script>
/*
$.ajax({
  type: "POST",
  url: "http://log.winseliu.com:20000",
  data: JSON.stringify({
    title: document.title,
    location: JSON.stringify(location),
    referrer: document.referrer,
    userAgent: navigator.userAgent
  }),
  contentType: "application/json; charset=utf-8",
  dataType: "json"
});
*/
</script>









</body>
</html>
