<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scala | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/scala/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2015-01-14T18:57:35+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scala Wordcount on Hadoop2]]></title>
    <link href="http://winseliu.com/blog/2014/09/12/scala-wordcount-on-hadoop/"/>
    <updated>2014-09-12T07:52:01+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/12/scala-wordcount-on-hadoop</id>
    <content type="html"><![CDATA[<p>从了解scala，到spark再次遇见scala，准备好好学学这门语言。函数式编程大势所趋，简洁的语法，更抽象好用的集合操作。土生土长的JVM的语言，以及凭借其与java的互操作性，发展前景一片光明。在云计算以及手机（android）开发都有其大展拳脚的地方。</p>

<p>工作中大部分时间写mapreduce，项目空白期实践了一下把scala搬上hadoop。整体来说用scala写个helloworld是比较简单的，就一些细节的东西比较繁琐。尽管用了几年的eclipse了，但是<a href="http://scala-ide.org/">scala-ide</a>还是需要再适应适应！scala-idea也没有大家说的那么好，和webstorm比差远了。</p>

<p><div><script src='https://gist.github.com/5df39f77e8bd59348a7a.js'></script>
<noscript><pre><code>package com.github.winse.hadoop

import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.Reducer
import org.apache.hadoop.io.Text
import org.apache.hadoop.io.IntWritable
import org.apache.hadoop.io.LongWritable
import org.apache.hadoop.mapreduce.Mapper
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
import org.apache.hadoop.fs.Path
import scala.Array.canBuildFrom
import org.apache.hadoop.conf.Configured
import org.apache.hadoop.util.Tool
import org.apache.hadoop.util.ToolRunner

class ScalaMapper extends Mapper[LongWritable, Text, Text, IntWritable] {

  val one = new IntWritable(1);

  override def map(key: LongWritable, value: Text, context: Mapper[LongWritable, Text, Text, IntWritable]#Context) {
    value.toString().split(&quot;\\s+&quot;).map(word =&gt; context.write(new Text(word), one))
  }

}

class ScalaReducer extends Reducer[Text, IntWritable, Text, IntWritable] {

  override def reduce(key: Text, values: java.lang.Iterable[IntWritable], context: Reducer[Text, IntWritable, Text, IntWritable]#Context) {
    var sum: Int = 0

    val itr = values.iterator()
    while (itr.hasNext()) {
      sum += itr.next().get()
    }
    context.write(key, new IntWritable(sum))
  }

}

object HelloScalaMapRed extends Configured with Tool {

  override def run(args: Array[String]): Int = {

    val job = Job.getInstance(getConf(), &quot;WordCount Scala.&quot;)
    job.setJarByClass(getClass())

    job.setOutputKeyClass(classOf[Text])
    job.setOutputValueClass(classOf[IntWritable])

    job.setMapperClass(classOf[ScalaMapper])
    job.setCombinerClass(classOf[ScalaReducer])
    job.setReducerClass(classOf[ScalaReducer])

    FileInputFormat.addInputPath(job, new Path(&quot;/scala/in/&quot;));
    FileOutputFormat.setOutputPath(job, new Path(&quot;/scala/out/&quot;));

    job.waitForCompletion(true) match {
      case true =&gt; 0
      case false =&gt; 1
    }

  }

  def main(args: Array[String]) {
    val res: Int = ToolRunner.run(new Configuration(), this, args)
    System.exit(res);
  }

}</code></pre></noscript></div>
</p>

<p>使用scala主要原因：</p>

<ul>
<li>写JavaBean更简单方便</li>
<li>多返回值无需定义Result实体类</li>
<li>集合更抽象的方法真的很好用</li>
<li>trait可以更便捷的进行操作层面的聚合，也就是可以把操作分离出来，进行组合就可以实现新的功能。这不就是decorate模式嘛！java的decorate多麻烦的！加点东西太麻烦了！！！</li>
</ul>


<p>上面的scala代码和java的比较类似，主要在集合操作上不同而已，变量定义简单化。</p>

<p>编写好代码后就是运行调试。</p>

<p>前面其他的文章已经说过了，默认<code>mapreduce.framework.name</code>的配置是本地<code>local</code>，所以直接运行就像运行一个普通的本地java程序。这就不多将了。
这里主要讲讲怎么把代码打包放到真实的集群环境运行，相比java的版本要添加那些步骤。</p>

<p>从项目的maven pom中可以发现，其实就是多了scala-lang的新依赖而已，其他都是hadoop自带的公共包。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHV6AAJoCAABANktCWmk664.png" alt="" /></p>

<p>所以运行程序只需要指定把scala-lang.jar添加到运行环境的classpath中即可。使用maven打包后的项目结构如下：</p>

<pre><code>[hadoop@master1 scalamapred-1.0.5]$ cd lib/
[hadoop@master1 lib]$ ls -l
total 8
drwxrwxr-x. 2 hadoop hadoop 4096 Sep 11 23:10 common
drwxrwxr-x. 2 hadoop hadoop 4096 Sep 11 23:56 core
[hadoop@master1 lib]$ ll core/
total 12
-rw-r--r--. 1 hadoop hadoop 11903 Sep 11 23:55 scalamapred-1.0.5.jar
[hadoop@master1 lib]$ ls common/
activation-1.1.jar                commons-lang-2.6.jar            hadoop-hdfs-2.2.0.jar                     jaxb-api-2.2.2.jar                      log4j-1.2.17.jar
aopalliance-1.0.jar               commons-logging-1.1.1.jar       hadoop-mapreduce-client-common-2.2.0.jar  jaxb-impl-2.2.3-1.jar                   management-api-3.0.0-b012.jar
asm-3.1.jar                       commons-math-2.1.jar            hadoop-mapreduce-client-core-2.2.0.jar    jersey-client-1.9.jar                   netty-3.6.2.Final.jar
avro-1.7.4.jar                    commons-net-3.1.jar             hadoop-yarn-api-2.2.0.jar                 jersey-core-1.9.jar                     paranamer-2.3.jar
commons-beanutils-1.7.0.jar       gmbal-api-only-3.0.0-b023.jar   hadoop-yarn-client-2.2.0.jar              jersey-grizzly2-1.9.jar                 protobuf-java-2.5.0.jar
commons-beanutils-core-1.8.0.jar  grizzly-framework-2.1.2.jar     hadoop-yarn-common-2.2.0.jar              jersey-guice-1.9.jar                    scala-library-2.10.4.jar
commons-cli-1.2.jar               grizzly-http-2.1.2.jar          hadoop-yarn-server-common-2.2.0.jar       jersey-json-1.9.jar                     servlet-api-2.5.jar
commons-codec-1.4.jar             grizzly-http-server-2.1.2.jar   jackson-core-asl-1.8.8.jar                jersey-server-1.9.jar                   slf4j-api-1.7.1.jar
commons-collections-3.2.1.jar     grizzly-http-servlet-2.1.2.jar  jackson-jaxrs-1.8.3.jar                   jersey-test-framework-core-1.9.jar      slf4j-log4j12-1.7.1.jar
commons-compress-1.4.1.jar        grizzly-rcm-2.1.2.jar           jackson-mapper-asl-1.8.8.jar              jersey-test-framework-grizzly2-1.9.jar  snappy-java-1.0.4.1.jar
commons-configuration-1.6.jar     guava-17.0.jar                  jackson-xc-1.8.3.jar                      jets3t-0.6.1.jar                        stax-api-1.0.1.jar
commons-daemon-1.0.13.jar         guice-3.0.jar                   jasper-compiler-5.5.23.jar                jettison-1.1.jar                        xmlenc-0.52.jar
commons-digester-1.8.jar          guice-servlet-3.0.jar           jasper-runtime-5.5.23.jar                 jetty-6.1.26.jar                        xz-1.0.jar
commons-el-1.0.jar                hadoop-annotations-2.2.0.jar    javax.inject-1.jar                        jetty-util-6.1.26.jar                   zookeeper-3.4.5.jar
commons-httpclient-3.1.jar        hadoop-auth-2.2.0.jar           javax.servlet-3.1.jar                     jsch-0.1.42.jar
commons-io-2.1.jar                hadoop-common-2.2.0.jar         javax.servlet-api-3.0.1.jar               jsp-api-2.1.jar
[hadoop@master1 lib]$ 
</code></pre>

<p>在lib文件夹下面包括common和core两放置jar的文件夹，common是项目的依赖包，core下面的是项目的源码jar。</p>

<p>接下来运行程序，通过libjar把<strong>scala-library的包加入到mapreduce的运行时classpath</strong>。当然也可以把scala-library加入到<code>mapreduce.application.classpath</code>（默认值为<code>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</code>）。</p>

<pre><code>[hadoop@master1 scalamapred-1.0.5]$ for j in `find . -name "*.jar"` ; do export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$j ; done
[hadoop@master1 scalamapred-1.0.5]$ 
[hadoop@master1 scalamapred-1.0.5]$ export HADOOP_CLASSPATH=
[hadoop@master1 scalamapred-1.0.5]$ export HADOOP_CLASSPATH=/home/hadoop/scalamapred-1.0.5/lib/core/*:/home/hadoop/scalamapred-1.0.5/lib/common/*
[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed -libjars lib/common/scala-library-2.10.4.jar 
</code></pre>

<h2>问题攻略</h2>

<p>上面如果不加libjar的话，会在nodemanager的代码中抛出异常。本来认为不加依赖包也就不能执行mapreduce里面的代码而已。问题的根源在哪里呢？</p>

<p>给代码添加远程调试的配置，然后运行一步步的查找问题（一次找不到就多运行调试几次）。</p>

<pre><code>[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed  -Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090"

// 我这里slaver就一台，取到机器上查看运行的程序

[hadoop@slaver1 nmPrivate]$ ps axu|grep java
hadoop    1427  0.6 10.5 1562760 106344 ?      Sl   Sep11   0:45 /opt/jdk1.7.0_60//bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=hadoop-hadoop-datanode-slaver1.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode
hadoop    2874  2.5 11.7 1599312 118980 ?      Sl   00:08   0:57 /opt/jdk1.7.0_60//bin/java -Dproc_nodemanager -Xmx1000m -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.home.dir= -Dyarn.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dyarn.policy.file=hadoop-policy.xml -server -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -classpath /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/etc/hadoop/nm-config/log4j.properties org.apache.hadoop.yarn.server.nodemanager.NodeManager
hadoop    3750  0.0  0.1 106104  1200 ?        Ss   00:43   0:00 /bin/bash -c /opt/jdk1.7.0_60//bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stderr 
hadoop    3759  0.1  1.8 737648 18232 ?        Sl   00:43   0:00 /opt/jdk1.7.0_60//bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster
hadoop    3778  0.0  0.0 103256   832 pts/0    S+   00:45   0:00 grep java

// 取到对应的目录下查看launcher.sh的脚本
// appmaster launcher

[hadoop@slaver1 nm-local-dir]$ cd nmPrivate/application_1410453720744_0007/
[hadoop@slaver1 application_1410453720744_0007]$ ll
total 4
drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:43 container_1410453720744_0007_01_000001
[hadoop@slaver1 application_1410453720744_0007]$ less container_1410453720744_0007_01_000001/
container_1410453720744_0007_01_000001.tokens       launch_container.sh                                 
.container_1410453720744_0007_01_000001.tokens.crc  .launch_container.sh.crc                            
[hadoop@slaver1 application_1410453720744_0007]$ less container_1410453720744_0007_01_000001/launch_container.sh 
#!/bin/bash

export NM_HTTP_PORT="8042"
export LOCAL_DIRS="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007"
export HADOOP_COMMON_HOME="/home/hadoop/hadoop-2.2.0"
export JAVA_HOME="/opt/jdk1.7.0_60/"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export HADOOP_YARN_HOME="/home/hadoop/hadoop-2.2.0"
export CLASSPATH="$PWD:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/share/hadoop/common/*:$HADOOP_COMMON_HOME/share/hadoop/common/lib/*:$HADOOP_HDFS_HOME/share/hadoop/hdfs/*:$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*:$HADOOP_YARN_HOME/share/hadoop/yarn/*:$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*:job.jar/job.jar:job.jar/classes/:job.jar/lib/*:$PWD/*"
export HADOOP_TOKEN_FILE_LOCATION="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001/container_tokens"
export NM_HOST="slaver1"
export APPLICATION_WEB_PROXY_BASE="/proxy/application_1410453720744_0007"
export JVM_PID="$$"
export USER="hadoop"
export HADOOP_HDFS_HOME="/home/hadoop/hadoop-2.2.0"
export PWD="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001"
export CONTAINER_ID="container_1410453720744_0007_01_000001"
export HOME="/home/"
export NM_PORT="40888"
export LOGNAME="hadoop"
export APP_SUBMIT_TIME_ENV="1410455811401"
export MAX_APP_ATTEMPTS="2"
export HADOOP_CONF_DIR="/home/hadoop/hadoop-2.2.0/etc/hadoop"
export MALLOC_ARENA_MAX="4"
export LOG_DIRS="/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001"
ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/10/job.jar" "job.jar"
ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/13/job.xml" "job.xml"
mkdir -p jobSubmitDir
ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/11/job.splitmetainfo" "jobSubmitDir/job.splitmetainfo"
mkdir -p jobSubmitDir
ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/12/job.split" "jobSubmitDir/job.split"
exec /bin/bash -c "$JAVA_HOME/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stderr "

// 去到TMP对应的目录下，查看整个运行的根目录

[hadoop@slaver1 ~]$ cd /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001
[hadoop@slaver1 container_1410453720744_0007_01_000001]$ ll
total 28
-rw-r--r--. 1 hadoop hadoop   95 Sep 12 00:43 container_tokens
-rwx------. 1 hadoop hadoop  468 Sep 12 00:43 default_container_executor.sh
lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:43 job.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/10/job.jar
drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:43 jobSubmitDir
lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:43 job.xml -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/13/job.xml
-rwx------. 1 hadoop hadoop 3005 Sep 12 00:43 launch_container.sh
drwx--x---. 2 hadoop hadoop 4096 Sep 12 00:43 tmp
[hadoop@slaver1 container_1410453720744_0007_01_000001]$ 
</code></pre>

<p>为了对应，我这里列出来在添加了libjar的TMP目录的列表：</p>

<pre><code>[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed  -Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090" -libjars lib/common/scala-library-2.10.4.jar 

[hadoop@slaver1 container_1410453720744_0007_01_000001]$ cd /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/container_1410453720744_0008_01_000001
[hadoop@slaver1 container_1410453720744_0008_01_000001]$ ll
total 32
-rw-r--r--. 1 hadoop hadoop   95 Sep 12 00:49 container_tokens
-rwx------. 1 hadoop hadoop  468 Sep 12 00:49 default_container_executor.sh
lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:49 job.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/filecache/10/job.jar
drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:49 jobSubmitDir
lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:49 job.xml -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/filecache/13/job.xml
-rwx------. 1 hadoop hadoop 3127 Sep 12 00:49 launch_container.sh
lrwxrwxrwx. 1 hadoop hadoop   85 Sep 12 00:49 scala-library-2.10.4.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/filecache/10/scala-library-2.10.4.jar
drwx--x---. 2 hadoop hadoop 4096 Sep 12 00:49 tmp
[hadoop@slaver1 container_1410453720744_0008_01_000001]$ 
</code></pre>

<p>windows本地使用eclipse和进行跟踪调试代码。</p>

<p><img src="http://file.bmob.cn/M00/0E/A1/wKhkA1QUG0aARyPVAAMnUXGDgbY378.png" alt="" /></p>

<p>此时可以通过8088的网页查看状态，当前有一个mrappmaster在执行，如果第一个失败，会尝试执行第二次。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHDGAe0anAAEfiNTmB1k734.png" alt="" /></p>

<p>运行调试多次后，<strong>最终确定问题</strong>所在。在master中会检查是否为链式mr，而加载该class的时刻，同时要加载父类的class，即scala的类，所以在这里会抛出异常。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHFOAWJulAAPOawkAbgo349.png" alt="" /></p>

<p>去到查看程序运行的日志，可以看到程序抛出的异常<strong>NoClassDefFoundError</strong>。</p>

<pre><code>[hadoop@slaver1 ~]$ less /home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410448728371_0003/*/syslog
2014-09-11 22:55:12,616 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1410448728371_0003_000001
...
2014-09-11 22:55:18,677 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1410448728371_0003 to jobTokenSecretManager
2014-09-11 22:55:19,119 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster
java.lang.NoClassDefFoundError: scala/Function1
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:190)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1277)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1217)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:135)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1420)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1358)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:972)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:134)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1227)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1035)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1445)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1441)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1374)
Caused by: java.lang.ClassNotFoundException: scala.Function1
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 22 more
2014-09-11 22:55:19,130 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster received a signal. Signaling RMCommunicator and JobHistoryEventHandler.
</code></pre>

<h2>意外收获</h2>

<ul>
<li>推测执行初始化代码</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHHCATFHtAAMeDcCHWzU166.png" alt="" /></p>

<ul>
<li>OutputFormat的获取Committer代码</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHImAJAq1AALGEfA-F9k811.png" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://digifesto.com/2013/04/15/hadoop-with-scala-hacking-notes/">Hadoop with Scala: hacking notes</a></li>
<li><a href="https://github.com/derrickcheng/ScalaOnHadoop/blob/master/src/main/scala/WordCount.scala">ScalaOnHadoop WordCount.scala</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【笔记】Beginning Scala（1）]]></title>
    <link href="http://winseliu.com/blog/2014/09/08/note-beginning-scala-part1/"/>
    <updated>2014-09-08T07:36:57+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/08/note-beginning-scala-part1</id>
    <content type="html"><![CDATA[<p>Scala借鉴了python、ruby等函数式语言。从java转过来还是需要一个适应阶段，与groovy比似乎困难多了不少。一年前好奇接触过，看了一些官网的入门教程，觉得这就是一个异类，后面就放下了。</p>

<p>直到再次弄hadoop，接触spark。经过一个时间的过渡期后，发现Scala确实能处理java的一些繁琐问题，为我们的双手减负，写出更简洁更优雅的代码，或者说更”易懂“。</p>

<p><strong>这篇是第一章（About Scala and How to Install It）和第二章（Scala Syntax, Scripts, and Your First Scala Programs）的笔记。</strong></p>

<p>作者寄语：</p>

<blockquote><p>My Path was hard, and I hope yours will be easier.</p></blockquote>

<h2>历史与安装</h2>

<p>随着HotSpot对JVM的改进，JDK1.3的程序与C++写的程序一样快。Java程序可以运行几个星期、几个月、甚至一年都不用重启。</p>

<p>好的Java代码与C/C++的代码一样快，甚至更快。在同样功能下，经过深度调优的C/C++程序会比Java程序更高效，与C/C++相比Java程序需要更多的内存，但对于一个适度复杂的项目（非系统内核级别），JVM程序将比C/C++表现的更优异。</p>

<p>这么多年来，Java在语言级别还不成熟。Java语法停滞不前，Java上的web框架越来越笨重。处理XML，或者其他一些简单概念的实现，如字段生成前台的HTML表单，需要越来越多的代码。对Java越来越失望。
Java5增加了枚举和泛型，对语言而言这是一个可喜的消息，但编码方面我们不得不使用IDE来完成Java代码编写。</p>

<p>“写Scala”的Martin Odersky曾编写了java编译器和泛型功能。Scala(2001, first version in 2003)，语法表达能力如ruby，但同时有Java的强类型和高性能。</p>

<p>Scala即快又简洁，同时类型安全。Scala运行效率也很高，最终编译成Java字节码跑在JVM上，又能与Java代码互相调用。</p>

<blockquote><p>But most importantly, Scala taught me  to program and reason about programming differently. I stopped thinking in terms of allocating buffers, structs, and objects, and of changing those pieces of memory. Instead, I learned to think about most of my programs as transforming input to output. This change in thinking has lead to lower defect rates, more modular code, and more testable code. Scala has also given me the tools to write smaller, more modular units of code and asse mble them together into a whole that is maintainable, yet far more complex than anything that I could write in Java or Ruby for that matter.</p></blockquote>

<p>下载安装JDK6+配置PATH, <a href="http://scala-lang.org/download/2.10.4.html">Scala 2.10+</a>下载zip版本的，然后解压就行了。</p>

<pre><code>winse@Lenovo-PC /cygdrive/d/scala/bin
$ ls -1
fsc
fsc.bat
scala
scala.bat
scalac
scalac.bat
scaladoc
scaladoc.bat
scalap
scalap.bat

winse@Lenovo-PC /cygdrive/d/scala/bin
$ scala
Welcome to Scala version 2.10.4 (Java HotSpot(TM) Client VM, Java 1.7.0_02).
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt; def fact(n:Int)=1 to n reduceLeft(_*_) // n!
fact: (n: Int)Int

scala&gt; fact(5)
res0: Int = 120
</code></pre>

<h2>语法结构，第一个Scala程序</h2>

<p>运行程序的三种方式：</p>

<ul>
<li>命令行交互式的REPL（read-eval-print loop)</li>
<li>shell/cmd脚本</li>
<li>编译打包成jar后运行，跟Java一样</li>
</ul>


<h3>REPL</h3>

<p>进入到Scala的bin目录下，双击scala.bat打开。</p>

<pre><code>scala&gt; 1+1
res0: Int = 2

scala&gt; res0*8
res1: Int = 16

scala&gt; val x="hello world"
x: String = hello world

scala&gt; var xl=x.length
xl: Int = 11

scala&gt; import java.util._
import java.util._

scala&gt; val d = new Date
d: java.util.Date = Mon Sep 08 09:17:08 CST 2014
</code></pre>

<h3>脚本</h3>

<p>脚本中无需显示的定义main方法，当你运行脚本时，Scala把整个文件的内容添加到类的main方法中，编译代码，然后运行生成的main方法。你只需在脚本文件中编写scala代码即可。</p>

<pre><code>winse@Lenovo-PC ~
$ scala hello.scala
hello world

winse@Lenovo-PC ~
$ cat hello.scala
println("hello world")
</code></pre>

<h3>编译后运行</h3>

<p>运行方式和javac类似，会生成对应类的字节码class文件。</p>

<pre><code>winse@Lenovo-PC ~/scala-hello
$ scalac hello.scala

winse@Lenovo-PC ~/scala-hello
$ ll
total 13
-rwxr-xr-x  1 winse None 2067 Sep  8 09:27 hello$.class
-rwxr-xr-x  1 winse None  704 Sep  8 09:27 hello$delayedInit$body.class
-rwxr-xr-x  1 winse None  921 Sep  8 09:27 hello.class
-rw-r--r--+ 1 winse None   58 Sep  8 09:26 hello.scala

winse@Lenovo-PC ~/scala-hello
$ cat hello.scala
object hello extends App {

  println("hello world")

}

winse@Lenovo-PC ~/scala-hello
$ scala hello
hello world
</code></pre>

<p>编译器的启动是很耗时的操作，你可以使用fsc（fast Scala Compiler），fsc是单独运行在后台的编译进程。</p>

<p>如果你原有的项目中使用Ant或Maven，scala有对应的插件，可以很容易把Scala集成到项目中。</p>

<h3>First Scala Programs</h3>

<p>在Scala，你可以编写像ruby和python脚本语言代码。如输出“hello world”的println方法，封装了System.out.println()。因为太常用了，println被定义在Scala的Predef（预定义成员）中，每个程序都会自动加载，就像java.lang会自动引入到每个java程序一样。</p>

<pre><code>println("hello world")

for {i&lt;- 1 to 10}
  println(i)

for {i&lt;- 1 to 10
     j&lt;- 1 to 10}
  println(i*j)
</code></pre>

<p>99乘法表：</p>

<pre><code>scala&gt; for(i&lt;- 1 to 9){
     | for(j&lt;- 1 to i)
     | printf("%s*%s=%2s\t",j,i,i*j);
     |
     | println()
     | }
1*1= 1
1*2= 2  2*2= 4
1*3= 3  2*3= 6  3*3= 9
1*4= 4  2*4= 8  3*4=12  4*4=16
1*5= 5  2*5=10  3*5=15  4*5=20  5*5=25
1*6= 6  2*6=12  3*6=18  4*6=24  5*6=30  6*6=36
1*7= 7  2*7=14  3*7=21  4*7=28  5*7=35  6*7=42  7*7=49
1*8= 8  2*8=16  3*8=24  4*8=32  5*8=40  6*8=48  7*8=56  8*8=64
1*9= 9  2*9=18  3*9=27  4*9=36  5*9=45  6*9=54  7*9=63  8*9=72  9*9=81
</code></pre>

<p>编写复杂点的程序，可以使用<a href="http://scala-ide.org/">Scala-IDE</a>。</p>

<pre><code>import scala.io._   // like java import scala.io.*

def toInt(in: String): Option[Int] =
    try {
      Some(Integer.parseInt(in.trim))
    } catch {
      case e: NumberFormatException =&gt; None
    }

def sum(in: Seq[String]) = {
    val ints = in.flatMap(s =&gt; toInt(s))
    ints.foldLeft(0)((a, b) =&gt; a + b)
}

println("Enter some numbers and press CTRL+C")

val input = Source.fromInputStream(System.in)
val lines = input.getLines.toSeq

println("Sum " + sum(lines))
</code></pre>

<p>Option是包含一个或零个对象的容器。如果不包含元素，返回的是单例的None。如果包括一个元素，就是新的Some(theElement)的实例。Option是Scala中避免空指针异常（null pointer）和显示进行null检查的处理一种方式。如果是None，一个业务逻辑将应用到0个元素，是Some就应用到一个元素上。</p>

<p>方法没有显示的return语句，默认就是方法“最后”（逻辑上最后执行的）一个语句的返回值。</p>

<p>sum方法的参数Seq是一个trait（类似java interface），是Array，List以机构其他顺序集合的父trait。trait拥有java interface的所有特性，同时traits可以包括方法的实现。你可以混合很多的traits成一个类。Traits除了不能定义有参构造函数外，其他和类一样。trait使得“多重继承”简单化，无需担忧<strong> the diamond problem</strong>（有点类似近亲结婚 ^ v ^）。</p>

<p>如：当BC都实现了M方法，D不知道用谁的M，会有歧义！！</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Diamond_inheritance.svg/220px-Diamond_inheritance.svg.png" alt="" /></p>

<p>在Scala中，定义参数分为val和var，val类似于java final，var类似于java的变量定义。对于不变化的变量，定义为val可以减少代码错误几率，进行防御性的编程。</p>

<p>接下来运行程序， 输入一些数字后按CTRL+C结束，就会输出计算的和。</p>

<pre><code>E:\local\home\Administrator\scala-hello&gt;scala Sum.scala
Enter some numbers and press CTRL+C
12
23
34
45
Sum 114
终止批处理操作吗(Y/N)?
</code></pre>

<h3>基本的语法Basic Syntax</h3>

<p>Scala的全部语法和语言的定义可以查看<a href="http://www.scala-lang.org/docu/files/ScalaReference.pdf">Scala Language Specification</a></p>

<h4>数字、字符串和XML常量</h4>

<ul>
<li><p>; 行结束符可以忽略</p></li>
<li><p>和Java一样的常量定义</p>

<blockquote><p>Integer: 1882, -1
Boolean: true, false
Double: 1.0, 1d, 1e3
Long: 42L
Float: 78.9f
Characters: &lsquo;4&rsquo;, &lsquo;?&rsquo;, &lsquo;z&rsquo;
Strings: &ldquo;Hello World&rdquo;</p></blockquote></li>
<li><p>Scala支持多行的字符串</p>

<blockquote><p>&ldquo;&rdquo;&ldquo;Hello
Multiline
World&rdquo;&ldquo;&rdquo;</p></blockquote></li>
<li><p>Scala支持XML常量，包括内嵌的Scala代码</p>

<blockquote><p>
<b>Foll</b>
<ul>{(1 to 3).map(i => <li>{i}</li>)}</ul>
</p></blockquote></li>
</ul>


<h4>包package和import</h4>

<p>package定义在源代码非注释的第一行。和java一样。</p>

<p>import则比java的更加灵活。基本的用法使用：</p>

<pre><code>import scala.xml._
</code></pre>

<p>scala中的import可以基于前面的imports语句。如再导入<code>scala.xml.transform</code>：</p>

<pre><code>import transform._
</code></pre>

<p>也可以导入一个具体的class和object：</p>

<pre><code>import scala.collection.mutable.HashMap
</code></pre>

<p>一次性倒入一个package下的几个class或object：</p>

<pre><code>import scala.collection.immutable.{TreeMap, TreeSet}
</code></pre>

<p>甚至可以给原有的class或object定义一个别名。</p>

<pre><code>import scala.util.parsing.json.{JSON =&gt; JsonParser}
</code></pre>

<p>import可以定义在任何代码块中，并且只会在当前作用域内有效。还可以引入objects的method，相当于java的import static。</p>

<pre><code>class Frog {
    import scala.xml._
    def n: NodeSeq = NodeSeq.Empty
}

object Moose {
    def bark = "woof"
}

import Moose._
bark
</code></pre>

<h4>Class, Trait和Object</h4>

<p>Scala的对象语法和规则比Java的更加复杂。</p>

<p>Scala去掉了一个文件中只能定义一个public类的限制。你想在一个文件里面放n个类都可以，同时文件的名称也没有限制（Java文件名需要和public的类同名）。</p>

<p>Scala中默认访问级别是public的。</p>

<pre><code>// scala
class Foo

// java
public class Foo {
}
</code></pre>

<p>如果构造函数、方法没有参数，可以省略参数列表（即不需要输入括号）。</p>

<pre><code>new Foo

new Foo()

class Bar(name: String)

new Bar("Working...")

class Baz(name: String) {
    // constructor code is inline
    if(name == null) throw new Exception("Name is null")
}
</code></pre>

<p>Scala的trait，和java中的interface类似。同时trait可以包括具体实现的方法，这是一个非常方便的特性，你不必在定义复杂的类继承关系来实现代码的重用，在Scala中，把代码写在trait中即可。Scala traits类似于Ruby mixins</p>

<pre><code>trait Dog

class Fizz2(name: String) extends Bar(name) with Dog

trait Cat {
    def meow(): String
}

trait FuzzyCat extends Cat {
    override def meow(): String = "Meeeeeeeeeow"
}

trait OtherThing {
    def hello() = 4
}

class Yep extends FuzzyCat with OtherThing

(new Yep).meow()
(new Yep).hello()
</code></pre>

<p>Scala中不支持static关键字，可以使用<code>object</code>单例对象来实现类似的功能。当object对象第一次访问才会被初始化，在对应的访问域内仅有一个该实例。Scala object还有一个优势，由于是类的实例，所以可以作为方法参数进行传递。</p>

<pre><code>object Simple

object OneMethod {
    def myMethod() = "Only One"
}

object Dude extends Yep

object Dude2 extends Yep {
    override def meow() = "Dude looks like a cat"
}

object OtherDude extends Yep {
    def twoMeows(otherparam: Yep) = meow + ", " + otherparam.meow
}

OtherDude.meow // Meeeeeeeeeow
OtherDude.twoMeows(Dude) // Meeeeeeeeeow, Meeeeeeeeeow
OtherDude.twoMeows(Dude2) // Meeeeeeeeeow, Dude looks like a cat
</code></pre>

<p>如果object嵌套定义在class, trait, object内部的时刻，在其作用域下每个<strong>实例</strong>会创建一个object的单例。</p>

<pre><code>class HasYep {
    object myYep extends Yep {
    override def meow = "Moof"
  }
}

(new HasYep).myYep.meow // 每个HasYep实例会有一个单独的myYep
</code></pre>

<p>同样Classes，Objects，traits也可以嵌套在classes，objects，traits。</p>

<pre><code>class HasClass {
    private class MyDude extends FuzzyCat
    def makeOne(): FuzzyCat = new MyDude
}
</code></pre>

<h4>类继承Class Hierarchy</h4>

<p>除了方法（method），其他一切都是对象(an instance of a class)。Java的primitives类型在Scala也被当做对象，如int(Int)。当两个Ints相加时，Scala编译器会对字节码进行优化最终和java的两个ints相加时一样的。如果使用了Int的方法hashCode和toString，当primitive类型被用于需要引用类型时(expects an Any)，Scala编译器会对其进行装箱，如把Int值加入到HashMap。</p>

<p>为了保持命名的规范化，即所有类的第一个单词都是大写的。在Scala中的原始类型对应为Int,Long,Double,Float,Boolean,Char,Short,Byte，他们都是AnyVal的子类。java的void对应Unit， 同样是AnyVal的子类。你也可以使用<code>()</code>来显示的返回Unit类型实例。</p>

<pre><code>val v = ()

List(v) // List[Unit] = List(())
</code></pre>

<p><code>Nothing</code>是很酷，任何方法返回Nothing，表示它不是正常返回，肯定是抛出了异常。<code>None</code>是一个<code>Option[Nothing]</code>的实例，它的get方法会返回<code>Nothing</code>，也就是说get方法会抛出异常，而不是返回底层的值类型null。</p>

<p>Any是Scala中所有类的基类，想Object在Java中的地位。但是，Nothing/primitives等等，所以需要在Object下面定义Scala的根基类。</p>

<p>AnyVal是Scala中primitives对象的包装类的基类。
AnyRef与Java中的Object类似。<code>eq</code>,<code>ne</code>,<code>==</code>,<code>!=</code>这些方法的含义不同。<code>==</code>编译后最终调用java的equals方法，如果需要进行对象引用的比较，使用<code>eq</code>进行处理。</p>

<h4>方法声明</h4>

<p>类型推测很强大也很有用，但是需要小心使用，当类型返回类型不明确时，需要显示进行声明。</p>

<pre><code>def myMethod(): String = "Moof"

def myOtherMethod() = "Moof" // not have to explicity declare the return type

def foo(a: Int): String = a.toString

def f2(a: Int, b: Boolean): String = if(b) b.toString else "false"

def list[T](p: T): List[T] = p :: Nil

list(1)
list("Hello")

// 可变参数， Seq[Int]
def largest(as: Int*): Int = as.reduceLeft((a,b) =&gt; a max b)

largest(1)
largest(2, 3, 99)
largest(33, 22, 33, 22)

def mkString[T](as: T*): String = as.foldLeft("")(_ + _.toString)

def sum[T &lt;: Number](as: T*): Double = as.foldLeft(0d)(_ + _.doubleValue)
</code></pre>

<p>方法可以定义在<strong>任何方法块</strong>中，除了最外层即classes，traits，objects定义的地方。方法中可以使用当前作用域类的所有的成员。</p>

<pre><code>def readLines(br: BufferedReader) = {
    var ret: List[String] = Nil

    def readAll(): Unit = br.readLine match { 
        case null =&gt;
        case s =&gt; ret ::= s; readAll()
    }

    readAll()
    ret.reverse
}
</code></pre>

<p>方法重写和java的不一样，被重写的方法必须带上override的修饰符。重写抽象的方法可以不带override的修饰符。</p>

<pre><code>abstract class Base {
    def thing: String
}

class One extends Base {
    def thing = "Moof"
}
</code></pre>

<p>不带参数的方法和变量可以使用<strong>相同的方式访问</strong>，重写父类方法时可以使用val代替def。</p>

<pre><code>class Two extends One {
    override val thing = (new java.util.Date).toString
}

class Three extends One {
    override lazy val thing = super.thing + (new java.util.Date).toString
}
</code></pre>

<h4>变量声明</h4>

<p>和声明方法类似，不过关键字使用val, var, lazy val。var
可以在设置值以后再次进行修改，类似于java中的变量。val在运行到该作用域时就初始化。lazy val仅在访问的时刻进行计算一次。</p>

<pre><code>var y: String = "Moof"
val x: String = "Moof"
lazy val lz: String = someLongQuery()
</code></pre>

<p>在编程时，不推荐使用var变量除非一定要用变量。Given that mutability leads to unexpected defects, minimizing mutability in code minimizes mutability-related defects.</p>

<p>Scala类型推测对变量一样有效，在参数类型明确的情况下，定义参数时可以不用指定类型。</p>

<pre><code>var y2 = "Moof"
val x2 = "Moof"
</code></pre>

<p>Scala支持同时接受多个参数值。 If a code block or method returns a Tuple, the Tuple can be assigned to a val variable.</p>

<pre><code>val (i1: Int, s1: String) = Pair(33, "Moof")
val (i2, s2) = Pair(43, "Moof")
</code></pre>

<p>运行的效果如下：</p>

<pre><code>scala&gt; val (i2,s2)=Pair(43,"W")
i2: Int = 43
s2: String = W

scala&gt; i2
res0: Int = 43
</code></pre>

<h4>代码块</h4>

<p>方法和参数定义都可以定义在单行。</p>

<pre><code>def meth9 = "hello world"
</code></pre>

<p>或者定义在大括号包围的代码块中。代码块可以去嵌套。代码块的返回值是最后一个行的运行结果。</p>

<pre><code>def meth3(): String = {"Moof"}
def meth4(): String = {
    val d = new java.util.Date()
    d.toString()
}
</code></pre>

<p>参数定义同样可以使用代码块，适合于有少量计算的赋值操作。</p>

<pre><code>val x3: String = {
    val d = new java.util.Date()
    d.toString()
}
</code></pre>

<h4>Call-by-Name</h4>

<p>在java中，所有方法是按call-by-reference或者call-by-value（原始类型）调用。也就是说，在调用栈中的参数的值或者引用（AnyRef）会传递给调用者。</p>

<p>Scala提供另一种传递参数给方法（函数）的方式：call-by-name，可以把方法块传给调用者。 Each time the callee accesses the parameter, the code block is executed and the value is calculated.</p>

<p>Call-by-name容许我们把耗时的操作（但可能不会用到的）当做参数。For example, in a call to the logger you can use call-by-name, and the express to print is only calculated if it’s going to be logged。Call-by-name同样容许我们创建（如while/doWhile）自定义的控制结构。</p>

<pre><code>def nano() ={
    println("Getting nano")
    System.nanoTime
}

def delayed(t: =&gt; Long) = {
    println("In delayed method")
    println("Param: " + t)
    t
}

scala&gt; delayed(nano())
In delayed method
Getting nano
Param: 198642874346225
Getting nano
res1: Long = 198642875202814

def notDelayed(t: Long) = {
    println("In not delayed method")
    println("Param: " + t)
    t
}

scala&gt; notDelayed(nano)
Getting nano
In not delayed method
Param: 199944029171474
res5: Long = 199944029171474
</code></pre>

<p>注意println输出的位置和次数。</p>

<h4>方法调用</h4>

<pre><code>instance.method()
instance.method

instance.method(param)
instance method param
</code></pre>

<p>方法没有参数时可以省略括号。当只有可以参数时，可以省去点和括号。</p>

<p>实际运行效果：</p>

<pre><code>scala&gt; "abc" toUpperCase
warning: there were 1 feature warning(s); re-run with -feature for details
res0: String = ABC

scala&gt; "abc".toUpperCase
res1: String = ABC

scala&gt; "abc".charAt 1
&lt;console&gt;:1: error: ';' expected but integer literal found.
       "abc".charAt 1
                    ^

scala&gt; "abc" charAt 1
res2: Char = b

scala&gt; "abc" concat "efg"
res3: String = abcefg
</code></pre>

<p>Scala允许方法名中包括+/-/*/?， Scala’s dotless method notation creates a syntactically neutral way of invoking methods that are hard-coded operators in Java.</p>

<pre><code>scala&gt; 2.1.*(4.3)
res4: Double = 9.03

scala&gt; 2.1 * 4.3
res5: Double = 9.03
</code></pre>

<p>多参数的方法调用和java一样。</p>

<pre><code>instance.method(p1, p2)
</code></pre>

<p>Scala中的泛型方法，编译器可以进行类型推断。当然你也可以显示的指定类型。</p>

<pre><code>instance.method[TypeParam](p1, p2)
</code></pre>

<h4>Functions, apply, update, and Compiler Magic</h4>

<p>Scala是一门函数语言，也意味着你可以传递函数，可以把函数作为返回值在函数和方法中返回。</p>

<p>函数是一个带有参数和返回值的代码块。
在JVM中是不容许传递代码块的。Scala中使用特定接口的匿名内部类作为函数内部实现。当传递一个函数时，其实就是传递一个特定接口(trait)的对象。</p>

<p>定义函数的trait使用一个参数和一个返回值:</p>

<pre><code>Function1[A, B]
</code></pre>

<p>其中A是参数类型，B是返回值类型。</p>

<p>所有的函数接口都有一个apply的方法，用于函数的调用。</p>

<pre><code>Function1.apply(p: A): B
</code></pre>

<p>Thus, you can define a method that takes a function and invokes the function with the parameter 42:</p>

<pre><code>def answer(f: Function1[Int, String]) = f.apply(42)
</code></pre>

<p>如果（只要）对象包括apply方法，可以省略apply，直接把参数跟在函数名后面。</p>

<pre><code>def answer(f: Function1[Int, String]) = f(42)
</code></pre>

<p>Scala提供的语法糖，在编译时f(42)会编译成f.apply(42)。这样使用可以让代码更简洁漂亮，同时看起来更像函数调用的写法。</p>

<p>更多的语法糖：</p>

<pre><code>Function1[Int, String]
Int =&gt; String

def answer(f: Int =&gt; String) = f(42)
</code></pre>

<p>这种语法糖适用于所有包括apply方法对象。</p>

<pre><code>scala&gt; class Ap {
     | def apply(in: Int) = in.toString
     | }
defined class Ap

scala&gt; new Ap()(44)
res0: String = 44

scala&gt; new Ap(44)
&lt;console&gt;:9: error: too many arguments for constructor Ap: ()Ap
              new Ap(44)
              ^

scala&gt; val a = new Ap
a: Ap = Ap@18258b2

scala&gt; a(44)
res2: String = 44
</code></pre>

<p>如果类包括update方法，编译解析赋值操作时，会调用两个参数的update方法。</p>

<pre><code>scala&gt; class Up {
     | def update(k: Int, v: String) = println("Hey: " + k + " " + v)
     | }
defined class Up

scala&gt; val u = new Up
u: Up = Up@7bfd80

scala&gt; u(33) = "hello"
Hey: 33 hello

scala&gt; class Update {
     | def update(what: String) = println("Singler: " + what)
     | def update(a: Int, b: Int, what: String) = println("2d update")
     | }
defined class Update

scala&gt; val u = new Update
u: Update = Update@4bd4d2

scala&gt; u() = "Foo"
Singler: Foo

scala&gt; u(3,4) = "Howdy"
2d update
</code></pre>

<p>Scala中Array和HashMap使用update的方式进行设值。使用这种方式我们可以编写和Scala类似特性的库。</p>

<p>Scala的这些特性可以让我们编写更易理解的代码。同时理解Scala的这些语法糖，能更好的与java类库一起协作。</p>

<h4>Case Classes</h4>

<p>Scala has a mechanism for creating classes that have the common stuff filled in. Most of the time, when I define a class, I have to write the toString, hashCode, and equals methods.  These methods are boilerplate. Scala provides the case class mechanism for filling in these blanks as well as support for pattern matching.</p>

<p>A case class provides the same facilities as a normal class, but the compiler generates toString,  hashCode, and  equals methods (which you can override).</p>

<p>Case classes can be instantiated without the use of the  new statement. By default, all the parameters in the case class’s constructor become properties on the case class. Here’s how to create a case class:</p>

<pre><code>scala&gt; case class Stuff(name: String, age: Int)
defined class Stuff

scala&gt; val s = Stuff("David", 45)
s: Stuff = Stuff(David,45)

scala&gt; s.toString
res0: String = Stuff(David,45)

scala&gt; s == Stuff("David", 45) // == 相当于java中的equals
res1: Boolean = true

scala&gt; s == Stuff("David", 42)
res2: Boolean = false

scala&gt; s.name
res4: String = David

scala&gt; s.age
res5: Int = 45
</code></pre>

<p>手写case class功能的类：</p>

<pre><code>class Stuff(val name: String, val age: Int) {
    override def toString = "Stuff(" + name + "," + age + ")"
    override def hashCode = name.hashCode + age
    override def equals(other: AnyRef) = other match {
        case s: Stuff =&gt; this.name == s.name &amp;&amp; this.age = s.age
        case _ =&gt; false
    }
}

object Stuff {
    def apply(name: String, age: Int) = new Stuff(name, age)
    def unapply(s: Stuff) = Some((s.name, s.age))
}
</code></pre>

<h4>Basic Pattern Matching</h4>

<p>模式匹配（Pattern matching）可以使用很少的代码编写非常复杂的判断。Scala Pattern matching和Java switch语句类似， but you can test against almost anything, and you can even assign pieces of the matched value to variables. Like everything in Scala, pattern matching is an expression, so it result s in a value that may be assigned or returned. The most basic pattern matching is like Java’s switch, except there is no  break in each case as the cases do not fall through to each other.</p>

<pre><code>44 match {
    case 44 =&gt; true
    case _ =&gt; false
}
</code></pre>

<p>可以对String进行match操作，类似于C#。</p>

<pre><code>"David" match {
    case "David" =&gt; 45
    case "Elwood" =&gt; 77
    case _ =&gt; 0
}
</code></pre>

<p>可以多case classes进行模式匹配（pattern match）操作。Case classes提供了非常适合与pattern-matching的语法。下面的例子，用于匹配Stuff的name==David以及age==45的对象。</p>

<pre><code>Stuff("David", 45) match {
    case Stuff("David", 45) =&gt; true
    case _ =&gt; false
}
</code></pre>

<p>仅匹配名字：</p>

<pre><code>Stuff("David", 45) match {
    case Stuff("David", _) =&gt; "David"
    case _ =&gt; "Other"
}
</code></pre>

<p>还可以把值提取出来，如把age的值赋给howOld变量：</p>

<pre><code>Stuff("David", 45) match {
    case Stuff("David", howOld) =&gt; "David, age: " + howOld
    case _ =&gt; "Other"
}
</code></pre>

<p>还可以在pattern和=>之间添加条件。如年龄小于30的返回young David，其他的结果为old David。</p>

<pre><code>Stuff("David", 45) match {
    case Stuff("David", age) if age &lt; 30 =&gt; "young David"
    case Stuff("David", _) =&gt; "old David"
    case _ =&gt; "Other"
}
</code></pre>

<p>Pattern matching还可以根据类型进行匹配：</p>

<pre><code>x match {
    case d: java.util.Date =&gt; "The date in milliseconds is " + d.getTime
    case u: java.net.URL =&gt; "The URL path: " + u.getPath
    case s: String =&gt; "String: " + s
    case _ =&gt; "Something else"
}
</code></pre>

<p>如果使用Java代码的话，需要多很多的转换！！</p>

<pre><code>if(x instanceof Date) return "The date in milliseconds is " + ((Date)x).getTime();
if(x instanceof URL) return "The URL path: " + ((URL)x).getPath();
if(x instanceof String) return "String: " + ((String)x);
return "Something else"
</code></pre>

<h4>if/else and while</h4>

<p>while在Scala中比较少用。if/else使用频率高一些，比java的三目赋值操作符（?:）使用频率更高。if和while表达式总是返回Unit（相当于Java的Void）。if/else的返回值更具各个部分表单时类型确定。</p>

<pre><code>if(exp) println("yes")

// multiline
if(exp) {
    println("Line one")
    println("Line two")
}

val i: Int = if(exp) 1 else 3

val i: Int = if(exp) 1 
else {
    val j = System.currentTimeMillis
    (j % 100L).toInt
}
</code></pre>

<p>while executes its code block as long  as its expression evaluates to  true, just like Java. In practice, using recursion, a method calling itself, provides more readab le code and enforces the concept of transforming input to output rather than changing, mutating, variables. Recursive methods can be as efficient as a while loop.</p>

<pre><code>while (exp) println("Working...")
while (exp) {
    println("Working...")
}
</code></pre>

<h4>for</h4>

<pre><code>for { i &lt;- 1 to 3} println(i)

for { i &lt;- 1 to 3
        j &lt;- 1 to 3
    } println(i*j)

def isOdd(in: Int) = in % 2 == 1
for {i &lt;- 1 to 5 if ifOdd(i)} println(i)

for {i &lt;- 1 to 5
        j &lt;- 1 to 5 if isOdd(i*j)} println(i*j)

val lst = (1 to 18 by 3).toList
for {i &lt;- lst if isOdd(i)} yield i

for {i &lt;- lst; j &lt;- lst if isOdd(i*j)} yield i*j
</code></pre>

<p>将在第三章-集合中更详细的讲解for使用方法。</p>

<h4>throw, try/catch/finally, and synchronized</h4>

<p>try/finally的写法和java类似：</p>

<pre><code>throw new Exception("Working...")

try{
    throw new Exception("Working...")
} finally {
    println("This will always be printed")
}
</code></pre>

<p>try/catch的语法不大一样，catch对异常进行了封装，首先它是一个表达式其返回值是一个值；使用case（pattern matched）来匹配异常类型。</p>

<pre><code>try {
    file.write(stuff)
} catch {
    case e: java.io.IOException =&gt; // handle IO Exception
    case n: NullPointerException =&gt; // handle null Exception
}

try { Integer.parseInt("dog") } catch { case _ =&gt; 0 } //0
try { Integer.parseInt("44") } catch { case _ =&gt; 0 } //44
</code></pre>

<p>基于对象的同步操作，每个类都自带了synchronized方法。</p>

<pre><code>obj.synchronized {
    // do something that needs to be serialized
}
</code></pre>

<p>不像java有synchronized方法修饰符。在Scala中同步方法定义使用：</p>

<pre><code>def foo(): Int = synchronized {
    42
}
</code></pre>

<h4>Comments</h4>

<p>注释基本上类C的语言都一样，单行<code>//</code>、多上<code>/* ... */</code>。</p>

<p>在Scala中还可以嵌套的注释。</p>

<pre><code>/*
  This is an outer comment
  /* And this comment
     is nested
  */
  Outer comment
*/
</code></pre>

<h4>Scala vs Java vs Ruby</h4>

<p><strong>类和实例</strong></p>

<p>java有原始类型。Scala中操作都是方法调用，所有东西都是对象，无需为了原始类型而进行额外的判断/处理。</p>

<pre><code>1.hashCode
2.toString
</code></pre>

<p>我们可以定义一个方法，传递函数（从一个Int到另一个Int转换操作）。</p>

<pre><code>def with42(in: Int =&gt; Int) = in(42)
with42( 33 + )
</code></pre>

<p>在语言级别，如果所有东西都是统一的，在进行编程设计时就会很方便和简单。同时Scala编译时会针对JVM原始类型进行优化，使得scala的代码在效率上非常接近Java。</p>

<p><strong>Traits, Interfaces, and Mixins</strong></p>

<p>在java中除了Object对象，其他对象都有一个唯一的父类。Java类可以实现一个或者多个接口（定义实现类必须实现方法的约定）。这是依赖注入和测试mocks，以及其他抽象模式的基础。</p>

<p>Scala使用traits， Traits提供了Java接口拥有的所有特性。同时Traits可以包括方法的实现以及参数的定义。方法实现一次，把所有继承traits的方法混入子类中。</p>

<p><strong>Object, Static, and Singletons</strong></p>

<p>在Java中，可以定义类的（静态）方法和属性，提供了访问方法的唯一入口，同时不需要实例化对象。类（静态）属性提供了在JVM中全局共享数据的方式。
Scala提供了类似的机制：Objects。Objects是单例模式的实现。在类加载的时刻实例化该对象。这种方式同样可以共享全局状态。而且，objects也是Scala完全的面向对象的一种体现，objects是一个类的实例，而不是某种类级别的常量（some class-level constant）。可以把objects作为参数来进行传递。</p>

<p><strong>Functions, Anonymous Inner Class, and  Lambdas/Procs</strong></p>

<p>The Java construct to pass units of computation as parameters to methods is anonymous inner class. 匿名内部类在Swing UI库非常的常见。在Swing中，许多UI事件处理的接口定义1-2个方法，在编写程序时，实现事件接口的内部类能访问外部类的私有成员数据。</p>

<p>Scala functions对应的就是匿名内部类。Scala functions实现了统一的接口，调用函数时执行接口的apply方法。和Java匿名内部类相比，Scala创建函数的语法更加简洁和优雅。同时，访问本地参数的规则也更加灵活。在Java匿名内部类只能访问final的参数，而Scala functions能访问和修改vars参数。</p>

<p>Scala和Ruby的面向对象模型和函数式编程很相似。同时Scala在访问类库和静态类型方面和Java很类似。Scala博采众长，把Java和Ruby的优点都囊括了。</p>

<h3>总结</h3>

<p>这一章首相讲了安装和运行Scala程序，然后围绕Scala编程的语法结构来展开。下一章讲解Scala的数据类型，使用很少的代码编写功能健壮的程序，同时编码量的减少也能有效的控制bugs的数量。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[计算出从1到100之间所有奇数的平方之和]]></title>
    <link href="http://winseliu.com/blog/2014/09/04/scala-quadratic-sum-of-odd-num-in-100/"/>
    <updated>2014-09-04T14:15:40+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/04/scala-quadratic-sum-of-odd-num-in-100</id>
    <content type="html"><![CDATA[<p><a href="http://freewind.github.io/posts/scala-group-entry-problem/">计算出从1到100之间所有奇数的平方之和，代码50字符内（QQ群的验证框长度限制为50）</a>。</p>

<p>如题，题目没啥难度，这50字符的条件莫名的增添压迫感。其实java写也不用50个字符就能搞定的 ！</p>

<pre><code>// (1 to 50) foreach {x =&gt; print("0")}
00000000000000000000000000000000000000000000000000

// java
int sum=0;for(int i=0;i&lt;100;i+=2)sum+=i*i;

// scala
(1 to 100).map(a=&gt;if(a%2==1)a*a else 0).foldLeft(0)(_+_)
(0 to 100).foldLeft(0)(_+((a:Int)=&gt;if(a%2==1)a*a else 0)(_))
var sum=0;for(i&lt;- 1 to 100)if(i%2==1)sum+=i*i
var sum=0;for(i&lt;- 1 to 100; if i%2==1)sum+=i*i

(1 to 100 by 2).foldLeft(0)(_+((a:Int)=&gt;a*a)(_))
(1 to 100 by 2).map(a=&gt;a*a).foldLeft(0)(_+_)
var sum=0;for(i&lt;- 1 to 100 by 2)sum+=i*i
(1 to 100 by 2).map(a=&gt;a*a).reduce(_+_)
</code></pre>

<p><code>(1 to 100 by 2).map(a=&gt;a*a).reduce(_+_)</code>是里面最短的应该也是最好的了，既没有定义变量同时意义清晰一看就懂。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala Shell #! 惊叹号井号]]></title>
    <link href="http://winseliu.com/blog/2014/09/03/linux-shell-shebang-tanjinghao/"/>
    <updated>2014-09-03T12:55:32+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/03/linux-shell-shebang-tanjinghao</id>
    <content type="html"><![CDATA[<p>工作中主要是写java代码，shell也只是用于交互性操作，写脚本的次数比较少。对于<code>#!</code><strong>井号叹号</strong>仅仅是教条式的添加在脚本开头，并且基本上都是<code>#!/bin/sh</code>。</p>

<p>今天在看scala官方的<a href="http://www.scala-lang.org/documentation/getting-started.html">入门教程</a>尽然发现<code>!#</code>的写法，很是困惑，Google查询也不知道怎么描述关键字，一般搜索引擎都把这些操作符过滤掉了的。</p>

<pre><code>#!/bin/sh
exec scala "$0" "$@"
!#
object HelloWorld extends App {
  println("Hello, world!")
}
HelloWorld.main(args)
</code></pre>

<p>首先了解下<code>#!</code>作用：如果<code>#!</code>在脚本的最开始，脚本程序会把第一行的剩余部分当做解析器指令；使用当前的解析器来执行程序，同时把当前脚本的路径作为参数传递给解析器。</p>

<blockquote><p>In computing, a shebang is the character sequence consisting of the characters number sign and exclamation mark (that is, &ldquo;#!&rdquo;) at the beginning of a script.</p>

<p>Under Unix-like operating systems, when a script with a shebang is run as a program, the program loader parses the rest of the script&rsquo;s initial line as an interpreter directive; the specified interpreter program is run instead, passing to it as an argument the path that was initially used when attempting to run the script.</p></blockquote>

<p>如果把<code>!#</code>去掉，再执行上面的脚本则会报错：<strong>error: script file does not close its header with !# or ::!#</strong>，查寻一番后，这原来是Scala的脚本功能的内部处理。通过SourceFile.scala关键字搜索到了<a href="http://www.cnblogs.com/agateriver/archive/2010/09/07/scala_pound_bang.html">该文</a>列出了具体的位置，还有<a href="http://alvinalexander.com/scala/scala-shell-script-example-exec-syntax">A Scala shell script example</a>和我有同样疑问。</p>

<p><img src="http://file.bmob.cn/M00/0B/B1/wKhkA1QGuE-AP-ihAAA1mwvYd5E865.png" alt="" /></p>

<p>可以在《Programing in Scala &ndash; A comprehensive step-by-step guide》一书的附录A中 Scala scripts on Unix and Windows 查找到相应的描述：把<code>#!</code>和<code>!#</code>之间的内容忽略掉了。</p>

<p>语法糖的疑惑解决了，针对上面的脚本还有个问题：exec执行完了，下面的内容不执行了？在exec命令的前面打上调试语句，也只输出了<strong>sh start</strong>。</p>

<pre><code>winse@Lenovo-PC ~
$ cat script.scala
#!/bin/sh
echo 'sh start'
exec scala "$0" "$@"
echo 'sh end'
!#
object HelloWorld extends App {
    print("hello world")
}

HelloWorld.main(args)

winse@Lenovo-PC ~
$ sh script.scala
sh start
hello world
</code></pre>

<blockquote><p>exec 使用 exec 方式运行script时, 它和 source 一样, 也是让 script 在当前process内执行, 但是 process 内的原代码剩下部分将被终止. 同样, process 内的环境随script 改变而改变.</p></blockquote>

<p>所以，整个脚本流程就是：执行shell，调用exec来调用scala的解释器执行整个脚本内容，而解释器会过滤掉<code>#!</code>和<code>!#</code>之间内容，执行完后，exec退出脚本，实现scala脚本执行的功能。这样折中的使用方式，应该是为了处理<strong>参数传递</strong>*的问题！</p>

<h2>参考</h2>

<ul>
<li><a href="http://bbs.chinaunix.net/thread-3583927-1-1.html">井号加叹号的作用是什么</a></li>
<li><a href="http://en.wikipedia.org/wiki/Shebang_%28Unix%29">Shebang (Unix)</a></li>
<li><a href="http://bbs.chinaunix.net/thread-218853-1-1.html">shell 十三問? </a></li>
<li><a href="http://www.cnblogs.com/agateriver/archive/2010/09/07/scala_pound_bang.html">Scala 脚本的 pound bang 魔术</a></li>
<li><a href="http://alvinalexander.com/scala/scala-shell-script-example-exec-syntax">A Scala shell script example (and discussion)</a></li>
<li><a href="http://tldp.org/LDP/abs/html/abs-guide.html">Advanced Bash-Scripting Guide An in-depth exploration of the art of shell scripting</a></li>
<li><a href="http://blog.chinaunix.net/uid-27653755-id-4385938.html">linux中fork, source和exec的区别 </a></li>
<li><a href="http://ss64.com/bash/exec.html">exec</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
