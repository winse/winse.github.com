<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Recommend | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/recommend/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2016-10-13T19:03:29+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[整理] Hadoop入门]]></title>
    <link href="http://winseliu.com/blog/2016/04/23/hadoop-guide-catalog/"/>
    <updated>2016-04-23T15:45:34+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/23/hadoop-guide-catalog</id>
    <content type="html"><![CDATA[<h2>1. 环境准备</h2>

<p>工欲善事其必先利其器。不要吝啬硬件上投入，找一个适合自己的环境！</p>

<ul>
<li>Windows

<ul>
<li><a href="/blog/2014/02/23/quickly-open-program-in-windows/">快速打开程序</a></li>
<li>Cygwin：Windows本地编译需要，执行命令比 cmd 更方便</li>
</ul>
</li>
<li><a href="/blog/2011/02/28/win7-install-fedora-linux/">Windows + Linux双系统</a></li>
<li>Linux

<ul>
<li><a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a></li>
<li><a href="/blog/2015/09/13/review-linux-101-hacks/">【linux 101 Hacks】读后感</a>

<ul>
<li><a href="/images/blogs/linux-101-hacks-review-securecrt-config.png">Socket5代理</a></li>
</ul>
</li>
<li><a href="/blog/2016/03/11/install-and-config-openvpn/">OpenVPN</a></li>
<li>docker

<ul>
<li><a href="/blog/2014/09/27/docker-start-guide-on-centos/">Docker入门</a></li>
<li><a href="/blog/2014/09/30/docker-ssh-on-centos/">配置ssh</a></li>
<li><a href="/blog/2014/10/18/docker-dnsmasq-handler-hosts-build-hadoop-cluster/">Dnsmasq</a></li>
</ul>
</li>
</ul>
</li>
</ul>


<h2>2. 安装部署hadoop/spark</h2>

<h4>编译安装</h4>

<ul>
<li>Hadoop安装与升级:

<ul>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-1-install-in-docker/">Docker中安装</a></li>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-2-hadoop-upgrade/">2.2升级到2.6</a></li>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-3-ha/">HA配置</a></li>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-4-ha-upgrade/">HA升级</a></li>
</ul>
</li>
<li><a href="/blog/2015/03/08/vmware-build-hadoop2-dot-6/">Centos6 Build hadoop2.6</a></li>
<li><a href="/blog/2015/03/09/windows-build-hadoop-2-dot-6/">Windows Build hadoop2.6</a></li>
<li><a href="/blog/2014/10/16/spark-build-and-configuration/">各版本Spark编译/搭建环境</a></li>
</ul>


<h4>功能优化</h4>

<ul>
<li><a href="/blog/2014/09/01/hadoop2-mapreduce-compress/">Hadoop2 Mapreduce输入输出压缩</a></li>
<li><a href="/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading/">Hadoop2 ShortCircuit Local Reading</a></li>
<li><a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>

<ul>
<li><a href="/blog/2016/04/08/snappy-centos5-on-hive-on-spark/">Hive-on-spark Snappy on Centos5</a></li>
</ul>
</li>
<li><a href="/blog/2016/05/05/hdfs-heterogeneous-storage.markdown">HDFS RamDisk内存缓冲</a></li>
</ul>


<h4>维护</h4>

<ul>
<li><a href="/blog/2013/02/22/hadoop-cluster-increases-nodes/">Hadoop集群增加节点</a></li>
<li><a href="/blog/2014/07/29/safely-remove-datanode/">安全的关闭datanode</a></li>
<li><a href="/blog/2015/03/25/deploy-separated-yarn-on-exists-hdfs-cluster/">已有HDFS上部署yarn</a></li>
<li><a href="/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs/">Hadoop不同版本yarn和hdfs混搭，spark-yarn环境配置</a></li>
</ul>


<h4>旧版本安装</h4>

<ul>
<li><a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a></li>
<li><a href="/blog/2013/03/24/pseudo-distributed-hadoop-in-windows/"><del>Windows配置hadoop伪分布式环境(续)</del></a> 不再推荐cygwin下部署Hadoop。</li>
<li><a href="/blog/2013/03/02/quickly-build-a-second-hadoop-cluster/">快速搭建第二个hadoop分布式集群环境</a></li>
<li><a href="/blog/2013/03/27/run-on-hadoop-on-ant/"><del>Ant实现hadoop插件Run-on-Hadoop</del></a></li>
</ul>


<h2>3. 进阶</h2>

<h4>配置深入理解</h4>

<ul>
<li><a href="/blog/2014/08/02/hadoop-datanode-config-should-equals/">Hadoop的datanode数据节点机器配置</a></li>
<li><a href="/blog/2016/03/17/hadoop-memory-opts-and-args/">Hadoop内存环境变量和参数</a></li>
<li><a href="/blog/2016/04/11/spark-on-yarn-memory-allocate/">Spark-on-yarn内存分配</a></li>
<li><a href="/blog/2016/03/25/spark-sql-executors-dynamic-on-yarn/">SparkSQL-on-YARN的Executors池(动态)配置</a></li>
</ul>


<h4>问题定位</h4>

<ul>
<li><a href="/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/">在windows开发测试mapreduce几种方式</a></li>
<li><a href="/blog/2014/04/22/remote-debug-hadoop2/">远程调试hadoop2以及错误处理方法</a></li>
<li><a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">逐步定位Java程序OOM的异常</a></li>
</ul>


<h4>读码</h4>

<ul>
<li>Hadoop2 Balancer磁盘空间平衡

<ul>
<li><a href="/blog/2014/08/06/read-hadoop-balancer-source-part1/">上</a></li>
<li><a href="/blog/2014/09/05/read-hadoop-balancer-source-part2/">中</a></li>
<li><a href="/blog/2014/09/05/read-hadoop-balancer-source-part3/">下</a></li>
</ul>
</li>
<li><a href="/blog/2015/03/13/hadoop-distcp/">Hadoop Distcp</a></li>
</ul>


<h4>其他</h4>

<ul>
<li><a href="/blog/2014/09/12/scala-wordcount-on-hadoop/">Scala Wordcount on Hadoop2</a></li>
<li><a href="/blog/2014/12/07/hadoop-mr-rest-api/">MR Rest接口</a></li>
</ul>


<h2>4. Hadoop平台</h2>

<ul>
<li>zookeeper</li>
<li>hive

<ul>
<li><a href="/blog/2014/06/21/upgrade-hive/">Upgrade Hive: 0.12.0 to 0.13.1</a></li>
<li>tez:

<ul>
<li><a href="/blog/2014/06/18/hadoop-tez-firststep/">Tez编译及使用</a></li>
<li><a href="/blog/2016/01/12/tez-ui-config-and-run/">配置TEZ-UI</a></li>
</ul>
</li>
<li>hive on spark

<ul>
<li><a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a></li>
<li><a href="/blog/2016/04/08/snappy-centos5-on-hive-on-spark/">Hive-on-spark Snappy on Centos5</a></li>
<li><a href="/blog/2016/03/29/limit-on-sparksql-and-hive/">Limit on Sparksql and Hive</a></li>
</ul>
</li>
<li><a href="/blog/2016/04/08/dbcp-parameters/">DBCP参数在Hive JDBC上的实践</a></li>
<li><a href="/blog/2016/04/13/hiveserver2-ui-and-upgrade-hive2-dot-0-0/">Hiveserver2 Ui and Upgrade hive2.0.0</a></li>
</ul>
</li>
<li>kafka

<ul>
<li><a href="/blog/2015/01/08/kafka-guide/">Kafka快速入门</a></li>
</ul>
</li>
<li>alluxio(tachyon)

<ul>
<li><a href="/blog/2015/04/15/tachyon-quickstart/">Tachyon入门指南</a></li>
<li><a href="/blog/2015/04/18/tachyon-deep-source/">Tachyon剖析</a></li>
<li><a href="/blog/2016/04/15/alluxio-quickstart2/">Alluxio入门大全2</a></li>
</ul>
</li>
</ul>


<h2>5. 监控与自动化部署</h2>

<h4>监控</h4>

<ul>
<li><a href="/blog/2013/02/26/linux-top-command-mannual/">top</a></li>
<li>nagios

<ul>
<li><a href="/blog/2015/09/25/nagios-start-guide/">Nagios监控主机</a></li>
</ul>
</li>
<li><del>cacti</del>    Ganglia更简单

<ul>
<li><a href="/blog/2015/09/22/cacti-start-guide/">Cacti监控主机</a></li>
<li><a href="/blog/2015/10/13/cacti-batch-adding-configurations/">Cacti批量添加配置</a></li>
</ul>
</li>
<li>ganglia

<ul>
<li><a href="/blog/2014/07/18/install-ganglia-on-redhat/"><del>Install Ganglia on Redhat5+</del></a> 手动安装依赖太麻烦了！</li>
<li><a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a></li>
<li><a href="/blog/2016/02/01/ganglia-python-extension/">Ganglia扩展-Python</a></li>
<li><a href="/blog/2016/02/25/ganglia-web-ui-views/">Ganglia页自定义视图</a></li>
</ul>
</li>
</ul>


<h4>自动化</h4>

<ul>
<li>git:

<ul>
<li><a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a></li>
<li><a href="/blog/2014/02/19/maven-package-dependent-git-projects/">打包依赖的git项目</a></li>
<li><a href="/blog/2013/05/27/handle-git-conflict/">处理git冲突</a></li>
</ul>
</li>
<li><a href="/blog/2014/09/07/expect-automate-and-batch-config-ssh/">expect-批量实现SSH无密钥登录</a></li>
<li>puppet

<ul>
<li><a href="/blog/2016/04/08/puppet-install/">puppet4.4.1入门安装</a></li>
<li><a href="/blog/2016/04/21/puppet-domain-fdqn/">puppet入门之域名证书</a></li>
<li><a href="/blog/2016/04/21/puppetdb-install-and-config/">puppetdb安装配置</a>

<ul>
<li><a href="/blog/2015/12/13/postgresql-start-guide/">postgresql入门</a></li>
</ul>
</li>
<li>puppet-ui

<ul>
<li><a href="/blog/2016/05/05/puppetboard-install/">puppetboard安装</a></li>
<li><a href="/blog/2016/04/21/puppetexplorer-setting/">puppetexplorer设置</a></li>
<li>foreman</li>
</ul>
</li>
<li><a href="/blog/2016/04/04/rpm-build-your-package/">RPM打包</a></li>
<li>puppet基本使用以及配置集群</li>
<li>mcollective

<ul>
<li><a href="/blog/2016/04/28/mcollective-quick-start/">安装配置</a></li>
<li><a href="/blog/2016/04/28/mcollective-plugins/">插件安装</a></li>
</ul>
</li>
<li><a href="/blog/2016/05/03/hiera-and-facts/">Hiera</a></li>
</ul>
</li>
</ul>


<p>&hellip;</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hive on Spark]]></title>
    <link href="http://winseliu.com/blog/2016/03/28/hive-on-spark/"/>
    <updated>2016-03-28T18:20:46+08:00</updated>
    <id>http://winseliu.com/blog/2016/03/28/hive-on-spark</id>
    <content type="html"><![CDATA[<p>先看官网的资源<a href="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started">Hive on Spark: Getting Started</a> 。文档是值得信任和有保证的，但是有前提：<strong>Spark版本</strong>得是hive/pom.xml中指定的。</p>

<h2>重新编译spark(assembly包中去掉hive、hadoop)</h2>

<p>这里hive-1.2.1用的是spark-1.3.1 !!!</p>

<pre><code>[hadoop@cu2 spark-1.3.1]$ ./make-distribution.sh --name "hadoop2.6.3-without-hive" --tgz --mvn "$(which mvn)" -Pyarn,hadoop-provided,hadoop-2.6,parquet-provided -Dhadoop.version=2.6.3 -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</code></pre>

<p>拷贝打包好的 spark-1.3.1-bin-hadoop2.6.3-without-hive.tgz 到服务器。解压并做一个软链接到spark(或者指定 <strong>SPARK_HOME</strong> 环境变量 )，Hive不遗余力啊，把所有想的jar通过各种办法拿到 ( <code>sparkHome=$(readlink -f $bin/../../spark)</code> )。</p>

<pre><code>[hadoop@hadoop-master2 ~]$ ln -s spark-1.3.1-bin-hadoop2.6.3-without-hive spark

把压缩包传到hdfs，这样每次启动任务就少传几百M的数据。后面spark.yarn.jar配置会用到
[hadoop@hadoop-master2 ~]$ cd spark/lib/
[hadoop@hadoop-master2 lib]$ hadoop fs -put spark-assembly-1.3.1-hadoop2.6.3.jar /spark/
</code></pre>

<p>做好软链接后效果：</p>

<pre><code>[hadoop@hadoop-master2 ~]$ ll | grep -E "hive|spark"
drwxrwxr-x   9 hadoop hadoop 4096 1月  14 08:08 apache-hive-1.2.1-bin
lrwxrwxrwx   1 hadoop hadoop   21 1月  14 08:07 hive -&gt; apache-hive-1.2.1-bin
lrwxrwxrwx   1 hadoop hadoop   40 3月  28 16:38 spark -&gt; spark-1.3.1-bin-hadoop2.6.3-without-hive
drwxrwxr-x  10 hadoop hadoop 4096 3月  28 16:31 spark-1.3.1-bin-hadoop2.6.3-without-hive
drwxrwxr-x  12 hadoop hadoop 4096 3月  25 16:18 spark-1.6.0-bin-2.6.3
drwxrwxr-x  11 hadoop hadoop 4096 3月  28 11:15 spark-1.6.0-bin-hadoop2-without-hive
</code></pre>

<p>这里的spark-1.6.0是教训啊！记住最好最好用hive/pom.xml中spark的版本！！！</p>

<h2>修改hive配置</h2>

<p>由于spark会加载很多的class，需要把permsize调大。</p>

<pre><code>[hadoop@hadoop-master2 ~]$ less ~/hive/conf/hive-env.sh
export HADOOP_OPTS="$HADOOP_OPTS -XX:MaxPermSize=256m -Dhive.home=${HIVE_HOME} "
</code></pre>

<p>在conf目录下增加spark-defaults.conf文件，指定spark的配置。动态资源分配查看：<a href="http://spark.apache.org/docs/1.3.1/job-scheduling.html#dynamic-resource-allocation">dynamic-resource-allocation</a>：</p>

<pre><code>[hadoop@hadoop-master2 conf]$ cat spark-defaults.conf 
spark.yarn.jar    hdfs:///spark/spark-assembly-1.3.1-hadoop2.6.3.jar

spark.dynamicAllocation.enabled    true
spark.shuffle.service.enabled      true
spark.dynamicAllocation.executorIdleTimeout    600
spark.dynamicAllocation.minExecutors    160 
spark.dynamicAllocation.maxExecutors    1800
spark.dynamicAllocation.schedulerBacklogTimeout   5

spark.driver.memory    10g
spark.driver.maxResultSize   0

spark.eventLog.enabled  true
spark.eventLog.compress  true
spark.eventLog.dir    hdfs:///spark-eventlogs
spark.yarn.historyServer.address hadoop-master2:18080


spark.serializer        org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max    512m
</code></pre>

<ul>
<li>minExecutors <strong>最好应该是和datanode机器数量差不多，每台一个executor才能本地计算嘛！</strong></li>
<li>dynamicAllocation需要yarn的配合，具体查看前一篇文章，或者直接看官网的资料。</li>
<li>eventlog查看历史记录需要，配置好后每个任务的信息会存储到eventlog.dir的路径。通过18080端口可以看到历史记录。</li>
</ul>


<h2>跑起来</h2>

<p><code>spark.master</code> 默认是 <strong>yarn-cluster</strong>， 这里先本地(local)跑一下看下效果。然后再改成yarn-cluster/yarn-client就可以了(推荐使用yarn-client，如果yarn-cluster模式AppMaster同时也是Driver，内存比较难控制，日志看起来也麻烦)。</p>

<pre><code>[hadoop@hadoop-master2 hive]$ hive --hiveconf hive.execution.engine=spark 

hive&gt; set spark.master=local;
hive&gt; select count(*) from t_house_info ;
Query ID = hadoop_20160328163952_93dafddc-c8b1-4bc9-b851-5e51f6d26fa8
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&lt;number&gt;
In order to set a constant number of reducers:
  set mapreduce.job.reduces=&lt;number&gt;
Starting Spark Job = 0

Query Hive on Spark job[0] stages:
0
1

Status: Running (Hive on Spark job[0])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
2016-03-28 16:40:02,077 Stage-0_0: 0(+1)/1      Stage-1_0: 0/1
2016-03-28 16:40:03,078 Stage-0_0: 1/1 Finished Stage-1_0: 1/1 Finished
Status: Finished successfully in 2.01 seconds
OK
1
Time taken: 10.169 seconds, Fetched: 1 row(s)
hive&gt; 
</code></pre>

<p>再回过头看其实挺简单，和官方文档中的差不多。</p>

<p>注意：hive的日志级别可以通过 <strong>hive-log4j.properties</strong> 来配置。</p>

<p>有一个问题，不管yarn-cluser还是yarn-client（hive1.2.1-on-spark1.3.1），application强制kill掉以后，再查询会失败，应该是application杀了但是session还在！</p>

<pre><code>[hadoop@file1 ~]$ yarn application -kill application_1460379750886_0012
16/04/13 08:47:17 INFO client.RMProxy: Connecting to ResourceManager at file1/192.168.102.6:8032
Killing application application_1460379750886_0012
16/04/13 08:47:18 INFO impl.YarnClientImpl: Killed application application_1460379750886_0012

    &gt; select count(*) from t_info where edate=20160413;
Query ID = hadoop_20160413084736_ac8f88bb-5ee1-4941-9745-f4a8a504f2f3
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&lt;number&gt;
In order to set a constant number of reducers:
  set mapreduce.job.reduces=&lt;number&gt;
Starting Spark Job = eb7e038a-2db0-45d7-9b0d-1e55d354e5e9
Status: Failed
FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
</code></pre>

<h2>坑坑坑</h2>

<p>刚开始弄的时刻，没管spark的版本的。直接上spark-1.6.0，然后完全跑不通，看hive.log的日志，啥都看不出来。最后查看<a href="http://markmail.org/message/reingwn556e7e37y">http://markmail.org/message/reingwn556e7e37y</a>Hive on Spark的老大邮件列表的回复，把 <strong>spark.master=local</strong> 设置成本地跑才看到一点点有用的错误信息。</p>

<pre><code>hive&gt; set hive.execution.engine=spark;
hive&gt; select count(*) from t_ods_access_log2 where day=20160327;
Query ID = hadoop_20160328083028_a9fb9860-38dc-4288-8415-b5b2b88f920a
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=&lt;number&gt;
In order to set a constant number of reducers:
  set mapreduce.job.reduces=&lt;number&gt;
Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
</code></pre>

<p>日志里面'毛'有用信息都没有！</p>

<p>把日志级别调成debug（hive-log4j.properties），并把 <code>set spark.master=local;</code> 设置成本地。再跑日志：</p>

<pre><code>2016-03-28 15:13:52,549 DEBUG internal.PlatformDependent (Slf4JLogger.java:debug(71)) - Javassist: unavailable
2016-03-28 15:13:52,549 DEBUG internal.PlatformDependent (Slf4JLogger.java:debug(71)) - You don't have Javassist in your class path or you don't have enough permission to load dynamically generated classes.  Please check the configuration for better performance.

2016-03-28 15:14:56,594 DEBUG storage.BlockManager (Logging.scala:logDebug(62)) - Putting block broadcast_0_piece0 without replication took  8 ms
2016-03-28 15:14:56,597 ERROR util.Utils (Logging.scala:logError(95)) - uncaught error in thread SparkListenerBus, stopping SparkContext
java.lang.AbstractMethodError
        at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:62)
        at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
        at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
        at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)
        at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1180)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)
</code></pre>

<p><strong>调用抽象方法</strong>的错误。然后查看了hive-1.2.1中 SparkListener实现类JobMetricsListener 确实没有(spark-1.6.0)62行错误的onBlockUpdated方法实现。然后把spark换成1.3.1一切就好了，其他就是文章前面写的。</p>

<p><strong>心得</strong>: 刚刚开始用一个新东西的时刻，还是安装官网指定的版本来用省心。等到自己熟悉后，在玩其他的。</p>

<h2><strong>hive on spark</strong> VS <strong>SparkSQL</strong> VS <strong>hive on tez</strong></h2>

<p>前一篇已经弄好了SparkSQL，SparkSQL也有thriftserver服务，这里说说为啥还选择搞hive-on-spark：</p>

<ul>
<li>SparkSQL-Thriftserver所有结果全部内存，快是快，但是不能满足查询大量数据的需求。如果查询几千万的数据，SparkSQL是搞不定的。而hive-on-spark除了计算用spark其他逻辑都是hive的，返回的结果会先写hdfs，再慢慢返回给客户端。</li>
<li>SparkSQL-Thriftserver代码的是全部用scala重写的，和已有hive业务不一定兼容！！</li>
<li>SparkSQL-Thriftserver有一个最大的优势就是整个server相当于hive-on-spark的一个session，网页监控漂亮清晰。而hive-on-spark不同的session那就相当于不同的application！！（2016-4-13 20:57:23）用了动态分配，没感觉SparkSQLThriftserver快很多。</li>
<li>SparkSQL由于基于内存，再一些调度方面做了优化。如[limit]: hive是死算，sparksql递增数据量的一次次的试。sparksql可以这么做的，毕竟算好的数据在内存里面放着。</li>
</ul>


<p>hive和sparksql的理念不同，hive的存储是HDFS，而sparksql只是把HDFS作为持久化工具，它的数据基本都放内存。</p>

<p>查看hive的日志，可以看到返回结果后有写HDFS的动作体现，会有类似日志：</p>

<pre><code>2016-03-28 19:39:25,687 INFO  exec.FileSinkOperator (Utilities.java:mvFileToFinalPath(1882))
 - Moving tmp dir: hdfs://zfcluster/hive/scratchdir/hadoop/de2b263e-9601-4df7-bc38-ba932ae83f42/hive_2016-03-28_19-38-08_834_7914607982986605890-1/-mr-10000/.hive-staging_hive_2016-03-28_19-38-08_834_7914607982986605890-1/_tmp.-ext-10001 
 to: hdfs://zfcluster/hive/scratchdir/hadoop/de2b263e-9601-4df7-bc38-ba932ae83f42/hive_2016-03-28_19-38-08_834_7914607982986605890-1/-mr-10000/.hive-staging_hive_2016-03-28_19-38-08_834_7914607982986605890-1/-ext-10001
</code></pre>

<ul>
<li>tez的优势spark都有，并且tez其实缓冲优势并不大。而spark的缓冲效果更明显，而且可以快速返回。例如：你查3万条数据，tez是要全部查询然后再返回的，而sparksql取到3万条其他就不算了（效果看起来是这样子，具体没看源码实现；md hive-on-spark还是会全部跑）。</li>
<li>tez任务缓冲不能共享，spark更加细化，可以有process级别缓冲（就是用上次计算过的结果，加载过的缓冲）！例如，你查数据记录同时又要返回count，这时有些操作是prcess_local级别的，这个tez是不能比的！</li>
<li>spark的日志UI看起来更便捷，呵呵</li>
</ul>


<p>单就从用的角度，spark全面取胜啊。</p>

<h2>参考</h2>

<ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started">https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started</a></li>
<li><a href="http://spark.apache.org/docs/1.3.1/configuration.html">http://spark.apache.org/docs/1.3.1/configuration.html</a></li>
<li><a href="http://spark.apache.org/docs/1.3.1/job-scheduling.html#dynamic-resource-allocation">http://spark.apache.org/docs/1.3.1/job-scheduling.html#dynamic-resource-allocation</a></li>
<li>cloudera-hos优化: <a href="http://www.cloudera.com/documentation/enterprise/latest/topics/admin_hos_tuning.html">http://www.cloudera.com/documentation/enterprise/latest/topics/admin_hos_tuning.html</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装配置Ganglia(2)]]></title>
    <link href="http://winseliu.com/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/"/>
    <updated>2016-01-23T17:47:28+08:00</updated>
    <id>http://winseliu.com/blog/2016/01/23/install-and-config-ganglia-on-redhat-2</id>
    <content type="html"><![CDATA[<p>前一篇介绍了全部手工安装Ganglia的文章，当时安装测试的环境比较简单。按照网上的步骤安装好，看到图了以为就懂了。Ganglia的基本多播/单播的概念都没弄懂。</p>

<p>这次有机会把Ganglia安装到正式环境，由于网络复杂一些，遇到新的问题。也更进一步的了解了Ganglia。</p>

<p>后端Gmetad(ganglia meta daemon)和Gmond(ganglia monitoring daemon)是Ganglia的两个组件。</p>

<p>Gmetad负责收集各个cluster的数据，并更新到rrd数据库中；Gmond把本机的数据UDP广播（或者单播给某台机），同时收集集群节点的数据供Gmetad读取。Gmetad并不用于监控数据的汇总，是对已经采集好的全部数据处理并存储到rrdtool数据库。</p>

<h2>搭建yum环境</h2>

<p>由于正式环境没有提供外网环境，所以需要把安装光盘拷贝到机器，作为yum的本地源。</p>

<pre><code>mount -t iso9660 -o loop rhel-server-6.4-x86_64-dvd\[ED2000.COM\].iso iso/
ln -s iso rhel6.4

vi /etc/yum.repos.d/rhel.repo 
[os]
name = Linux OS Packages
baseurl = file:///opt/rhel6.4
enabled=1
gpgcheck = 0
</code></pre>

<p>再极端点，yum程序都没有安装。到 Packages 目录用 rpm 安装 <code>yum*</code> 。</p>

<p>安装httpd后，把 rhel6.4 源建一个软链接到 <code>/var/www/html/rhel6.4</code> ，其他机器就可以使用该源来进行安装软件了。</p>

<pre><code>cat /etc/yum.repos.d/rhel.repo
[http]
name=LOCAL YUM server
baseurl = http://cu-omc1/rhel6.4
enabled=1
gpgcheck=0
</code></pre>

<p>注意：如果用CentOS的ISO会有两个光盘，两个地址用逗号分隔全部加到baseurl（http方式也一样）：</p>

<pre><code>[centos-local]
name=Centos Local
baseurl=file:///mnt/cdrom,file:///mnt/cdrom2 
failovermethod=priority
enabled=1
gpgcheck=0
</code></pre>

<h2>使用yum安装依赖</h2>

<pre><code>yum install -y gcc gd httpd php php-devel php-mysql php-pear php-common php-gd php-mbstring php-cli 

yum install -y rrdtool 

yum install -y apr*

# 编译Ganglia时加 --with-libpcre=no 可以不安装pcre
yum install -y pcre*

# yum install -y zlib-devel
</code></pre>

<h2>(仅)编译安装Ganglia</h2>

<p>下载下面的软件(yum没有这些软件)：</p>

<ul>
<li><a href="http://rpm.pbone.net/index.php3/stat/4/idpl/15992683/dir/scientific_linux_6/com/rrdtool-devel-1.3.8-6.el6.x86_64.rpm.html">rrdtool-devel-1.3.8-6.el6.x86_64.rpm</a></li>
<li><a href="http://download.savannah.gnu.org/releases/confuse/">confuse-2.7.tar.gz</a></li>
<li><a href="http://sourceforge.net/projects/ganglia/files/ganglia%20monitoring%20core/">ganglia</a></li>
<li><a href="http://sourceforge.net/projects/ganglia/files/ganglia-web/">ganglia-web</a></li>
</ul>


<p>安装：</p>

<pre><code>umask 0022 # 临时修改下，不然后面会遇到权限问题

rpm -ivh rrdtool-devel-1.3.8-6.el6.x86_64.rpm 

# 如果yum可以安装的话：yum install -y libconfuse*
tar zxf confuse-2.7.tar.gz
cd confuse-2.7
./configure CFLAGS=-fPIC --disable-nls
make &amp;&amp; make install

tar zxf ganglia-3.7.2.tar.gz 
cd ganglia-3.7.2
./configure --with-gmetad --enable-gexec --enable-status --prefix=/usr/local/ganglia
# 可选项，用于指定默认配置位置 `-sysconfdir=/etc/ganglia`

make &amp;&amp; make install

cp gmetad/gmetad.init /etc/init.d/gmetad
chkconfig gmetad on
# 查看gmetad的情况
chkconfig --list | grep gm

df -h # 把rrds目录放到最大的分区，再做个链接到data目录下
mkdir -p /data/ganglia/rrds
chown nobody:nobody /data/ganglia/rrds
ln -s /usr/local/ganglia/sbin/gmetad /usr/sbin/gmetad

gmetad -h # 查看默认的config位置。下面步骤AB 二选一 根据是否配置 sysconfdir 选项
# 步骤A
# cp gmetad/gmetad.conf /etc/ganglia/
# 步骤B
vi /etc/init.d/gmetad 
  /usr/local/ganglia/etc/gmetad.conf #修改原来的默认配置路径

cd ganglia-3.7.2/gmond/
ln -s /usr/local/ganglia/sbin/gmond /usr/sbin/gmond
cp gmond.init /etc/init.d/gmond
chkconfig gmond on
chkconfig --list gmond

gmond -h # 查看默认的config位置。
./gmond -t &gt;/usr/local/ganglia/etc/gmond.conf
vi /etc/init.d/gmond 
  /usr/local/ganglia/etc/gmond.conf #修改原来的默认配置路径
</code></pre>

<h2>配置</h2>

<ul>
<li>Ganglia配置</li>
</ul>


<pre><code>vi /usr/local/ganglia/etc/gmetad.conf
  datasource "HADOOP" hadoop-master1
  datasource "CU" cu-ud1
  rrd_rootdir "/data/ganglia/rrds"
  gridname "bigdata"

vi /usr/local/ganglia/etc/gmond.conf
  cluster {
   name = "CU"

  udp_send_channel {
   bind_hostname = yes
</code></pre>

<p><a href="http://ixdba.blog.51cto.com/2895551/1149003">http://ixdba.blog.51cto.com/2895551/1149003</a></p>

<p>Ganglia的收集数据工作可以工作在单播（unicast)或多播(multicast)模式下，默认为多播模式。</p>

<ul>
<li>单播：发送自己 <strong>收集</strong> 到的监控数据到特定的一台或几台机器上，可以跨网段</li>
<li>多播：发送自己收集到的监控数据到同一网段内所有的机器上，同时收集同一网段内的所有机器发送过来的监控数据。因为是以广播包的形式发送，因此需要同一网段内。但同一网段内，又可以定义不同的发送通道。</li>
</ul>


<p>主机多网卡(多IP)情况下需要绑定到特定的IP，设置bind_hostname来设置要绑定的IP地址。单IP情况下可以不需要考虑。</p>

<p>多播情况下只能在单一网段进行，如果集群存在多个网段，可以分拆成多个子集群（data_source)，或者使用单播来进行配置。期望配置简单点的话，配置多个 data_source 。</p>

<ul>
<li><code>data_source "cluster-db" node1 node2</code>  定义集群名称，以及获取集群监控数据的节点。由于采用multicast模式，每台gmond节点都有本集群内节点服务器的所有监控数据，因此不必把所有节点都列出来。node1 node2是or的关系，如果node1无法下载，则才会尝试去node2下载，所以它们应该都是同一个集群的节点，保存着同样的数据。</li>
<li><code>cluster.name</code> 本节点属于哪个cluster，需要与data_source对应。</li>
<li><code>host.location</code> 类似于hostname的作用。</li>
<li><code>udp_send_channel.mcast_join/host</code> 多播地址，工作在239.2.11.71通道下。如果使用单播模式，则要写host=node1，单播模式下可以配置多个upd_send_channel</li>
<li><code>udp_recv_channel.mcast_join</code></li>
</ul>


<p><strong>参考思路</strong> (未具体实践)：多网段情况可以用单播解决，要是单网段要配置多个data_source(集群)那就换个多播的端口吧！</p>

<h2>启动以及测试</h2>

<pre><code>service httpd restart
service gmetad start
service gmond start

[root@cu-omc1 ganglia]# netstat -anp | grep gm
tcp        0      0 0.0.0.0:8649                0.0.0.0:*                   LISTEN      916/gmond           
tcp        0      0 0.0.0.0:8651                0.0.0.0:*                   LISTEN      12776/gmetad        
tcp        0      0 0.0.0.0:8652                0.0.0.0:*                   LISTEN      12776/gmetad        
udp        0      0 239.2.11.71:8649            0.0.0.0:*                               916/gmond           
udp        0      0 192.168.31.11:60126         239.2.11.71:8649            ESTABLISHED 916/gmond           
unix  2      [ ]         DGRAM                    1331526917 12776/gmetad        
[root@cu-omc1 ganglia]# bin/gstat -a
CLUSTER INFORMATION
       Name: CU
      Hosts: 0
Gexec Hosts: 0
 Dead Hosts: 0
  Localtime: Wed Jun 15 20:17:36 2016

There are no hosts up at this time



netstat -anp | grep -E "gmond|gmetad"

# 启动如果有问题，使用调试模式启动查找问题
/usr/sbin/gmetad -d 10

/usr/local/ganglia/bin/gstat -a
/usr/local/ganglia/bin/gstat -a -i hadoop-master1

telnet localhost 8649
telnet localhost 8651
</code></pre>

<p>问题：多播地址绑定失败</p>

<p>如果telnet8649没有数据，查看下route是否有 [hostname对应的IP] 到 [239.2.11.71] 的路由！(多网卡多IP的时刻，可能default的路由并非主机名对应IP的地址)</p>

<blockquote><p><a href="http://llydmissile.blog.51cto.com/7784666/1411239">http://llydmissile.blog.51cto.com/7784666/1411239</a>
<a href="http://www.cnblogs.com/Cherise/p/4350581.html">http://www.cnblogs.com/Cherise/p/4350581.html</a></p>

<p>测试过程中可能会出现以下错误：Error creating multicast server mcast_join=239.2.11.71 port=8649 mcast_if=NULL family=&lsquo;inet4&rsquo;. Will try again&hellip;，系统不支持多播，需要将多播ip地址加入路由表，使用route add -host 239.2.11.71 dev eth0命令即可，将该命令加入/etc/rc.d/rc.local文件中，一劳永逸</p></blockquote>

<pre><code>[root@hadoop-master4 ~]# gmond -d 10
loaded module: core_metrics
loaded module: cpu_module
loaded module: disk_module
loaded module: load_module
loaded module: mem_module
loaded module: net_module
loaded module: proc_module
loaded module: sys_module
udp_recv_channel mcast_join=239.2.11.71 mcast_if=NULL port=8649 bind=239.2.11.71 buffer=0
Error creating multicast server mcast_join=239.2.11.71 port=8649 mcast_if=NULL family='inet4'.  Will try again...
</code></pre>

<p>环境的default route被清理掉了(或者是由于网关和本机不在同一网段)。需要手动添加一条到网卡的route。</p>

<pre><code>[root@hadoop-master4 ~]# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.32.0    *               255.255.255.0   U     0      0        0 bond0
192.168.31.0    192.168.32.254  255.255.255.0   UG    0      0        0 bond0
link-local      *               255.255.0.0     U     1006   0        0 bond0
[root@hadoop-master4 ~]# route add -host 239.2.11.71 dev bond0
[root@hadoop-master4 ~]# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
239.2.11.71     *               255.255.255.255 UH    0      0        0 bond0
192.168.32.0    *               255.255.255.0   U     0      0        0 bond0
192.168.31.0    192.168.32.254  255.255.255.0   UG    0      0        0 bond0
link-local      *               255.255.0.0     U     1006   0        0 bond0
</code></pre>

<h2>安装GWeb</h2>

<pre><code>cd ~/ganglia-web-3.7.1
vi Makefile # 一次性配置好，不再需要去修改conf_default.php
    GDESTDIR = /var/www/html/ganglia
    GCONFDIR = /usr/local/ganglia/etc/
    GWEB_STATEDIR = /var/www/html/ganglia
    # Gmetad rootdir (parent location of rrd folder)
    GMETAD_ROOTDIR = /data/ganglia
    APACHE_USER = apache
make install

# 注意：内网还是需要改下 conf_default.php 一堆jquery的js。
# 如果Web不能访问，查看下防火墙以及SELinux
</code></pre>

<ul>
<li>httpd登录密码配置</li>
</ul>


<pre><code>htpasswd -c /var/www/html/ganglia/etc/htpasswd.users gangliaadmin 

vi /etc/httpd/conf/httpd.conf 

    &lt;Directory "/var/www/html/ganglia"&gt;
    #  SSLRequireSSL
       Options None
       AllowOverride None
       &lt;IfVersion &gt;= 2.3&gt;
          &lt;RequireAll&gt;
             Require all granted
    #        Require host 127.0.0.1

             AuthName "Ganglia Access"
             AuthType Basic
             AuthUserFile /var/www/html/ganglia/etc/htpasswd.users
             Require valid-user
          &lt;/RequireAll&gt;
       &lt;/IfVersion&gt;
       &lt;IfVersion &lt; 2.3&gt;
          Order allow,deny
          Allow from all
    #     Order deny,allow
    #     Deny from all
    #     Allow from 127.0.0.1

          AuthName "Ganglia Access"
          AuthType Basic
          AuthUserFile /var/www/html/ganglia/etc/htpasswd.users
          Require valid-user
       &lt;/IfVersion&gt;
    &lt;/Directory&gt;

service httpd restart
</code></pre>

<p>如果在nginx做权限控制，一样很简单：</p>

<pre><code>location /ganglia {
        proxy_pass http://localhost/ganglia;
        auth_basic "Ganglia Access";
        auth_basic_user_file "/var/www/html/ganglia/etc/htpasswd.users";
}
</code></pre>

<h2>集群配置</h2>

<pre><code>cd /usr/local 
# for h in cu-ud{1,2} hadoop-master{1,2} ; do echo $h ; done
for h in cu-ud1 cu-ud2 hadoop-master1 hadoop-master2 ; do 
    cd /usr/local;
    rsync -vaz  ganglia $h:/usr/local/ ;
    ssh $h ln -s /usr/local/ganglia/sbin/gmond /usr/sbin/gmond ;
    scp /etc/init.d/gmond $h:/etc/init.d/ ;
    ssh $h "chkconfig gmond on" ;
    ssh $h "yum install apr* -y" ; 
    ssh $h "service gmond start" ; 
done

# 不同的集群，gmond.conf的cluster.name需要修改

telnet hadoop-master1 8649
netstat -anp | grep gm
</code></pre>

<p>要是集群有变动，添加还好，删除的话，会存在原来的旧数据，页面会提示机器down掉了。可以删除rrds目录下对应集群中节点的数据，然后重庆gmetad/httpd即可。</p>

<h2>参考</h2>

<h3>内容</h3>

<pre><code>防火墙规则设置
iptables -I INPUT 3 -p tcp -m tcp --dport 80 -j ACCEPT
iptables -I INPUT 3 -p udp -m udp --dport 8649 -j ACCEPT

service iptables save
service iptables restart

关闭selinux
vi /etc/selinux/config
SELINUX=disabled
setenforce 0
</code></pre>

<p>实际应用中，需要监控的机器往往在不同的网段内，这个时候，就不能用gmond默认的多播方式（用于同一个网段内）来传送数据，必须使用单播的方法。</p>

<p>gmond可以配置成为一个cluster，这些gmond节点之间相互发送各自的监控数据。所以每个gmond节点上实际上都会有 cluster内的所有节点的监控数据。gmetad只需要去某一个节点获取数据就可以了。</p>

<p>web front-end 一个基于web的监控界面，通常和Gmetad安装在同一个节点上(还需确认是否可以不在一个节点上，因为php的配置文件中ms可配置gmetad的地址及端口)，它从Gmetad取数据，并且读取rrd数据库，生成图片，显示出来。</p>

<p>gmetad周期性的去gmond节点或者gmetad节点poll数据。一个gmetad可以设置多个datasource，每个datasource可以有多个备份，一个失败还可以去其他host取数据。Gmetad只有tcp通道，一方面他向datasource发送请求，另一方面会使用一个tcp端口，发 布自身收集的xml文件，默认使用8651端口。所以gmetad即可以从gmond也可以从其他的gmetad得到xml数据。</p>

<p>对于IO来说，Gmetad默认15秒向gmond取一次xml数据，如果gmond和gmetad都是在同一个节点，这样就相当于本地io请求。同时gmetad请求完xml文件后，还需要对其解析，也就是说按默认设置每15秒需要解析一个10m级别的xml文件，这样cpu的压力就会很大。同时它还有写入RRD数据库，还要处理来自web客户端的解析请求，也会读RRD数据库。这样本身的IO CPU 网络压力就很大，因此这个节点至少应该是个空闲的而且能力比较强的节点。</p>

<ul>
<li>多播模式配置
这个是默认的方式，基本上不需要修改配置文件，且所有节点的配置是一样的。这种模式的好处是所有的节点上的 gmond 都有完备的数据，gmetad 连接其中任意一个就可以获取整个集群的所有监控数据，很方便。
其中可能要修改的是 mcast_if 这个参数，用于指定多播的网络接口。如果有多个网卡，要填写对应的内网接口。</li>
<li>单播模式配置
监控机上的接收 Channel 配置。我们使用 UDP 单播模式，非常简单。我们的集群有部分机器在另一个机房，所以监听了 0.0.0.0，如果整个集群都在一个内网中，建议只 bind 内网地址。如果有防火墙，要打开相关的端口。</li>
<li>最重要的配置项是 data_source: <code>data_source "my-cluster" localhost:8648</code> 如果使用的是默认的 8649 端口，则端口部分可以省略。如果有多个集群，则可以指定多个 data_source，每行一个。</li>
<li>最后是 gridname 配置，用于给整个 Grid 命名</li>
<li><a href="https://github.com/ganglia/gmond_python_modules">https://github.com/ganglia/gmond_python_modules</a></li>
</ul>


<h3>网址</h3>

<ul>
<li><a href="http://yhz.me/blog/Install-Ganglia-On-CentOS.html">在 CentOS 6.5 上安装 Ganglia 3.6.0</a></li>
<li>*<a href="http://ixdba.blog.51cto.com/2895551/1149003">分布式监控系统ganglia配置文档</a></li>
<li><p>*<a href="http://www.3mu.me/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%80%E6%BA%90%E7%9B%91%E6%8E%A7%E8%BD%AF%E4%BB%B6ganglia-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/">企业级开源监控软件Ganglia 安装与配置</a></p></li>
<li><p>*<a href="http://jerrypeng.me/2014/07/04/server-side-java-monitoring-ganglia/">Java 服务端监控方案（二. Ganglia 篇）</a></p></li>
<li><a href="http://jerrypeng.me/2014/07/22/server-side-java-monitoring-nagios/">Java 服务端监控方案（三. Nagios 篇）</a></li>
<li><p><a href="https://github.com/ganglia/ganglia-web/wiki/Nagios-Integration">https://github.com/ganglia/ganglia-web/wiki/Nagios-Integration</a></p></li>
<li><p><a href="https://ganglia.wikimedia.org/latest/">维基百科Ganglia</a></p></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Supervisor安装配置]]></title>
    <link href="http://winseliu.com/blog/2015/08/24/manual-install-supervisor/"/>
    <updated>2015-08-24T16:24:25+08:00</updated>
    <id>http://winseliu.com/blog/2015/08/24/manual-install-supervisor</id>
    <content type="html"><![CDATA[<ul>
<li><a href="https://en.wikipedia.org/wiki/Process_supervision">进程监管</a>

<ul>
<li><a href="http://supervisord.org/">supervisord</a></li>
<li><a href="http://www.skarnet.org/software/s6/">s6</a></li>
</ul>
</li>
</ul>


<p>supervisor使用ini的方式配置其实挺讨厌的，但有一点好的就是它不需要配置为boot进程（process id 1）。</p>

<h2>在线安装</h2>

<ul>
<li><a href="http://supervisord.org/installing.html">http://supervisord.org/installing.html</a></li>
</ul>


<p>下载 <a href="https://pypi.python.org/pypi/supervisor">https://pypi.python.org/pypi/supervisor</a> ，并安装:</p>

<pre><code># yum的supervisor版本太低(2.1)了，使用tar.gz源码来安装
[root@cu2 supervisor-3.2.3]# yum list supervisor
Loaded plugins: fastestmirror, priorities
Loading mirror speeds from cached hostfile
 * epel: ftp.cuhk.edu.hk
Available Packages
supervisor.noarch                                                                              2.1-9.el6                                                                               epel


# 不支持python3，安装之前先保证安装有python2.4+
[root@cu2 ~]# python --version
Python 2.6.6
[root@cu2 ~]# cd supervisor-3.2.3/
[root@cu2 supervisor-3.2.3]# python setup.py install
...
creating dist
creating 'dist/supervisor-3.2.3-py2.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it
removing 'build/bdist.linux-x86_64/egg' (and everything under it)
Processing supervisor-3.2.3-py2.6.egg
creating /usr/lib/python2.6/site-packages/supervisor-3.2.3-py2.6.egg
Extracting supervisor-3.2.3-py2.6.egg to /usr/lib/python2.6/site-packages
Adding supervisor 3.2.3 to easy-install.pth file
Installing echo_supervisord_conf script to /usr/bin
Installing pidproxy script to /usr/bin
Installing supervisorctl script to /usr/bin
Installing supervisord script to /usr/bin

Installed /usr/lib/python2.6/site-packages/supervisor-3.2.3-py2.6.egg
Processing dependencies for supervisor==3.2.3
Searching for meld3&gt;=0.6.5
Reading http://pypi.python.org/simple/meld3/
Best match: meld3 1.0.2
Downloading https://pypi.python.org/packages/45/a0/317c6422b26c12fe0161e936fc35f36552069ba8e6f7ecbd99bbffe32a5f/meld3-1.0.2.tar.gz#md5=3ccc78cd79cffd63a751ad7684c02c91
Processing meld3-1.0.2.tar.gz
Running meld3-1.0.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-nMSEmq/meld3-1.0.2/egg-dist-tmp-vkwrOp
zip_safe flag not set; analyzing archive contents...
Adding meld3 1.0.2 to easy-install.pth file

Installed /usr/lib/python2.6/site-packages/meld3-1.0.2-py2.6.egg
Finished processing dependencies for supervisor==3.2.3
</code></pre>

<h2>(离线安装)下载依赖以及安装包，并安装</h2>

<p>安装官网的版本下载: <a href="http://supervisord.org/installing.html#installing-to-a-system-without-internet-access">Installing To A System Without Internet Access</a></p>

<pre><code>python -V

tar zxvf setuptools-18.2.tar.gz 
cd setuptools-18.2
python setup.py install

tar zxvf meld3-0.6.5.tar.gz 
cd meld3-0.6.5
python setup.py install

tar zxvf elementtree-1.2.6-20050316.tar.gz 
cd elementtree-1.2.6-20050316
python setup.py install

tar zxvf supervisor-3.1.3.tar.gz 
cd supervisor-3.1.3
python setup.py  install
</code></pre>

<h2>启动</h2>

<pre><code># http://supervisord.org/installing.html#creating-a-configuration-file
[root@cu2 supervisor-3.2.3]# echo_supervisord_conf &gt;/etc/supervisord.conf
[root@cu2 supervisor-3.2.3]# supervisord
/usr/lib/python2.6/site-packages/supervisor-3.2.3-py2.6.egg/supervisor/options.py:296: UserWarning: Supervisord is running as root and it is searching for its configuration file in default locations (including its current working directory); you probably want to specify a "-c" argument specifying an absolute path to a configuration file for improved security.
  'Supervisord is running as root and it is searching '

[root@cu2 supervisor-3.2.3]# ps aux | grep supervisor | grep -v grep
root     63599  0.0  0.1 196600 10404 ?        Ss   16:57   0:00 /usr/bin/python /usr/bin/supervisord

[root@cu2 supervisor-3.2.3]# supervisorctl 
supervisor&gt; help

default commands (type help &lt;topic&gt;):
=====================================
add    exit      open  reload  restart   start   tail   
avail  fg        pid   remove  shutdown  status  update 
clear  maintail  quit  reread  signal    stop    version

supervisor&gt; pid
63599
supervisor&gt; shutdown
Really shut the remote supervisord process down y/N? y
Shut down
supervisor&gt; exit
</code></pre>

<p>ctl命令查看状态：</p>

<ul>
<li>reread: Reload the daemon&rsquo;s configuration files</li>
<li><strong>update</strong>: Reload config and add/remove as necessary</li>
<li>reload: Restart the remote supervisord.</li>
<li>pid: supervisord的进程号</li>
<li>status： Get all process status info</li>
<li>avail： Display all configured processes</li>
</ul>


<p>网页管理：</p>

<p><img src="/images/blogs/supervisord-web.png" alt="" /></p>

<h2>配置</h2>

<p><a href="http://supervisord.org/configuration.html">http://supervisord.org/configuration.html</a></p>

<p>默认server(supervisord)-client(supervisorctl)通过 <code>unix domain socket</code> 文件的方式来通信，为了方便网页查看<strong>同时</strong>开启web配置 <code>inet_http_server</code>。</p>

<p>program区域的配置要细读，主要配置工作都在这个上面。被管理的程序 <strong>不能后台运行</strong> (例如：java程序不要加 <code>nohup</code> 以及 <code>&amp;</code> )！！</p>

<blockquote><p>Controlled programs should themselves not be daemons, as supervisord assumes it is responsible for daemonizing its subprocesses</p></blockquote>

<pre><code>[root@cu2 supervisor-3.2.3]# vi /etc/supervisord.conf 
...
[inet_http_server]         ; inet (TCP) server disabled by default
port=0.0.0.0:9001        ; (ip_address:port specifier, *:port for all iface)
...
[include]
files = /etc/supervisord.d/*.ini


[root@cu2 ~]# cat /etc/supervisord.d/redis.ini
[program:redis]
command=/home/hadoop/redis/bin/redis-server /home/hadoop/redis/redis.conf --port 1637%(process_num)01d
process_name=%(program_name)s_1637%(process_num)01d
numprocs=4
numprocs_start=0
priority=1
autostart=true
startsecs=0
startretries=3
autorestart=true
directory=/home/hadoop/redis

[root@cu2 ~]# supervisorctl shutdown
Shut down
[root@cu2 ~]# 
[root@cu2 ~]# supervisord

[root@cu2 ~]# ps aux | grep redis
root     50458  0.1  0.0 137444  2384 ?        Sl   18:14   0:00 /home/hadoop/redis/bin/redis-server *:16372                                   
root     50460  0.0  0.0 137444  2380 ?        Sl   18:14   0:00 /home/hadoop/redis/bin/redis-server *:16373                                   
root     50461  0.0  0.0 137444  2384 ?        Sl   18:14   0:00 /home/hadoop/redis/bin/redis-server *:16370                                   
root     50462  0.1  0.0 137444  2388 ?        Sl   18:14   0:00 /home/hadoop/redis/bin/redis-server *:16371                                   
root     51853  0.0  0.0 103248   888 pts/2    S+   18:14   0:00 grep redis

# 测试：随便kill掉一个，再ps查看，进程没少，但是刚刚kill掉的redis-server进程号变了

# 可以通过supervisord.log查看启动的日志。默认在/tmp下面
[root@cu2 tmp]# less supervisord.log 

# 如果在 配置program块 没有指定stdout和stderr的位置，可以在/tmp下找到对应的日志文件：
[root@cu2 tmp]# ll | grep redis
-rw------- 1 root   root      0 May  2 18:14 redis_16370-stderr---supervisor-pZ5pzl.log
-rw------- 1 root   root   2649 May  2 18:17 redis_16370-stdout---supervisor-GyrHYw.log
-rw------- 1 root   root      0 May  2 18:14 redis_16371-stderr---supervisor-XfyvMv.log
-rw------- 1 root   root   5298 May  2 18:20 redis_16371-stdout---supervisor-NOmTQJ.log
-rw------- 1 root   root      0 May  2 18:14 redis_16372-stderr---supervisor-SHMEi3.log
-rw------- 1 root   root   4967 May  2 18:19 redis_16372-stdout---supervisor-fvKDa8.log
-rw------- 1 root   root      0 May  2 18:14 redis_16373-stderr---supervisor-ncUWuE.log
-rw------- 1 root   root   2326 May  2 18:14 redis_16373-stdout---supervisor-ZxiqIj.log
</code></pre>

<h2>supervisor服务</h2>

<ul>
<li><a href="https://github.com/Supervisor/initscripts/blob/master/redhat-init-equeffelec">https://github.com/Supervisor/initscripts/blob/master/redhat-init-equeffelec</a></li>
<li><a href="https://github.com/Supervisor/initscripts/blob/master/redhat-sysconfig-equeffelec">https://github.com/Supervisor/initscripts/blob/master/redhat-sysconfig-equeffelec</a></li>
</ul>


<pre><code># 调整下启动脚本 start方法 中 --pidfile 的位置
[root@cu2 ~]# ll /etc/init.d/supervisord 
-rwxr-xr-x 1 root root 2977 May  2 19:29 /etc/init.d/supervisord
[root@cu2 ~]# grep daemon /etc/init.d/supervisord 
    daemon $supervisord --pidfile=${pidfile} $OPTIONS
[root@cu2 ~]# ll /etc/sysconfig/supervisord 
-rw-r--r-- 1 root root 723 May  2 19:17 /etc/sysconfig/supervisord

[root@cu2 ~]# service supervisord status
supervisord (pid  39549) is running...
</code></pre>

<h2>参考</h2>

<ul>
<li><a href="http://supervisord.org/installing.html#installing-to-a-system-without-internet-access">http://supervisord.org/installing.html#installing-to-a-system-without-internet-access</a></li>
<li><a href="http://supervisord.org/configuration.html#programx-section">http://supervisord.org/configuration.html#programx-section</a></li>
<li>工具使用的文章

<ul>
<li>supervisor

<ul>
<li><a href="http://supervisord.org/installing.html">http://supervisord.org/installing.html</a></li>
<li><a href="http://blog.csdn.net/heyjackie/article/details/12995187">http://blog.csdn.net/heyjackie/article/details/12995187</a></li>
</ul>
</li>
<li>s6 <a href="https://blog.tutum.co/2014/12/02/docker-and-s6-my-new-favorite-process-supervisor/">https://blog.tutum.co/2014/12/02/docker-and-s6-my-new-favorite-process-supervisor/</a></li>
<li>daemontools版本太老了，用s6吧

<ul>
<li><a href="https://isotope11.com/blog/manage-your-services-with-daemontools">https://isotope11.com/blog/manage-your-services-with-daemontools</a></li>
<li><a href="http://wrox.cn/article/100007143/">http://wrox.cn/article/100007143/</a></li>
</ul>
</li>
<li>runit <a href="http://jtimberman.housepub.org/blog/2012/12/29/process-supervision-solved-problem/">http://jtimberman.housepub.org/blog/2012/12/29/process-supervision-solved-problem/</a></li>
</ul>
</li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[编译/搭建Spark环境]]></title>
    <link href="http://winseliu.com/blog/2014/10/16/spark-build-and-configuration/"/>
    <updated>2014-10-16T20:02:46+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/16/spark-build-and-configuration</id>
    <content type="html"><![CDATA[<p>记录spark编译和打包成tar的整个流程。包括各个版本的编译过程，使用make-distribution脚本打包，搭建本地、standalone、yarn的spark环境。</p>

<ul>
<li>2016-1 spark-1.6.0</li>
<li>2015-04 【Spark-1.3.0】单独附在最后，添加了spark-sql功能使用和spark-HA的配置</li>
</ul>


<h2>编译和打包</h2>

<ul>
<li>spark-1.6.0</li>
</ul>


<pre><code>// java version "1.7.0_17" &amp; Apache Maven 3.3.9 &amp; CentOS release 6.6 (Final)
[hadoop@cu2 spark-1.6.0]$ export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
[hadoop@cu2 spark-1.6.0]$ mvn package eclipse:eclipse -Phadoop-2.6 -Dhadoop.version=2.6.3 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests

[hadoop@cu2 spark-1.6.0]$ vi make-distribution.sh 
BUILD_COMMAND=("$MVN" package -DskipTests $@)

[hadoop@cu2 spark-1.6.0]$ ./make-distribution.sh --tgz --mvn "$(which mvn)"  -Dhadoop-2.6 -Dhadoop.version=2.6.3 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests 
[hadoop@cu2 spark-1.6.0]$ ll spark-1.6.0-bin-2.6.3.tgz 

// examples
[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ export HADOOP_CONF_DIR=~/hadoop-2.6.3/etc/hadoop
[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client lib/spark-examples-1.6.0-hadoop2.6.3.jar 10

[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ export HADOOP_CONF_DIR=~/hadoop-2.6.3/etc/hadoop
[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ export SPARK_PRINT_LAUNCH_COMMAND=true
// export HADOOP_ROOT_LOGGER=DEBUG,console Spark的脚本不认这个变量
[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ bin/spark-submit --master yarn --deploy-mode client --class org.apache.spark.examples.streaming.HdfsWordCount lib/spark-examples-1.6.0-hadoop2.6.3.jar /data

// --driver-java-options "-Dhadoop.root.logger=WARN,console" 
// --driver-java-options "-Dhadoop.root.logger=WARN,console -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"

// org.apache.spark.deploy.yarn.Client#copyFileToRemote
// --conf "spark.yarn.jar=hdfs://hadoop-master2:9000/spark-assembly-1.6.0-hadoop2.6.3.jar"

// http://spark.apache.org/docs/latest/running-on-yarn.html
</code></pre>

<ul>
<li>spark-1.5</li>
</ul>


<pre><code>-- jdk8-x64 &amp; spark-1.5.2 &amp; maven-3.3.9
set or export MAVEN_OPTS=-Xmx2g
mvn package eclipse:eclipse -Phadoop-2.6 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
-- 注释掉pom.xml中的&lt;useZincServer&gt;true&lt;/useZincServer&gt; @see http://stackoverflow.com/questions/31844848/building-spark-with-maven-error-finding-javac-but-path-is-correct
-- 公司网络不稳定，遇到下载maven包报错，多重试几次！！
</code></pre>

<ul>
<li>spark-1.4.1</li>
</ul>


<pre><code>[hadoop@cu2 spark-1.4.1]$ export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"

[hadoop@cu2 spark-1.4.1]$ mvn package -Phadoop-2.6 -Dhadoop.version=2.7.1 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests

-- 打包：
-- // 修改BUILD_COMMAND变量
[hadoop@cu2 spark-1.4.1]$ vi make-distribution.sh 
BUILD_COMMAND=("$MVN"  package -DskipTests $@)

[hadoop@cu2 spark-1.4.1]$ ./make-distribution.sh --mvn `which mvn` --tgz  --skip-java-test   -Phadoop-2.6 -Dhadoop.version=2.7.1 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</code></pre>

<ul>
<li>spark-1.1.0</li>
</ul>


<p>官网提供的hadoop版本没有2.5的。这里我自己下载源码再进行编译。先下载spark-1.1.0.tgz，解压然后执行命令编译：</p>

<pre><code>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive -X -DskipTests clean package

-- mvn package eclipse:eclipse -Phadoop-2.2 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</code></pre>

<p>注意事项：用64位的JDK！！加上maven参数，不然很可能出现OOM（甚至各种稀奇古怪的问题）。编译的时间也挺长的，可以先去吃个饭。或者取消一些功能的编译（如hive）。</p>

<p>编译完后，在assembly功能下会生成包括所有spark及其依赖的jar文件。</p>

<pre><code>[root@docker scala-2.10]# cd spark-1.1.0/assembly/target/scala-2.10/
[root@docker scala-2.10]# ll -h
total 135M
-rw-r--r--. 1 root root 135M Oct 15 21:18 spark-assembly-1.1.0-hadoop2.5.1.jar
</code></pre>

<p>打包:</p>

<p>上面我们已经编译好了spark程序，这里对其进行打包集成到一个压缩包。使用程序自带的make-distribution.sh即可。</p>

<p>为了减少重新编译的巨长的等待时间，修改下脚本<code>make-distribution.sh</code>的maven编译参数，去掉maven的clean阶段操作（最好直接注释掉mvn那行），修改最终结果如下：</p>

<pre><code>#BUILD_COMMAND="mvn clean package -DskipTests $@"
BUILD_COMMAND="mvn package -DskipTests $@"
</code></pre>

<p>然后执行命令：</p>

<pre><code>[root@docker spark-1.1.0]# sh -x make-distribution.sh --tgz  --skip-java-test -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive 
[root@docker spark-1.1.0]# ll -h
total 185M
...
-rw-r--r--. 1 root root 185M Oct 16 00:09 spark-1.1.0-bin-2.5.1.tgz
</code></pre>

<p>最终会在目录行打包生成tgz的文件。</p>

<h2>本地运行</h2>

<p>把本机ip主机名写入到hosts，方便以后windows本机查看日志</p>

<pre><code>[root@docker spark-1.1.0-bin-2.5.1]# echo 192.168.154.128 docker &gt;&gt; /etc/hosts
[root@docker spark-1.1.0-bin-2.5.1]# cat /etc/hosts
...
192.168.154.128 docker
</code></pre>

<ul>
<li>运行helloworld：</li>
</ul>


<pre><code>[root@docker spark-1.1.0-bin-2.5.1]# bin/run-example SparkPi 10
Spark assembly has been built with Hive, including Datanucleus jars on classpath
...
14/10/16 00:22:36 INFO SparkContext: Job finished: reduce at SparkPi.scala:35, took 2.848632007 s
Pi is roughly 3.139344
14/10/16 00:22:36 INFO SparkUI: Stopped Spark web UI at http://docker:4040
...
</code></pre>

<ul>
<li>交互式操作：</li>
</ul>


<pre><code>[root@docker spark-1.1.0-bin-2.5.1]# bin/spark-shell --master local[2]
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_60)
...
14/10/16 00:25:57 INFO SparkUI: Started SparkUI at http://docker:4040
14/10/16 00:25:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/10/16 00:25:58 INFO Executor: Using REPL class URI: http://192.168.154.128:39385
14/10/16 00:25:58 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@docker:57417/user/HeartbeatReceiver
14/10/16 00:25:58 INFO SparkILoop: Created spark context..
Spark context available as sc.

scala&gt; 
</code></pre>

<p>说明下环境：我使用windows作为开发环境，使用虚拟机中的linux作为测试环境。同时通过ssh连接的隧道来实现windows无缝的访问虚拟机linux操作系统（可以通过浏览器socket5代理查看web页面）。</p>

<p>启动交互式访问后，就可以通过浏览器访问4040查看spark程序的状态。</p>

<p><img src="http://file.bmob.cn/M00/1E/4B/wKhkA1Q_3NOALefuAAEimqVy6-s418.png" alt="" /></p>

<p>任务已经启动，接下来就可以进行操作：</p>

<pre><code>scala&gt; val textFile=sc.textFile("README.md")
textFile: org.apache.spark.rdd.RDD[String] = README.md MappedRDD[1] at textFile at &lt;console&gt;:12

scala&gt; textFile.count()
res0: Long = 141

scala&gt; textFile.first()
res1: String = # Apache Spark

scala&gt; val linesWithSpark = textFile.filter(line=&gt;line.contains("Spark"))
linesWithSpark: org.apache.spark.rdd.RDD[String] = FilteredRDD[2] at filter at &lt;console&gt;:14

scala&gt; textFile.filter(line=&gt;line.contains("Spark")).count()
res2: Long = 21

scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)
res3: Int = 15

scala&gt; import java.lang.Math
import java.lang.Math

scala&gt; textFile.map(_.split(" ").size).reduce((a,b)=&gt;Math.max(a,b))
res4: Int = 15

scala&gt; val wordCounts = textFile.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_)
wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at &lt;console&gt;:15

scala&gt; wordCounts.collect()
res5: Array[(String, Int)] = Array((means,1), (under,2), (this,4), (Because,1), (Python,2), (agree,1), (cluster.,1), (its,1), (follows.,1), (general,2), (have,2), (YARN,,3), (pre-built,1), (locally.,1), (locally,2), (changed,1), (MRv1,,1), (several,1), (only,1), (sc.parallelize(1,1), (This,2), (learning,,1), (basic,1), (requests,1), (first,1), (Configuration,1), (MapReduce,2), (CLI,1), (graph,1), (without,1), (documentation,1), ("yarn-client",1), ([params]`.,1), (any,2), (setting,2), (application,1), (prefer,1), (SparkPi,2), (engine,1), (version,3), (file,1), (documentation,,1), (&lt;http://spark.apache.org/&gt;,1), (MASTER,1), (entry,1), (example,3), (are,2), (systems.,1), (params,1), (scala&gt;,1), (provides,1), (refer,1), (MLLib,1), (Interactive,2), (artifact,1), (configure,1), (can,8), (&lt;art...
</code></pre>

<p>执行了上面一些操作后，通过网页查看状态变化：</p>

<p><img src="http://file.bmob.cn/M00/1E/4C/wKhkA1Q_3w6AM6njAAF-MCCYh2s170.png" alt="" /></p>

<h2>Spark-standalone集群</h2>

<p>部署集群需要用到多个服务器，这里我使用docker来进行部署。</p>

<p>本来应该早早完成本文的实践，但是在搭建docker-hadoop集群时花费了很多的时间。关于搭建集群dnsmasq处理域名问题参见下一篇文章。
最终实现可以参考：<a href="https://github.com/winse/docker-hadoop/tree/spark-yarn">docker-hadoop</a></p>

<pre><code>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver1 -h slaver1 spark-yarn
[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver2 spark-yarn
[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name master -h master spark-yarn

[root@docker docker-hadoop]# docker ps | grep spark | awk '{print $1}' | xargs -I{} docker inspect -f ' ' {} &gt; /etc/dnsmasq.hosts
[root@docker docker-hadoop]# cat /etc/dnsmasq.hosts 
172.17.0.29 master
172.17.0.28 slaver2
172.17.0.27 slaver1
[root@docker docker-hadoop]# service dnsmasq restart
[root@docker docker-hadoop]# ssh hadoop@master

[hadoop@master ~]$ ssh-copy-id master
[hadoop@master ~]$ ssh-copy-id localhost
[hadoop@master ~]$ ssh-copy-id slaver1
[hadoop@master ~]$ ssh-copy-id slaver2
[hadoop@master spark-1.1.0-bin-2.5.1]$ sbin/start-all.sh 
[hadoop@master spark-1.1.0-bin-2.5.1]$ /opt/jdk1.7.0_67/bin/jps  -m
266 Jps -m
132 Master --ip master --port 7077 --webui-port 8080
</code></pre>

<p>通过网页可以查看集群的状态：</p>

<p><img src="http://file.bmob.cn/M00/1E/F8/wKhkA1RClV2AE0biAAEmpXJlzTc914.png" alt="" /></p>

<p>运行任务连接到master：</p>

<pre><code>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-shell --master spark://master:7077
...
14/10/17 11:31:08 INFO BlockManagerMasterActor: Registering block manager slaver2:55473 with 265.4 MB RAM
14/10/17 11:31:09 INFO BlockManagerMasterActor: Registering block manager slaver1:33441 with 265.4 MB RAM

scala&gt; 
</code></pre>

<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmG-AO--XAAD84ATrCew955.png" alt="" /></p>

<p>从上图可以看到，程序已经正确连接到spark集群，master为driver，任务节点为slaver1和slaver2。下面运行下程序，然后通过网页查看运行的状态。</p>

<pre><code>scala&gt; val textFile=sc.textFile("README.md")
scala&gt; textFile.count()
scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)
</code></pre>

<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmdmAB3M9AAFIzMb4yk0370.png" alt="" /></p>

<p>系统安装好了，启动spark-standalone集群和hadoop-yarn一样。配置ssh、java，然后启动，配合网页8080/4040可以实时的了解任务的指标。</p>

<h2>yarn集群</h2>

<p>注意：如果你是按照前面的步骤来操作的，需要先把spark-standalone的集群停掉。端口8080和yarn web使用端口冲突，会导致yarn启动失败。</p>

<p>修改spark-env.sh，添加HADOOP_CONF_DIR参数。然后提交任务到yarn上执行就行了。</p>

<pre><code>[hadoop@master spark-1.1.0-bin-2.5.1]$ cat conf/spark-env.sh
#!/usr/bin/env bash

JAVA_HOME=/opt/jdk1.7.0_67 

HADOOP_CONF_DIR=/opt/hadoop-2.5.1/etc/hadoop

[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.1.0-hadoop2.5.1.jar  10
</code></pre>

<p><img src="http://file.bmob.cn/M00/1E/FD/wKhkA1RCszeAALCPAAK1Nzk6faQ330.png" alt="" /></p>

<p>运行的结果输出在driver的slaver2节点，对应输出型来说不是很直观。spark-yarn提供了另一种方式，driver直接本地运行<em>yarn-client</em>。</p>

<pre><code>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client lib/spark-examples-1.1.0-hadoop2.5.1.jar  10
...
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8248 ms on slaver1 (1/10)
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 231 ms on slaver1 (2/10)
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 158 ms on slaver1 (3/10)
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 284 ms on slaver1 (4/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 175 ms on slaver1 (5/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 301 ms on slaver1 (6/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 175 ms on slaver1 (7/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 143 ms on slaver1 (8/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 164 ms on slaver1 (9/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO cluster.YarnClientSchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@slaver2:51923/user/Executor#1132577949] with ID 1
14/10/17 13:31:04 INFO util.RackResolver: Resolved slaver2 to /default-rack
14/10/17 13:31:04 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 397 ms on slaver1 (10/10)
14/10/17 13:31:04 INFO cluster.YarnClientClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
14/10/17 13:31:04 INFO scheduler.DAGScheduler: Stage 0 (reduce at SparkPi.scala:35) finished in 26.084 s
14/10/17 13:31:04 INFO spark.SparkContext: Job finished: reduce at SparkPi.scala:35, took 28.31400558 s
Pi is roughly 3.140248
</code></pre>

<p>thrift连接yarn运行时时受容器内存最大值限制，需要修改yarn-site.xml。</p>

<pre><code>cat yarn-site.xml 
&lt;property&gt;
  &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
  &lt;value&gt;32000&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
  &lt;value&gt;32768&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
  &lt;value&gt;2048&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;
  &lt;value&gt;32768&lt;/value&gt;
&lt;/property&gt;

./sbin/start-thriftserver.sh --executor-memory 29g --master yarn-client
</code></pre>

<p>不能把executor-memory的内存设置为等于最大值，否则会报错：</p>

<pre><code>Exception in thread "main" java.lang.IllegalArgumentException: Required executor memory (30720+2150 MB) is above the max threshold (32768 MB) of this cluster!
</code></pre>

<h2>总结</h2>

<p>本文主要是搭建spark的环境搭建，本地运行、以及在docker中搭建spark集群、yarn集群三种方式。本地运行最简单方便，但是没有模拟到集群环境；spark提供了yarn框架上的实现，直接提交任务到yarn即可；spark集群相对比较简单和方便，接下来的远程调试主要通过spark伪分布式集群方式来进行。</p>

<h2>参考</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/building-with-maven.html">Building Spark with Maven</a></li>
<li><a href="http://spark.apache.org/docs/latest/quick-start.html">Quick Start</a></li>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html">Spark Configuration</a></li>
<li><a href="http://www.07net01.com/linux/zuixindnsmasqanzhuangbushuxiangjie_centos6__653221_1381214991.html">DNS</a></li>
<li>[spark上安装mysql与hive](<a href="http://blog.csdn.net/hwssg/article/details/38424529">http://blog.csdn.net/hwssg/article/details/38424529</a>]</li>
</ul>


<h2>后记 Spark-1.3.0</h2>

<h3>编译1.3.0(cygwin)</h3>

<p>正式环境用的hadoop-2.2，不是开发环境，没有maven等工具。先本地编译后，再方式去。（由于是添加计算的工具，可以随便一点）。</p>

<pre><code>export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
mvn package eclipse:eclipse -Phadoop-2.2 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests

-- // 删除生成的eclipse文件中的including
find . -name ".classpath" | xargs -I{} sed -i 's/ including="\*\*\/\*\.java"//' {}

dos2unix make-distribution.sh
./make-distribution.sh --mvn `which mvn` --tgz  --skip-java-test -Phadoop-2.2 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests

-- linux环境部署
-- // 这个版本，windows-cygwin编译的shell文件也是**windows的换行符**！！需要注意下！
[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ find bin/* -perm /u+x | xargs -I{} sed -i 's/^M//g' {} 
[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ find sbin/* -perm /u+x | xargs -I{} sed -i 's/^M//g' {} 
</code></pre>

<h3>spark-1.3.0运行spark-sql</h3>

<ol>
<li><p>连接到hive-engine</p></li>
<li><p>依赖tez</p></li>
</ol>


<p>hive的<code>hive.execution.engine</code>的tez，添加tez的jar和hive-site到CLASSPATH。</p>

<p>包的导入以及配置：（如果使用meta-service的就不用这么麻烦）</p>

<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ vi conf/spark-env.sh 
...
JAVA_HOME=/home/eshore/jdk1.7.0_60

# log4j

__add_to_classpath() {

  root=$1

  if [ -d "$root" ] ; then
    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
      else
        export SPARK_DIST_CLASSPATH=$f
      fi
    done
  fi

}

__add_to_classpath "/home/eshore/tez-0.4.0-incubating"
__add_to_classpath "/home/eshore/tez-0.4.0-incubating/lib"
__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"

export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
export SPARK_CLASSPATH=/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR

不能直接把hive的包全部加进去，hive-0.13.1a和hive-0.13.1的部分包不一致！！

    java.lang.NoSuchMethodException: org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(com.esotericsoftware.kryo.Kryo, java.io.InputStream, java.lang.Class)

    private static java.lang.Object org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(org.apache.hive.com.esotericsoftware.kryo.Kryo,java.io.InputStream,java.lang.Class)

* 如果不依赖tez，可以直接把datanucleus的三个包拷贝到lib目录下。

[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ll lib
total 262364
-rw-rw-r-- 1 hadoop hadoop    339666 Mar 25 19:35 datanucleus-api-jdo-3.2.6.jar
-rw-rw-r-- 1 hadoop hadoop   1890075 Mar 25 19:35 datanucleus-core-3.2.10.jar
-rw-rw-r-- 1 hadoop hadoop   1809447 Mar 25 19:35 datanucleus-rdbms-3.2.9.jar
-rwxr-xr-x 1 hadoop hadoop   4136686 Mar 31 13:05 spark-1.3.0-yarn-shuffle.jar
-rwxr-xr-x 1 hadoop hadoop 154198768 Mar 31 13:05 spark-assembly-1.3.0-hadoop2.2.0.jar
-rwxr-xr-x 1 hadoop hadoop 106275583 Mar 31 13:05 spark-examples-1.3.0-hadoop2.2.0.jar

[eshore@bigdatamgr1 conf]$ ll
...
lrwxrwxrwx 1 eshore biadmin   50 Mar 31 13:26 hive-site.xml -&gt; /home/eshore/apache-hive-0.13.1/conf/hive-site.xml
-rw-r--r-- 1 eshore biadmin  632 Mar 31 15:12 log4j.properties
lrwxrwxrwx 1 eshore biadmin   44 Mar 31 10:20 slaves -&gt; /data/opt/ibm/biginsights/hadoop-conf/slaves
-rwxr-xr-x 1 eshore biadmin 3380 Mar 31 16:17 spark-env.sh
lrwxrwxrwx 1 eshore biadmin   62 Mar 31 16:17 tez-site.xml -&gt; /data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop/tez-site.xml
</code></pre>

<p>上面用的是hive-site.xml直接连接数据库的方式。也可以起hive-metaserver，然后spark通过连接meta即可：</p>

<pre><code># 起meta服务
nohup bin/hive --service metastore &gt; metastore.log 2&gt;&amp;1 &amp;

# hive客户端配置
vi hive-site.xml
&lt;property&gt;
  &lt;name&gt;hive.metastore.uris&lt;/name&gt;
  &lt;value&gt;thrift://DataNode2:9083&lt;/value&gt;
  &lt;description&gt;Thrift uri for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt;
&lt;/property&gt;
</code></pre>

<ol>
<li>运行：</li>
</ol>


<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$  bin/spark-sql 2&gt;sql.log
SET spark.sql.hive.version=0.13.1
spark-sql&gt; show databases;
default
neva2dta
spark-sql&gt; show tables;
pokes   false
t_neva2_dps_xdr false
t_neva2_ipdr_xdr        false
spark-sql&gt; select count(*) from pokes;
500
spark-sql&gt; 

[eshore@bigdatamgr1 conf]$ vi spark-env.sh 
#!/usr/bin/env bash

JAVA_HOME=/home/eshore/jdk1.7.0_60
SPARK_CLASSPATH='/home/eshore/apache-hive-0.13.1/lib/*:/home/eshore/tez-0.4.0-incubating/*:/home/eshore/tez-0.4.0-incubating/lib/*'

# 同步
[eshore@bigdatamgr1 ~]$ for h in `cat ~/spark-1.3.0-bin-2.2.0/conf/slaves` ; do rsync -vaz /data/opt/ibm/biginsights/hadoop-2.2.0 $h:/data/opt/ibm/biginsights/  ; done
</code></pre>

<p>运行hivesever服务</p>

<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ cat start_thrift.sh 
#!/bin/bash
# hive-classpath已经在spark-env.sh中添加

./sbin/start-thriftserver.sh --master spark://bigdatamgr1:7077 --executor-memory 16g
[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ./start_thrift.sh 

[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ bin/beeline -u jdbc:hive2://bigdatamgr1:10001 -n eshore -p '' 
</code></pre>

<p>在不依赖外部的jar时，spark的启动脚本是没有问题的，但是我们添加了很多依赖的jar这么写就有问题了，尽管thrift启动正常，但是shell总是打印错误：</p>

<pre><code>failed to launch org.apache.spark.sql.hive.thriftserver.HiveThriftServer2:
  ========================================

full log in /home/eshore/spark-1.3.0-bin-2.2.0/sbin/../logs/spark-eshore-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-bigdatamgr1.out
</code></pre>

<p>比较隐晦，问题在<code>sbin/spark-daemon.sh</code>，启动完后通过<code>if [[ ! $(ps -p "$newpid" -o args=) =~ $command ]]; then</code>（其中<code>=~</code>表示正则匹配，最终<code>spark-class.sh</code>调用java会加上classpath），而上面的classpath会很长，导致上面的匹配失败！！</p>

<pre><code>[hadoop@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ vi bin/spark-class
...
  exec "$RUNNER" -cp "$CLASSPATH" $JAVA_OPTS "$@"
fi

-- 匹配失败时的值
[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ps -p 1925344 -o args=
/home/eshore/jdk1.7.0_60/bin/java -cp :/home/eshore/spark-1.3.0-bin-2.2.0/sbin/../conf:/home/eshore/spark-1.3.0-bin-2.2.0/lib/spark-assembly-1.3.0-hadoop2.2.0.jar:/home/eshore/spark
</code></pre>

<h4>解决办法</h4>

<p>先看实验：</p>

<pre><code>[dpi@dacs tmp]$ java -cp ~/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar:. JDBCConnTest

[dpi@dacs tmp]$ echo $CLASSPATH
.
[dpi@dacs tmp]$ export CLASSPATH=~/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar
[dpi@dacs tmp]$ java JDBCConnTest
错误: 找不到或无法加载主类 JDBCConnTest
[dpi@dacs tmp]$ java -cp . JDBCConnTest
java.lang.ClassNotFoundException: com.mysql.jdbc.Driver

[dpi@dacs tmp]$ echo $CLASSPATH
/home/dpi/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar
[dpi@dacs tmp]$ export CLASSPATH=~/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar:.
[dpi@dacs tmp]$ java JDBCConnTest
</code></pre>

<p>设置cp后会覆盖CLASSPATH。所以问题的解决方法：直接把cp的路径删掉（不添加），前面export的classpath路径。java程序会去主动获取改环境变量。</p>

<pre><code>  export CLASSPATH
  exec "$RUNNER" $JAVA_OPTS "$@"
</code></pre>

<p>效果如下：</p>

<pre><code>++ ps -p 1932338 -o args=
+ [[ ! /home/eshore/jdk1.7.0_60/bin/java -XX:MaxPermSize=128m -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --executor-memory 48g spark-internal =~ org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 ]]
</code></pre>

<ul>
<li>[=~的作用]<a href="http://bbs.chinaunix.net/thread-1623121-1-1.html">http://bbs.chinaunix.net/thread-1623121-1-1.html</a></li>
<li><a href="http://docs.oracle.com/javase/tutorial/essential/environment/paths.html">http://docs.oracle.com/javase/tutorial/essential/environment/paths.html</a></li>
<li><a href="https://docs.oracle.com/javase/8/docs/technotes/tools/windows/classpath.html">https://docs.oracle.com/javase/8/docs/technotes/tools/windows/classpath.html</a></li>
</ul>


<h3>Spark-HA</h3>

<p>仅需要配置，重启spark集群即可。</p>

<pre><code>[eshore@bigdata8 spark-1.3.0-bin-2.2.0]$ cat conf/spark-env.sh
...
SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark"

[eshore@bigdatamgr1 conf]$ vi spark-defaults.conf 
spark.master                     spark://bigdatamgr1:7077,bigdata8:7077
...
</code></pre>

<p>各个master要单独的启动:</p>

<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ sbin/start-all.sh 
[eshore@bigdata8 spark-1.3.0-bin-2.2.0]$ sbin/start-master.sh 
</code></pre>

<p>通过查看<a href="http://bigdata8:8080/">http://bigdata8:8080/</a>当前的状态为<strong>STANDBY</strong>。Workers列表为空。</p>

<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ sbin/stop-master.sh 
</code></pre>

<p>停了bigdatamgr1后，刷新<code>bigdata8:8080</code>页面等1分钟左右就变成ALIVE，然后其他所有的节点也连接到bigdata8了。</p>

<ul>
<li><a href="http://www.cnblogs.com/byrhuangqiang/p/3937654.html">http://www.cnblogs.com/byrhuangqiang/p/3937654.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html#high-availability">http://spark.apache.org/docs/latest/spark-standalone.html#high-availability</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
