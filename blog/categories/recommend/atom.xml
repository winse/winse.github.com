<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Recommend | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/recommend/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2015-09-22T17:34:20+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[配置ssh登录docker-centos]]></title>
    <link href="http://winseliu.com/blog/2014/09/30/docker-ssh-on-centos/"/>
    <updated>2014-09-30T00:10:02+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/30/docker-ssh-on-centos</id>
    <content type="html"><![CDATA[<p>上一篇写的是docker的入门知识，并没有进行实战。这些记录下使用ssh登录centos容器。</p>

<p>前文中参考的博客介绍了使用ssh登录tutorial容器（ubuntu），然后进行tomcat的安装，以及通过端口映射在客户机进行访问的例子。</p>

<h1>尝试</h1>

<pre><code>docker pull learn/tutorial
docker run -i -t learn/tutorial /bin/bash
    apt-get update
    apt-get install openssh-server
    which sshd
    /usr/sbin/sshd
    mkdir /var/run/sshd
    passwd #输入用户密码，我这里设置为123456，便于SSH客户端登陆使用
    exit #退出
docker ps -l
docker commit 51774a81beb3 learn/tutorial # 提交后，下次启动就可以基于容器更改的系统
docker run -d -p 49154:22 -p 80:8080 learn/tutorial /usr/sbin/sshd -D
ssh root@127.0.0.1 -p 49154
    # 在ubuntu 12.04上安装oracle jdk 7
    apt-get install python-software-properties
    add-apt-repository ppa:webupd8team/java
    apt-get update
    apt-get install -y wget
    apt-get install oracle-java7-installer
    java -version
    # 下载tomcat 7.0.47
    wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-7/v7.0.47/bin/apache-tomcat-7.0.47.tar.gz
    # 解压，运行
    tar xvf apache-tomcat-7.0.47.tar.gz
    cd apache-tomcat-7.0.47
    bin/startup.sh
</code></pre>

<p>然而在centos上，运行是不成功的。总结操作如下：</p>

<pre><code>[root@docker ~]# docker pull centos:centos6
[root@docker ~]# docker run -i -t  centos:centos6 /bin/bash
    yum install which openssh-server openssh-clients

    /usr/sbin/sshd # 这里会报错，需要手动生成key
    ssh-keygen -f /etc/ssh/ssh_host_rsa_key
    ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key

    vi /etc/pam.d/sshd  # 修改pam_loginuid.so为optional
    # /bin/sed -i 's/.*session.*required.*pam_loginuid.so.*/session optional pam_loginuid.so/g' /etc/pam.d/sshd

    passwd # 添加密码
</code></pre>

<ul>
<li>提交保存成果</li>
</ul>


<pre><code>[root@docker ~]# docker ps -l
[root@docker ~]# docker commit 3a7b6994bb2a winse/hadoop # 保存为自己使用的版本

[root@docker ~]# docker run -d winse/hadoop /usr/sbin/sshd
f5cb57f6ec22dd9d257bf610322e2bd547ea0064262fcad63308b932c0490670
[root@docker ~]# docker ps -l
CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS                     PORTS               NAMES
f5cb57f6ec22        winse/hadoop:latest   /usr/sbin/sshd      2 seconds ago       Exited (0) 2 seconds ago                       sharp_rosalind      

[root@docker ~]# docker run -d -p 8888:22 winse/hadoop /usr/sbin/sshd -D
f9814253159373e8a8df3261904200a733b41c63f55708db3cb56a7ebf650cef
[root@docker ~]# docker ps -l
CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS                  NAMES
f98142531593        winse/hadoop:latest   /usr/sbin/sshd -D   5 seconds ago       Up 4 seconds        0.0.0.0:8888-&gt;22/tcp   boring_bell         
[root@docker ~]# ssh localhost -p 8888
The authenticity of host '[localhost]:8888 ([::1]:8888)' can't be established.
RSA key fingerprint is f5:5e:be:ae:ea:b1:ed:e8:49:43:28:9e:80:87:0d:86.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[localhost]:8888' (RSA) to the list of known hosts.
root@localhost's password: 
Last login: Mon Sep 29 14:48:23 2014 from localhost
-bash-4.1# 
</code></pre>

<p>参数<code>-D</code>表示sshd运行在前台。这样当前的docker容器就会一直有程序在运行，不至于执行完指定的任务就被关闭掉了。</p>

<p>在centos配置ssh登录需要进行额外参数的设置。这个还是挺折腾人的。关于把<code>/etc/pam.d/sshd</code>中的<code>pam_loginuid.so</code>修改为optional，<a href="(http://stackoverflow.com/questions/21391142/why-is-it-needed-to-set-pam-loginuid-to-its-optional-value-with-docker">stackoverflow</a>)上的回答还是挺中肯的。</p>

<p>连上ssh后，下一步就和你远程操作服务器一样了。其实docker运行一个容器后，就会分配一个ip，你也可以根据这个ip来连接。</p>

<pre><code>[root@docker ~]# docker run -t -i winse/hadoop /bin/bash
bash-4.1# ssh localhost
ssh: connect to host localhost port 22: Connection refused
bash-4.1# service sshd start
Starting sshd:                                             [  OK  ]
bash-4.1# ifconfig
eth0      Link encap:Ethernet  HWaddr 1E:2B:23:16:98:7E  
          inet addr:172.17.0.31  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::1c2b:23ff:fe16:987e/64 Scope:Link

# 新开一个终端
[root@docker ~]# ssh 172.17.0.31
The authenticity of host '172.17.0.31 (172.17.0.31)' can't be established.
RSA key fingerprint is f5:5e:be:ae:ea:b1:ed:e8:49:43:28:9e:80:87:0d:86.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '172.17.0.31' (RSA) to the list of known hosts.
root@172.17.0.31's password: 
Last login: Mon Sep 29 14:48:23 2014 from localhost
-bash-4.1#           
</code></pre>

<h2>使用Dockerfile脚本安装</h2>

<pre><code>[root@docker ~]# mkdir hadoop
[root@docker ~]# cd hadoop/
[root@docker hadoop]# touch Dockerfile
[root@docker hadoop]# vi Dockerfile
    # hadoop2 on docker-centos
    FROM centos:centos6
    MAINTAINER Winse &lt;fuqiuliu2006@qq.com&gt;
    RUN yum install -y which openssh-clients openssh-server #-y表示交互都输入yes

    RUN ssh-keygen -f /etc/ssh/ssh_host_rsa_key
    RUN ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key

    RUN echo 'root:hadoop' |chpasswd

    RUN sed -i '/pam_loginuid.so/c session    optional     pam_loginuid.so'  /etc/pam.d/sshd

    EXPOSE 22
    CMD /usr/sbin/sshd -D

[root@docker hadoop]# docker build -t="winse/hadoop" .

[root@docker hadoop]# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
winse/hadoop        latest              9d7f115ef0ec        5 minutes ago       289.1 MB
...

[root@docker hadoop]# docker run -d --name slaver1 winse/hadoop
[root@docker hadoop]# docker run -d --name slaver2 winse/hadoop
[root@docker hadoop]# docker run -d --name master1 -P --link slaver1:slaver1 --link slaver2:slaver2  winse/hadoop

[root@docker hadoop]# docker restart slaver1 slaver2 master1
slaver1
slaver2
master1

[root@docker hadoop]# docker port master1 22
0.0.0.0:49159
[root@docker hadoop]# ssh localhost -p 49159
... 
-bash-4.1# cat /etc/hosts
172.17.0.31     7ef63f98e2d1
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.29     slaver1
172.17.0.30     slaver2
</code></pre>

<h2>参考</h2>

<ul>
<li><a href="http://www.blogjava.net/yongboy/archive/2013/12/12/407498.html">Docker学习笔记之一，搭建一个JAVA Tomcat运行环境</a></li>
<li><a href="http://www.csdn123.com/html/topnews201408/36/1236.htm">Docker之配置Centos_ssh</a></li>
<li><a href="http://linux.die.net/man/8/pam_loginuid">pam_loginuid(8) - Linux man page</a></li>
<li><a href="http://stackoverflow.com/questions/21391142/why-is-it-needed-to-set-pam-loginuid-to-its-optional-value-with-docker">Why is it needed to set <code>pam_loginuid</code> to its <code>optional</code> value with docker?</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在windows开发测试mapreduce几种方式]]></title>
    <link href="http://winseliu.com/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/"/>
    <updated>2014-09-17T12:55:38+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature</id>
    <content type="html"><![CDATA[<blockquote><p>备注： 文后面的maven打包、以及执行的shell脚本还是极好的&hellip;</p></blockquote>

<p>hadoop提供的两大组件HDFS、MapReduce。其中HDFS提供了丰富的API，最重要的有类似shell的脚本进行操作。而编写程序，要很方便的调试测试，其实是一件比较麻烦和繁琐的事情。</p>

<p>首先可能针对拆分的功能进行<strong>单独的方法</strong>级别的单元测试，然后到map/reduce的一个<strong>完整的处理过程</strong>的测试，再就是针对<strong>整个MR</strong>的测试，前面说的都是在IDE中完成后，最后需要到<strong>测试环境</strong>对其进行验证。</p>

<ul>
<li>单独的方法这里就不必多讲，直接使用eclipse自带的junit即可完成。</li>
<li>mrunit，针对map/reduce的测试，以至于整个MR流程的测试，但是mrunit的输入是针对小数据量的。</li>
<li>本地模式运行程序，模拟正式的环境来进行测试，数据直接从hdfs获取。</li>
<li>测试环境远程调试，尽管经过前面的步骤可能还会遇到各种问题，此时可结合<code>remote debug</code>来定位问题。</li>
</ul>


<h3>mrunit测试map/reduce</h3>

<p>首先去到<a href="http://mrunit.apache.org/">官网下载</a>，把对应的jar加入到你项目的依赖。懒得去手工下载的话直接使用maven。</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.mrunit&lt;/groupId&gt;
        &lt;artifactId&gt;mrunit&lt;/artifactId&gt;
        &lt;version&gt;1.1.0&lt;/version&gt;
        &lt;classifier&gt;hadoop2&lt;/classifier&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
</code></pre>

<p>可以对mapreduce的各种情况（map/reduce/map-reduce/map-combine-reduce）进行简单的测试，验证逻辑上是否存在问题。<a href="https://cwiki.apache.org/confluence/display/MRUNIT/MRUnit+Tutorial">官方文档的例子</a>已经很具体详细了。</p>

<p>先新建初始化driver（MapDriver/ReduceDriver/MapReduceDriver)，然后添加配置配置信息（configuration），再指定withInput来进行输入数据，和withOutput对应的输出数据。运行调用runTest方法就会模拟mr的整个运行机制来对单条的记录进行处理。因为都是在一个jvm中执行，调试是很方便的。</p>

<pre><code>    private MapReduceDriver&lt;LongWritable, Text, KeyWrapper, ValueWrapper, Text, Text&gt; mrDriver;

    @Before
    public void setUp() {
        AccessLogMapper mapper = new AccessLogMapper();
        AccessLogReducer reducer = new AccessLogReducer();
        // AccessLogCombiner combiner = new AccessLogCombiner();

        mrDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer);

        // mDriver = MapDriver.newMapDriver(mapper);
        // mcrDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer, combiner);
    }

    private String[] datas;

    @After
    public void run() throws IOException {
        if (datas != null) {
            // 配置
            ...
            mrDriver.setConfiguration(config);
            // mrDriver.getConfiguration().addResource("job_1399189058775_0627_conf.xml");

          // 输入输出
            Text input = new Text();
            int i = 0;
            for (String data : datas) {
                input.set(data);
                mrDriver.withInput(new LongWritable(++i), new Text(data));
            }
            mrDriver.withOutputFormat(MultipleFileOutputFormat.class, TextInputFormat.class);
            mrDriver.runTest();
        }
    }

    // / datas

    private String[] datas() {
        return ...;
    }

    @Test
    public void testOne() throws IOException {
        datas = new String[] { datas()[0] };
    }
</code></pre>

<h2>local方式进行本地测试</h2>

<p>mapreduce默认提供了两种任务框架： local和yarn。YARN环境需要把程序发布到nodemanager上去运行，对于开发测试来讲，还是太繁琐了。</p>

<p>使用local的方式，既不用打包同时拥有IDE本地调试的便利，同时数据直接从HDFS中获取，也就是说，除了任务框架不同，其他都一样，程序的输入输出，任务代码的业务逻辑。为全面开发调试/测试提供了极其重要的方式。</p>

<p>只需要指定服务为local的服务框架，再加上输入输出即可。如果本地用户和hdfs的用户不同，设置下环境变量<code>HADOOP_USER_NAME</code>。同样map、reduce通过线程来模拟，都运行的同一个JVM中，断点调试也很方便。</p>

<pre><code>public class WordCountTest {

    static {
        System.setProperty("HADOOP_USER_NAME", "hadoop");
    }

    private static final String HDFS_SERVER = "hdfs://umcc97-44:9000";

    @Test
    public void test() throws Exception {
        WordCount.main(new String[]{
                "-Dmapreduce.framework.name=local", 
                "-Dfs.defaultFS=" + HDFS_SERVER, 
                HDFS_SERVER + "/user/hadoop/dta/001.tar.gz", 
                HDFS_SERVER + "/user/hadoop/output/"});
    }

}
</code></pre>

<h3>测试环境打包测试</h3>

<p>放到测试环境后，appmanager、map、reduce都是运行在不同的jvm；还有就是需要对程序进行打包，挺啰嗦而且麻烦的事情，依赖包多的话，包还挺大，每次job都需要传递这么大一个文件，也挺浪费的。</p>

<p>提供两种打包方式，一种是直接jar运行的，一种是所有的jar压缩包tar.gz方式。可以结合distributecache减少每次执行程序需要传递给nodemanager的数据量，以及结合mapreduce运行时配置参数可以进行远程调试。</p>

<pre><code>调试appmanager
-Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090" 
调试map
-Dmapreduce.map.java.opts
调试reduce
-Dmapreduce.reduce.java.opts
</code></pre>

<h3>小结</h3>

<p>通过以上3中方式基本上能处理工作终于到的大部分问题了。大部分的功能使用mrunit测试就可以了，还可以单独的测试map，或者reduce挺不错的。</p>

<h3>附录：maven打包</h3>

<pre><code>    &lt;profile&gt;
        &lt;id&gt;jar&lt;/id&gt;
        &lt;build&gt;
            &lt;plugins&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                    &lt;executions&gt;
                        &lt;execution&gt;
                            &lt;id&gt;make-assembly&lt;/id&gt;
                            &lt;phase&gt;package&lt;/phase&gt;
                            &lt;goals&gt;
                                &lt;goal&gt;single&lt;/goal&gt;
                            &lt;/goals&gt;
                        &lt;/execution&gt;
                    &lt;/executions&gt;
                    &lt;configuration&gt;
                        &lt;descriptorRefs&gt;
                            &lt;descriptorRef&gt;
                                jar-with-dependencies
                            &lt;/descriptorRef&gt;
                        &lt;/descriptorRefs&gt;
                    &lt;/configuration&gt;
                &lt;/plugin&gt;

            &lt;/plugins&gt;
        &lt;/build&gt;
    &lt;/profile&gt;

    &lt;profile&gt;
        &lt;id&gt;tar&lt;/id&gt;
        &lt;build&gt;
            &lt;plugins&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                    &lt;executions&gt;
                        &lt;execution&gt;
                            &lt;id&gt;make-assembly&lt;/id&gt;
                            &lt;phase&gt;package&lt;/phase&gt;
                            &lt;goals&gt;
                                &lt;goal&gt;single&lt;/goal&gt;
                            &lt;/goals&gt;
                        &lt;/execution&gt;
                    &lt;/executions&gt;
                    &lt;configuration&gt;
                        &lt;appendAssemblyId&gt;true&lt;/appendAssemblyId&gt;
                        &lt;descriptors&gt;
                            &lt;descriptor&gt;${basedir}/../assemblies/application.xml&lt;/descriptor&gt;
                        &lt;/descriptors&gt;
                    &lt;/configuration&gt;
                &lt;/plugin&gt;
            &lt;/plugins&gt;
        &lt;/build&gt;
    &lt;/profile&gt;
</code></pre>

<p>打包成tar.gz的描述文件：</p>

<pre><code>    &lt;assembly&gt;
        &lt;id&gt;dist-${env}&lt;/id&gt;
        &lt;formats&gt;
            &lt;format&gt;tar.gz&lt;/format&gt;
        &lt;/formats&gt;
        &lt;includeBaseDirectory&gt;true&lt;/includeBaseDirectory&gt;
        &lt;fileSets&gt;
            &lt;fileSet&gt;
                &lt;directory&gt;${basedir}/src/main/scripts&lt;/directory&gt;
                &lt;outputDirectory&gt;/bin&lt;/outputDirectory&gt;
                &lt;includes&gt;
                    &lt;include&gt;*.sh&lt;/include&gt;
                &lt;/includes&gt;
                &lt;fileMode&gt;0755&lt;/fileMode&gt;
                &lt;lineEnding&gt;unix&lt;/lineEnding&gt;
            &lt;/fileSet&gt;
            &lt;fileSet&gt;
                &lt;directory&gt;${basedir}/target/classes&lt;/directory&gt;
                &lt;outputDirectory&gt;/conf&lt;/outputDirectory&gt;
                &lt;includes&gt;
                    &lt;include&gt;*.xml&lt;/include&gt;
                    &lt;include&gt;*.properties&lt;/include&gt;
                &lt;/includes&gt;
            &lt;/fileSet&gt;
            &lt;fileSet&gt;
                &lt;directory&gt;${basedir}/target&lt;/directory&gt;
                &lt;outputDirectory&gt;/lib/core&lt;/outputDirectory&gt;
                &lt;includes&gt;
                    &lt;include&gt;${project.artifactId}-${project.version}.jar
                    &lt;/include&gt;
                &lt;/includes&gt;
            &lt;/fileSet&gt;
        &lt;/fileSets&gt;
        &lt;dependencySets&gt;
            &lt;dependencySet&gt;
                &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt;
                &lt;outputDirectory&gt;/lib/common&lt;/outputDirectory&gt;
                &lt;scope&gt;runtime&lt;/scope&gt;
            &lt;/dependencySet&gt;
        &lt;/dependencySets&gt;
    &lt;/assembly&gt;
</code></pre>

<p>运行整个程序的shell脚本</p>

<pre><code>#!/bin/sh

bin=`which $0`
bin=`dirname ${bin}`
bin=`cd "$bin"; pwd`

export ANAYSER_HOME=`dirname "$bin"`

export ANAYSER_LOG_DIR=$ANAYSER_HOME/logs

export ANAYSER_OPTS="-Dproc_dta_analyser -server -Xms1024M -Xmx2048M -Danalyser.log.dir=${ANAYSER_LOG_DIR}"

export HADOOP_HOME=${HADOOP_HOME:-/home/hadoop/hadoop-2.2.0}
export ANAYSER_CLASSPATH=$ANAYSER_HOME/conf
export ANAYSER_CLASSPATH=$ANAYSER_CLASSPATH:$HADOOP_HOME/etc/hadoop

for f in $ANAYSER_HOME/lib/core/*.jar ; do
  export ANAYSER_CLASSPATH+=:$f
done

for f in $ANAYSER_HOME/lib/common/*.jar ; do
  export ANAYSER_CLASSPATH+=:$f
done

if [ ! -d $ANAYSER_LOG_DIR ] ; then
  mkdir -p $ANAYSER_LOG_DIR
fi

[ -w "$ANAYSER_PID_DIR" ] ||  mkdir -p "$ANAYSER_PID_DIR"

nohup ${JAVA_HOME}/bin/java $ANAYSER_OPTS -cp $ANAYSER_CLASSPATH com.analyser.AnalyserStarter &gt;$ANAYSER_LOG_DIR/stdout 2&gt;$ANAYSER_LOG_DIR/stderr &amp;
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读读书]Redis入门指南]]></title>
    <link href="http://winseliu.com/blog/2014/07/27/start-redis/"/>
    <updated>2014-07-27T01:20:44+08:00</updated>
    <id>http://winseliu.com/blog/2014/07/27/start-redis</id>
    <content type="html"><![CDATA[<p>《Redis入门指南》的基本使用笔记，<a href="/blog/categories/redis/">jemalloc/tcmalloc功能和redis3集群的安装参考</a>。</p>

<h2>第一章 简介</h2>

<ul>
<li>讲了redis的产生的缘由</li>
<li>Salvtore Sanfilippo/Pieter Noordhuis被招到VMware专门负责redis</li>
<li>redis的源码可以从github下载编译。</li>
</ul>


<p>redis相比keyvalue，提供了更加丰富的值类型：字符串/散列/列表/集合/有序集合，数据提供多种持久化(RDB/AOF)的方式。</p>

<p>在一台普通的笔记本电脑上，Redis可以在一秒内读写超过十万个键值。</p>

<p>功能丰富，提供TTL，可以做(阻塞)队列、缓冲系统、发布/订阅消息模式。redis是单线程模型，相比memcached的多线程，可以启动多个redis实例。</p>

<h2>第二章 准备</h2>

<p>默认的生产环境使用linux，windows操作系统下也有对应的版本但是版本比较旧。
在linux下，下载完成后直接<code>make</code>就可以使用src目录下生成的命令了，<code>make install</code>会把命令拷贝到/usr/local/bin目录下。同时有介绍iOS和Windows下怎么安装redis。</p>

<h3>启动Redis2.8.3</h3>

<pre><code>src/redis-server # default port 6379
src/redis-server --port 6380
</code></pre>

<p>初始化脚本启动Redis</p>

<pre><code>    #!/bin/sh
    #
    # Simple Redis init.d script conceived to work on linux systems
    # as it does use of the /proc filesystem.

    REDISPORT=6379
    EXEC=/usr/local/bin/redis-server
    CLIEXEC=/usr/local/bin/redis-cli

    PIDFILE=/var/run/redis_${REDISPORT}.pid
    CONF=/etc/redis/${REDISPORT}.conf

    case "$1" in
    start)
        if [ -f $PIDFILE ]
        then
            echo "$PIDFILE exists, process is already running or crashed"
        else
            echo "Starting Redis server..."
            $EXEC $CONF
        fi
        ::
    stop)
        if [ ! -f $PIDFILE ]
        then
            echo "$PIDFILE does not exists, process is not running"
        else
            PID=$(cat $PIDFILE)
            echo "Stopping..."
            $CLIEXEC -p $REDISPORT shutdown
            while [ -x /proc/$PID ]
            do 
                echo "Waiting for Redis to shutdown..."
                sleep 1
            done
            echo "Redis stopped"
        fi
        ::
    *)
        echo "Please use start or stop as first argument"
        ::
    esac
</code></pre>

<h3>停止Redis</h3>

<p>不要直接强制终止程序(<code>kill -9</code>)。使用redis提供的shutdown来停，会等所有操作都flush到磁盘后再关闭。保证数据不会丢失。
当然也可以使用SIGTERM信号来处理，使用<code>kill PID</code>命令，Redis妥善的处理与发送shutdown命令效果一样。</p>

<pre><code>src/redis-cli shutdown
</code></pre>

<h3>命令行客户端(cli Command-Line-Interface)</h3>

<pre><code>redis-cli -h IP -p PORT

[hadoop@master1 src]$ ./redis-cli PING
PONG

[hadoop@master1 src]$ ./redis-cli
127.0.0.1:6379&gt; PING
PONG
127.0.0.1:6379&gt; echo hi
"hi"
</code></pre>

<p>各种返回值</p>

<pre><code>127.0.0.1:6379&gt; errorcommand
(error) ERR unknown command 'errorcommand'
127.0.0.1:6379&gt; incr foo
(integer) 1
127.0.0.1:6379&gt; get foo
"1"
127.0.0.1:6379&gt; get noexists
(nil)
127.0.0.1:6379&gt; keys *
1) "foo"
</code></pre>

<h3>配置</h3>

<pre><code>redis-server CONFPATH --loglevel warning
</code></pre>

<p>也可以通过客户端设置值</p>

<pre><code>127.0.0.1:6379&gt; config set loglevel warning
OK
127.0.0.1:6379&gt; config get loglevel
1) "loglevel"
2) "warning"
</code></pre>

<h3>多数据库</h3>

<p>默认启动的程序启用了16个库（0-15，<code>databases 16</code>），客户端与Redis建立连接后，会自动选择0号数据库，不过可以通过SELECT命令更换数据库:</p>

<pre><code>127.0.0.1:6379&gt; select 1
OK
127.0.0.1:6379[1]&gt; get foo
(nil)
127.0.0.1:6379[1]&gt; set foo 1
OK
127.0.0.1:6379[1]&gt; get foo
"1"
</code></pre>

<p>redis不支持为每个数据库设置不同的访问密码，一个客户端要么可以访问全部数据库，要么连一个数据库也没有权限访问。最重要的一点是多个数据库并不是完全的隔离，比如flushall命令可以清空Redis实例中所有的数据库中的数据。所以这些数据库更像是一个命名空间，而不是适合存储不同应用的数据。</p>

<p>但是可以使用0号数据库存储A应用的生产数据而使用1号数据库存储A应用的测试数据，不同的应用应该使用不同的Redis实例存储数据。由于Redis非常轻量级，一个空Redis实例占用内存只有1M左右，所以不用担心多个Redis实例会额外占用很多内存。</p>

<h2>第三章 入门</h2>

<h3>热身</h3>

<p>获取符合规则的键名（glob风格 ?/*/\X/[]） : <code>KEYS pattern</code></p>

<pre><code>127.0.0.1:6379[1]&gt; KEYS *
1) "foq"
2) "foo"
3) "fop"
127.0.0.1:6379[1]&gt; keys fo[a-p]
1) "foo"
2) "fop"

127.0.0.1:6379[1]&gt; exists foa
(integer) 0 #不存在
127.0.0.1:6379[1]&gt; exists foo
(integer) 1 #存在

127.0.0.1:6379[1]&gt; del foo
(integer) 1
127.0.0.1:6379[1]&gt; del foa
(integer) 0
127.0.0.1:6379[1]&gt; keys *
1) "fop"
</code></pre>

<p>keys会遍历Redis中的所有键，当数量比较多是会影响性能，不建议在生产环境使用。</p>

<p>del可以删除多个键值，返回值为删除的个数。del命令的参数不支持通配符，但可以通过linux的实现批量删除<code>redis-cli DEL $(redis-cli KEYS "user:*")</code>（有长度限制）来达到效果，效果比xargs效果更好。</p>

<p>获取keyvalue值的类型</p>

<pre><code>127.0.0.1:6379&gt; set foo 1
OK
127.0.0.1:6379&gt; lpush foo 1
(error) WRONGTYPE Operation against a key holding the wrong kind of value
127.0.0.1:6379&gt; lpush foa 1
(integer) 1
127.0.0.1:6379&gt; type foo
string
127.0.0.1:6379&gt; type foa
list
</code></pre>

<h3>字符串类型</h3>

<pre><code>set key value
get key

incr key # 对应的值需为数值

set foo 1
incr foo
set foo b
incr foo
# (error) ERR value is not an integer or out of range

# 增加指定的整数

incrby key increment
decr key 
decr key decrement
increbyfloat key increment

append key value
strlen key # 字节数，和java字符串的length不同

mget key [key ...]
mset key value [key value ...]

getbit key offset
setbit key offset value
bitcount key [start] [end]
bitop operation destkey key [key ...] # AND OR XOR NOT

set foo1 bar
set foo2 aar
BITOP OR res foo1 foo2 # 位操作命令可以非常紧凑地存储布尔值
GET res
</code></pre>

<h3>散列值</h3>

<pre><code>hset key field value
hget key field
hmset key field value [field value ...]
hmget key field [field ...]
hgetall key

hexists key field
hsetnx key field value # 当字段不存在时赋值 if not exists

hincrby key field increment

hdel key field [field ...]

hkeys key # 仅key
hvals key # 仅value
hlen key  # 字段数量
</code></pre>

<h3>列表</h3>

<p>双端队列型列表</p>

<pre><code>lpush key value [value ...]
rpush key value [value ...]
lpop key
rpop key
llen key
lrange key start stop # 可以使用负索引，从0开始，包括最右边的元素

lrem key count value 
# 删除列表中前count个值为value的元素，返回的是实际删除的元素个数。
# count为负数是从右边开始删除
# count为0时删除所有值为value的元素

# 获得/设置指定索引的元素值

lindex key index # index为负数是从右边开始
lset key index value

ltrim key start end # 只保留列表指定的片段
linsert key BEFORE/AFTER pivotvalue value

poplpush source destination # 将元素从给一个列表转到另一个列表
</code></pre>

<h3>集合类型</h3>

<pre><code>sadd key member [member ...]
srem key member [member ...]
smembers key # 获取集合中的元素
sismember key member # 判断元素是否在集合中

sdiff key [key ...] # 差集 A-B
sinter key [key ...] # A ∩ B
sunion key [key ...] # A ∪ B

scard key # 获取集合中元素个数

sdiffstore destination key [key ...]
sinterstore destination key [key ...]
sunionstore destination key [key ...]

srandmember key [count] 
# 随机获取集合中的元素，count参数来一次性获取多个元素
# count为负数时，会随机从集合里获得|count|个的元素，这里元素有可能相同。

spop key # 从集合中随机弹出一个元素
</code></pre>

<h3>有序集合</h3>

<p>列表类型是通过链表实现的，获取靠近两端的数据速度极快，而当元素增多后，访问中间数据的速度会较慢，所以它更加适合实现和“新鲜事”或“日志”这样很少访问中间元素的应用。有序集合类型是使用散列和跳跃表（Skip list）实现的，所以即使读取位于中间的数据也很快（时间复杂度是O(log(N))）。列表中不能简单地调整某个元素的位置，但是有序集合可以（通过更改这个元素的分数）。有序集合要比列表类型更耗费内存。</p>

<pre><code>zadd key score member [score member ...]
# 如果该元素已经存在则会用新的分数替换原有的分数。zadd命令的返回值是新加入到集合中的元素个数（不包含之前已经存在的元素）。
# 其中+inf和-inf分别表示正无穷和负无穷

zscore key member

zrange key start stop [withscores] # 获取排名在某个范围的元素列表
zrevrange key start stop [withscores] 
# 负数代表从后向前查找（-1表示最后一个元素），O(logn+m)

zrangebyscore key min max [withscores] [limit offset  count]

# 命令按照元素分数从小到大的顺序返回分数的min和max之间（包含min和max）的元素。
# 如果希望分数范围不包含端点值，可以在分数前加上"("符号。例如，希望返回80分到100分的数据，可以含80分，但不包含100分。则稍微修改一下上面的命令即可：
zrangebyscore scoreboard 80 (100
zrangebyscore scoreboard (80 +inf
# 本命令中LIMIT offset count与SQL中的用法基本相同。获取分数低于或等于100的前3个人
zrevrangebyscore scoreboard 100 0 limit 0 3

zincrby key increment memeber # 增加某个元素的分数

zcard key # 获取集合中元素的数量
zcount key min max # 获得指定分数范围内的元素个数
zrem key member [memeber ...] # 删除一个或多个元素，返回成功删除的元素数量

# 按照排名范围删除元素, 并返回删除的元素数量
zremrangebyrank key start stop
# 按照分数范围删除元素
zremrangebyscore key min max

zrank key member
zrevrank key memeber

zinterstore destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [aggregate sum|min|max]
zunionstore ...
</code></pre>

<h2>第四章 进阶</h2>

<h3>事务</h3>

<pre><code>multi
sadd "user:1:following" 2
sadd "user:2:followers" 1
exec
</code></pre>

<p>脚本语法有错，命令不能执行。但是当数据类型等逻辑运行错误时，事务里面的命令会被redis接受并执行。</p>

<p>如果事务里的一条命令出现错误，事务里的其他命令依然会继续执行（包括出错到最后的命令）。对应的返回值会返回错误信息。</p>

<pre><code>127.0.0.1:6379&gt; multi
OK
127.0.0.1:6379&gt; set key 1
QUEUED
127.0.0.1:6379&gt; sadd key 2
QUEUED
127.0.0.1:6379&gt; set key 3
QUEUED
127.0.0.1:6379&gt; exec
1) OK
2) (error) WRONGTYPE Operation against a key holding the wrong kind of value
3) OK
127.0.0.1:6379&gt; get key
"3"
</code></pre>

<p>redis的事务没有回滚的功能，出现错误事务时必须自己负责收拾剩下的摊子（将数据库复原事务执行前的状态等）。不过由于redis不支持回滚功能，也使得redis在事务上可以保持简洁和快速。其中语法错误完全可以再开发时找出并解决。另外如果能够很好的规划数据库（保证键名规范等）的使用，是不会出现命令与数据类型不匹配这样的错误的。</p>

<p><strong>watch命令</strong></p>

<p>在一个事务中只有当所有命令都依次执行完后才能得到每个结果的返回值。可是有些情况下需要先获得一条命令的返回值，然后再根据这个值执行下一条命令。
如increment的操作，在增加1的是时刻没法保证数据还是原来的数据。为了解决这个问题，可以在GET获取值后保证该键值不会被其他客户端修改，知道函数执行完成后才允许其他客户端修改该键值，这样也可以防止竞态条件。watch命令可以监控一个或多个键，一旦其中一个键被修改（或删除），之后的事务就不会被执行。监控一直持续到exec命令。</p>

<pre><code>127.0.0.1:6379&gt; watch key
OK
127.0.0.1:6379&gt; set key 2
OK
127.0.0.1:6379&gt; multi
OK
127.0.0.1:6379&gt; set key 3
QUEUED
127.0.0.1:6379&gt; exec
(nil)
127.0.0.1:6379&gt; get key
"2"
</code></pre>

<p>执行exec命令会取消对所有键的监控，如果不想执行事务中的命令也可以使用unwatch命令来取消监控。</p>

<h3>生存时间TTL</h3>

<pre><code>expire key seconds

ttl key

127.0.0.1:6379&gt; get key
"2"
127.0.0.1:6379&gt; ttl key
(integer) -1
127.0.0.1:6379&gt; expire key 10
(integer) 1
127.0.0.1:6379&gt; ttl key
(integer) 6
127.0.0.1:6379&gt; ttl key
(integer) 1
127.0.0.1:6379&gt; ttl key
(integer) -2

pexpire milliseconds #时间的单位为毫秒
expireat UTC
pexpireat 毫秒（UTC*1000）
</code></pre>

<p>除了persist命令之外，使用set和getset命令为键赋值也会同时清除键的生存时间。使用expire命令会重新设置键的生存时间。其他对键值进行操作的命令（如incr、lpush、hset、zrem）均不会影响键的生存时间。</p>

<p>提示： 如果使用watch命令监测一个拥有生存时间的键，该键时间到期自动删除并不会被watch命令认为该键被改变。</p>

<h3>缓冲</h3>

<p>expire + maxmemory maxmemory-policy(LRU)</p>

<h3>排序</h3>

<p>可以使用multi, zintestore, zrange, del, exec来实现，但太麻烦！<a href="https://gist.github.com/winse/30f9db38a4c41aaf5f9d">实际操作日志</a>。</p>

<p>sort命令，可用于集合、列表类型和有序集合类型</p>

<pre><code>sort key [ALPHA] [BY PREFIXKYE:*-&gt;property] [DESC] [LIMIT offset count] 

127.0.0.1:6379&gt; lpush mylist 7 1 3 9 0
(integer) 5
127.0.0.1:6379&gt; sort mylist
1) "0"
2) "1"
3) "3"
4) "7"
5) "9"
</code></pre>

<p>针对有序集合排序时会忽略元素的分数，只针对元素自身的值进行排序。
集合类型中所有元素是无序的，但经常被用于存储对象的ID，很多情况下都是整数。所以redis多这种情况进行了特殊的优化，元素的顺序是有序的。</p>

<pre><code>127.0.0.1:6379&gt; sadd myset 5 2 6 1 8 1 9 0
(integer) 7
127.0.0.1:6379&gt; smembers myset
1) "0"
2) "1"
3) "2"
4) "5"
5) "6"
6) "8"
7) "9"
</code></pre>

<p>除了直接对元素排序排序外，还可以通过BY操作来获取关联值来进行排序。BY参数的语法为“BY参考键”，其中参考键可以使字符串类型或者是散列类型键的某个字段（表示为键名->字段名）。如果提供了BY参数，sort命令将不再依据元素自身的值进行排序，而是对每个元素使用元素的值替换参考键中的第一个<code>*</code>并获取取值，然后依据该值对元素排序。</p>

<pre><code>sort tag:ruby:posts BY post:*-&gt;time desc
sort sortbylist BY itemsore:* desc
</code></pre>

<p>当参考键不包括<code>*</code>时（即常量键名，与元素值无关）。SORT命令将不会执行排序操作，因为redis认为这种情况没有意义（因为所有要比较的值都一样）。没有执行排序操作，在不需要排序但需要借组sort命令获得与元素相关联的数据时，常量键名是很有用的！</p>

<p>如果几个元素的参考键值相同，则SORT命令会在比较元素本身的值来决定元素的顺序。
当某个元素的参考键不存在时，会默认参考键的值为0。
参考键虽然支持散列类型，但是<code>*</code>只能在<code>-&gt;</code>符号前面（即键名部分）才有用，在<code>-&gt;</code>后（即字段名部分）会被当成字段名本身名本身而不会作为占位符被元素的值替换，即常量键名。但是实际运行时会发现一个有趣的结果。</p>

<pre><code>sort sortbylist BY somekey-&gt;somefield:* 
</code></pre>

<p>上面提到了当参考键名是常量键名时SORT命令将不会执行排序操作，然而上例中却是进行了排序，而且只是对元素本身进行排序。这是因为Redis判断参考键名是不是常量键名的方式是判断参考键名中是否包含<code>*</code>，而<code>somekey-&gt;somefield:*</code>中包含<code>*</code>所以不是常量键名。所以在排序的时刻Redis对每个元素都会读取键somekey中的<code>somefield:*</code>字段（<code>*</code>不会被替换）。无论能否获得其值，每个元素的参考键值是相同的，所以redis被按照元素本身的大小排序。</p>

<p>GET参考不影响排序，它的作用是使SORT命令的返回结果不在是元素自身的值。而是GET参数中指定的键值。GET参数的规则和BY参数一样，GET参数也支持字符串类型和散列类型的值，并使用<code>*</code>作为占位符。要实现在排序后直接返回ID对应的违章标题，可以这样写：</p>

<pre><code>127.0.0.1:6379&gt; lpush tag:ruby:posts 1 2 3
(integer) 3
127.0.0.1:6379&gt; hmset post:1 time 140801 name HelloWorld
OK
127.0.0.1:6379&gt; hmset post:2 time 140802 name HelloWorld2
OK
127.0.0.1:6379&gt; hmset post:3 time 140803 name HelloWorld3
OK
127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc
1) "3"
2) "2"
3) "1"
127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time DESC GET post:*-&gt;name
1) "HelloWorld3"
2) "HelloWorld2"
3) "HelloWorld"
</code></pre>

<p>一个sort命令中可以有多个GET参数（而BY参数只能有一个），所以还可以这样用：</p>

<pre><code>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc GET post:*-&gt;name GET post:*-&gt;time
1) "HelloWorld3"
2) "140803"
3) "HelloWorld2"
4) "140802"
5) "HelloWorld"
6) "140801"
</code></pre>

<p>如果还需要返回文章ID，可以使用<code>GET #</code>获得，也就是返回元素本身的值。</p>

<pre><code>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc GET post:*-&gt;name GET post:*-&gt;time GET #
1) "HelloWorld3"
2) "140803"
3) "3"
4) "HelloWorld2"
5) "140802"
6) "2"
7) "HelloWorld"
8) "140801"
9) "1"
</code></pre>

<p>默认情况下SORT会直接返回排序结果，如果希望保存排序结果，可以使用STORE参数。保存后的键的类型为列表类型，如果键已经存在则会覆盖它，加上STORE参数后的SORT命令的返回值的结果的个数。</p>

<pre><code>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc GET post:*-&gt;name GET post:*-&gt;time GET # STORE tag.ruby.posts.sort
(integer) 9
127.0.0.1:6379&gt; lrange tag.ruby.posts.sort 0 -1
1) "HelloWorld3"
2) "140803"
3) "3"
4) "HelloWorld2"
5) "140802"
6) "2"
7) "HelloWorld"
8) "140801"
9) "1"
</code></pre>

<p>SORT命令的时间复杂度是O(n+mlogm)，其中n表示要排序的列表（集合或有序集合）中的元素个数，m表示要返回的元素个数。当n较大时SORT命令的性能相对较低，并且redis在排序前会建立一个长度为n的容器来存储排序的元素（当键类型为有序集合且参考键为常量键名时容器大小为m而不是n），虽然是一个临时的过程，但如果同时进行较多的大数据量排序操作则会严重影响性能。</p>

<h3>消息通知</h3>

<p>producer/consumer，松耦合，易于扩展，而且可以分布在不同的服务器中！</p>

<pre><code>BLPOP key [key ...] timeout
BRPOP key [key ...] timeoutseconds
# 超时时间设置为0时，表示不限制等待的时间，即如果没有新元素加入列表就会永远阻塞下去。
</code></pre>

<p>BRPOP可以同时接收多个键，同时检测多个键，如果所有键都没有元素则阻塞，其中有一个键有元素则会从该键中弹出元素。如果存在键都有元素则从左到右的顺序取第一个键中的一个元素。借此特性可以实现优先级的队列任务。</p>

<p>publish/subscribe模式，发布/订阅模式同样可以实现进程间的消息传递。</p>

<pre><code>PUBLISH channel.1 hi
SUBSCRIBE channel.1
</code></pre>

<p>执行SUBSCRIBE命令后，客户端会进入订阅状态，处于此状态下客户端不能使用SUBSCRIBE/UNSUBSCRIBE/PSUBSCRIBE（支持glob风格通配符格式）/PUNSUBSCRIBE这4个属于发布/订阅模式的命令之外的命令，否则会报错。</p>

<p>消息类型： subscribe/message/unsubscribe</p>

<pre><code>psubscribe channel.?*
</code></pre>

<h3>管道pipelining</h3>

<p>在执行多个命令时每条命令都需要等待上一条命令执行完才能执行，即使命令不需要上一条命令的执行结果。通过管道可以一次性发送多条命令并在执行完后一次性将结果返回，当一组命令中每条命令都不依赖与之前命令的执行结果就可以将这一组命令一起通过管道发出。管道通过减少客户端与redis的通信次数来实现降低往返时延。（</p>

<h3>节省空间</h3>

<ul>
<li>精简键名和键值 <code>VIP&lt;-very.important.person</code></li>
<li>内部编码优化（存储和效率的取舍）</li>
</ul>


<p>如果想查看一个键的内部编码方式可以使用<code>OBJECT ENCODING foo</code></p>

<h2>第五章 实践</h2>

<ul>
<li>php用户登录，忘记密码邮件发送队列</li>
<li>ruby自动完成</li>
<li>python在线好友</li>
<li>nodejs的IP段地址查询</li>
</ul>


<h2>第六章 脚本</h2>

<p>代码块多次请求，以及事务竞态等问题，需要用到WATCH，多次请求在网络传输上浪费很多时间。redis的脚本类似于数据库的function，在服务端执行。这种方式不仅代码简单、没有竞态条件（redis的命令都是原子的），而且减少了通过网络发送和接收命令的传输开销。</p>

<p>从2.6开始，允许开发者使用Lua语言编写脚本传到redis中执行。在Lua脚本中可以调用大部分的redis命令。减少网络传输时延，原子操作，复用（发送的脚本永久存储在redis中，其他客户端可以复用）。</p>

<p><strong>访问频率</strong></p>

<pre><code>localtimes=redis.call('incr', KEYS[1])
if times==1 then
redis.call('expire', KEYS[1], ARGV[1])
end

if times&gt;tonumber(ARGV[2]) then
return 0
end

return 1
# redis-cli --eval ratelimiting.lua rate.limiting:127.0.0.1 , 10 3 逗号前的是键，后面的是参数
</code></pre>

<h3>lua语法（和shell脚本有点像，更简洁）</h3>

<pre><code>本地变量 local x=10
注释 --xxx
多行注释 --[[xxxx]]
赋值 local a,b=1,2 # a=1, b=2
   local a={1,2,3}
   a[1]=5
数字操作符的操作数如果是字符串会自动转成数字
tonumber
tostring
只要操作数不是nil或者false，逻辑操作符就认为操作数为真，否则为假！
用..来实现字符串连接
取长度 print(#"hello") -- 5
</code></pre>

<h3>使用脚本</h3>

<pre><code>EVAL script numkeys key [key ...] arg [arg ...]

redis&gt; eval "return redis.call('SET', KEYS[1], ARGV[1])" 1 foo bar

EVALSHA sha1 numkeys key [key ...] arg [arg ...]
</code></pre>

<p>同时获取多个散列类型键的键值</p>

<pre><code>local result={}
for i,v in ipairs(KEYS) do
result[i]=redis.call("HGETALL", v)
end
return result
</code></pre>

<p>获取并删除有序集合中分数最小的元素</p>

<pre><code>local element=redis.call("ZRANGE", KEY[1], 0, 0)[1]
if element the
redis.call('ZREM', KEYS[1], element)
end
return element
</code></pre>

<p>处理JSON</p>

<pre><code>local sum=0
local users=redis.call('mget', unpack(KEYS))
for _,user in ipairs(users) do 
local courses=cjson.decode(user).course
for _,score in pairs(courses) do
sum=sum+score
end
end
return sum
</code></pre>

<p>redis脚本禁用使用lua标准库中与文件或系统调用相关的函数，在脚本中只允许对redis的数据进行处理。并且redis还通过禁用脚本的全局变量的方式保证每个脚本都是相对隔离的们不会互相干扰。
使用沙盒不仅是为了保证服务器的安全性，而且还确保了脚本的执行结果值和脚本本身和执行时传递的参数有关，不依赖外界条件（如系统时间、系统中某个文件的内存。。）。这是因为在执行复制和AOF持久化操作时记录的是脚本的内容而不是脚本调用的命令，所以必须保证在脚本内容和参数一样的前提下脚本的执行进行特殊的处理。</p>

<pre><code>script load 'return 1'
script exists sha1
script flush #清空脚本缓冲

script kill
script nosave
</code></pre>

<p>为了限制某个脚本执行时间过长导致redis无法提供服务（如死循环），redis提供了lua-time-limit参数限制脚本的最长运行时间，默认5s。</p>

<h2>第七章 管理</h2>

<ul>
<li>持久化 rdb/AOF
```
save 900 1
save 300 10
save 60 10000
SAVE
BGSAVE
appendonly yes
appendfilename appendonly.aof
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
BGREWRITEAOF

<h1>appendfsync always</h1>

appendfsync everysec

<h1>appendfsync no</h1>

<p>```</p></li>
<li>复制
<code>
redis-server --port 6380 --slaveof 127.0.0.1 6379
SLAVEOF 127.0.0.1 6379
SLAVEOF NO ONE
</code></li>
<li>读写分离</li>
<li>耗时日志查询
<code>
SLOWLOG GET # slowlog-log-slower-than slowlog-max-len
MONITOR
</code></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Windows下部署/配置/调试hadoop2]]></title>
    <link href="http://winseliu.com/blog/2014/04/21/hadoop2-windows-startguide/"/>
    <updated>2014-04-21T16:27:11+08:00</updated>
    <id>http://winseliu.com/blog/2014/04/21/hadoop2-windows-startguide</id>
    <content type="html"><![CDATA[<p>Windows作为开发屌丝必备，在windows上如何跑集群方便开发调试，以及怎么把eclipse写好的任务mapreduce提交到测试的集群(linux)上面去跑？这些都是需要直面并解决的问题。</p>

<p>本文主要记录在windows上hadoop集群的环境准备，以及eclipse调试功能等。</p>

<ol>
<li>windows伪分布式部署

<ul>
<li>cmd</li>
<li>cygwin shell</li>
</ul>
</li>
<li>windows-eclipse提交任务到linux集群</li>
<li>导入源码到eclipse</li>
</ol>


<p>这篇文章并非按照操作的时间顺序来进行编写。而是，如果再安装第二遍的话，自己应该如何去操作来组织下文。</p>

<h2>一、Windows伪分布式部署</h2>

<p>尽管一直用windows，但是对windows自带的cmd命令很是不屑！想在cygwin下部署，现在想来，最终用的是windows的java！在cygwin下不就是把路径转换后再传给java执行吗！</p>

<p>所以，如果把cygwin环境搭建好了的话，其实已经把windows的环境也搭建好了！同样hadoop的windows环境配置好了，cygwin环境也同样配置好了。但是，在cygwin下面提交mapreduce任务会有各种"凌乱"的问题！</p>

<p>先说说在windows环境搭建的步骤，然后再讲cygwin下运行。</p>

<ol>
<li>需要用到的软件环境</li>
<li>编译windows环境变量配置</li>
<li>编译hadoop-common源代码生成本地依赖库</li>
<li>伪分布式配置</li>
<li>windows下运行</li>
<li>cygwin下运行</li>
</ol>


<h3>1.1 需要用到的软件环境</h3>

<ul>
<li>Win7-x86</li>
<li>hadoop-2.2.0.tar.gz</li>
<li>git</li>
<li>cygwin (源码编译时需要执行sh命令)</li>
<li>visual studio 2010（如果与.net framework4有关的问题请查阅： <a href="http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval">[*]</a> <a href="http://stackoverflow.com/questions/12267158/failure-during-conversion-to-coff-file-invalid-or-corrupt">[*]</a> <a href="http://social.msdn.microsoft.com/Forums/vstudio/en-US/eb4a7699-0f3c-4701-9790-199787f1b359/vs-2010-error-lnk1123-failure-during-conversion-to-coff-file-invalid-or-corrupt?forum=vcgeneral">[*]</a>）</li>
<li>protoc(protoc-2.5.0-win32.zip)(<strong>解压，然后把路径加入到PATH</strong>)</li>
</ul>


<p>搭建环境之前，<strong>建议您看看<a href="http://wiki.apache.org/hadoop/Hadoop2OnWindows">wiki-Hadoop2OnWindows</a></strong>。最终有用的步骤都在上面了！不过在自己瞎折腾的过程中也弄了不少东西，记录下来！</p>

<h3>1.2 编译windows环境变量配置</h3>

<table>
<thead>
<tr>
<th style="text-align:left;"> 变量              </th>
<th style="text-align:left;"> windows</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> Platform          </td>
<td style="text-align:left;"> Win32</td>
</tr>
<tr>
<td style="text-align:left;"> ANT_HOME          </td>
<td style="text-align:left;"> D:\local\usr\apache-ant-1.9.0</td>
</tr>
<tr>
<td style="text-align:left;"> MAVEN_HOME        </td>
<td style="text-align:left;"> D:\local\usr\apache-maven-3.0.4</td>
</tr>
<tr>
<td style="text-align:left;"> JAVA_HOME         </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02</td>
</tr>
<tr>
<td style="text-align:left;"> PATH              </td>
<td style="text-align:left;"> C:\cygwin\bin;C:\protoc;D:\local\usr\apache-maven-3.0.4\bin;D:\local\usr\apache-ant-1.9.0/bin;D:\Java\jdk1.7.0_02\bin;%PATH%</td>
</tr>
</tbody>
</table>


<p><del>编译时，在打开的命令行加入cygwin的路径即可。</del>
在maven编译最后需要用到sh的shell命令，需要把<code>c:\cygwin\bin</code>目录加入到path环境变量。
这里先不配置hadoop的环境变量，因为我只需要用到编译后的本地库而已！！</p>

<h3>1.3 编译源代码生成本地依赖库(dll, exe)</h3>

<p>hadoop2.2.0操作本地文件针对平台的进行了处理。也就是只要在windows运行集群，不管怎么样，你都得先把winutils.exe、hadoop.dll编译出来，用来处理对本地文件赋权、软链接等（类似Linux-Shell的功能）。否则会看到下面的错误：</p>

<ul>
<li>命令执行出错，少了winutils.exe
```
14/04/14 20:07:58 ERROR util.Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
  at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
  at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
  at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)</li>
</ul>


<p>14/04/17 21:22:32 INFO service.AbstractService: Service org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerServic
e failed in state INITED; cause: java.lang.NullPointerException
java.lang.NullPointerException
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1010)
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
        at org.apache.hadoop.util.Shell.run(Shell.java:379)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:678)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:661)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:639)
        at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:435)
```</p>

<ul>
<li>少了hadoop.dll的本地库文件
<code>
14/04/17 21:30:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-ja
va classes where applicable
14/04/17 21:30:29 FATAL datanode.DataNode: Exception in secureMain
java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
      at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
      at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:435)
      at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)
      at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)
      at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)
      at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:147)
      at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:1698)
</code></li>
</ul>


<h4>下载源码进行编译</h4>

<p>下面需要用到visual studio修改项目配置信息（或者直接修改sln文件也行），然后再使用maven进行编译。</p>

<p>这里仅编译hadoop-common项目，最后把生成winutils.exe/hadoop.dll放到hadoop程序bin目录下。</p>

<p>第一步 下载源码</p>

<pre><code>/*  https://github.com/apache/hadoop-common.git  */

Administrator@WINSELIU /e/git/hadoop-common (master)
$ git checkout branch-2.2.0
Checking out files: 100% (5536/5536), done.
Branch branch-2.2.0 set up to track remote branch branch-2.2.0 from origin.
Switched to a new branch 'branch-2.2.0'
</code></pre>

<p>第二步 应用补丁patch-native-win32</p>

<p>jira: <a href="https://issues.apache.org/jira/browse/HADOOP-9922">https://issues.apache.org/jira/browse/HADOOP-9922</a>   <br/>
patch: <a href="https://issues.apache.org/jira/secure/attachment/12600760/HADOOP-9922.patch">https://issues.apache.org/jira/secure/attachment/12600760/HADOOP-9922.patch</a></p>

<p>native.sln-patch有点问题，下面通过vs修改，使用Visual Studio修改native的活动平台</p>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFebmAcNJsAATafRp0jDs763.png" alt="" /></p>

<p>第三步 在<code>Visual Studio 命令提示(2010)</code>命令行进行Maven编译(仅需编译hadoop-common)</p>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFeeGAfZgdAAWIDc_jnSM492.png" alt="" /></p>

<pre><code>E:\git\hadoop-common\hadoop-common-project\hadoop-common&gt;mvn package -Pdist,native-win -DskipTests -Dtar -Dmaven.javadoc.skip=true

/*  native files  */

Administrator@winseliu /cygdrive/e/git/hadoop-common/hadoop-common-project/hadoop-common
$ ls -1 target/bin/
hadoop.dll
hadoop.exp
hadoop.lib
hadoop.pdb
libwinutils.lib
winutils.exe
winutils.pdb

Administrator@winseliu /cygdrive/e/git/hadoop-common/hadoop-common-project/hadoop-common
$ cp target/bin/* ~/hadoop/bin/
</code></pre>

<p>windows的本地库的路径就是PATH环境变量。所以<strong>windows下最好还是把dll放到bin目录下，同时把<code>HADOOP_HOME/bin</code>加入到环境变量中！！</strong>
修改PATH环境变量。</p>

<p>可以把dll放到自定义的位置，但是同样最好把该路径加入到PATH环境变量。java默认会到PATH路径下找动态链接库dll。</p>

<h3>1.4 修改hadoop配置，部署伪分布式环境</h3>

<p>可以直接把linux伪分布式的配置cp过来用。然后修改namenode/datanode/yarn文件的存储路径就可以了。
这里有个坑，<code>hdfs-default.xml</code>中的路径前面都加了<code>file://</code>前缀！所以hdfs配置中涉及到路径的，这里都得进行了修改。</p>

<p><strong>Notepad++的Ctrl+D是一个好功能啊</strong></p>

<table>
<thead>
<tr>
<th style="text-align:left;"> 属性                                    </th>
<th style="text-align:left;"> 值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> <strong>slaves</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> localhost</td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> <strong>core-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> fs.defaultFS                            </td>
<td style="text-align:left;"> hdfs://localhost:9000</td>
</tr>
<tr>
<td style="text-align:left;"> io.file.buffer.size                     </td>
<td style="text-align:left;"> 10240</td>
</tr>
<tr>
<td style="text-align:left;"> hadoop.tmp.dir                          </td>
<td style="text-align:left;"> file:///e:/tmp/hadoop</td>
</tr>
<tr>
<td style="text-align:left;"> <strong>hdfs-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> dfs.replication                         </td>
<td style="text-align:left;"> 1</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.namenode.secondary.http-address     </td>
<td style="text-align:left;"> localhost:9001 #设置为空可以禁用</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.namenode.name.dir                   </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/name</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.datanode.data.dir                   </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/data</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.namenode.checkpoint.dir             </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/namesecondary</td>
</tr>
<tr>
<td style="text-align:left;"> <del>dfs.namenode.shared.edits.dir</del>       </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/shared/edits</td>
</tr>
<tr>
<td style="text-align:left;"> <strong>mapred-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> mapreduce.framework.name                </td>
<td style="text-align:left;"> yarn</td>
</tr>
<tr>
<td style="text-align:left;"> mapreduce.jobhistory.address            </td>
<td style="text-align:left;"> localhost:10020</td>
</tr>
<tr>
<td style="text-align:left;"> mapreduce.jobhistory.webapp.address     </td>
<td style="text-align:left;"> localhost:19888</td>
</tr>
<tr>
<td style="text-align:left;"> <strong>yarn-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> yarn.nodemanager.aux-services           </td>
<td style="text-align:left;"> mapreduce_shuffle</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.nodemanager.aux-services.mapreduce_shuffle.class  </td>
<td style="text-align:left;"> org.apache.hadoop.mapred.ShuffleHandler</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.address            </td>
<td style="text-align:left;"> localhost:8032</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.scheduler.address  </td>
<td style="text-align:left;"> localhost:8030</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.resource-tracker.address  </td>
<td style="text-align:left;"> localhost:8031</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.admin.address      </td>
<td style="text-align:left;"> localhost:8033</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.webapp.address     </td>
<td style="text-align:left;"> localhost:8088</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.application.classpath              </td>
<td style="text-align:left;"> %HADOOP_CONF_DIR%, %HADOOP_COMMON_HOME%/share/hadoop/common/<em>, %HADOOP_COMMON_HOME%/share/hadoop/common/lib/</em>, %HADOOP_HDFS_HOME%/share/hadoop/hdfs/<em>, %HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/</em>, %HADOOP_YARN_HOME%/share/hadoop/yarn/<em>, %HADOOP_YARN_HOME%/share/hadoop/yarn/lib/</em></td>
</tr>
</tbody>
</table>


<p>注意点：</p>

<ul>
<li>yarn.application.classpath必须定义！尽管程序中有判断不同平台的默认值不同，但是在yarn-default.xml中已经有值了！

<ul>
<li>yarn.application.classpath对启动程序没影响，但是在运行mapreduce时影响巨大破坏力极强！</li>
</ul>
</li>
<li>自定library的路径是个坑！！

<ul>
<li>在windows下，执行java程序java.library.path默认到PATH路径找。这也是需要定义环境变量HADOOP_HOME，以及把bin加入到PATH的原因吧！</li>
</ul>
</li>
</ul>


<hr />

<h3>1.5 Windows直接运行cmd启动</h3>

<p>如果是用windows的cmd的话，到这里已经基本ok了！<strong>格式化namenode</strong>（<code>hadoop namenode -format</code>），启动就ok了！
<del>发现自己其实很傻×，固执的要用cygwin启动运行！用windows的cmd启动，然后用cygwin的终端查看数据不就行了！两不耽误！</del></p>

<p>cmd命令<strong>默认</strong>是去bin目录下找hadoop.dll的，同时hadoop命令会把bin加入到java.library.path路径下。再次强调/推荐：直接把hadoop.dll放到bin路径。
设置环境变量，启动文件系统：</p>

<pre><code>/* **设置环境变量** */
HADOOP_HOME=E:\local\libs\big\hadoop-2.2.0 
PATH=%HADOOP_HOME%\bin;%PATH%

/* 格式化namenode */
hadoop namenode -format

/* 操作HDFS */
set HADOOP_ROOT_LOGGER=DEBUG,console

E:\local\libs\big\hadoop-2.2.0&gt;sbin\start-dfs.cmd

E:\local\libs\big\hadoop-2.2.0&gt;hdfs dfs -put README.txt /   # 很弱，fs简化操作都不兼容！

E:\local\libs\big\hadoop-2.2.0&gt;hdfs dfs -ls /
Found 1 items
-rw-r--r--   1 Administrator supergroup       1366 2014-04-22 22:20 /README.txt
</code></pre>

<p>JAVA_HOME的路径中最好不要有空格！否则测试下面的方式进行处理：</p>

<blockquote><p>instead e.g. c:\Progra~1\Java... instead of c:\Program Files\Java.&hellip;</p></blockquote>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFehCATjV7AAVxPp-3G94526.png" alt="" /></p>

<p>好处也是明显的，直接是windows执行，可以使用jdk自带的工具查看运行情况。</p>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFejOAbQmkAAZxcxSbXv0535.png" alt="" /></p>

<p>?疑问： log日志都写在hadoop.log文件中了？反正我是没看到hadoop.log的文件！</p>

<p>HDFS操作文件OK，如果按照上面步骤或者<a href="http://wiki.apache.org/hadoop/Hadoop2OnWindows">官网的wiki</a>操作，则运行mapreduce也是不会出问题的!!</p>

<pre><code>E:\local\libs\big\hadoop-2.2.0&gt;sbin\start-yarn.cmd

E:\local\libs\big\hadoop-2.2.0&gt;hadoop org.apache.hadoop.examples.WordCount /README.txt /out

E:\local\libs\big\hadoop-2.2.0&gt;hdfs dfs -ls /out
Found 2 items
-rw-r--r--   1 Administrator supergroup          0 2014-04-22 22:22 /out/_SUCCESS
-rw-r--r--   1 Administrator supergroup       1306 2014-04-22 22:22 /out/part-r-00000
</code></pre>

<p>如果你使用上面的hadoop命令执行不了命令，请把hadoop.cmd的换行（下载下来后是unix的）转成windows的换行！</p>

<h4>问题原因分析</h4>

<p>如果你运行mapreduce失败，不外乎三种情况：没有定义HADOOP_HOME系统环境变量，hadoop.dll没有放在PATH路径下，以及yarn.application.classpath没有设置。这三个问题导致。如果你不幸碰到了，那我们如何来确认问题呢？</p>

<p>下面一步步的来解读这个处理过程。在运行mapreduce时报错，可以使用远程调试方式来确认发生的具体位置。（如果你还没有弄好本地开发环境，请先看[三、导入源码到eclipse]）</p>

<p>第一步 调试NodeManager，从根源下手</p>

<p>由于windows的hadoop的程序都是<strong>直接</strong>运行的，不像linux还要ssh再登陆然后在启动。所以这里直接设置HADOOP_NODEMANAGER_OPTS就可以了。</p>

<pre><code>set HADOOP_NODEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8092"

E:\local\libs\big\hadoop-2.2.0\sbin\start-yarn.cmd
starting yarn daemons

E:\&gt;hadoop org.apache.hadoop.examples.WordCount /in /out
</code></pre>

<p>运行任务之前，在ContainerLaunch#call#171行打个断点（可以查看执行的java命令脚本内容，#254writeLaunchEnv写入cmd文件）。同时可以去到<code>nm-local-dir/nmPrivate</code>目录下查看任务的本地临时文件。application_XXX/containter_XXX/launch_container.cmd文件是MRAppMaster/YarnChild/YarnChild的启动脚本。</p>

<ul>
<li><p>调试。备份生成的脚本文件，开启死循环拷贝模式，把缓存留下来慢慢看</p>

<pre><code class="``">  while true ; do cp -rf nm-local-dir/ backup/ ; sleep 0.5; done
</code></pre>

<p>  <img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFe0aAKlntAAg2A_A0xS0493.png" alt="" /></p></li>
<li><p>查看缓存文件</p>

<ul>
<li>真正启动Mapreduce(yarnchild)的脚本文件launch_container.cmd</li>
<li><p>查看系统日志，确定错误</p>

<p>  <img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFd1GAMcbxAAIU6umDgu4394.png" alt="" /></p></li>
<li><p>classpath路径</p>

<p>  <img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFdsqAVo5pAAlRz9SLM3s104.png" alt="" /></p></li>
<li><p>Job任务类型。第三个参数！</p>

<p>  <img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFd4qAJKJAAAr2lBOh9yU947.png" alt="" /></p></li>
</ul>
</li>
</ul>


<p>这里可以查看脚本，确认HADOOP的相关目录是否正确！以及查看classpath的MANIFEST.MF查看依赖的jar是否完整！也可以通过任务的名称了解相关信息。</p>

<ul>
<li>路径问题，不影响大局（可以不关注/不修改）</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFd8uAR1y0AAcQFk5KVVo655.png" alt="" /></p>

<ul>
<li>调试map/reduce</li>
</ul>


<p>调试程序mapreduce比较好办了，毕竟代码都是自己写的好弄。可以使用mrunit。</p>

<p>map和reduce的进程都是动态的，既不能通过命令行的OPTS参数指定。如果要调试map/reduce需要在opts中传递给它们。</p>

<pre><code>hadoop org.apache.hadoop.examples.WordCount  "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090" /in /out
</code></pre>

<ul>
<li>library问题</li>
</ul>


<p>如果因为library的问题报access$0的错，提交任务都不成功，可以把自定义的dll路径加入java.library.path尝试一下。</p>

<pre><code>hadoop org.apache.hadoop.examples.WordCount "-Dmapreduce.map.java.opts= -Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native" "-Dmapreduce.reduce.java.opts=-Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native"  /in /out
</code></pre>

<hr />

<h3>1.6 cygwin下运行</h3>

<p>要在cygwin下面把hadoop弄起来，你要把cygwin与java的路径区分，理清楚路径，配置工作就成功一半咯！既然用的还是windows的java程序。配置文件也是最终提供给java执行的，所以配置都不需要修改。</p>

<p>要在cygwin中运行hadoop，仅仅搞定脚本就ok了！在执行java命令之前，把cygwin的路径转换为windows。</p>

<ul>
<li>修改了hadoop-env.sh的内容：</li>
</ul>


<pre><code>export JAVA_HOME=/cygdrive/d/Java/jdk1.7.0_02 #本来已经在环境变量中定义了，但是执行后台批处理的时刻不会调用环境变量的配置！
export HADOOP_HEAPSIZE=512
export HADOOP_PID_DIR=${HADOOP_PID_DIR:-${HADOOP_LOG_DIR}}
</code></pre>

<p>cygwin也就是linux的默认加载native的路径是libs/native！！拷一份过去把！！或者配置JAVA_LIBRARY_PATH，参见下面的修改Shell脚本部分。</p>

<p>cygwin自带的工具有个优势：运行脚本和java命令都不出现乱码。（或许把SecureCRT改成GBK编码也行）</p>

<ul>
<li>修改shell脚本命令</li>
</ul>


<p>由于java在windows和linux在识别文件路径上也有差异。如/data传给java，在windows会加上当前路径的盘符(e.g. E)，那写入数据目录就为<code>e:/data</code>。</p>

<p>同时，不同操作系统的classpath的组织方式也不同。(1)需要对classpath已经文件夹的路径进行转换，才能在cygwin下正常的运行java程序。
所以，只要在执行java命令之前对路径和classpath进行转换即可。(2)还需要对getconf返回值的换行符进行处理。涉及到下列的文件：</p>

<pre><code>libexec/hadoop-config.sh
bin/hadoop
bin/hdfs
bin/mapred
bin/yarn
sbin/start-dfs.sh
sbin/stop-dfs.sh
</code></pre>

<p>重点修改两个问题如下：</p>

<ul>
<li>配置</li>
</ul>


<pre><code>/* hadoop-config.sh */

# 定义时注意，处理cygwin路径时只处理了以/cygdrive开头的路径！ 
export JAVA_LIBRARY_PATH=/cygdrive/e/local/libs/big/hadoop-2.2.0/bin
</code></pre>

<p>由于windows配置时，把hadoop.dll的动态链接库放到bin目录下，而linux（cygwin）的sh脚本默认是去lib/native下面，所以需要定义一下链接库的查找路径。</p>

<ul>
<li>脚本</li>
</ul>


<pre><code>/* hadoop-config.sh */

/* 在调用java命令前，调用该方法 */
function Cygwin_Patch_PathConvert() {

    cygwin=false
    case "`uname`" in
    CYGWIN*) cygwin=true;;
    esac

    # cygwin path translation
    if $cygwin; then
        CLASSPATH=`cygpath -p -w "$CLASSPATH"`
        # ssh过来执行命令是不从.bash_profile获取参数！
        if [ "X$HADOOP_HOME" != "X" ]; then
            HADOOP_HOME=`cygpath -w "$HADOOP_HOME"`
        fi
        HADOOP_LOG_DIR=`cygpath -w "$HADOOP_LOG_DIR"`
        if [ "X$TOOL_PATH" != "X" ]; then
            TOOL_PATH=`cygpath -p -w "$TOOL_PATH"`
        fi

HADOOP_COMMON_HOME=`cygpath -w "$HADOOP_COMMON_HOME"`
JAVA_HOME=`cygpath -w "$JAVA_HOME"`
HADOOP_YARN_HOME=`cygpath -w "$HADOOP_YARN_HOME"`
HADOOP_HDFS_HOME=`cygpath -w "$HADOOP_HDFS_HOME"`
HADOOP_CONF_DIR=`cygpath -w "$HADOOP_CONF_DIR"`

# HOME

        # 把带/cygdrive/[abc]形式的路径转换为windows路径
        HADOOP_OPTS=`echo $HADOOP_OPTS | awk -F" " '{for(i=1;i&lt;=NF;i++)print $i}' | awk -F"=" ' {if($2~/^\/cygdrive\/[a|b|c|d|e]/){print $1;system("cygpath -w -p " $2 )}else{ print $0 }; print ""}' | awk 'BEGIN{opt="";last=""}{if($0~/^$/){ opt=opt " "; last="" }else{ if(last!=""){ opt=opt "="} opt=opt $0; last=$0; }; }END{ print opt }' `

        YARN_OPTS=`echo $YARN_OPTS | awk -F" " '{for(i=1;i&lt;=NF;i++)print $i}' | awk -F"=" ' {if($2~/^\/cygdrive\/[a|b|c|d|e]/){print $1;system("cygpath -w -p " $2 )}else{ print $0 }; print ""}' | awk 'BEGIN{opt="";last=""}{if($0~/^$/){ opt=opt " "; last="" }else{ if(last!=""){ opt=opt "="} opt=opt $0; last=$0; }; }END{ print opt }' `

        JAVA_LIBRARY_PATH=`cygpath -p -w "$JAVA_LIBRARY_PATH"`

    fi
}

/* 系统的换行符不同，需要转换 */
SECONDARY_NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -secondarynamenodes 2&gt;/dev/null | sed 's/^M//g' )
</code></pre>

<p>在解析OPTS时执行cygpath转换的时刻，也需要加上-p的参数！OPTS中有java.library.path的环境变量！</p>

<ul>
<li>HDFS文件系统测试</li>
</ul>


<pre><code>bin/hadoop namenode -format
sbin/start-dfs.sh
ps
</code></pre>

<p>jps没有作用了；或者也可以通过任务管理器/<strong>ProcessExplorer</strong>查看java.exe，命令行列还可以查看具体的执行命令，对应的什么服务。</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> 映像名称     </th>
<th style="text-align:left;"> 用户名         </th>
<th style="text-align:left;"> 命令行</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_namenode -Xmx512m &hellip; org.apache.hadoop.hdfs.server.namenode.NameNode</td>
</tr>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_datanode -Xmx512m &hellip; org.apache.hadoop.hdfs.server.datanode.DataNode</td>
</tr>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_secondarynamenode &hellip; org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode</td>
</tr>
</tbody>
</table>


<p>修改了hadoop的脚本，启动环境（cygwin下启动和windows启动都可以），就可以操作HDFS了。</p>

<pre><code>Administrator@winseliu ~
$ hadoop/bin/hadoop fs -put job.xml /
14/04/22 23:53:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

Administrator@winseliu ~
$ export JAVA_LIBRARY_PATH=/cygdrive/e/local/libs/big/hadoop-2.2.0/bin

Administrator@winseliu ~
$ hadoop/bin/hadoop fs -ls /
Found 4 items
-rw-r--r--   1 Administrator supergroup       1366 2014-04-22 22:20 /README.txt
-rw-r--r--   1 Administrator supergroup      66539 2014-04-22 23:53 /job.xml
drwxr-xr-x   - Administrator supergroup          0 2014-04-22 23:34 /out
drwx------   - Administrator supergroup          0 2014-04-22 22:21 /tmp
</code></pre>

<p>如果执行权限问题，可以使用设置HADOOP_USER_NAME的方式处理：</p>

<pre><code>Administrator@winseliu ~/hadoop
$ export HADOOP_USER_NAME=Administrator

Administrator@winseliu ~/hadoop
$ bin/hadoop fs -rmr /out
</code></pre>

<h4>MapReduce任务测试</h4>

<pre><code>sbin/start-yarn.sh
ps
</code></pre>

<p>yarn资源框架启动后，任务管理又会添加两个java的程序：</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> 映像名称     </th>
<th style="text-align:left;"> 用户名         </th>
<th style="text-align:left;"> 命令行</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_resourcemanager &hellip;  org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</td>
</tr>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_nodemanager &hellip; org.apache.hadoop.yarn.server.nodemanager.NodeManager</td>
</tr>
</tbody>
</table>


<h4>提交任务，执行任务处理</h4>

<p>在cygwin环境下，hdfs和yarn都启动成功了，并且能传文件到HDFS中。但是由于cygwin环境最终还是使用windows的java程序集群执行任务！</p>

<p>（可考虑[2.2 Eclipse提交MapReduce]）</p>

<ul>
<li>已处理问题一： cygwin下启动nodemanager，路径没转换</li>
</ul>


<p>由于在cygwin下面启动，大部分的环境变量都是从cygwin带过来的！解析conf中的变量时会使用nodemanager中对应变量的值，如HADOOP_MAPRED_HOME等。</p>

<p>在cygwin使用start-yarn.sh调用java启动程序之前需要转换路径为windows下的路径。在上面的操作已经进行了处理。</p>

<pre><code># 在临时目录下生成了launch_container.cmd文件，用于执行命令，而里面环境变量的值有些cygwin环境下的！

# 设置端口调试nodemanager
Administrator@winseliu ~/hadoop
$ grep "8092" etc/hadoop/*
etc/hadoop/yarn-env.sh:export YARN_NODEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8092"

## windows下这个方法大有文章，会把客户端传递的CLASSPATH写入jar的MANIFEST.MF中！
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv()
</code></pre>

<ul>
<li>已处理问题二：执行mapreduce任务时，缺少环境变量（使用Process Explorer工具查看）</li>
</ul>


<pre><code># 设置远程调试map
hadoop org.apache.hadoop.examples.WordCount  "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090" /job.xml /out

# mapred-site.xml设置超时时间
    &lt;property&gt;
        &lt;name&gt;mapred.task.timeout&lt;/name&gt;
        &lt;value&gt;1800000&lt;/value&gt;
    &lt;/property&gt;

# 结束任务
Administrator@winseliu ~/hadoop
$ bin/hadoop job -kill job_1398407971082_0003
</code></pre>

<ul>
<li>取不到HADOOP_HOME环境变量，查找winutils.exe时报错！

<ul>
<li>在hadoop-env.sh中增加定义HADOOP_HOME！</li>
</ul>
</li>
<li>library路径问题，解析动态链接库hadoop.dll失败！

<ul>
<li>增加-D参数吧！</li>
</ul>
</li>
</ul>


<pre><code>hadoop org.apache.hadoop.examples.WordCount "-Dmapreduce.map.java.opts= -Djava.library.path=E:\local\libs\big\hadoop-2.2.0\bin" "-Dmapreduce.reduce.java.opts=-Djava.library.path=E:\local\libs\big\hadoop-2.2.0\bin"  /job.xml /out
</code></pre>

<p>windows泽腾啊。</p>

<ul>
<li>问题二：直接提交任务到linux集群，环境变量不匹配</li>
</ul>


<pre><code>Administrator@winseliu ~/hadoop
$ bin/hadoop  fs -ls hdfs://192.168.1.104:9000/

Administrator@winseliu ~/hadoop
$ export HADOOP_USER_NAME=hadoop

Administrator@winseliu ~/hadoop
$  bin/hadoop  org.apache.hadoop.examples.WordCount   -fs hdfs://192.168.1.104:9000 -jt 192.168.1.104 /in /out
</code></pre>

<p>由于本地是windows的java执行任务提交到集群，所以使用了<code>%JAVA_HOME%</code>，以及windows下的CLASSPATH！执行任务时，同时把nodemanager节点的临时目录备份下来再慢慢查看：</p>

<pre><code>[hadoop@slave temp]$ while true ; do cp -rf nm-local-dir/ backup/ ; sleep 0.1; done
[hadoop@slave temp]$ find . -name "*.sh"
</code></pre>

<p>修复该问题，可以参考[2.2 Eclipse提交MapReduce]。</p>

<h3>参考</h3>

<hr />

<h2>二、Windows下使用eclipse连接linux集群</h2>

<h3>2.1 java代码操作HDFS</h3>

<pre><code>public class HelloHdfs {

    public static boolean FINISH_CLEAN = true;

    public static void main(String[] args) throws IOException {
        System.setProperty("HADOOP_USER_NAME", "hadoop"); // 设置用户，否则会有读取权限的问题

        FileSystem fs = FileSystem.get(new Configuration());

        fs.mkdirs(new Path("/java/folder"));
        OutputStream os = fs.create(new Path("/java/folder/hello.txt"));
        Writer w = new BufferedWriter(new OutputStreamWriter(os, "UTF-8"));
        w.write("hello hadoop!");
        w.flush();
        w.close();
        os.close();

        FSDataInputStream is = fs.open(new Path("/java/folder/hello.txt"));
        BufferedReader br = new BufferedReader(new InputStreamReader(is, "UTF-8"));
        System.out.println(br.readLine());
        br.close();
        is.close();

        // IOUtils.copyBytes(in, out, 4096, true);

        if (FINISH_CLEAN)
            fs.delete(new Path("/java"), true);
    }

}
</code></pre>

<p>对于访问linux集群的hdfs，只要编译通过，对集群HDFS文件系统的CRUD基本没有不会遇到什么问题。写代码过程中遇到过下面两个问题：</p>

<ul>
<li><p>如果你也引入了hive的包，可能会抛不能重写final方法的错误！由于hive中也就了proto的代码（final），调整下顺序先加载proto的包就可以了！</p>

<pre><code class="``">  log4j:WARN Please initialize the log4j system properly.
  Exception in thread "main" java.lang.VerifyError: class org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
      at java.lang.ClassLoader.defineClass1(Native Method)
      at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
</code></pre></li>
<li><p>Permission denied: user=Administrator, access=WRITE, inode=&ldquo;/&rdquo;:hadoop:supergroup:drwxr-xr-x
  这个问题的处理方式有很多。</p>

<ul>
<li>hadoop fs -chmod 777 /</li>
<li>在hdfs的配置文件中，将dfs.permissions修改为False</li>
<li>System.setProperty(&ldquo;user.name&rdquo;, &ldquo;hduser&rdquo;)/System.setProperty(&ldquo;HADOOP_USER_NAME&rdquo;, &ldquo;hduser&rdquo;)/configuration.set(&ldquo;hadoop.job.ugi&rdquo;, &ldquo;hduser&rdquo;);</li>
</ul>
</li>
</ul>


<h3>2.2 Eclipse提交MapReduce</h3>

<ul>
<li><p>需要设置HADOOP_HOME/hadoop.home.dir的环境变量，即在该目录下面有bin\winutils.exe的文件。否则会报错：</p>

<pre><code class="``">  14/04/14 20:07:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  14/04/14 20:07:58 ERROR util.Shell: Failed to locate the winutils binary in the hadoop binary path
  java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
      at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
      at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
      at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:293)
</code></pre></li>
<li><p>任务端(map/reduce)执行命令的classpath变量在客户端Client拼装的！</p>

<p>  浏览官网的<a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655" title="Remote job submit from windows to a linux hadoop cluster fails due to wrong classpath">jira</a>，然后下载并应用<a href="https://issues.apache.org/jira/secure/attachment/12616981/MRApps.patch">MRApps.patch</a>和<a href="https://issues.apache.org/jira/secure/attachment/12616982/YARNRunner.patch">YARNRunner.patch</a>两个补丁。</p>

<p>  其实就是修改Apps#addToEnvironment(Map&lt;String, String>, String, String)来拼装特定操作系统的classpath。以及JAVA_HOME等一些环境变量的值（<code>$JAVA_HOME</code> or <code>%JAVA_HOME%</code>）</p>

<p>  使用<code>patch -p1 &lt; PATCH</code>进行修复。如果patch文件不在项目根路径，可以删除补丁内容前面文件夹路径，直接与源文件放一起然后应用patch就行了。当然你根据修改的内容手动修改也是OK的。</p></li>
</ul>


<p>如果仅仅是作为客户端client提交任务时使用。如仅在eclipse中运行main提交任务，那么就没有必要打包！直接放到需要项目源码中即可。</p>

<pre><code>* 把应用了补丁的YARNRunner和MRApps加入到项目中
* 然后再configuration中加入`config.set("mapred.remote.os", "Linux")`
* 把mapreduce的任务打包为jar，然后`job.setJar("helloyarn.jar")`
* 最后`Run As -&gt; Java Application`运行提交
</code></pre>

<p>如果很多项目使用，可以打包出来，然后把它添加到classpath中，同时添加加入自定义的xml配置。</p>

<pre><code>lib-ext&gt;jar tvf window-client-mapreduce-patch.jar
    25 Wed Apr 16 11:21:26 CST 2014 META-INF/MANIFEST.MF
 26684 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapred/YARNRunner.class
 24397 Tue Apr 15 10:32:28 CST 2014 org/apache/hadoop/mapred/YARNRunner.java
  1406 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps$1.class
  2450 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps$TaskAttemptStateUI.class
 19887 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps.class
 18879 Tue Apr 15 11:42:42 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps.java
</code></pre>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFe3eATYhNAAifW_Xk8HI644.png" alt="" /></p>

<h3>参考：</h3>

<ul>
<li><a href="http://zy19982004.iteye.com/blog/2031172">Hadoop学习三十二：Win7下无法提交MapReduce Job到集群环境</a></li>
<li><a href="http://blog.csdn.net/fansy1990/article/details/22896249">Eclipse调用hadoop2运行MR程序</a></li>
<li><a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655">jira-Remote job submit from windows to a linux hadoop cluster fails due to wrong classpath</a></li>
</ul>


<hr />

<h2>三、导入源码到eclipse</h2>

<h3>环境</h3>

<p>参考前面的【window伪分布式部署】</p>

<h3>打开Visual Studio的命令行工具</h3>

<pre><code>启动\所有程序\Microsoft Visual Studio 2010\Visual Studio Tools\Visual Studio 命令提示(2010)
</code></pre>

<h3>获取源码，检查2.2.0的分支</h3>

<pre><code>git clone git@github.com:apache/hadoop-common.git
git checkout branch-2.2.0
</code></pre>

<p>也可以下载src的源码包，但是如果想修改点东西的话，clone源码应该是最佳的选择了。</p>

<ul>
<li>前面说的win32的patch，如果记得打上哦！参见[1.3 编译源代码生成本地依赖库(dll, exe)]</li>
<li>编译hadoop-auth项目的时刻报错，需要在pom中添加jetty-util的依赖，参考<a href="http://www.cnblogs.com/sysuys/p/3492791.html">找不到org.mortbay.component.AbstractLifeCycle的类文件</a>。</li>
</ul>


<h3>编译生成打包</h3>

<pre><code>set PATH=c:\cygwin\bin;%PATH%
mvn package -Pdist,native-win -DskipTests -Dmaven.javadoc.skip=true
</code></pre>

<p>最好加上skipTests条件，不然编译等待时间不是一般的长！！</p>

<h3>导入eclipse</h3>

<pre><code>mvn eclipse:eclipse
</code></pre>

<p>然后使用eclipse导入已经存在的工程(existing projects into workspace)，导入后存在两个问题：</p>

<ol>
<li>stream工程的conf源码包找不到。修改为在.project文件中引用，然后把conf引用加入到.classpath。</li>
<li>common下的test代码报错。把<code>target/generated-test-sources/java</code>文件夹的也作为源码包即可。</li>
</ol>


<p><img src="http://file.bmob.cn/M00/0B/29/wKhkA1QFeTmAStwwAAuCI-ODX3Q346.png" alt="" /></p>

<p>eclipse的maven插件你得安装了（要用到M2_REPO路径），同时引用正确conf\settings.xml的Maven配置路径。</p>

<p>注意： 不要使用eclipse导入已经存在的maven方式！eclipse的m2e有些属性和插件还不支持，导入后会报很多错！而使用<code>mvn eclipse:eclipse</code>的方式是把依赖的jar加入到<code>.classpath</code>。</p>

<h3>参考</h3>

<ul>
<li><a href="http://www.cnblogs.com/zhengcong/p/3592490.html">使用Maven将Hadoop2.2.0源码编译成Eclipse项目</a></li>
</ul>


<hr />

<h2>四、胡乱噗噗</h2>

<h3>查看Debug日志</h3>

<pre><code>[hadoop@umcc97-44 ~]$ export HADOOP_ROOT_LOGGER=DEBUG,console
[hadoop@umcc97-44 ~]$ hadoop fs -ls /
</code></pre>

<h3>java加载动态链接库的环境变量java.library.path</h3>

<pre><code>D:\local\cygwin\Administrator\test&gt;java LoadLib

D:\local\cygwin\Administrator\test&gt;java -Djava.library.path=. LoadLib
Exception in thread "main" java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860)
        at java.lang.Runtime.loadLibrary0(Runtime.java:845)
        at java.lang.System.loadLibrary(System.java:1084)
        at LoadLib.main(LoadLib.java:3)

D:\local\cygwin\Administrator\test&gt;java -Djava.library.path=".;%PATH%" LoadLib
</code></pre>

<p>没有定义的时刻，会去PATH路径下找。一旦定义了java.library.path只会在给定的路径下查找！</p>

<h3>hadoop的本地native-library的位置</h3>

<p>文件具体放什么位置，随便运行一个命令，通过debug的日志就可以看到默认Library的路径。</p>

<pre><code>Administrator@winseliu ~/hadoop
$ export HADOOP_ROOT_LOGGER=DEBUG,console

Administrator@winseliu ~/hadoop
$ bin/hadoop fs -ls /

14/04/18 09:48:39 DEBUG util.NativeCodeLoader: java.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native
14/04/18 09:48:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</code></pre>

<h3>cygwin下运行java程序，路径问题</h3>

<pre><code>Administrator@winseliu ~/test
$ ls
ENV.class  ENV.java  w3m-0.5.2

Administrator@winseliu ~/test
$ java ENV
Windows 7

Administrator@winseliu ~/test
$ jar cvf test.jar *.class
已添加清单
正在添加: ENV.class(输入 = 475) (输出 = 307)(压缩了 35%)

Administrator@winseliu ~/test
$ ls -l
总用量 18
-rwxr-xr-x  1 Administrator None 475 四月  4 15:00 ENV.class
-rw-r--r--  1 Administrator None 117 四月  4 15:00 ENV.java
-rwxr-xr-x  1 Administrator None 758 四月 19 12:03 test.jar
drwxr-xr-x+ 1 Administrator None   0 四月 12 20:31 w3m-0.5.2

Administrator@winseliu ~/test
$ java -cp test.jar ENV
Windows 7

Administrator@winseliu ~/test
$ java -cp /home/Administrator/test/test.jar ENV
错误: 找不到或无法加载主类 ENV

Administrator@winseliu ~/test
$ set -x

Administrator@winseliu ~/test
$ java -cp `cygpath -w /home/Administrator/test/test.jar` ENV
++ cygpath -w /home/Administrator/test/test.jar
+ java -cp 'D:\local\cygwin\Administrator\test\test.jar' ENV
Windows 7
</code></pre>

<h3>[cygwin]ssh单独用户权限问题</h3>

<pre><code>Administrator@winseliu ~
$ hadoop/bin/hadoop  fs -put .bash_profile /bash.info
put: Permission denied: user=Administrator, access=WRITE, inode="/":cyg_server:supergroup:drwxr-xr-x
</code></pre>

<ul>
<li>设置环境变量<code>HADOOP_USER_NAME=hadoop</code></li>
<li>可以使用dfs.permissions属性设置为false。</li>
<li>给位置chown/chmod赋权: <code>hadoop fs -chmod 777 /</code></li>
<li>也可以使用ssh-host-config的<code>Should privilege separation be used? (yes/no) no</code>设置为<strong>no</strong>。使用当前用户进行管理。
  <code>
  Administrator@winseliu /var
  $ chown Administrator:None empty/
  Administrator@winseliu ~
  $ /usr/sbin/sshd.exe # 启动，也可以弄个脚本到启动项，开机启动
  Administrator@winseliu ~/hadoop
  $ ps | grep ssh
       4384       1    4384       4384  ?        500 02:41:21 /usr/sbin/sshd
 </code></li>
</ul>


<h3>Visual Studio处理winutils工程</h3>

<ul>
<li><p>winutils的32位编译
  .net framework4, vs2010, 属性修改设置
  <a href="http://stackoverflow.com/questions/12267158/failure-during-conversion-to-coff-file-invalid-or-corrupt">http://stackoverflow.com/questions/12267158/failure-during-conversion-to-coff-file-invalid-or-corrupt</a>
  <a href="http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval">http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval</a>
  <a href="http://social.msdn.microsoft.com/Forums/vstudio/en-US/eb4a7699-0f3c-4701-9790-199787f1b359/vs-2010-error-lnk1123-failure-during-conversion-to-coff-file-invalid-or-corrupt?forum=vcgeneral">http://social.msdn.microsoft.com/Forums/vstudio/en-US/eb4a7699-0f3c-4701-9790-199787f1b359/vs-2010-error-lnk1123-failure-during-conversion-to-coff-file-invalid-or-corrupt?forum=vcgeneral</a></p>

<p>  <a href="http://hi.baidu.com/dreamthief/item/aa690d1494e2caca38cb306d">http://hi.baidu.com/dreamthief/item/aa690d1494e2caca38cb306d</a></p>

<p>  在cygwin安装的时刻也看过这篇，用64位环境maven的是可以编译的
  <a href="http://www.srccodes.com/p/article/38/build-install-configure-run-apache-hadoop-2.2.0-microsoft-windows-os">http://www.srccodes.com/p/article/38/build-install-configure-run-apache-hadoop-2.2.0-microsoft-windows-os</a></p>

<p>  <a href="http://stackoverflow.com/questions/18630019/running-apache-hadoop-2-1-0-on-windows">http://stackoverflow.com/questions/18630019/running-apache-hadoop-2-1-0-on-windows</a></p></li>
</ul>


<h3>[cygwin]ipv6的问题，改成ipv4后不能登陆！</h3>

<p>可能是新版本的openssh的bug！！！</p>

<pre><code>Administrator@winseliu /cygdrive/h/documents
$ ssh -o AddressFamily=inet localhost -v
OpenSSH_6.5, OpenSSL 1.0.1g 7 Apr 2014
debug1: Reading configuration data /etc/ssh_config
debug1: Connecting to localhost [127.0.0.1] port 22.
debug1: Connection established.
debug1: identity file /home/Administrator/.ssh/id_rsa type 1
debug1: identity file /home/Administrator/.ssh/id_rsa-cert type -1
debug1: identity file /home/Administrator/.ssh/id_dsa type -1
debug1: identity file /home/Administrator/.ssh/id_dsa-cert type -1
debug1: identity file /home/Administrator/.ssh/id_ecdsa type -1
debug1: identity file /home/Administrator/.ssh/id_ecdsa-cert type -1
debug1: identity file /home/Administrator/.ssh/id_ed25519 type -1
debug1: identity file /home/Administrator/.ssh/id_ed25519-cert type -1
debug1: Enabling compatibility mode for protocol 2.0
debug1: Local version string SSH-2.0-OpenSSH_6.5
ssh_exchange_identification: read: Connection reset by peer

Administrator@winseliu ~
$ ping localhost

正在 Ping winseliu [::1] 具有 32 字节的数据:
来自 ::1 的回复: 时间&lt;1ms
来自 ::1 的回复: 时间&lt;1ms
</code></pre>

<p>还不能在hosts文件中加！如，指定localhost为127.0.0.1后，得到结果为：</p>

<pre><code>Administrator@winseliu ~/hadoop
$ ssh localhost
ssh_exchange_identification: read: Connection reset by peer
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GIT操作记录手册]]></title>
    <link href="http://winseliu.com/blog/2014/03/30/git-cheatsheet/"/>
    <updated>2014-03-30T22:15:11+08:00</updated>
    <id>http://winseliu.com/blog/2014/03/30/git-cheatsheet</id>
    <content type="html"><![CDATA[<p>Git的每次提交都有一个<strong>唯一的ID</strong>与之对应，所有的TAG/Branch/Master/HEAD等等，都是一个<strong>软链接/别名</strong>而已！这个是理解好Git的基础！</p>

<h2>提交最佳实践</h2>

<ul>
<li>commit 只改一件事情。</li>
<li>如果一个文档有多个变更，使用<code>git add --patch</code>只选择文档中的<strong>部分变更</strong>进入stage。具体怎么使用，键入命令后在输入<code>?</code></li>
<li>写清楚 commit message</li>
</ul>


<h2>配置</h2>

<h3>内建的图形化 git：</h3>

<pre><code>gitk
</code></pre>

<h3>git服务器</h3>

<p>搭建git服务器也很方便，有很多web-server的版本，我试用了下<a href="http://www.scm-manager.org/download/">scm-manager</a>使用挺简单的！
如果已经有了SVN的服务器，可以直接使用git-svn检出到本地！！</p>

<h3>配置环境</h3>

<pre><code>git config --global user.email "XXX"
git config --global user.name "XXX"
</code></pre>

<h3>换行（\r\n）提交检出均不转换</h3>

<p>基本上都在windows操作系统上工作，不需要进行转换！</p>

<pre><code>git config --global core.autocrlf false
</code></pre>

<ul>
<li>true 提交时转换为LF，检出时转换为CRLF</li>
<li>input 提交时转换为LF，检出时不转换</li>
<li>false 提交检出均不转换</li>
</ul>


<h3>core.safecrlf</h3>

<ul>
<li>true 拒绝提交包含混合换行符的文件</li>
<li>false 允许提交包含混合换行符的文件</li>
<li>warn 提交包含混合换行符的文件时给出警告</li>
</ul>


<h3>默认分支</h3>

<p>.git/config如下的内容：</p>

<pre><code>[branch "master"]
    remote = origin
    merge = refs/heads/master
</code></pre>

<p>这等于告诉git两件事:
1. 当你处于master branch, 默认的remote就是origin。
2. 当你在master branch上使用git pull时，没有指定remote和branch，那么git就会采用默认的remote（也就是origin）来merge在master branch上所有的改变</p>

<p>如果不想或者不会编辑config文件的话，可以在bush上输入如下命令行：</p>

<pre><code>$ git config branch.master.remote origin 
$ git config branch.master.merge refs/heads/master 
</code></pre>

<p>之后再重新git pull下。最后git push你的代码，到此步顺利完成时，则可以在Github上看到你新建的仓库以及你提交到仓库中文件了OK。</p>

<h3>修改默认Git编辑器</h3>

<pre><code>$ git config core.editor vim

$ git config --global core.editor vi
</code></pre>

<h2>常用基本操作</h2>

<table>
<thead>
<tr>
<th style="text-align:left;"> 操作                                          </th>
<th style="text-align:left;"> 说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> git init                                      </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git init &ndash;bare                               </td>
<td style="text-align:left;"> 服务端使用bare（空架子，赤裸）的方式</td>
</tr>
<tr>
<td style="text-align:left;"> git status                                    </td>
<td style="text-align:left;"> 使用git打的最多的就是status命令，查看状态的同时会提示下一步的操作！</td>
</tr>
<tr>
<td style="text-align:left;"> git diff                                      </td>
<td style="text-align:left;"> 工作空间和index/stage进行对比</td>
</tr>
<tr>
<td style="text-align:left;"> git diff &ndash;cached                             </td>
<td style="text-align:left;"> index/stage与本地仓库进行对比</td>
</tr>
<tr>
<td style="text-align:left;"><strong>增加到变更(index/stage)</strong>                    </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git add .                                     </td>
<td style="text-align:left;"> 将当前目录添加到git仓库中，常用命令！</td>
</tr>
<tr>
<td style="text-align:left;"> git add -A                                    </td>
<td style="text-align:left;"> 添加所有改动的文档</td>
</tr>
<tr>
<td style="text-align:left;"> git add -u                                    </td>
<td style="text-align:left;"> 只加修改过的文件,新增的文件不加入</td>
</tr>
<tr>
<td style="text-align:left;"> git rm &ndash;cached <file>&hellip;                     </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"><strong>添加到本地库</strong>                               </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git commit                                    </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git commit -m &ldquo;msg&rdquo;                           </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git commit -a                                 </td>
<td style="text-align:left;"> -a是把所有的修改的（tracked）文件都commit</td>
</tr>
<tr>
<td style="text-align:left;"> git commit &ndash;amend -m &ldquo;commit message.&rdquo;       </td>
<td style="text-align:left;"> 未push到远程分支的提交，快捷的回退再提交。修补提交（修补最近一次的提交而不创建新的提交），可结合git add使用！</td>
</tr>
<tr>
<td style="text-align:left;"> git commit -v                                 </td>
<td style="text-align:left;"> -v 可以看到文件哪些内容被修改</td>
</tr>
<tr>
<td style="text-align:left;"> git commit -m &lsquo;v1.2.0-final&rsquo; &ndash;allow-empty    </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git reset                                     </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git reset HEAD^                               </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git reset &ndash;hard HEAD^                        </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git checkout file                             </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git checkout &ndash;orphan <branchname>; git rm &ndash;cached -r . </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git rebase                                    </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git merge                                     </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"><strong>日志</strong>                                       </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log                                       </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;oneline &ndash;decorate &ndash;graph          </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;stat                                </td>
<td style="text-align:left;"> 查看提交信息及更新的文件</td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;stat -p -1 &ndash;format=raw             </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log -3 <file-name>                        </td>
<td style="text-align:left;"> 文件的最近3次提交的历史版本记录</td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;stat -2                             </td>
<td style="text-align:left;"> 查看最近两次的提交描述及修改文件信息</td>
</tr>
<tr>
<td style="text-align:left;"> git log -p -2                                 </td>
<td style="text-align:left;"> 展开显示每次提交的内容差异，类似git show功能</td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;name-status                         </td>
<td style="text-align:left;"> 仅显示文件的D/M/A的状态</td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;summary                             </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;dirstat -5                          </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;pretty=format:&ldquo;%h %s&rdquo; &ndash;graph       </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;pretty=oneline                      </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git reflog                                    </td>
<td style="text-align:left;"> 查看本地操作历史。 ref log</td>
</tr>
<tr>
<td style="text-align:left;"> git show                                      </td>
<td style="text-align:left;"> 查看某版本文件的内容，版本库中最新提交的diff！</td>
</tr>
<tr>
<td style="text-align:left;"> git show master:index.md                      </td>
<td style="text-align:left;"> 查看历史版本的文件内容</td>
</tr>
<tr>
<td style="text-align:left;"> git show &lt;哈希值:文件目录/文件>               </td>
<td style="text-align:left;"> 查看内容</td>
</tr>
<tr>
<td style="text-align:left;"> git cat-file                                  </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"><strong>分支</strong>                                       </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git branch                                    </td>
<td style="text-align:left;"> 查看本地分支</td>
</tr>
<tr>
<td style="text-align:left;"> git branch <branch>                           </td>
<td style="text-align:left;"> 添加新分支，新分支创建后不会自动切换！！</td>
</tr>
<tr>
<td style="text-align:left;"> git branch &ndash;set-upstream branch-name origin/branch-name      </td>
<td style="text-align:left;"> * 建立本地分支和远程分支的关联</td>
</tr>
<tr>
<td style="text-align:left;"> git branch -a                                 </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git branch &ndash;list &ndash;merged                    </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git branch -r                                 </td>
<td style="text-align:left;"> 查看远程分支</td>
</tr>
<tr>
<td style="text-align:left;"> git checkout &ndash;orphan <new-branch>            </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git checkout <branch>                         </td>
<td style="text-align:left;"> 切换分支</td>
</tr>
<tr>
<td style="text-align:left;"> git checkout -b [new_branch_name]             </td>
<td style="text-align:left;"> 创建新分支并立即切换到新分支。git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致</td>
</tr>
<tr>
<td style="text-align:left;"> git branch -d branch_name                     </td>
<td style="text-align:left;"> -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项</td>
</tr>
<tr>
<td style="text-align:left;"> git branch -d -r remote_name/branch_name      </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git merge origin/local-branch                 </td>
<td style="text-align:left;"> 本地分支与主分支合并</td>
</tr>
<tr>
<td style="text-align:left;"><strong>推/拉</strong>                                      </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git pull                                      </td>
<td style="text-align:left;"> * 等价于git fetch &amp;&amp; git merge</td>
</tr>
<tr>
<td style="text-align:left;"> git fetch                                     </td>
<td style="text-align:left;"> 先把git的东西fetch到你本地然后merge后再push</td>
</tr>
<tr>
<td style="text-align:left;"> git push &ndash;rebase                             </td>
<td style="text-align:left;"> *</td>
</tr>
<tr>
<td style="text-align:left;"> git push                                      </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git push &ndash;set-upstream origin <branch>       </td>
<td style="text-align:left;"> To push the current branch and set the remote as upstream</td>
</tr>
<tr>
<td style="text-align:left;"> git push origin branch-name                   </td>
<td style="text-align:left;"> 创建远程分支(本地分支push到远程)，从本地推送分支。如果推送失败，先用git pull抓取远程的新提交</td>
</tr>
<tr>
<td style="text-align:left;"> git push -u origin master                     </td>
<td style="text-align:left;"> 将代码从本地回传到仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git push origin test:master                   </td>
<td style="text-align:left;"> 提交本地test分支作为远程的master分支</td>
</tr>
<tr>
<td style="text-align:left;"> git push -f                                   </td>
<td style="text-align:left;"> * 强推(&ndash;force)，即利用强覆盖方式用你本地的代码替代git仓库内的内容，这种方式不建议使用。</td>
</tr>
<tr>
<td style="text-align:left;"> git pull [remoteName] [localBranchName]       </td>
<td style="text-align:left;"> 获取远程版本库提交与本地提交进行合并</td>
</tr>
<tr>
<td style="text-align:left;"> git push [remoteName] [localBranchName]       </td>
<td style="text-align:left;"> 提交、推送远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git push &ndash;tags                               </td>
<td style="text-align:left;"> 提交时带上标签信息</td>
</tr>
<tr>
<td style="text-align:left;"> git push <git-url> master                     </td>
<td style="text-align:left;"> 把本地仓库提交到远程仓库的master分支中</td>
</tr>
<tr>
<td style="text-align:left;"> git push origin :branch_name                  </td>
<td style="text-align:left;"> 删除远端分支,(如果:左边的分支为空，那么将删除:右边的远程的分支。)远程的test将被删除，但是本地还会保存的，不用担心。</td>
</tr>
<tr>
<td style="text-align:left;"> git push origin :/refs/tags/tagname           </td>
<td style="text-align:left;"> 删除远端标签</td>
</tr>
<tr>
<td style="text-align:left;"> git clone <a href="http://path/to/git.git">http://path/to/git.git</a>              </td>
<td style="text-align:left;"> clone的内容会放在当前目录下的新目录</td>
</tr>
<tr>
<td style="text-align:left;"> git clone &ndash;branch <remote-branch> <git-url>  </td>
<td style="text-align:left;"> 获取指定分支，检出远程版本的分支。 git clone &ndash;branch unity /d/winsegit/hello helloclone</td>
</tr>
<tr>
<td style="text-align:left;"><strong>TAG</strong>                                        </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git tag                                       </td>
<td style="text-align:left;"> 查看标签</td>
</tr>
<tr>
<td style="text-align:left;"> git tag <tag>                                 </td>
<td style="text-align:left;"> 添加标签</td>
</tr>
<tr>
<td style="text-align:left;"> git tag -d <tag>                              </td>
<td style="text-align:left;"> 删除标签</td>
</tr>
<tr>
<td style="text-align:left;"> git tag -r                                    </td>
<td style="text-align:left;"> 查看远程标签</td>
</tr>
<tr>
<td style="text-align:left;"> git show <tag>                                </td>
<td style="text-align:left;"> 查看标签的信息</td>
</tr>
<tr>
<td style="text-align:left;"> git tag -a <tag> <msg>                        </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"><strong>REMOTE</strong>                                     </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git remote [show]                             </td>
<td style="text-align:left;"> 查看远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git remote -v                                 </td>
<td style="text-align:left;"> 查看远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git remote add [name] [url]                   </td>
<td style="text-align:left;"> 添加远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git remote set-url &ndash;push[name][newUrl]       </td>
<td style="text-align:left;"> 修改远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git remote show origin                        </td>
<td style="text-align:left;"> 远程库origin的详细信息。缺省值推送分支，有哪些远端分支还没有同步到本地，哪些已同步到本地的远端分支在远端服务器上已被删除，git pull 时将自动合并哪些分支！</td>
</tr>
<tr>
<td style="text-align:left;"> git remote show <remote-name>                 </td>
<td style="text-align:left;"> 远程版本信息查看</td>
</tr>
<tr>
<td style="text-align:left;"> git remote add origin <git-url>               </td>
<td style="text-align:left;"> 设置仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git remote rm [name]                          </td>
<td style="text-align:left;"> 删除远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"><strong>文件列表</strong>                                   </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;">git ls-tree &ndash;name-only  -rt <SHA-ID></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"><strong>打包</strong>                                       </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git archive &ndash;format tar &ndash;output <tar> master</td>
<td style="text-align:left;"> 将 master以tar格式打包到指定文件</td>
</tr>
</tbody>
</table>


<pre><code>$ git config core.filemode false
</code></pre>

<h2>按功能点完整的操作步骤</h2>

<h3>查看指定版本文件内容</h3>

<pre><code>Administrator@WINSELIU /e/git/hello (master)
$ git ls-tree master
100644 blob 139b30f9054cf77bd2eeabcebaf6ca3f32cd1d50    abc

Administrator@WINSELIU /e/git/hello (master)
$ git cat-file -p 139b30f9054cf77bd2eeabcebaf6ca3f32cd1d50
</code></pre>

<h3>回归指定的版本文件</h3>

<pre><code># 通过上面的命令得到该文件的版本
winse@Lenovo-PC ~/eshore/git
$ git checkout 4179d96 eshore/DTA/ISMI_CU/docs/测试/省汇聚平台/dta/kettle/file/hive2mysql.ktr
</code></pre>

<h3>查看提交版本的指定文件内容</h3>

<pre><code>git log abc  # 获取文件提交ID
git cat-file -p &lt;commit-id&gt;  # 获取treeID
git cat-file -p &lt;tree-id&gt;  # 获取当前tree的列表
git cat-file -p &lt;file-blob-id&gt;
</code></pre>

<h3>根据格式输出日志</h3>

<pre><code>$ git log --pretty=oneline
$ git log --pretty=short
$ git log --pretty=format:'%h was %an, %ar, message: %s'
$ git log --pretty=format:'%h : %s' --graph
$ git log --pretty=format:'%h : %s' --topo-order --graph
$ git log --pretty=format:'%h : %s' --date-order --graph
</code></pre>

<p>你也可用‘medium',&lsquo;full&rsquo;,&lsquo;fuller&rsquo;,&lsquo;email&rsquo; 或‘raw'. 如果这些格式不完全符合你的相求， 你也可以用‘&ndash;pretty=format'参数(参见：git log)来创建你自己的"格式“.</p>

<h3>本地提交后再次修改</h3>

<p><strong>修改注释</strong></p>

<pre><code>git commit --amend 
</code></pre>

<p><strong>内容修改</strong></p>

<pre><code> # edit file
git add file
git commit --amend
</code></pre>

<p><strong>提交了不该提交的，并撤回</strong></p>

<p>刚刚提交的不完整，想修改一些东西，加到刚才的提交中</p>

<p>commit -> modify -> add -> amend</p>

<pre><code>git reset HEAD^
git status
cat abc
git diff
git commit -a -m "for test reset"
git log
git diff

vi abc
git add abc
git commit --amend

git status
git diff
git show master:abc
git log
</code></pre>

<h3>没有push到远程库的提交，本地可以做的事情</h3>

<ul>
<li>git reset: 用于回溯，回到原来的提交节点，多次提交合并为一个</li>
<li>git rebase <origin>：在origin分支的基础上，合并当前分支上的提交，形成线性提交历史。 会把当前分支的提交保存为patch，然后切到origin分支应用patch，形成线性的提交，common-origin-current。</li>
</ul>


<p>rebase冲突处理时，使用git add &amp;&amp; git rebase &ndash;continue。如果你使用了git add &amp;&amp; git commit，那么当前冲突使用git rebase &ndash;skip即可。</p>

<h3>处理本地和服务器之间冲突的方式</h3>

<ul>
<li>以本地为主。 git push -f</li>
<li>归并merge。 git pull 或者 git fetch &amp;&amp; git merge</li>
<li></li>
</ul>


<p>从stash恢复出现冲突，可以先提交，然后在pop，最后处理冲突。一般提交到本地index中的数据才是自己想要的，从stash中获取的数据只是临时的，可以直接用HEAD的数据内容覆盖，省去处理冲突的时间。</p>

<pre><code>$ git add -u
$ git commit -m 'update XXXX'
$ git stash pop

$ git status | grep 'both modified'  | grep ' ssh-config' | awk -F: '{print $2}' | while read line ; do git show HEAD:"$line" &gt; "$line" ; done
</code></pre>

<h3>从Github远程服务上拿其他分支：</h3>

<pre><code>Administrator@WINSELIU /e/git/to-markdown (master)
$ git branch -r
  origin/HEAD -&gt; origin/master
  origin/gh-pages
  origin/jquery
  origin/master

$ git checkout -b jquery origin/jquery
</code></pre>

<h3>把本地的git项目发布到Github</h3>

<pre><code>touch README.md
git init
git add README.md
git commit -m "first commit"
git remote add origin git@github.com:winse/flickr-uploader.git
git push -u origin master
</code></pre>

<p>Push an existing repository from the command line：</p>

<pre><code>git remote add origin git@github.com:winse/flickr-uploader.git
git push -u origin master
</code></pre>

<p>如果已经存在remote origin，使用下面的方式修改远程的地址：</p>

<pre><code>Administrator@WINSELIU /d/winsegit/flickr_uploader/chrome (master)
$ git remote set-url --add origin  git@github.com:winse/flickr-uploader.git

Administrator@WINSELIU /d/winsegit/flickr_uploader/chrome (master)
$ git remote show origin
Warning: Permanently added 'github.com,192.30.252.128' (RSA) to the list of known hosts.
* remote origin
  Fetch URL: git@github.com:winse/flickr-uploader.git
  Push  URL: git@github.com:winse/flickr-uploader.git
  HEAD branch: (unknown)
</code></pre>

<h3>reset后撤回</h3>

<p>可能存在已经更新的数据，先提交到临时缓存区</p>

<pre><code>git stash
</code></pre>

<p>然后通过reflog得到需要撤回到的版本号</p>

<pre><code>$ git reflog
ef9ccf7 HEAD@{0}: reset: moving to HEAD^^
4f317fe HEAD@{1}: commit (amend): 2015-03-04 d
</code></pre>

<p>仍然使用reset回退</p>

<pre><code>git reset --hard 4f317fe
git stash pop
# 处理冲突
# git add 冲突文件
git reset
</code></pre>

<h3>git查看本地领先远程的提交</h3>

<pre><code>Administrator@WINSELIU /d/winsegit/winse.github.com (master)
$ git status
# On branch master
# Your branch is ahead of 'origin/master' by 2 commits.
#   (use "git push" to publish your local commits)
#
# Changes not staged for commit:
#   (use "git add &lt;file&gt;..." to update what will be committed)
#   (use "git checkout -- &lt;file&gt;..." to discard changes in working directory)
#
#       modified:   about.md
#       modified:   blog/_posts/2014-01-21-monitoring-mobile-networks.md
#
# Untracked files:
#   (use "git add &lt;file&gt;..." to include in what will be committed)
#
#       default.html
no changes added to commit (use "git add" and/or "git commit -a")

Administrator@WINSELIU /d/winsegit/winse.github.com (master)
$ git log --oneline --decorate -5
0425ec5 (HEAD, master) 增加日志Github修改历史功能
f3a4a58 把TAG定位到首页，并分页以及按照年分类
d5097e3 (origin/master, origin/HEAD) plugins disabled in github page!
e75d62b test
876dd42 修复根目录下的md不能通过npp-windows请求编辑的BUG

Administrator@WINSELIU /d/winsegit/winse.github.com (master)
$ git cherry
+ f3a4a58cfead3aa76e4b92de3342bee5970accb7
+ 0425ec548aa4e3dd29cd6fbfa1b656543e85058e
</code></pre>

<h3>找回游离的提交</h3>

<p><strong>绑定到新分支</strong></p>

<pre><code>git reflog # 查看本地操作历史
git branch head23 HEAD@{23} # 把分支head23指向/绑定到游离的提交
</code></pre>

<p>git的版本都是从分支开始查找的，如果没有被分支管理的提交就游离在版本库中！
所以在reset重新修改时，最好建立分支然后再提交！
如果发现类似的提交问题，就需要尽快的修复，不然提交的ID找不到就S了！</p>

<blockquote><p>那些老的提交会被丢弃。 如果运行垃圾收集命令(pruning garbage collection), 这些被丢弃的提交就会删除. （请查看 git gc)</p></blockquote>

<p><strong>重置HEAD</strong></p>

<pre><code>git reset --hard HEAD@{23}
</code></pre>

<h3>删除提交</h3>

<p>删除提交E：</p>

<pre><code>$ git tag F
$ git tag E HEAD^
$ git tag D HEAD^^
$ git checkout D
$ git cherry-pick master # 把master-patch应用到TAG-D
# fix conflicts
$ git status # 提交
$ git checkout master # checkout到master分支
$ git reset --hard HEAD@{1} # 重置master到删除E后的提交
</code></pre>

<h3>Git浏览特定版本的文件列表</h3>

<pre><code>git ls-tree --name-only  -rt &lt;SHA-ID&gt;
</code></pre>

<h3>删除没有被git track的文件</h3>

<pre><code>git clean -fd # -f force branch switch/ignore unmerged entries， -d if you have new directory
git clean -x -fd

git reset --hard ( or git reset then back to 1. )
git checkout . ( or specify with file names )
git reset --hard ( or git reset then back to 3. )
</code></pre>

<h3>检出SVN项目</h3>

<pre><code>Administrator@ZGC-20130605LYE /e/git
$ git svn clone http://chrome-hosts-manager.googlecode.com/svn/trunk/
</code></pre>

<p><a href="http://www.worldhello.net/2010/02/01/339.html">http://www.worldhello.net/2010/02/01/339.html</a>下面提到的有意思：</p>

<blockquote><p>Git-svn 是 Subversion 的最佳伴侣，可以用 Git 来操作 Subversion 版本库。这带来一个非常有意思的副产品——部分检出：
可以用 git-svn 来对 Subversion 代码库的任何目录进行克隆，克隆出来的是一个git版本库
可以在部分克隆的版本库中用 Git 进行本地提交。
部分克隆版本库中的本地提交可以提交到上游 Subversion 版本库的相应目录中</p></blockquote>

<p>如果需要密码的，使用方面的方式会报错<strong>git-svn died signal 11</strong>。可以先init，然后在fetch。</p>

<pre><code>Kevin@Kevin-PC /cygdrive/d/dta-git
$ git svn init URL --username=NAME
Initialized empty Git repository in /cygdrive/d/dta-git/.git/

$ git svn fetch
Authentication realm: &lt;https://IP:PORT&gt; Subversion Repositories
Password for 'NAME':
# 输入密码后，ctrl+c退出后再重新下载

$ git svn fetch &gt; fetch.log 2&gt;&amp;1
</code></pre>

<ul>
<li><a href="http://rongjih.blog.163.com/blog/static/3357446120107111449543/">如何将SVN仓库转换为Git仓库 </a></li>
</ul>


<h3>Github添加项目主页github page(gh-pages)</h3>

<p>提交后就可以访问了<a href="http://winse.github.io/flickr-uploader/popup.html">页面</a>了。</p>

<pre><code>Administrator@WINSELIU /d/winsegit/flickr_uploader/chrome (master)
$ git branch -a
* master
  remotes/origin/master

Administrator@WINSELIU /d/winsegit/flickr_uploader/chrome (master)
$ git push origin master:gh-pages
Warning: Permanently added 'github.com,192.30.252.128' (RSA) to the list of known hosts.
Total 0 (delta 0), reused 0 (delta 0)
To git@github.com:winse/flickr-uploader.git
 * [new branch]      master -&gt; gh-pages

Administrator@WINSELIU /d/winsegit/flickr_uploader/chrome (master)
$ git branch -a
* master
  remotes/origin/gh-pages
  remotes/origin/master
</code></pre>

<p><strong><a href="https://help.github.com/articles/creating-project-pages-manually">Creating Project Pages manually</a></strong></p>

<blockquote><pre><code>  cd repository

  git checkout --orphan gh-pages
  # Creates our branch, without any parents (it's an orphan!)
  # Switched to a new branch 'gh-pages'

  git rm -rf .
  # Remove all files from the old working tree
  # rm '.gitignore'

  echo "My GitHub Page" &gt; index.html
  git add index.html
  git commit -a -m "First pages commit"
  git push origin gh-pages
</code></pre></blockquote>

<h3>子模块操作</h3>

<p><a href="http://josephjiang.com/entry.php?id=342">git-submodule教程！</a></p>

<pre><code>Administrator@WINSELIU /d/winsegit/jae_winse (master)
$ git submodule add git@github.com:winse/flickr-uploader.git src/main/webapp/flickr

Administrator@WINSELIU /d/winsegit/jae_winse (master)
$ git submodule status
 635090c5a754eebf5ce6566b7f8c65446b764f51 src/main/webapp/flickr (heads/master)

Administrator@WINSELIU /d/winsegit/jae_winse (master)
$ git commit -m "add submodule"
[master c7dc8c7] add submodule
warning: LF will be replaced by CRLF in .gitmodules.
The file will have its original line endings in your working directory.
 2 files changed, 4 insertions(+)
 create mode 100644 .gitmodules
 create mode 160000 src/main/webapp/flickr
</code></pre>

<p>如：$ git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs</p>

<p>初始化子模块：$ git submodule init &mdash;-只在首次检出仓库时运行一次就行</p>

<p>更新子模块：$ git submodule update &mdash;-每次更新或切换分支后都需要运行一下</p>

<p>删除子模块：（分4步走哦）</p>

<ol>
<li>$ git rm &ndash;cached [path]</li>
<li>编辑“.gitmodules”文件，将子模块的相关配置节点删除掉</li>
<li>编辑“.git/config”文件，将子模块的相关配置节点删除掉</li>
<li>手动删除子模块残留的目录</li>
</ol>


<h2>其他偶尔使用命令</h2>

<pre><code>git diff --check # 检查行尾有没有多余的空白
git remote prune &lt;remotename&gt;
git ls-remote --heads origin
git gc --prune=now
git ls-remote --heads &lt;remote-name&gt;
git rm -r --cached *
</code></pre>

<h2>参考</h2>

<ul>
<li><a href="http://blog.csdn.net/trochiluses/article/details/8996431">回溯 与 合并:git rebase与git reset</a></li>
<li><a href="http://ihower.tw/git/">git教程的一个站点</a></li>
<li><a href="http://ihower.tw/git/basic.html">git基本操作</a></li>
<li><a href="http://ihower.tw/git/vcs.html">版本管理介绍</a></li>
<li><a href="http://blog.csdn.net/ithomer/article/details/7529841">速查表</a></li>
<li><a href="http://git-scm.com/book/zh/Git-%E5%9F%BA%E7%A1%80-%E6%9F%A5%E7%9C%8B%E6%8F%90%E4%BA%A4%E5%8E%86%E5%8F%B2">Git 基础 - 查看提交历史</a></li>
<li><p><a href="http://gitbook.liuhui998.com/3_4.html">查看历史 －Git日志</a></p></li>
<li><p><a href="http://git-scm.com/book/zh/Git-%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2">http://git-scm.com/book/zh/Git-%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2</a></p></li>
<li><a href="http://ihower.tw/blog/archives/2622">http://ihower.tw/blog/archives/2622</a></li>
<li><a href="http://git-scm.com/docs/git-rebase">http://git-scm.com/docs/git-rebase</a></li>
<li><a href="http://xiewenbo.iteye.com/blog/1285693">http://xiewenbo.iteye.com/blog/1285693</a></li>
<li><a href="http://gitready.com/">http://gitready.com/</a></li>
<li><a href="http://git-scm.com/book/zh/Git-%E5%9F%BA%E7%A1%80-%E6%9F%A5%E7%9C%8B%E6%8F%90%E4%BA%A4%E5%8E%86%E5%8F%B2">http://git-scm.com/book/zh/Git-%E5%9F%BA%E7%A1%80-%E6%9F%A5%E7%9C%8B%E6%8F%90%E4%BA%A4%E5%8E%86%E5%8F%B2</a></li>
<li><a href="http://josephjiang.com/entry.php?id=342">http://josephjiang.com/entry.php?id=342</a> git-submodule没有更好的教程了</li>
<li><a href="http://www.cnblogs.com/william9/archive/2012/09/01/2666767.html">http://www.cnblogs.com/william9/archive/2012/09/01/2666767.html</a></li>
<li><a href="http://marklodato.github.io/visual-git-guide/index-zh-cn.html">http://marklodato.github.io/visual-git-guide/index-zh-cn.html</a></li>
<li><a href="http://www.16kan.com/question/detail/321093.html">http://www.16kan.com/question/detail/321093.html</a></li>
<li><a href="http://gitbook.liuhui998.com/3_4.html">http://gitbook.liuhui998.com/3_4.html</a></li>
<li><a href="http://www.bootcss.com/p/git-guide/">http://www.bootcss.com/p/git-guide/</a></li>
<li><a href="http://blog.csdn.net/ithomer/article/details/7529022">http://blog.csdn.net/ithomer/article/details/7529022</a></li>
<li><a href="http://git-scm.com/book/zh/Git-%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF">http://git-scm.com/book/zh/Git-%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF</a></li>
<li><a href="http://git-scm.com/book/zh/Git-%E5%9F%BA%E7%A1%80-%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8">http://git-scm.com/book/zh/Git-%E5%9F%BA%E7%A1%80-%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8</a></li>
<li><a href="http://blog.csdn.net/trochiluses/article/details/14517379">http://blog.csdn.net/trochiluses/article/details/14517379</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
