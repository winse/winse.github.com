<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Debug | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/debug/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2017-11-17T05:57:35+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[远程调试hadoop2以及错误处理方法]]></title>
    <link href="http://winseliu.com/blog/2014/04/22/remote-debug-hadoop2/"/>
    <updated>2014-04-22T06:47:48+08:00</updated>
    <id>http://winseliu.com/blog/2014/04/22/remote-debug-hadoop2</id>
    <content type="html"><![CDATA[<p>了解程序运行过程，除了一行行代码的扫射源代码。更快捷的方式是运行调试源码，通过F6/F7来一步步的带领我们熟悉程序。针对特定细节具体数据，打个断点调试则是水到渠成的方式。</p>

<h2>Java远程调试</h2>

<pre><code> * JDK 1.3 or earlier -Xnoagent -Djava.compiler=NONE -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=6006
 * JDK 1.4(linux ok) -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=6006
 * newer JDK(win7 &amp; jdk7) -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=6006
</code></pre>

<h2>同一操作系统任务提交</h2>

<p>windows提交到windows，linux提交到linux，可以直接通过命令行添加参数调试wordcount任务：</p>

<pre><code>E:\local\dotfile&gt;hdfs dfs -rmr /out # native-lib放在非path路径下，cmd脚本中有对其进行处理

E:\local\dotfile&gt;hadoop org.apache.hadoop.examples.WordCount  "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090 -Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native -Dmapreduce.reduce.java.opts=-Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native"  /in /out
</code></pre>

<p><strong>suspend设置为y，会等待客户端连接再运行</strong>。在eclipse中在WordCount$TokenizerMapper#map打个断点，然后再使用<code>Remote Java Application</code>就可以调试程序了。</p>

<h2>Hadoop集群环境下调试任务</h2>

<p>hadoop有很多的程序，同样有对应的环境变量选项来进行设置！</p>

<ul>
<li>主程序-调试Job提交

<ul>
<li><code>set HADOOP_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"</code></li>
<li>可以在配置文件中进行设置。需要注意可能会覆盖已经设置的该参数的值。</li>
</ul>
</li>
<li>Nodemanager调试

<ul>
<li><code>set HADOOP_NODEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8092"</code></li>
<li>(linux下需要定义在文件中)<code>YARN_NODEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8092"</code></li>
</ul>
</li>
<li>ResourceManager调试

<ul>
<li>HADOOP_RESOURCEMANAGER_OPTS</li>
<li><code>export YARN_RESOURCEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8091"</code></li>
</ul>
</li>
</ul>


<p>Linux上的设置略有不同，通过SSH再调用的进程(如NodeManager)需要把其OPTS写到命令行脚本文件中！！
linux需要远程调试NodeManager的话，需要写到etc/hadoop/yarn-env.sh文件中！不然，nodemanger不生效（通过ssh去执行的）！</p>

<h3>其他调试技巧</h3>

<p>调试测试集群环境，比本地windows开发环境复杂点。毕竟本地windows的就一个主一个从。而把<strong>任务放到分布式集群</strong>上时，例如调试分布式缓存的！
那么就需要一些小技巧来获取任务运行所在的机器！下面的步骤中有具体操作命令。</p>

<h3>任务配置及运行</h3>

<p>eclipse下windows提交job到linux的补丁，查阅<a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655">[MAPREDUCE-5655]</a></p>

<pre><code># 配置
    &lt;property&gt;
        &lt;name&gt;mapred.remote.os&lt;/name&gt;
        &lt;value&gt;Linux&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.job.jar&lt;/name&gt;
        &lt;value&gt;dta-analyser-all.jar&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;
        &lt;value&gt;-Xmx1024m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;mapred.task.timeout&lt;/name&gt;
        &lt;value&gt;1800000&lt;/value&gt;
    &lt;/property&gt;

# 代码，map/reduce数都设置为1   
job.setNumReduceTasks(1);
job.getConfiguration().setInt(MRJobConfig.NUM_MAPS, 1);
</code></pre>

<ul>
<li>调试的时刻把超时时间设置的久一点，否则：</li>
</ul>


<pre><code> Got exception: java.net.SocketTimeoutException: Call From winseliu/127.0.0.1 to winse.com:2850 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch :
</code></pre>

<ul>
<li>调试main方法参数设置</li>
</ul>


<p>调试main（转瞬即逝的把suspend设置为true！），map的调试选项的语句写在配置文件里面</p>

<pre><code>export HADOOP_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8073"

Administrator@winseliu ~/hadoop
$ sh -x bin/hadoop org.apache.hadoop.examples.WordCount /in /out 
</code></pre>

<h3>遍历所有子节点，查找节点运行map程序的信息</h3>

<p>map调试的端口配置为18090，根据这个选项来查找程序运行的机器。</p>

<pre><code>[hadoop@umcc97-44 ~]$ for h in `cat hadoop-2.2.0/etc/hadoop/slaves` ; do ssh $h 'ps aux|grep java | grep 18090'; echo $h;  done
hadoop    8667  0.0  0.0  63888  1268 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
umcc97-142
hadoop   12686  0.0  0.0  63868  1260 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
umcc97-143
hadoop   23516  0.0  0.0  63856  1108 ?        Ss   18:11   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1605/container_1397006359464_1605_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 57576 attempt_1397006359464_1605_m_000000_0 2 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002/stderr 
hadoop   23522  0.0  0.0 605136 15728 ?        Sl   18:11   0:00 /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1605/container_1397006359464_1605_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 57576 attempt_1397006359464_1605_m_000000_0 2
hadoop   23665  0.0  0.0  63856  1264 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
umcc97-144
</code></pre>

<p>仅打印运行map的节点名称</p>

<pre><code>[hadoop@umcc97-44 ~]$ for h in `cat hadoop-2.2.0/etc/hadoop/slaves` ; do ssh $h 'if ps aux|grep -v grep | grep java | grep 18090 | grep -v bash 2&gt;&amp;1 1&gt;/dev/null ; then echo `hostname`; fi'; done
umcc97-142
[hadoop@umcc97-44 ~]$ 
</code></pre>

<p>后面的操作就和普通的java程序调试步骤一样了。不再赘述。</p>

<h2>任务运行过程中的数据</h2>

<h4>辅助运行的两个bash程序</h4>

<p>运行的第一个程序（000001）为AppMaster，第二程序（000002）才是我们提交job的map任务。</p>

<pre><code>[hadoop@umcc97-143 ~]$ cd hadoop-2.2.0/tmp/nm-local-dir/nmPrivate
[hadoop@umcc97-143 nmPrivate]$ ls -Rl
.:
total 12
drwxrwxr-x 4 hadoop hadoop 4096 Apr 21 18:34 application_1397006359464_1606
-rw-rw-r-- 1 hadoop hadoop    6 Apr 21 18:34 container_1397006359464_1606_01_000001.pid
-rw-rw-r-- 1 hadoop hadoop    6 Apr 21 18:34 container_1397006359464_1606_01_000002.pid

./application_1397006359464_1606:
total 8
drwxrwxr-x 2 hadoop hadoop 4096 Apr 21 18:34 container_1397006359464_1606_01_000001
drwxrwxr-x 2 hadoop hadoop 4096 Apr 21 18:34 container_1397006359464_1606_01_000002

./application_1397006359464_1606/container_1397006359464_1606_01_000001:
total 8
-rw-r--r-- 1 hadoop hadoop   95 Apr 21 18:34 container_1397006359464_1606_01_000001.tokens
-rw-r--r-- 1 hadoop hadoop 3121 Apr 21 18:34 launch_container.sh

./application_1397006359464_1606/container_1397006359464_1606_01_000002:
total 8
-rw-r--r-- 1 hadoop hadoop  129 Apr 21 18:34 container_1397006359464_1606_01_000002.tokens
-rw-r--r-- 1 hadoop hadoop 3532 Apr 21 18:34 launch_container.sh
[hadoop@umcc97-143 nmPrivate]$ 
[hadoop@umcc97-143 nmPrivate]$ jps
4692 NodeManager
4173 DataNode
13497 YarnChild
7538 HRegionServer
13376 MRAppMaster
13574 Jps
[hadoop@umcc97-143 nmPrivate]$ cat *.pid
13366
13491
[hadoop@umcc97-143 nmPrivate]$ ps aux | grep 13366
hadoop   13366  0.0  0.0  63868  1088 ?        Ss   18:34   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001/stderr 
hadoop   13594  0.0  0.0  61204   760 pts/2    S+   18:36   0:00 grep 13366
[hadoop@umcc97-143 nmPrivate]$ ps aux | grep 13491
hadoop   13491  0.0  0.0  63868  1100 ?        Ss   18:34   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/container_1397006359464_1606_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 52046 attempt_1397006359464_1606_m_000000_0 2 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002/stderr 
hadoop   13599  0.0  0.0  61204   760 pts/2    S+   18:37   0:00 grep 13491
[hadoop@umcc97-143 nmPrivate]$ 
</code></pre>

<h4>程序运行本地缓存数据</h4>

<pre><code>[hadoop@umcc97-143 container_1397006359464_1606_01_000002]$ ls -l
total 28
-rw-r--r-- 1 hadoop hadoop  129 Apr 21 18:34 container_tokens
-rwx------ 1 hadoop hadoop  516 Apr 21 18:34 default_container_executor.sh
lrwxrwxrwx 1 hadoop hadoop   65 Apr 21 18:34 filter.io -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/filecache/10/filter.io
lrwxrwxrwx 1 hadoop hadoop  120 Apr 21 18:34 job.jar -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/filecache/10/job.jar
lrwxrwxrwx 1 hadoop hadoop  120 Apr 21 18:34 job.xml -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/filecache/13/job.xml
-rwx------ 1 hadoop hadoop 3532 Apr 21 18:34 launch_container.sh
drwx--x--- 2 hadoop hadoop 4096 Apr 21 18:34 tmp
[hadoop@umcc97-143 container_1397006359464_1606_01_000002]$ 
</code></pre>

<h2>处理问题方法</h2>

<ul>
<li>打印DEBUG日志：<code>export HADOOP_ROOT_LOGGER=DEBUG,console</code>

<ul>
<li>日志文件放置在nodemanager节点的logs/userlogs目录下。</li>
</ul>
</li>
<li>打印DEBUG日志也搞不定时，可以在源码里面sysout信息然后把<strong>class覆盖</strong>，来进行定位配置的问题。</li>
<li>如果不清楚shell的执行过程，可以通过<code>sh -x [CMD]</code>，或者在脚本文件的操作前加上<code>set -x</code>。相当于windows-batch的<code>echo on</code>功能。</li>
</ul>


<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/975271/remote-debugging-a-java-application">remote debugger opts</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
