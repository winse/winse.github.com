<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: #nfs | Winse Blog]]></title>
  <link href="http://winse.github.io/blog/categories/nfs/atom.xml" rel="self"/>
  <link href="http://winse.github.io/"/>
  <updated>2024-01-16T12:59:42+08:00</updated>
  <id>http://winse.github.io/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[K8S官方例子NFS]]></title>
    <link href="http://winse.github.io/blog/2022/04/19/k8s-nfs-run-example/"/>
    <updated>2022-04-19T18:34:01+08:00</updated>
    <id>http://winse.github.io/blog/2022/04/19/k8s-nfs-run-example</id>
    <content type="html"><![CDATA[<h2>参考</h2>

<p>源码</p>

<ul>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs">https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs</a></li>
<li><a href="https://github.com/kubernetes/examples/blob/master/staging/volumes/nfs/nfs-server-deployment.yaml">https://github.com/kubernetes/examples/blob/master/staging/volumes/nfs/nfs-server-deployment.yaml</a></li>
</ul>


<p>本地volumn</p>

<ul>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#local">https://kubernetes.io/zh/docs/concepts/storage/volumes/#local</a></li>
</ul>


<h2>运行</h2>

<h3>目录结构</h3>

<pre><code>[ec2-user@k8s ~]$ git clone https://github.com/kubernetes/examples

[ec2-user@k8s ~]$ cd examples/staging/volumes/nfs/
[ec2-user@k8s nfs]$ ls -l
total 48
-rw-rw-r-- 1 ec2-user ec2-user  823 Apr 19 20:34 nfs-busybox-deployment.yaml
drwxrwxr-x 2 ec2-user ec2-user   77 Apr 19 20:34 nfs-data
-rw-rw-r-- 1 ec2-user ec2-user  193 Apr 19 20:34 nfs-pvc.yaml
-rw-rw-r-- 1 ec2-user ec2-user 9379 Apr 19 20:34 nfs-pv.png
-rw-rw-r-- 1 ec2-user ec2-user  234 Apr 19 20:34 nfs-pv.yaml
-rw-rw-r-- 1 ec2-user ec2-user  729 Apr 19 20:34 nfs-server-deployment.yaml
-rw-rw-r-- 1 ec2-user ec2-user  212 Apr 19 20:34 nfs-server-service.yaml
-rw-rw-r-- 1 ec2-user ec2-user  673 Apr 19 20:34 nfs-web-deployment.yaml
-rw-rw-r-- 1 ec2-user ec2-user  120 Apr 19 20:34 nfs-web-service.yaml
drwxrwxr-x 2 ec2-user ec2-user   98 Apr 19 20:34 provisioner
-rw-rw-r-- 1 ec2-user ec2-user 6540 Apr 19 20:34 README.md
[ec2-user@k8s nfs]$ ls -l provisioner/
total 12
-rw-rw-r-- 1 ec2-user ec2-user 291 Apr 19 20:34 nfs-server-azure-pv.yaml
-rw-rw-r-- 1 ec2-user ec2-user 324 Apr 19 20:34 nfs-server-cdk-pv.yaml
-rw-rw-r-- 1 ec2-user ec2-user 215 Apr 19 20:34 nfs-server-gce-pv.yaml
</code></pre>

<ul>
<li>nfs-data 是nfs-server镜像构建Dockerfile相关文件。</li>
<li>nfs-server：

<ul>
<li>nfs-server-deployment.yaml</li>
<li>nfs-server-service.yaml</li>
<li>provisioner</li>
</ul>
</li>
<li>web

<ul>
<li>nfs-pv.yaml</li>
<li>nfs-pvc.yaml</li>
<li>nfs-web-deployment.yaml</li>
<li>nfs-web-service.yaml</li>
<li>nfs-busybox-deployment.yaml</li>
</ul>
</li>
</ul>


<p>按照结构，一步步的来进行配置和运行。</p>

<h3>搭建NFS Server的本地存储卷</h3>

<ul>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#local">https://kubernetes.io/zh/docs/concepts/storage/volumes/#local</a></li>
</ul>


<pre><code>[ec2-user@k8s ~]$ sudo mkdir /nfs
[ec2-user@k8s ~]$ sudo chmod 777 /nfs

[ec2-user@k8s nfs]$ cat server-pv.yml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 10Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
# https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /nfs
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - k8s

[ec2-user@k8s nfs]$ kubectl apply -f server-pv.yml
persistentvolume/nfs-pv created
</code></pre>

<p>创建server pvc：</p>

<pre><code>[ec2-user@k8s nfs]$ kubectl create namespace nfs
namespace/nfs-server created

[ec2-user@k8s nfs]$ cat provisioner/nfs-server-gce-pv.yaml   
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pv-provisioning-demo
  labels:
    demo: nfs-pv-provisioning
spec:
  accessModes: [ "ReadWriteOnce" ]
  resources:
    requests:
      storage: 10Gi
  storageClassName: local-storage

[ec2-user@k8s nfs]$ kubectl apply -f provisioner/nfs-server-gce-pv.yaml -n nfs
persistentvolumeclaim/nfs-pv-provisioning-demo created

[ec2-user@k8s nfs]$ kubectl get pv 
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS    REASON   AGE
nfs-pv   10Gi       RWO            Delete           Bound    nfs/nfs-pv-provisioning-demo   local-storage            21s
[ec2-user@k8s nfs]$ kubectl get pvc -n nfs 
NAME                       STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
nfs-pv-provisioning-demo   Bound    nfs-pv   10Gi       RWO            local-storage   18s
</code></pre>

<h3>启动nfs-server</h3>

<pre><code># 下载镜像要代理的，可以替换image或者下载后改tag

[ec2-user@k8s nfs]$ cat nfs-server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-server
spec:
  replicas: 1
  selector:
    matchLabels:
      role: nfs-server
  template:
    metadata:
      labels:
        role: nfs-server
    spec:
      containers:
      - name: nfs-server
        image: k8s.gcr.io/volume-nfs:0.8
        ports:
          - name: nfs
            containerPort: 2049
          - name: mountd
            containerPort: 20048
          - name: rpcbind
            containerPort: 111
        securityContext:
          privileged: true
        volumeMounts:
          - mountPath: /exports
            name: mypvc
      volumes:
        - name: mypvc
          persistentVolumeClaim:
            claimName: nfs-pv-provisioning-demo
[ec2-user@k8s nfs]$ cat nfs-server-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: nfs-server
spec:
  ports:
    - name: nfs
      port: 2049
    - name: mountd
      port: 20048
    - name: rpcbind
      port: 111
  selector:
    role: nfs-server

[ec2-user@k8s nfs]$ kubectl apply -f nfs-server-deployment.yaml -f nfs-server-service.yaml -n nfs
deployment.apps/nfs-server created
service/nfs-server created

[ec2-user@k8s nfs]$ kubectl get all -n nfs -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP             NODE   NOMINATED NODE   READINESS GATES
pod/nfs-server-64886b598f-57mnx   1/1     Running   0          18s   10.244.0.146   k8s    &lt;none&gt;           &lt;none&gt;

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE   SELECTOR
service/nfs-server   ClusterIP   10.111.29.73   &lt;none&gt;        2049/TCP,20048/TCP,111/TCP   18s   role=nfs-server

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                      SELECTOR
deployment.apps/nfs-server   1/1     1            1           18s   nfs-server   k8s.gcr.io/volume-nfs:0.8   role=nfs-server

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                      SELECTOR
replicaset.apps/nfs-server-64886b598f   1         1         1       18s   nfs-server   k8s.gcr.io/volume-nfs:0.8   pod-template-hash=64886b598f,role=nfs-server
</code></pre>

<p>根据nodeAffinity容器部署在了k8s的机器。</p>

<h3>启动web应用</h3>

<p>创建容器使用的卷：</p>

<pre><code># 不知为何，后面容器挂载卷的时刻识别不了域名... 
# mount的时刻可能是在node节点上执行的，所以解析不了域名。
# 把主机的 nameserver设置为（/etc/resolv.conf） nameserver 10.96.0.10 就可以了。
[ec2-user@k8s nfs]$ cat nfs-pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs
spec:
  capacity:
    storage: 1Mi
  accessModes:
    - ReadWriteMany
  nfs:
    server: nfs-server.nfs.svc.cluster.local
    path: "/"
  mountOptions:
    - nfsvers=4.2

[ec2-user@k8s nfs]$ kubectl apply -f nfs-pv.yaml 
persistentvolume/nfs created
[ec2-user@k8s nfs]$ kubectl get pv nfs
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
nfs    1Mi        RWX            Retain           Available                                   12s


[ec2-user@k8s nfs]$ cat nfs-pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: ""
  resources:
    requests:
      storage: 1Mi
  volumeName: nfs

# kubectl get pv | tail -n+2 | awk '$5 == "Released" {print $1}' | xargs -I{} kubectl patch pv {} --type='merge' -p '{"spec":{"claimRef": null}}
# 
# [ec2-user@k8s nfs]$ kubectl patch pv nfs -p '{"spec":{"claimRef": null}}'                
# persistentvolume/nfs patched
[ec2-user@k8s nfs]$ kubectl apply -f nfs-pvc.yaml
persistentvolumeclaim/nfs created


[ec2-user@k8s nfs]$ kubectl get pv 
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS    REASON   AGE
nfs      1Mi        RWX            Retain           Bound    default/nfs                                                 56s
nfs-pv   10Gi       RWO            Delete           Bound    nfs/nfs-pv-provisioning-demo   local-storage            3m8s
[ec2-user@k8s nfs]$ kubectl get pvc -A 
NAMESPACE   NAME                       STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
nfs         nfs                        Bound    nfs      1Mi        RWX                            22s
nfs         nfs-pv-provisioning-demo   Bound    nfs-pv   10Gi       RWO            local-storage   3m
</code></pre>

<p>部署web应用的容器：</p>

<pre><code>[ec2-user@k8s nfs]$ cat nfs-web-deployment.yaml 
# This pod mounts the nfs volume claim into /usr/share/nginx/html and
# serves a simple web page.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-web
spec:
  replicas: 2
  selector:
    matchLabels:
      role: web-frontend
  template:
    metadata:
      labels:
        role: web-frontend
    spec:
      containers:
      - name: web
        image: nginx
        ports:
          - name: web
            containerPort: 80
        volumeMounts:
            # name must match the volume name below
            - name: nfs
              mountPath: "/usr/share/nginx/html"
      volumes:
      - name: nfs
        persistentVolumeClaim:
          claimName: nfs

[ec2-user@k8s nfs]$ cat nfs-web-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: nfs-web
spec:
  ports:
    - port: 80
  selector:
    role: web-frontend

[ec2-user@k8s nfs]$ kubectl apply -f nfs-web-deployment.yaml -f nfs-web-service.yaml
deployment.apps/nfs-web created
service/nfs-web created

[ec2-user@k8s nfs]$ kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/nfs-web-5554f77845-6rq5p   1/1     Running   0          4m56s
pod/nfs-web-5554f77845-8r885   1/1     Running   0          4m56s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP   33d
service/nfs-web      ClusterIP   10.108.129.153   &lt;none&gt;        80/TCP    4m56s

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nfs-web   2/2     2            2           4m56s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/nfs-web-5554f77845   2         2         2       4m56s
</code></pre>

<p>测试：deployment创建两个busybox，会在nfs共享目录下创建修改index.html，内容为：时间和主机名。</p>

<pre><code>[ec2-user@k8s nfs]$ kubectl apply -f nfs-busybox-deployment.yaml 
deployment.apps/nfs-busybox created

[ec2-user@k8s nfs]$ curl nfs-web.default.svc.cluster.local 
Wed Apr 20 00:15:20 UTC 2022
nfs-busybox-55c668489c-dxqcx
[ec2-user@k8s nfs]$ curl nfs-web.default.svc.cluster.local
Wed Apr 20 00:16:51 UTC 2022
nfs-busybox-55c668489c-sl7jz
</code></pre>

<h3>清理</h3>

<pre><code>[ec2-user@k8s nfs]$ kubectl delete -f nfs-busybox-deployment.yaml -f nfs-web-deployment.yaml -f nfs-web-service.yaml -f nfs-pvc.yaml -f nfs-pv.yaml 
[ec2-user@k8s nfs]$ kubectl delete ns nfs 
[ec2-user@k8s nfs]$ kubectl delete -f server-pv.yml 
</code></pre>

<h2>问题</h2>

<h3>NFS服务器的域名解析不了</h3>

<p>一开始没有改节点的dns，会出现域名解析不了的情况：</p>

<pre><code>[ec2-user@k8s nfs]$ kubectl describe pod nfs-web-7bc965b94f-k6mrj -n nfs 
...
Events:
  Type     Reason       Age               From               Message
  ----     ------       ----              ----               -------
  Normal   Scheduled    43s               default-scheduler  Successfully assigned nfs/nfs-web-7bc965b94f-k6mrj to worker1
  Warning  FailedMount  1s (x7 over 35s)  kubelet            MountVolume.SetUp failed for volume "nfs" : mount failed: exit status 32
Mounting command: mount
Mounting arguments: -t nfs -o nfsvers=4.2 nfs-server.nfs.svc.cluster.local:/ /var/lib/kubelet/pods/03049217-ded3-4d31-86e8-6a13c0f5b12f/volumes/kubernetes.io~nfs/nfs
Output: mount.nfs: Failed to resolve server nfs-server.nfs.svc.cluster.local: Name or service not known
mount.nfs: Operation already in progress
</code></pre>

<p>容器内是可以通过域名访问的，mount的时刻可能是在node节点上执行的，所以解析不了域名。</p>

<pre><code>[ec2-user@k8s ~]$ kubectl run -ti test --rm --image busybox -n nfs -- sh  
/ # ping nfs-server.nfs.svc.cluster.local
[ec2-user@k8s nfs]$ kubectl run -ti test --rm --image busybox -- sh

/ # cat /etc/resolv.conf 
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local localdomain
options ndots:5
</code></pre>

<p>把所有节点的dns设置为 kube-dns 的 10.96.0.10 就可以了。</p>

<h3>集群出问题</h3>

<pre><code>[ec2-user@k8s nfs]$ kubectl get pods -A 
,,,
ingress-nginx          ingress-nginx-controller-755447bb4d-55hjz    0/1     CrashLoopBackOff       32 (2m19s ago)   17d
kube-system            coredns-64897985d-4d5rx                      0/1     CrashLoopBackOff       40 (4m30s ago)   33d
kube-system            coredns-64897985d-m9p9q                      0/1     CrashLoopBackOff       40 (4m15s ago)   33d
,,,
kubernetes-dashboard   dashboard-metrics-scraper-799d786dbf-rj8rl   0/1     CrashLoopBackOff       33 (116s ago)    17d
kubernetes-dashboard   kubernetes-dashboard-fb8648fd9-22krh         0/1     CrashLoopBackOff       32 (3m35s ago)   17d
</code></pre>

<p>把所有的容器都清理了，然后重启服务器（或者kubelet）</p>

<pre><code>[ec2-user@k8s nfs]$ docker ps -a |  awk '{print $1}' | while read i ; do docker rm -f $i ; done 
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[k8s共享存储使用NFS]]></title>
    <link href="http://winse.github.io/blog/2022/04/14/k8s-nfs/"/>
    <updated>2022-04-14T01:23:14+08:00</updated>
    <id>http://winse.github.io/blog/2022/04/14/k8s-nfs</id>
    <content type="html"><![CDATA[<p>容器中的应用数据得保存下来，使用local/hostPath可以临时用用，还是得有一个共享的存储。</p>

<ul>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#volume-types">https://kubernetes.io/zh/docs/concepts/storage/volumes/#volume-types</a></li>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/_print/#types-of-persistent-volumes">https://kubernetes.io/zh/docs/concepts/storage/_print/#types-of-persistent-volumes</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner">https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#hostpath">https://kubernetes.io/docs/concepts/storage/volumes/#hostpath</a></li>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#local">https://kubernetes.io/zh/docs/concepts/storage/volumes/#local</a></li>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#cephfs">https://kubernetes.io/zh/docs/concepts/storage/volumes/#cephfs</a></li>
</ul>


<p>先使用最简单的NFS分区/卷。</p>

<ul>
<li><a href="http://www.lishuai.fun/2021/08/12/k8s-nfs-pv">http://www.lishuai.fun/2021/08/12/k8s-nfs-pv</a></li>
</ul>


<h2>安装NFS server on aws ec2</h2>

<ul>
<li><a href="https://segmentfault.com/a/1190000024512057">https://segmentfault.com/a/1190000024512057</a></li>
</ul>


<pre><code>#所有node节点安装nfs客户端
#yum -y install nfs-utils
#systemctl start nfs &amp;&amp; systemctl enable nfs


[ec2-user@k8s ~]$ sudo yum install nfs-utils 
Loaded plugins: langpacks, priorities, update-motd
amzn2-core                                                                                                                     | 3.7 kB  00:00:00     
Package 1:nfs-utils-1.3.0-0.54.amzn2.0.2.x86_64 already installed and latest version
Nothing to do

[ec2-user@k8s ~]$ sudo mkdir /backup
[ec2-user@k8s ~]$ sudo chmod -R 755 /backup
[ec2-user@k8s ~]$ sudo chown nfsnobody:nfsnobody /backup

[ec2-user@k8s ~]$ sudo vi /etc/exports
[ec2-user@k8s ~]$ cat /etc/exports    
/backup 192.168.191.0/24(rw,sync,no_root_squash,no_all_squash)

# /k8s-fs *(rw,sync,no_root_squash,no_all_squash)

[ec2-user@k8s ~]$ sudo service nfs-server restart 
Redirecting to /bin/systemctl restart nfs-server.service
[ec2-user@k8s ~]$ 
[ec2-user@k8s ~]$ sudo exportfs
/backup         192.168.191.0/24
[ec2-user@k8s ~]$ sudo exportfs -arv 
exporting 192.168.191.0/24:/backup

[ec2-user@k8s ~]$ rpcinfo -p localhost
   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
    100024    1   udp  56847  status
    100024    1   tcp  60971  status
    100005    1   udp  20048  mountd
    100005    1   tcp  20048  mountd
    100005    2   udp  20048  mountd
    100005    2   tcp  20048  mountd
    100005    3   udp  20048  mountd
    100005    3   tcp  20048  mountd
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    3   tcp   2049  nfs_acl
    100003    3   udp   2049  nfs
    100227    3   udp   2049  nfs_acl
    100021    1   udp  47545  nlockmgr
    100021    3   udp  47545  nlockmgr
    100021    4   udp  47545  nlockmgr
    100021    1   tcp  40703  nlockmgr
    100021    3   tcp  40703  nlockmgr
    100021    4   tcp  40703  nlockmgr
[ec2-user@k8s ~]$ showmount -e 192.168.191.131
Export list for 192.168.191.131:
/backup 192.168.191.0/24
</code></pre>

<p>也可以通过docker来启动nfs server：</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2020/09/20/setup-a-nfs-server-with-docker/">https://blog.ruanbekker.com/blog/2020/09/20/setup-a-nfs-server-with-docker/</a></li>
<li><a href="https://westzq1.github.io/k8s/2019/06/28/nfs-server-on-K8S.html">https://westzq1.github.io/k8s/2019/06/28/nfs-server-on-K8S.html</a></li>
<li><a href="https://github.com/kubernetes/examples/blob/master/staging/volumes/nfs/nfs-server-deployment.yaml">https://github.com/kubernetes/examples/blob/master/staging/volumes/nfs/nfs-server-deployment.yaml</a></li>
</ul>


<pre><code>[ec2-user@k8s ~]$ sudo mkdir -p /data/kubernetes-volumes
[ec2-user@k8s ~]$ docker run --privileged -itd --name nfs -p 2049:2049 -e SHARED_DIRECTORY=/data -v /data/kubernetes-volumes:/data itsthenetwork/nfs-server-alpine:12 
f84b70dcca6bd5abb275fbee50fd161d8befdd709ce6523b3a514f04b7af8677

[ec2-user@k8s ~]$ docker logs f84b70dcca6bd5abb2 
Writing SHARED_DIRECTORY to /etc/exports file
The PERMITTED environment variable is unset or null, defaulting to '*'.
This means any client can mount.
The READ_ONLY environment variable is unset or null, defaulting to 'rw'.
Clients have read/write access.
The SYNC environment variable is unset or null, defaulting to 'async' mode.
Writes will not be immediately written to disk.
Displaying /etc/exports contents:
/data *(rw,fsid=0,async,no_subtree_check,no_auth_nlm,insecure,no_root_squash)

Starting rpcbind...
Displaying rpcbind status...
   program version netid     address                service    owner
    100000    4    tcp6      ::.0.111               -          superuser
    100000    3    tcp6      ::.0.111               -          superuser
    100000    4    udp6      ::.0.111               -          superuser
    100000    3    udp6      ::.0.111               -          superuser
    100000    4    tcp       0.0.0.0.0.111          -          superuser
    100000    3    tcp       0.0.0.0.0.111          -          superuser
    100000    2    tcp       0.0.0.0.0.111          -          superuser
    100000    4    udp       0.0.0.0.0.111          -          superuser
    100000    3    udp       0.0.0.0.0.111          -          superuser
    100000    2    udp       0.0.0.0.0.111          -          superuser
    100000    4    local     /var/run/rpcbind.sock  -          superuser
    100000    3    local     /var/run/rpcbind.sock  -          superuser
Starting NFS in the background...
rpc.nfsd: knfsd is currently down
rpc.nfsd: Writing version string to kernel: -2 -3 +4 +4.1 +4.2
rpc.nfsd: Created AF_INET TCP socket.
rpc.nfsd: Created AF_INET6 TCP socket.
Exporting File System...
exporting *:/data
/data           &lt;world&gt;
Starting Mountd in the background...These
Startup successful.

[ec2-user@k8s ~]$ sudo mount -v -o vers=4,loud 127.0.0.1:/ nfsmnt
mount.nfs: timeout set for Thu Apr 14 08:26:48 2022
mount.nfs: trying text-based options 'vers=4.1,addr=127.0.0.1,clientaddr=127.0.0.1'

[ec2-user@k8s ~]$ df -h | grep nfsmnt
127.0.0.1:/      25G  9.8G   16G  39% /home/ec2-user/nfsmnt

[ec2-user@k8s ~]$ touch nfsmnt/$(hostname).txt
[ec2-user@k8s ~]$ ls -l nfsmnt/
total 0
-rw-rw-r-- 1 ec2-user ec2-user 0 Apr 14 08:25 k8s.txt
[ec2-user@k8s ~]$ ls -l /data/kubernetes-volumes/
total 0
-rw-rw-r-- 1 ec2-user ec2-user 0 Apr 14 08:25 k8s.txt
[ec2-user@k8s ~]$ 

# vi /etc/fstab
# 192.168.0.4:/   /mnt   nfs4    _netdev,auto  0  0

### pod
# kubectl create -f nfs-server.yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nfs-server
spec:
  replicas: 1           # &lt;- no more replicas
  template:
    metadata:
      labels:
        app: nfs-server
    spec:
      nodeSelector:     # &lt;- use selector to fix nfs-server on k8s2.zhangqiaoc.com
        kubernetes.io/hostname: k8s2.zhangqiaoc.com
      containers:
      - name: nfs-server
        image: itsthenetwork/nfs-server-alpine:latest
        volumeMounts:
        - name: nfs-storage
          mountPath: /nfsshare
        env:
        - name: SHARED_DIRECTORY
          value: "/nfsshare"
        ports:
        - name: nfs  
          containerPort: 2049   # &lt;- export port
        securityContext:
          privileged: true      # &lt;- privileged mode is mandentory.
      volumes:
      - name: nfs-storage  
        hostPath:               # &lt;- the folder on the host machine.
          path: /root/fileshare
# kubectl expose deployment nfs-server --type=ClusterIP
# kubectl get svc

# yum install -y nfs-utils
# mkdir /root/nfsmnt
# mount -v 10.101.117.226:/ /root/nfsmnt
</code></pre>

<p>client</p>

<pre><code># 所有work节点安装 nfs-utils rpcbind
[ec2-user@worker1 ~]$ sudo yum install nfs-utils 
Loaded plugins: langpacks, priorities, update-motd
amzn2-core                                                                                                                                        | 3.7 kB  00:00:00     
Package 1:nfs-utils-1.3.0-0.54.amzn2.0.2.x86_64 already installed and latest version
Nothing to do

[ec2-user@worker1 ~]$ sudo systemctl status nfs
● nfs-server.service - NFS server and services
   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; disabled; vendor preset: disabled)
   Active: inactive (dead)
[ec2-user@worker1 ~]$ sudo systemctl status rpcbind
● rpcbind.service - RPC bind service
   Loaded: loaded (/usr/lib/systemd/system/rpcbind.service; enabled; vendor preset: enabled)
   Active: active (running) since Wed 2022-04-13 20:44:34 CST; 1h 51min ago
  Process: 6979 ExecStart=/sbin/rpcbind -w $RPCBIND_ARGS (code=exited, status=0/SUCCESS)
 Main PID: 7025 (rpcbind)
    Tasks: 1
   Memory: 2.1M
   CGroup: /system.slice/rpcbind.service
           └─7025 /sbin/rpcbind -w

Apr 13 20:44:34 worker1 systemd[1]: Starting RPC bind service...
Apr 13 20:44:34 worker1 systemd[1]: Started RPC bind service.


[ec2-user@worker1 ~]$ sudo mkdir -p /data
[ec2-user@worker1 ~]$ sudo chmod 777 /data 
[ec2-user@worker1 ~]$ sudo mount -t nfs 192.168.191.131:/backup /data
[ec2-user@worker1 ~]$ df -h | grep 192.168.191.131
192.168.191.131:/backup   25G  9.6G   16G  39% /data


# vi /etc/fstab
# 172.17.30.22:/backup /data nfs defaults 0 0
</code></pre>

<h2>k8s中使用NFS</h2>

<ul>
<li><a href="http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/">http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/</a></li>
</ul>


<h3>容器直接挂载NFS</h3>

<pre><code>[ec2-user@k8s ~]$ cat nginx-1.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        volumeMounts:
        - name: data
          mountPath: /usr/share/nginx/html
      volumes:
      - name: data
        nfs:
          path: /backup
          server: 192.168.191.131

[ec2-user@k8s ~]$ kubectl apply -f nginx-1.yml

[ec2-user@k8s ~]$ kubectl get all 
NAME                                   READY   STATUS    RESTARTS   AGE
pod/nginx-deployment-67dcb957c-g2h8x   1/1     Running   0          2m50s
pod/nginx-deployment-67dcb957c-gfv28   1/1     Running   0          2m50s
pod/nginx-deployment-67dcb957c-rqwjs   1/1     Running   0          2m50s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   27d

NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-deployment   3/3     3            3           2m50s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-deployment-67dcb957c   3         3         3       2m50s

[ec2-user@k8s ~]$ kubectl exec -ti pod/nginx-deployment-67dcb957c-g2h8x -- bash 
root@nginx-deployment-67dcb957c-g2h8x:/# echo $(hostname) &gt;/usr/share/nginx/html/1.txt

root@nginx-deployment-67dcb957c-g2h8x:/# mount | grep 192
192.168.191.131:/backup on /usr/share/nginx/html type nfs4 (rw,relatime,vers=4.1,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.191.132,local_lock=none,addr=192.168.191.131)
root@nginx-deployment-67dcb957c-g2h8x:/# 


# 服务端查看文件内容
[ec2-user@k8s ~]$ cat /backup/1.txt 
nginx-deployment-67dcb957c-g2h8x


[ec2-user@k8s ~]$ kubectl delete -f nginx-1.yml 
deployment.apps "nginx-deployment" deleted
</code></pre>

<h3>pvc</h3>

<ul>
<li><a href="https://segmentfault.com/a/1190000040785500">https://segmentfault.com/a/1190000040785500</a></li>
<li><a href="http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#%E5%88%9B%E5%BB%BA%E7%B1%BB%E5%9E%8B%E4%B8%BAnfs%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%E5%8D%B7">http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#%E5%88%9B%E5%BB%BA%E7%B1%BB%E5%9E%8B%E4%B8%BAnfs%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%E5%8D%B7</a></li>
</ul>


<pre><code>[ec2-user@k8s ~]$ vi pv-nfs.yaml 
[ec2-user@k8s ~]$ cat pv-nfs.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany 
  nfs:
    path: /backup
    server: 192.168.191.131

[ec2-user@k8s ~]$ kubectl apply -f pv-nfs.yaml 
persistentvolume/pv-nfs created
[ec2-user@k8s ~]$ kubectl get pv 
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv-nfs   10Gi       RWX            Retain           Available                                   5s

[ec2-user@k8s ~]$ vi pvc-nfs.yaml 
[ec2-user@k8s ~]$ cat pvc-nfs.yaml 
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-nfs
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
[ec2-user@k8s ~]$ kubectl apply -f pvc-nfs.yaml 
persistentvolumeclaim/pvc-nfs created
[ec2-user@k8s ~]$ kubectl get pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-nfs   Bound    pv-nfs   10Gi       RWX                           7s
[ec2-user@k8s ~]$ kubectl get pv 
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   REASON   AGE
pv-nfs   10Gi       RWX            Retain           Bound    default/pvc-nfs                           79s

[ec2-user@k8s ~]$ vi dp-pvc.yaml
[ec2-user@k8s ~]$ cat dp-pvc.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: busybox
  labels:
    app: busybox
spec:
  replicas: 1
  selector:
    matchLabels:
      app: busybox
  template:
    metadata:
      labels:
        app: busybox
    spec:
      containers:
      - name: busybox
        image: busybox
        command: ['sh', '-c', 'echo "Hello, Kubernetes!" &amp;&amp; sleep 3600']
        volumeMounts:
        - name: data
          mountPath: /data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: pvc-nfs

[ec2-user@k8s ~]$ kubectl apply -f dp-pvc.yaml 
deployment.apps/busybox created
[ec2-user@k8s ~]$ 


[ec2-user@k8s ~]$ kubectl get all 
NAME                           READY   STATUS    RESTARTS   AGE
pod/busybox-6b99c495c9-qnvlp   1/1     Running   0          47s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   27d

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/busybox   1/1     1            1           47s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/busybox-6b99c495c9   1         1         1       47s

# 查看NFS中原来的数据
[ec2-user@k8s ~]$ kubectl exec -ti busybox-6b99c495c9-qnvlp -- cat /data/1.txt 
nginx-deployment-67dcb957c-g2h8x
</code></pre>

<p>测一下subPathExpr：</p>

<pre><code>[ec2-user@k8s ~]$ kubectl delete -f dp-pvc.yaml 
deployment.apps "busybox" deleted
[ec2-user@k8s ~]$ 
[ec2-user@k8s ~]$ vi dp-pvc.yaml 
[ec2-user@k8s ~]$ cat dp-pvc.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: busybox
  labels:
    app: busybox
spec:
  replicas: 1
  selector:
    matchLabels:
      app: busybox
  template:
    metadata:
      labels:
        app: busybox
    spec:
      containers:
      - name: busybox
        image: busybox
        command: ['sh', '-c', 'echo "Hello, Kubernetes!" &amp;&amp; sleep 3600']
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        volumeMounts:
        - name: data
          mountPath: /data
          subPathExpr: $(POD_NAME)
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: pvc-nfs

[ec2-user@k8s ~]$ kubectl apply -f dp-pvc.yaml 
deployment.apps/busybox created

[ec2-user@k8s ~]$ kubectl get all 
NAME                           READY   STATUS    RESTARTS   AGE
pod/busybox-5497486bf5-csr6q   1/1     Running   0          7s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   27d

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/busybox   1/1     1            1           7s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/busybox-5497486bf5   1         1         1       7s

[ec2-user@k8s ~]$ kubectl exec -ti pod/busybox-5497486bf5-csr6q -- sh 
/ # ls /data
/ # echo $(hostname) &gt; /data/pvc.txt
/ # exit


# 查看服务端目录下数据
[ec2-user@k8s ~]$ ll /backup/
total 4
-rw-r--r-- 1 root root 33 Apr 14 00:37 1.txt
drwxr-xr-x 2 root root 21 Apr 14 00:51 busybox-5497486bf5-csr6q
[ec2-user@k8s ~]$ cat /backup/busybox-5497486bf5-csr6q/pvc.txt   
busybox-5497486bf5-csr6q
</code></pre>

<p>把replicas改成2，再试试：</p>

<pre><code>
[ec2-user@k8s ~]$ kubectl apply -f dp-pvc.yaml 

[ec2-user@k8s ~]$ kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
busybox-5497486bf5-fkzls   1/1     Running   0          3m8s
busybox-5497486bf5-rv7k7   1/1     Running   0          3m8s

[ec2-user@k8s ~]$ kubectl exec busybox-5497486bf5-fkzls -- sh -c 'echo $(hostname) &gt;/data/$(hostname).txt'
[ec2-user@k8s ~]$ kubectl exec busybox-5497486bf5-rv7k7 -- sh -c 'echo $(hostname) &gt;/data/$(hostname).txt'                        


# 查看服务端目录结构
[ec2-user@k8s ~]$ ll -R /backup/
/backup/:
total 4
-rw-r--r-- 1 root root 33 Apr 14 00:37 1.txt
drwxr-xr-x 2 root root 21 Apr 14 00:51 busybox-5497486bf5-csr6q
drwxr-xr-x 2 root root 42 Apr 14 01:20 busybox-5497486bf5-fkzls
drwxr-xr-x 2 root root 42 Apr 14 01:20 busybox-5497486bf5-rv7k7

/backup/busybox-5497486bf5-csr6q:
total 4
-rw-r--r-- 1 root root 25 Apr 14 00:51 pvc.txt

/backup/busybox-5497486bf5-fkzls:
total 4
-rw-r--r-- 1 root root 25 Apr 14 01:20 busybox-5497486bf5-fkzls.txt

/backup/busybox-5497486bf5-rv7k7:
total 4
-rw-r--r-- 1 root root 25 Apr 14 01:20 busybox-5497486bf5-rv7k7.txt
[ec2-user@k8s ~]$ 
</code></pre>

<h3>NFS Subdir External Provisioner</h3>

<ul>
<li><a href="http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#NFS-Subdir-External-Provisioner">http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#NFS-Subdir-External-Provisioner</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#nfs">https://kubernetes.io/docs/concepts/storage/storage-classes/#nfs</a></li>
<li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner</a></li>
</ul>


<p>NFS subdir external provisioner 使用现有的的NFS 服务器来支持通过 Persistent Volume Claims 动态供应 Kubernetes Persistent Volumes。持久卷默认被配置为${namespace}-${pvcName}-${pvName}，使用这个必须已经拥有 NFS 服务器。</p>

<ul>
<li><a href="http://dockone.io/article/2598">http://dockone.io/article/2598</a> External NFS驱动的工作原理</li>
</ul>


<blockquote><p>K8S的外部NFS驱动，可以按照其工作方式（是作为NFS server还是NFS client）分为两类：</p>

<p>1.nfs-client:</p>

<p>也就是我们接下来演示的这一类，它通过K8S的内置的NFS驱动挂载远端的NFS服务器到本地目录；然后将自身作为storage provider，关联storage class。当用户创建对应的PVC来申请PV时，该provider就将PVC的要求与自身的属性比较，一旦满足就在本地挂载好的NFS目录中创建PV所属的子目录，为Pod提供动态的存储服务。</p>

<p>2.nfs:
与nfs-client不同，该驱动并不使用k8s的NFS驱动来挂载远端的NFS到本地再分配，而是直接将本地文件映射到容器内部，然后在容器内使用ganesha.nfsd来对外提供NFS服务；在每次创建PV的时候，直接在本地的NFS根目录中创建对应文件夹，并export出该子目录。</p>

<p>接下来我们来操作一个nfs-client驱动的例子，先对其有个直观的认识！</p>

<p>External NFS驱动的部署实例</p>

<p>这里，我们将nfs-client驱动做一个deployment部署到K8S集群中，然后对外提供存储服务。</p>

<p>1.部署nfs-client-provisioner</p>

<p>环境变量的PROVISIONER_NAME、NFS服务器地址、NFS对外提供服务的路径信息等需要设置好；部署所使用的yaml文件关键代码如下所示：</p>

<p>2.创建Storage Class</p>

<p>storage class的定义，需要注意的是：provisioner属性要等于驱动所传入的环境变量PROVISIONER_NAME的值。否则，驱动不知道知道如何绑定storage class。</p>

<p>3.创建PVC</p>

<p>这里指定了其对应的storage-class的名字为wise2c-nfs-storage，如下：</p>

<p>4.创建pod</p>

<p>指定该pod使用我们刚刚创建的PVC：henry-claim：</p>

<p>完成之后，如果attach到pod中执行一些文件的读写操作，就可以确定pod的/mnt已经使用了NFS的存储服务了。</p></blockquote>

<ul>
<li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner#without-helm">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner#without-helm</a></li>
</ul>


<p>官方文档中的脚本：</p>

<pre><code># Set the subject of the RBAC objects to the current namespace where the provisioner is being deployed
$ NS=$(kubectl config get-contexts|grep -e "^\*" |awk '{print $5}')
$ NAMESPACE=${NS:-default}
$ sed -i'' "s/namespace:.*/namespace: $NAMESPACE/g" ./deploy/rbac.yaml ./deploy/deployment.yaml
$ kubectl create -f deploy/rbac.yaml
</code></pre>

<p>操作步骤：</p>

<pre><code>[ec2-user@k8s ~]$ git clone https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/
[ec2-user@k8s ~]$ cd nfs-subdir-external-provisioner/

[ec2-user@k8s nfs-subdir-external-provisioner]$ NS=$(kubectl config get-contexts|grep -e "^\*" |awk '{print $5}')
[ec2-user@k8s nfs-subdir-external-provisioner]$ NAMESPACE=${NS:-default}
[ec2-user@k8s nfs-subdir-external-provisioner]$ sed -i'' "s/namespace:.*/namespace: $NAMESPACE/g" ./deploy/rbac.yaml ./deploy/deployment.yaml
[ec2-user@k8s nfs-subdir-external-provisioner]$ kubectl create -f deploy/rbac.yaml
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created


$ vi deploy/deployment.yaml

            - name: NFS_SERVER
              value: 192.168.191.131
            - name: NFS_PATH
              value: /backup
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.191.131
            path: /backup

$ vi deploy/class.yaml

parameters:
  archiveOnDelete: "false"
#Specifies a template for creating a directory path via PVC metadata's such as labels, annotations, name or namespace. To specify metadata use ${.PVC.&lt;metadata&gt;}. Example: If folder should be named like &lt;pvc-namespace&gt;-&lt;pvc-name&gt;, use ${.PVC.namespace}-${.PVC.name} as pathPattern.
#  pathPattern: "${.PVC.namespace}/${.PVC.annotations.nfs.io/storage-path}" # waits for nfs.io/storage-path annotation, if not specified will accept as empty string.
#  onDelete: delete


# 先把镜像拉下来 k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2

[ec2-user@k8s nfs-subdir-external-provisioner]$ kubectl apply -f deploy/deployment.yaml 
deployment.apps/nfs-client-provisioner created

[ec2-user@k8s nfs-subdir-external-provisioner]$ kubectl apply -f deploy/class.yaml 
storageclass.storage.k8s.io/nfs-client created
[ec2-user@k8s nfs-subdir-external-provisioner]$
</code></pre>

<p>测试：</p>

<pre><code># PVC内容
# https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/blob/master/deploy/test-claim.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi

[ec2-user@k8s nfs-subdir-external-provisioner]$ kubectl create -f deploy/test-claim.yaml -f deploy/test-pod.yaml
persistentvolumeclaim/test-claim created
pod/test-pod created

# kubectl delete -f deploy/test-pod.yaml -f deploy/test-claim.yaml
</code></pre>

<p><code>test pod</code> 在共享文件系统下写了一个 <code>touch /mnt/SUCCESS</code> 文件：</p>

<pre><code>[ec2-user@k8s nfs-subdir-external-provisioner]$ ll /backup/default-test-claim-pvc-9857153a-6c2b-42d7-b464-aa5fc2acbf90/
total 0
-rw-r--r-- 1 root root 0 Apr 14 02:14 SUCCESS
</code></pre>

<h3>NFS Ganesha server and external provisioner</h3>

<ul>
<li><a href="https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner">https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner</a></li>
<li><a href="http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#nfs-ganesha-server-and-external-provisioner">http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#nfs-ganesha-server-and-external-provisioner</a></li>
</ul>


<p>就是直接在k8s集群中装一个NFS server。感觉没有直接在系统安装NFS管理方便，先搁置了。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NFS on Centos7]]></title>
    <link href="http://winse.github.io/blog/2017/08/05/nfs-on-centos7/"/>
    <updated>2017-08-05T16:38:56+08:00</updated>
    <id>http://winse.github.io/blog/2017/08/05/nfs-on-centos7</id>
    <content type="html"><![CDATA[<h2>参考</h2>

<ul>
<li><a href="https://www.howtoforge.com/nfs-server-and-client-on-centos-7">https://www.howtoforge.com/nfs-server-and-client-on-centos-7</a></li>
<li><a href="http://blog.huatai.me/2014/10/14/CentOS-7-NFS-Server-and-Client-Setup/">http://blog.huatai.me/2014/10/14/CentOS-7-NFS-Server-and-Client-Setup/</a></li>
</ul>


<h2>指令</h2>

<p>安装</p>

<pre><code>[root@cu3 data]# yum install nfs-utils -y 
[root@cu3 data]# chmod -R 777 /data/k8s-dta

systemctl enable rpcbind
systemctl enable nfs-server
systemctl enable nfs-lock
systemctl enable nfs-idmap

systemctl start rpcbind
systemctl start nfs-server
systemctl start nfs-lock
systemctl start nfs-idmap
</code></pre>

<p>配置</p>

<pre><code>[root@cu3 data]# vi /etc/exports
/data/k8s-dta 192.168.0.0/24(rw,sync,no_root_squash,no_all_squash)
</code></pre>

<p>说明：</p>

<pre><code>/data/k8s-dta – 共享目录
192.168.0.0/24 – 允许访问NFS的客户端IP地址段
rw – 允许对共享目录进行读写
sync – 实时同步共享目录
no_root_squash – 允许root访问
no_all_squash - 允许用户授权
no_subtree_check - 如果卷的一部分被输出，从客户端发出请求文件的一个常规的调用子目录检查验证卷的相应部分。如果是整个卷输出，禁止这个检查可以加速传输。
no_subtree_check - If only part of a volume is exported, a routine called subtree checking verifies that a file that is requested from the client is in the appropriate part of the volume. If the entire volume is exported, disabling this check will speed up transfers. Setting Up an NFS Server
</code></pre>

<p></p>

<p>然后重启服务，并开放防火墙（或者关闭）</p>

<pre><code>systemctl restart nfs-server

firewall-cmd --permanent --zone=public --add-service=ssh
firewall-cmd --permanent --zone=public --add-service=nfs
firewall-cmd --reload
</code></pre>

<h2>客户端配置</h2>

<pre><code>[root@cu2 opt]# yum install -y nfs-utils

[root@cu2 opt]# mount cu3:/data/k8s-dta dta
[root@cu2 opt]# touch dta/abc
[root@cu2 opt]# ll dta
total 0
-rw-r--r-- 1 root root 0 Aug  3  2017 abc

[root@cu3 data]# ll k8s-dta/
total 0
-rw-r--r-- 1 root root 0 Aug  3 15:19 abc
</code></pre>

<p></p>

<h2>on ubuntu</h2>

<ul>
<li><a href="https://sysadmins.co.za/setup-a-nfs-server-on-ubuntu/">https://sysadmins.co.za/setup-a-nfs-server-on-ubuntu/</a></li>
</ul>


<pre><code># In this post 10.8.133.83 will be the IP of our NFS Server.
$ apt update &amp;&amp; sudo apt upgrade -y
$ sudo apt-get install nfs-kernel-server nfs-common -y

$ mkdir /vol
$ chown -R nobody:nogroup /vol

# We need to set in the exports file, the clients we would like to allow:
# 
# rw: Allows Client R/W Access to the Volume.
# sync: This option forces NFS to write changes to disk before replying. More stable and Consistent. Note, it does reduce the speed of file operations.
# no_subtree_check: This prevents subtree checking, which is a process where the host must check whether the file is actually still available in the exported tree for every request. This can cause many problems when a file is renamed while the client has it opened. In almost all cases, it is better to disable subtree checking.
# In order for the containers to be able to change permissions, you need to set (rw,async,no_subtree_check,no_wdelay,crossmnt,insecure,all_squash,insecure_locks,sec=sys,anonuid=0,anongid=0)
$ echo '/vol 10.8.133.83(rw,sync,no_subtree_check) 10.8.166.19(rw,sync,no_subtree_check) 10.8.142.195(rw,sync,no_subtree_check)' &gt;&gt; /etc/exports

$ sudo systemctl restart nfs-kernel-server
$ sudo systemctl enable nfs-kernel-server
</code></pre>

<p>Client Side:</p>

<pre><code>$ sudo apt-get install nfs-common -y

$ sudo mount 10.8.133.83:/vol /mnt
$ sudo umount /mnt
$ df -h

$ sudo bash -c "echo '10.8.133.83:/vol /mnt nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0' &gt;&gt; /etc/fstab"
$ sudo mount -a
$ df -h
</code></pre>

<h2>后记</h2>

<p>建好NFS服务后，可以把它作为k8s容器的存储，这样就不怕丢数据了。</p>

<ul>
<li><a href="https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#writing-to-stable-storage">https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#writing-to-stable-storage</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#nfs">https://kubernetes.io/docs/concepts/storage/volumes/#nfs</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/nfs">https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/nfs</a></li>
</ul>


<p></p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
