<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Spark | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2015-12-31T12:03:33+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop不同版本yarn和hdfs混搭，spark-yarn环境配置]]></title>
    <link href="http://winseliu.com/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs/"/>
    <updated>2015-06-10T18:48:19+08:00</updated>
    <id>http://winseliu.com/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs</id>
    <content type="html"><![CDATA[<p>hadoop分为存储和计算两个主要的功能，hdfs步入hadoop2后不论稳定性还是HA等等功能都比hadoop1要更吸引人。hadoop-2.2.0的hdfs已经比较稳定，但是yarn高版本有更加丰富的功能。本文主要关注spark-yarn下日志的查看，以及spark-yarn-dynamic的配置。</p>

<p>hadoop-2.2.0的hdfs原本已经在使用的环境，在这基础上搭建运行yarn-2.6.0，以及spark-1.3.0-bin-2.2.0。</p>

<ul>
<li>编译</li>
</ul>


<p>我是在虚拟机里面编译，共享了host主机的maven库。参考【VMware共享目录】，【VMware-Centos6 Build hadoop-2.6】注意<strong>cmake_symlink_library的异常，由于共享的windows目录下不能创建linux的软链接</strong></p>

<pre><code>tar zxvf ~/hadoop-2.6.0-src.tar.gz 
cd hadoop-2.6.0-src/
mvn package -Pdist,native -DskipTests -Dtar -Dmaven.javadoc.skip=true

# 由于hadoop-hdfs还是2.2的，这里编译spark需要用2.2版本！
# 如果用2.6会遇到[UnsatisfiedLinkError:org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray ](http://blog.csdn.net/zeng_84_long/article/details/44340441)
cd spark-1.3.0
export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=1g -XX:ReservedCodeCacheSize=512m"
mvn clean package -Phadoop-2.2 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests

vi make-distribution.sh #注释掉BUILD_COMMAND那一行，不重复执行package！
./make-distribution.sh  --mvn `which mvn` --tgz  --skip-java-test -Phadoop-2.6 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</code></pre>

<ul>
<li><p>配置注意点</p></li>
<li><p>core-site不要全部拷贝原来的，只要一些主要的配置即可。</p></li>
<li>yarn-site的<code>yarn.resourcemanager.webapp.address</code>需要填写具体的地址，不能写<code>0.0.0.0</code>。</li>
<li>yarn-site的<code>yarn.nodemanager.aux-services</code>添加spark_shuffle服务。<a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation</a></li>
<li>把hive-site的文件拷贝/链接到spark的conf目录下。</li>
<li>spark-yarn-dynamic配置: <a href="https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation">https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation</a></li>
</ul>


<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ cat conf/spark-defaults.conf 
# spark.master                     spark://bigdatamgr1:7077,bigdata8:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
# spark.executor.extraJavaOptions       -Xmx16g -Xms16g -Xmn256m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:ParallelGCThreads=10
spark.driver.memory              48g
spark.executor.memory            48g
spark.sql.shuffle.partitions     200

#spark.scheduler.mode FAIR
spark.serializer  org.apache.spark.serializer.KryoSerializer
spark.driver.maxResultSize 8g
#spark.kryoserializer.buffer.max.mb 2048

spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 4
spark.shuffle.service.enabled true

[eshore@bigdatamgr1 conf]$ cat spark-env.sh 
#!/usr/bin/env bash

JAVA_HOME=/home/eshore/jdk1.7.0_60

# log4j

__add_to_classpath() {

  root=$1

  if [ -d "$root" ] ; then
    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
      else
        export SPARK_DIST_CLASSPATH=$f
      fi
    done
  fi

}
# this add tail of SPARK_CLASSPATH
__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"

#export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
export HADOOP_CONF_DIR=/home/eshore/hadoop-2.6.0/etc/hadoop
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR

# HA
SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark" 

SPARK_PID_DIR=${SPARK_HOME}/pids
</code></pre>

<ul>
<li>同步</li>
</ul>


<pre><code>for h in `cat slaves ` ; do rsync -vaz hadoop-2.6.0 $h:~/ --delete --exclude=work --exclude=logs --exclude=metastore_db --exclude=data --exclude=pids ; done
</code></pre>

<ul>
<li>启动spark-hive-thrift</li>
</ul>


<p>./sbin/start-thriftserver.sh &ndash;executor-memory 29g &ndash;master yarn-client</p>

<p>对于多任务的集群来说，配置自动动态分配（类似资源池）更有利于资源的使用。可以通过【All Applications】-【ApplicationMaster】-【Executors】来观察执行进程的变化。</p>

<p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[编译/搭建Spark环境]]></title>
    <link href="http://winseliu.com/blog/2014/10/16/spark-build-and-configuration/"/>
    <updated>2014-10-16T16:55:39+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/16/spark-build-and-configuration</id>
    <content type="html"><![CDATA[<p>2015-04更新：【后记 Spark-1.3.0】</p>

<p>官网提供的hadoop版本没有2.5的。这里我自己下载源码再进行编译。先下载spark-1.1.0.tgz，解压然后执行命令编译：</p>

<pre><code>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive -X -DskipTests clean package

# mvn package eclipse:eclipse -Phadoop-2.2 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests

## jdk8-x64 &amp; spark-1.5.2 &amp; maven-3.3.9
# set export MAVEN_OPTS=-Xmx2g
# mvn package eclipse:eclipse -Phadoop-2.6 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
# 注释掉pom.xml中的&lt;useZincServer&gt;true&lt;/useZincServer&gt; @see http://stackoverflow.com/questions/31844848/building-spark-with-maven-error-finding-javac-but-path-is-correct
# 公司网络不稳定，遇到下载maven包报错，多重试几次！！
</code></pre>

<p>用64位的JDK！！加上maven参数，不然很可能出现OOM（甚至各种稀奇古怪的问题）。编译的时间也挺长的，可以先去吃个饭。或者取消一些功能的编译（如hive）。</p>

<p>编译完后，在assembly功能下会生成包括所有spark及其依赖的jar文件。</p>

<pre><code>[root@docker scala-2.10]# cd spark-1.1.0/assembly/target/scala-2.10/
[root@docker scala-2.10]# ll -h
total 135M
-rw-r--r--. 1 root root 135M Oct 15 21:18 spark-assembly-1.1.0-hadoop2.5.1.jar
</code></pre>

<h2>打包</h2>

<p>上面我们已经编译好了spark程序，这里对其进行打包集成到一个压缩包。使用程序自带的make-distribution.sh即可。</p>

<p>为了减少重新编译的巨长的等待时间，修改下脚本<code>make-distribution.sh</code>的maven编译参数，去掉maven的clean阶段操作（最好直接注释掉mvn那行），修改最终结果如下：</p>

<pre><code>#BUILD_COMMAND="mvn clean package -DskipTests $@"
#BUILD_COMMAND="mvn package -DskipTests $@"
</code></pre>

<p>然后执行命令：</p>

<pre><code>[root@docker spark-1.1.0]# sh -x make-distribution.sh --tgz  --skip-java-test -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive 
[root@docker spark-1.1.0]# ll -h
total 185M
...
-rw-r--r--. 1 root root 185M Oct 16 00:09 spark-1.1.0-bin-2.5.1.tgz
</code></pre>

<p>最终会在目录行打包生成tgz的文件。</p>

<h2>本地运行</h2>

<p>把本机ip主机名写入到hosts，方便以后windows本机查看日志</p>

<pre><code>[root@docker spark-1.1.0-bin-2.5.1]# echo 192.168.154.128 docker &gt;&gt; /etc/hosts
[root@docker spark-1.1.0-bin-2.5.1]# cat /etc/hosts
...
192.168.154.128 docker
</code></pre>

<h3>运行helloworld：</h3>

<pre><code>[root@docker spark-1.1.0-bin-2.5.1]# bin/run-example SparkPi 10
Spark assembly has been built with Hive, including Datanucleus jars on classpath
...
14/10/16 00:22:36 INFO SparkContext: Job finished: reduce at SparkPi.scala:35, took 2.848632007 s
Pi is roughly 3.139344
14/10/16 00:22:36 INFO SparkUI: Stopped Spark web UI at http://docker:4040
...
</code></pre>

<h3>交互式操作：</h3>

<pre><code>[root@docker spark-1.1.0-bin-2.5.1]# bin/spark-shell --master local[2]
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_60)
...
14/10/16 00:25:57 INFO SparkUI: Started SparkUI at http://docker:4040
14/10/16 00:25:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/10/16 00:25:58 INFO Executor: Using REPL class URI: http://192.168.154.128:39385
14/10/16 00:25:58 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@docker:57417/user/HeartbeatReceiver
14/10/16 00:25:58 INFO SparkILoop: Created spark context..
Spark context available as sc.

scala&gt; 
</code></pre>

<p>说明下环境，我使用windows作为开发环境，使用虚拟机中的linux作为测试环境。同时通过ssh连接的隧道来实现windows无缝的访问虚拟机linux操作系统。</p>

<p>启动交互式访问后，就可以通过浏览器访问4040查看spark程序的状态。</p>

<p><img src="http://file.bmob.cn/M00/1E/4B/wKhkA1Q_3NOALefuAAEimqVy6-s418.png" alt="" /></p>

<p>任务已经启动，接下来就可以进行操作：</p>

<pre><code>scala&gt; val textFile=sc.textFile("README.md")
textFile: org.apache.spark.rdd.RDD[String] = README.md MappedRDD[1] at textFile at &lt;console&gt;:12

scala&gt; textFile.count()
res0: Long = 141

scala&gt; textFile.first()
res1: String = # Apache Spark

scala&gt; val linesWithSpark = textFile.filter(line=&gt;line.contains("Spark"))
linesWithSpark: org.apache.spark.rdd.RDD[String] = FilteredRDD[2] at filter at &lt;console&gt;:14

scala&gt; textFile.filter(line=&gt;line.contains("Spark")).count()
res2: Long = 21

scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)
res3: Int = 15

scala&gt; import java.lang.Math
import java.lang.Math

scala&gt; textFile.map(_.split(" ").size).reduce((a,b)=&gt;Math.max(a,b))
res4: Int = 15

scala&gt; val wordCounts = textFile.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_)
wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at &lt;console&gt;:15

scala&gt; wordCounts.collect()
res5: Array[(String, Int)] = Array((means,1), (under,2), (this,4), (Because,1), (Python,2), (agree,1), (cluster.,1), (its,1), (follows.,1), (general,2), (have,2), (YARN,,3), (pre-built,1), (locally.,1), (locally,2), (changed,1), (MRv1,,1), (several,1), (only,1), (sc.parallelize(1,1), (This,2), (learning,,1), (basic,1), (requests,1), (first,1), (Configuration,1), (MapReduce,2), (CLI,1), (graph,1), (without,1), (documentation,1), ("yarn-client",1), ([params]`.,1), (any,2), (setting,2), (application,1), (prefer,1), (SparkPi,2), (engine,1), (version,3), (file,1), (documentation,,1), (&lt;http://spark.apache.org/&gt;,1), (MASTER,1), (entry,1), (example,3), (are,2), (systems.,1), (params,1), (scala&gt;,1), (provides,1), (refer,1), (MLLib,1), (Interactive,2), (artifact,1), (configure,1), (can,8), (&lt;art...
</code></pre>

<p>执行了上面一些操作后，通过网页查看状态变化：</p>

<p><img src="http://file.bmob.cn/M00/1E/4C/wKhkA1Q_3w6AM6njAAF-MCCYh2s170.png" alt="" /></p>

<h2>Spark-standalone集群</h2>

<p>部署集群需要用到多个服务器，这里我使用docker来进行部署。</p>

<p>本来应该早早完成本文的实践，但是在搭建docker-hadoop集群时花费了很多的时间。关于搭建集群dnsmasq处理域名问题参见下一篇文章。
最终实现可以参考：<a href="https://github.com/winse/docker-hadoop/tree/spark-yarn">docker-hadoop</a></p>

<pre><code>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver1 spark-yarn
[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver2 spark-yarn
[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name master -h master spark-yarn

[root@docker docker-hadoop]# docker ps | grep spark | awk '{print $1}' | xargs -I{} docker inspect -f ' ' {} &gt; /etc/dnsmasq.hosts
[root@docker docker-hadoop]# cat /etc/dnsmasq.hosts 
172.17.0.29 master
172.17.0.28 slaver2
172.17.0.27 slaver1
[root@docker docker-hadoop]# service dnsmasq restart
[root@docker docker-hadoop]# ssh hadoop@master

[hadoop@master ~]$ ssh-copy-id master
[hadoop@master ~]$ ssh-copy-id localhost
[hadoop@master ~]$ ssh-copy-id slaver1
[hadoop@master ~]$ ssh-copy-id slaver2
[hadoop@master spark-1.1.0-bin-2.5.1]$ sbin/start-all.sh 
[hadoop@master spark-1.1.0-bin-2.5.1]$ /opt/jdk1.7.0_67/bin/jps  -m
266 Jps -m
132 Master --ip master --port 7077 --webui-port 8080
</code></pre>

<p>通过网页可以查看集群的状态：</p>

<p><img src="http://file.bmob.cn/M00/1E/F8/wKhkA1RClV2AE0biAAEmpXJlzTc914.png" alt="" /></p>

<p>运行任务连接到master：</p>

<pre><code>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-shell --master spark://master:7077
...
14/10/17 11:31:08 INFO BlockManagerMasterActor: Registering block manager slaver2:55473 with 265.4 MB RAM
14/10/17 11:31:09 INFO BlockManagerMasterActor: Registering block manager slaver1:33441 with 265.4 MB RAM

scala&gt; 
</code></pre>

<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmG-AO--XAAD84ATrCew955.png" alt="" /></p>

<p>从上图可以看到，程序已经正确连接到spark集群，master为driver，任务节点为slaver1和slaver2。下面运行下程序，然后通过网页查看运行的状态。</p>

<pre><code>scala&gt; val textFile=sc.textFile("README.md")
scala&gt; textFile.count()
scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)
</code></pre>

<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmdmAB3M9AAFIzMb4yk0370.png" alt="" /></p>

<p>系统安装好了，启动spark-standalone集群和hadoop-yarn一样。配置ssh、java，然后启动，配合网页8080/4040可以实时的了解任务的指标。</p>

<h2>yarn集群</h2>

<p>如果你是按照前面的步骤来操作的，需要先把spark-standalone的集群停掉。端口8080和yarn web使用端口冲突，会导致yarn启动失败。</p>

<p>修改spark-env.sh，添加HADOOP_CONF_DIR参数。然后提交任务到yarn上执行就行了。</p>

<pre><code>[hadoop@master spark-1.1.0-bin-2.5.1]$ cat conf/spark-env.sh
#!/usr/bin/env bash

JAVA_HOME=/opt/jdk1.7.0_67 

HADOOP_CONF_DIR=/opt/hadoop-2.5.1/etc/hadoop

[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.1.0-hadoop2.5.1.jar  10
</code></pre>

<p><img src="http://file.bmob.cn/M00/1E/FD/wKhkA1RCszeAALCPAAK1Nzk6faQ330.png" alt="" /></p>

<p>运行的结果输出在driver的slaver2节点，对应输出型来说不是很直观。spark-yarn提供了另一种方式，driver直接本地运行<em>yarn-client</em>。</p>

<pre><code>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client lib/spark-examples-1.1.0-hadoop2.5.1.jar  10
...
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8248 ms on slaver1 (1/10)
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 231 ms on slaver1 (2/10)
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 158 ms on slaver1 (3/10)
14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 284 ms on slaver1 (4/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 175 ms on slaver1 (5/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 301 ms on slaver1 (6/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 175 ms on slaver1 (7/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 143 ms on slaver1 (8/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 164 ms on slaver1 (9/10)
14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, slaver1, PROCESS_LOCAL, 1228 bytes)
14/10/17 13:31:03 INFO cluster.YarnClientSchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@slaver2:51923/user/Executor#1132577949] with ID 1
14/10/17 13:31:04 INFO util.RackResolver: Resolved slaver2 to /default-rack
14/10/17 13:31:04 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 397 ms on slaver1 (10/10)
14/10/17 13:31:04 INFO cluster.YarnClientClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
14/10/17 13:31:04 INFO scheduler.DAGScheduler: Stage 0 (reduce at SparkPi.scala:35) finished in 26.084 s
14/10/17 13:31:04 INFO spark.SparkContext: Job finished: reduce at SparkPi.scala:35, took 28.31400558 s
Pi is roughly 3.140248
</code></pre>

<p>thrift连接yarn运行时时受容器内存最大值限制，需要修改yarn-site.xml。</p>

<pre><code>cat yarn-site.xml 
&lt;property&gt;
  &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
  &lt;value&gt;32000&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
  &lt;value&gt;32768&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
  &lt;value&gt;2048&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;
  &lt;value&gt;32768&lt;/value&gt;
&lt;/property&gt;

./sbin/start-thriftserver.sh --executor-memory 29g --master yarn-client
</code></pre>

<p>不能直接把executor的内存设置为最大值，否则会报错：</p>

<pre><code>Exception in thread "main" java.lang.IllegalArgumentException: Required executor memory (30720+2150 MB) is above the max threshold (32768 MB) of this cluster!
</code></pre>

<h2>总结</h2>

<p>本文主要是搭建spark的环境搭建，本地运行、以及在docker中搭建spark集群、yarn集群三种方式。本地运行最简单方便，但是没有模拟到集群环境；spark提供了yarn框架上的实现，直接提交任务到yarn即可；spark集群相对比较简单和方便，接下来的远程调试主要通过spark伪分布式集群方式来进行。</p>

<h2>参考</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/building-with-maven.html">Building Spark with Maven</a></li>
<li><a href="http://spark.apache.org/docs/latest/quick-start.html">Quick Start</a></li>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html">Spark Configuration</a></li>
<li><a href="http://www.07net01.com/linux/zuixindnsmasqanzhuangbushuxiangjie_centos6__653221_1381214991.html">DNS</a></li>
<li>[spark上安装mysql与hive](<a href="http://blog.csdn.net/hwssg/article/details/38424529">http://blog.csdn.net/hwssg/article/details/38424529</a>]</li>
</ul>


<h2>后后记 spark-1.4.1</h2>

<pre><code>[hadoop@cu2 spark-1.4.1]$ export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"

[hadoop@cu2 spark-1.4.1]$  mvn package -Phadoop-2.6 -Dhadoop.version=2.7.1 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests

[hadoop@cu2 spark-1.4.1]$ vi make-distribution.sh 
BUILD_COMMAND=("$MVN"  package -DskipTests $@)

[hadoop@cu2 spark-1.4.1]$ ./make-distribution.sh --mvn `which mvn` --tgz  --skip-java-test   -Phadoop-2.6 -Dhadoop.version=2.7.1 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</code></pre>

<h2>后记 Spark-1.3.0</h2>

<h3>编译1.3.0(cygwin)</h3>

<pre><code>export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
mvn package eclipse:eclipse -Phadoop-2.2 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests

# mvn package eclipse:eclipse -Phadoop-2.2 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
# find . -name ".classpath" | xargs -I{} sed -i 's/ including="\*\*\/\*\.java"//' {}

dos2unix make-distribution.sh
./make-distribution.sh --mvn `which mvn` --tgz  --skip-java-test -Phadoop-2.2 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests

# linux环境部署
[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ find bin/* -perm /u+x | xargs -I{} sed -i 's/^M//g' {} 
[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ find sbin/* -perm /u+x | xargs -I{} sed -i 's/^M//g' {} 
</code></pre>

<p>这个版本，windows-cygwin编译的shell文件也是<strong>windows的换行符</strong>！！需要注意下！</p>

<h3>spark-1.3.0运行spark-sql</h3>

<pre><code>$ export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
$ mvn package eclipse:eclipse -Phadoop-2.2 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests

$ ./make-distribution.sh --mvn `which mvn` --tgz  --skip-java-test -Phadoop-2.2 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</code></pre>

<p>1 连接到hive-engine</p>

<p>hive的<code>hive.execution.engine</code>的tez，添加tez的jar和hive-site到CLASSPATH。</p>

<p>包的导入以及配置：（如果使用meta-service的就不用这么麻烦）</p>

<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ vi conf/spark-env.sh 
...
JAVA_HOME=/home/eshore/jdk1.7.0_60

# log4j

__add_to_classpath() {

  root=$1

  if [ -d "$root" ] ; then
    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
      else
        export SPARK_DIST_CLASSPATH=$f
      fi
    done
  fi

}

__add_to_classpath "/home/eshore/tez-0.4.0-incubating"
__add_to_classpath "/home/eshore/tez-0.4.0-incubating/lib"
__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"

export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
export SPARK_CLASSPATH=/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR

不能直接把hive的包全部加进去，hive-0.13.1a和hive-0.13.1的部分包不一致！！

    java.lang.NoSuchMethodException: org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(com.esotericsoftware.kryo.Kryo, java.io.InputStream, java.lang.Class)

    private static java.lang.Object org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(org.apache.hive.com.esotericsoftware.kryo.Kryo,java.io.InputStream,java.lang.Class)

&amp;&amp; 如果不依赖tez，可以直接把datanucleus的三个包拷贝到lib目录下。

[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ll lib
total 262364
-rw-rw-r-- 1 hadoop hadoop    339666 Mar 25 19:35 datanucleus-api-jdo-3.2.6.jar
-rw-rw-r-- 1 hadoop hadoop   1890075 Mar 25 19:35 datanucleus-core-3.2.10.jar
-rw-rw-r-- 1 hadoop hadoop   1809447 Mar 25 19:35 datanucleus-rdbms-3.2.9.jar
-rwxr-xr-x 1 hadoop hadoop   4136686 Mar 31 13:05 spark-1.3.0-yarn-shuffle.jar
-rwxr-xr-x 1 hadoop hadoop 154198768 Mar 31 13:05 spark-assembly-1.3.0-hadoop2.2.0.jar
-rwxr-xr-x 1 hadoop hadoop 106275583 Mar 31 13:05 spark-examples-1.3.0-hadoop2.2.0.jar

[eshore@bigdatamgr1 conf]$ ll
...
lrwxrwxrwx 1 eshore biadmin   50 Mar 31 13:26 hive-site.xml -&gt; /home/eshore/apache-hive-0.13.1/conf/hive-site.xml
-rw-r--r-- 1 eshore biadmin  632 Mar 31 15:12 log4j.properties
lrwxrwxrwx 1 eshore biadmin   44 Mar 31 10:20 slaves -&gt; /data/opt/ibm/biginsights/hadoop-conf/slaves
-rwxr-xr-x 1 eshore biadmin 3380 Mar 31 16:17 spark-env.sh
lrwxrwxrwx 1 eshore biadmin   62 Mar 31 16:17 tez-site.xml -&gt; /data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop/tez-site.xml
</code></pre>

<p>也可以起hive-metaserver，然后spark通过连接meta即可：</p>

<pre><code># 起meta服务
nohup bin/hive --service metastore &gt; metastore.log 2&gt;&amp;1 &amp;

# hive客户端配置
vi hive-site.xml
&lt;property&gt;
  &lt;name&gt;hive.metastore.uris&lt;/name&gt;
  &lt;value&gt;thrift://DataNode2:9083&lt;/value&gt;
  &lt;description&gt;Thrift uri for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt;
&lt;/property&gt;
</code></pre>

<p>2 运行：</p>

<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$  bin/spark-sql 2&gt;sql.log
SET spark.sql.hive.version=0.13.1
spark-sql&gt; show databases;
default
neva2dta
spark-sql&gt; show tables;
pokes   false
t_neva2_dps_xdr false
t_neva2_ipdr_xdr        false
spark-sql&gt; select count(*) from pokes;
500
spark-sql&gt; 

[eshore@bigdatamgr1 conf]$ vi spark-env.sh 
#!/usr/bin/env bash

JAVA_HOME=/home/eshore/jdk1.7.0_60
SPARK_CLASSPATH='/home/eshore/apache-hive-0.13.1/lib/*:/home/eshore/tez-0.4.0-incubating/*:/home/eshore/tez-0.4.0-incubating/lib/*'

# 同步
[eshore@bigdatamgr1 ~]$ for h in `cat ~/spark-1.3.0-bin-2.2.0/conf/slaves` ; do rsync -vaz /data/opt/ibm/biginsights/hadoop-2.2.0 $h:/data/opt/ibm/biginsights/  ; done
</code></pre>

<p>运行hivesever服务</p>

<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ cat start_thrift.sh 
#!/bin/bash
# hive-classpath已经在spark-env.sh中添加

./sbin/start-thriftserver.sh --master spark://bigdatamgr1:7077 --executor-memory 16g
[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ./start_thrift.sh 

[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ bin/beeline -u jdbc:hive2://bigdatamgr1:10001 -n eshore -p '' 
</code></pre>

<p>上面这么写有个问题，尽管thrift启动正常，但是shell总是打印错误：</p>

<pre><code>failed to launch org.apache.spark.sql.hive.thriftserver.HiveThriftServer2:
  ========================================

full log in /home/eshore/spark-1.3.0-bin-2.2.0/sbin/../logs/spark-eshore-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-bigdatamgr1.out
</code></pre>

<p>比较隐晦，问题在<code>sbin/spark-daemon.sh</code>，启动完后通过<code>if [[ ! $(ps -p "$newpid" -o args=) =~ $command ]]; then</code>（其中<code>=~</code>表示正则匹配，最终<code>spark-class.sh</code>调用java会由于加上classpath），而上面的classpath会很长，导致上面的匹配失败！！</p>

<pre><code>[hadoop@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ vi bin/spark-class
...
  exec "$RUNNER" -cp "$CLASSPATH" $JAVA_OPTS "$@"
fi

# 匹配失败时的值
[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ps -p 1925344 -o args=
/home/eshore/jdk1.7.0_60/bin/java -cp :/home/eshore/spark-1.3.0-bin-2.2.0/sbin/../conf:/home/eshore/spark-1.3.0-bin-2.2.0/lib/spark-assembly-1.3.0-hadoop2.2.0.jar:/home/eshore/spark
</code></pre>

<h4>解决办法</h4>

<p>先看实验：</p>

<pre><code>[dpi@dacs tmp]$ java -cp ~/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar:. JDBCConnTest

[dpi@dacs tmp]$ echo $CLASSPATH
.
[dpi@dacs tmp]$ export CLASSPATH=~/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar
[dpi@dacs tmp]$ java JDBCConnTest
错误: 找不到或无法加载主类 JDBCConnTest
[dpi@dacs tmp]$ java -cp . JDBCConnTest
java.lang.ClassNotFoundException: com.mysql.jdbc.Driver

[dpi@dacs tmp]$ echo $CLASSPATH
/home/dpi/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar
[dpi@dacs tmp]$ export CLASSPATH=~/kettle/data-integration/lib/mysql-connector-java-5.1.31-bin.jar:.
[dpi@dacs tmp]$ java JDBCConnTest
</code></pre>

<p>设置cp后会覆盖CLASSPATH。所以问题的解决方法：直接把cp的路径删掉（不添加），前面export的classpath路径。java程序会去主动获取改环境变量。</p>

<pre><code>  export CLASSPATH
  exec "$RUNNER" $JAVA_OPTS "$@"
</code></pre>

<p>效果如下：</p>

<pre><code>++ ps -p 1932338 -o args=
+ [[ ! /home/eshore/jdk1.7.0_60/bin/java -XX:MaxPermSize=128m -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --executor-memory 48g spark-internal =~ org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 ]]
</code></pre>

<ul>
<li>[=~的作用]<a href="http://bbs.chinaunix.net/thread-1623121-1-1.html">http://bbs.chinaunix.net/thread-1623121-1-1.html</a></li>
<li><a href="http://docs.oracle.com/javase/tutorial/essential/environment/paths.html">http://docs.oracle.com/javase/tutorial/essential/environment/paths.html</a></li>
<li><a href="https://docs.oracle.com/javase/8/docs/technotes/tools/windows/classpath.html">https://docs.oracle.com/javase/8/docs/technotes/tools/windows/classpath.html</a></li>
</ul>


<h3>Spark-HA</h3>

<p>仅需要配置，重启spark集群即可。</p>

<pre><code>[eshore@bigdata8 spark-1.3.0-bin-2.2.0]$ cat conf/spark-env.sh
...
SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark"

[eshore@bigdatamgr1 conf]$ vi spark-defaults.conf 
spark.master                     spark://bigdatamgr1:7077,bigdata8:7077
...
</code></pre>

<p>各个master要单独的启动:</p>

<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ sbin/start-all.sh 
[eshore@bigdata8 spark-1.3.0-bin-2.2.0]$ sbin/start-master.sh 
</code></pre>

<p>通过查看<a href="http://bigdata8:8080/">http://bigdata8:8080/</a>当前的状态为<strong>STANDBY</strong>。Workers列表为空。</p>

<pre><code>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ sbin/stop-master.sh 
</code></pre>

<p>停了bigdatamgr1后，刷新<code>bigdata8:8080</code>页面等1分钟左右就变成ALIVE，然后其他所有的节点也连接到bigdata8了。</p>

<ul>
<li><a href="http://www.cnblogs.com/byrhuangqiang/p/3937654.html">http://www.cnblogs.com/byrhuangqiang/p/3937654.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html#high-availability">http://spark.apache.org/docs/latest/spark-standalone.html#high-availability</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读码] Spark1.1.0前篇--代码统计导入Eclipse]]></title>
    <link href="http://winseliu.com/blog/2014/10/12/spark-read-source-starter/"/>
    <updated>2014-10-12T13:12:57+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/12/spark-read-source-starter</id>
    <content type="html"><![CDATA[<p>看过亚太研究院的spark在线教学视频，说spark1.0的源码仅有3w+的代码，蠢蠢欲动。先具体看下源码的量，估算估算；然后搭建eclipse读码环境。</p>

<h2>计算源码行数</h2>

<pre><code>winse@Lenovo-PC ~/git/spark
$ git branch -v
* (detached from v1.1.0) 2f9b2bd [maven-release-plugin] prepare release v1.1.0-rc4
  master                 4d8ae70 [behind 1246] Cleanup on Connection and ConnectionManager

winse@Lenovo-PC ~/git/spark
$ find . -name "*.scala" | grep 'src/main' | xargs sed  -e 's:\/\*.*\*\/::' -e  '/\/\*/, /\*\//{
/\/\*/{
 s:\/\*.*::p
}
/\*\//{
 s:.*\*\/::p
}
d
}' | sed -e '/^\s*$/d' -e '/^\s*\/\//d' | grep -v '^import' | grep -v '^package' | wc -l
72967

winse@Lenovo-PC ~/git/spark
$ ^scala^java
1749

winse@Lenovo-PC ~/git/spark
$ ^src/main^core/src/main
877

winse@Lenovo-PC ~/git/spark
$ ^java^scala
38526
</code></pre>

<p>全部源码的数量（去掉测试）大概在7W左右，仅计算核心代码core下面的代码量在4W。从量上面来说还是比较乐观的，学习scala然后读spark的源码。</p>

<p>spark1.0.0的核心代码量在3w左右。1.1多了大概1w行！！</p>

<h2>Docker</h2>

<p>查看目录结构的时刻，看到spark1下面竟然有docker，不过看Dockerfile的内容只是简单的安装了scala、把本机的spark映射到docker容器、然后运行spark主从集群。</p>

<h2>导入eclipse</h2>

<p>spark使用主要使用scala编写，首先需要下载<a href="http://scala-ide.org/download/sdk.html">scala-ide</a>直接下载2.10的版本（基于eclipse，很多操作都类似）；然后下载<a href="https://github.com/apache/spark.git">spark的源码</a>检出v1.1.0的；然后使用maven生成eclipse工程文件。</p>

<p>(不推荐)使用<a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-Eclipse">sbt生成工程文件</a>。这种方式会缺少一些依赖的jar，处理比较麻烦，还不清楚到底是少了啥！</p>

<pre><code>$ cd sbt/
$ sed -i 's/^M//g' *
$ cd ..
$ sbt/sbt eclipse -mem 512
</code></pre>

<p>(推荐)使用MVN编译生成，<a href="http://spark.apache.org/docs/latest/building-with-maven.html">使用Maven生成官网文章</a></p>

<pre><code>winse@Lenovo-PC ~/git/spark
$ git clean -x -fd #清理非仓库代码

$ echo $SCALA_HOME #指定scala-home
/cygdrive/d/scala

# 这里我直接修改默认值，理论上加 -Phadoop-2.2 选项应该也是可以的
$ vi pom.xml # hadoop.version 2.2.0
$ mvn eclipse:eclipse

$ find . -name ".classpath" | xargs sed -i -e 's/including="\*\*\/\*.java"//' -e 's/excluding="\*\*\/\*.java"//'

#也可以把添加特性的操作/添加scala源码包操作批量处理掉
</code></pre>

<p>然后导入到eclipse，然后再针对性的处理报错：</p>

<ul>
<li>先把每个工程都<strong>添加scala特性</strong></li>
<li>把含有python源码包的去掉（手动删除.classpath中classpathentry即可）</li>
<li>确认下并加上<code>src/test/scala</code>的源码包。</li>
</ul>


<p>注意，进行上面的步骤之前，由于scala源文件比较多，编译的时间会比较长，先把Project->Build Automatically去掉，然后一次性把问题处理掉后再手动build！</p>

<ul>
<li>手动使用<code>existing maven projects</code>导入yarn/stable，然后把<strong>yarn/common以链接的形式引入</strong>，并添加到源码包。</li>
</ul>


<p><img src="http://file.bmob.cn/M00/1C/E7/wKhkA1Q7jQ2AMhweAAOC-l-jcz4872.png" alt="" /></p>

<p>还有一个<strong> value q is not a member of StringContext </strong><a href="http://docs.scala-lang.org/overviews/quasiquotes/intro.html">quasiquotes</a>的错误，有些类需要在2.10添加编译组件才能正常编译，修改scala编译首选项。</p>

<p><img src="http://file.bmob.cn/M00/1D/07/wKhkA1Q76GyAFNYPAAEYJfk_ZGw816.png" alt="" /></p>

<p>添加依赖的编译组件后，整个功能就能正常编译通过了。接下来就能调试看源码了。</p>

<p><strong>备注：</strong>clean后发现target目录下并没有重新编译生成class，去掉<code>-Xshow-phases</code>才行。</p>

<blockquote><p> -Xshow-phases                  Print a synopsis of compiler phases.</p></blockquote>

<h2>Maven编译spark</h2>

<p>如果使用的hadoop版本在官网没有集成assembly版本，可以使用maven手动构建。至于打包可以查看下一篇文章。</p>

<pre><code>$ export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
$ mvn -Pyarn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean package
</code></pre>

<p><code>yarn</code>的profile能够编译成可执行的jar文件（包括所有依赖的spark），具体内容下一篇讲。</p>

<h2>小结</h2>

<p>断断续续的写了两天，字数统计弄了大半天，主要在于多行注释的处理。时间最主要都消耗在sbt、maven构建eclipse项目文件（生成、fixed）上。编译scala量上去后确实非常非常的慢，不管是maven还是eclipse都慢！</p>

<p>下一篇将使用docker搭建spark环境，并使用远程调试连接到helloworld程序。</p>

<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/24800129/scala-maven-builder-doesnt-understand-quasiquotes">Scala maven builder doesn&rsquo;t understand quasiquotes</a></li>
<li><a href="http://docs.scala-lang.org/overviews/macros/paradise.html">Macro Paradise</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
