<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Books | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/books/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2016-11-10T13:29:11+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[读读书]Apache Spark源码剖析-Shell]]></title>
    <link href="http://winseliu.com/blog/2016/05/08/rrc-apache-spark-source-inside-shell/"/>
    <updated>2016-05-08T21:41:01+08:00</updated>
    <id>http://winseliu.com/blog/2016/05/08/rrc-apache-spark-source-inside-shell</id>
    <content type="html"><![CDATA[<p>本来第二篇应该是与 [第1章 初识Spark] 有关，但我们运行helloworld、以及提交任务都是通过脚本 <code>bin/spark-shell</code> ，完全不知道那些脚本是干啥的？而且，在开发环境运行shell来启动应用总觉得怪怪的，这篇先来简单了解脚本的功能、以及Launcher模块。</p>

<p><strong> 其实每个大数据的框架，shell脚本都是通用入口，也是研读源码的第一个突破口 </strong>。掌握脚本功能相当于熟悉了基本的API功能，把 spark/bin 目录下面的脚本理清楚，然后再去写搭建开发环境、编写调试helloworld就事半功倍了。</p>

<p>官网 <strong> Quick Start </strong> 提供的简短例子都是通过 bin/spark-shell 来运行的。Submit页面提供了 bin/spark-submit 提交jar发布任务的方式。 spark-shell，spark-submit 就是两个非常重要的脚本，这里就来看下这两个脚本。</p>

<h2>spark-shell - 对应[3.1 spark-shell]章节</h2>

<p>spark-shell 脚本的内容相对多一些，主要代码如下（其他代码都是为了兼容cygwin弄的，我们这里不关注）：</p>

<pre><code>SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"
trap onExit INT     # 程序终止(interrupt)信号, 在用户键入INTR字符(通常是Ctrl + C)时触发

export SPARK_SUBMIT_OPTS
"${SPARK_HOME}"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"
</code></pre>

<p>最终调用 bin/spark-submit 脚本。其实和我们自己提交 helloworld.jar 命令一样：</p>

<pre><code>$ bin/spark-submit \
  --class "HelloWorld" \
  --master local[2] \
  target/scala-2.10/helloworld_2.10-1.0.jar
</code></pre>

<p>不过通过 bin/spark-shell 提交运行的类是spark自带，没有附加（不需要）额外的jar。这个后面再讲，我们也可以通过这种方式类运行公共位置的jar，可以减少一些不必要的网络带宽。</p>

<h2>spark-submit</h2>

<p>submit脚本更简单。就是把 <strong>org.apache.spark.deploy.SparkSubmit</strong> 和 <strong>输入参数</strong> 全部传递给脚本 bin/spark-class 。</p>

<pre><code>exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"
</code></pre>

<h2>spark-class</h2>

<p>主要的功能都集中在 bin/spark-class。bin/spark-class脚本最终启动java、调用 <strong>Launcher模块</strong> 。而 <strong>Launcher模块</strong> 解析输入参数并输出 <strong>最终输出Driver启动的命令</strong>，然后shell再通过 <strong>exec</strong> 来运行Driver程序。</p>

<p>要讲清楚 bin/spark-class 相对复杂点：通过脚本传递参数，调用java处理参数，又输出脚本，最后运行脚本才真正运行了Driver。所以这里通过 <strong>脚本</strong> 和 <strong>程序</strong> 来进行说明。</p>

<h4>脚本</h4>

<ul>
<li>先加载环境变量配置文件</li>
<li>再获取 assembly.jar 位置</li>
<li>然后调用 <code>org.apache.spark.launcher.Main</code> ， Main类根据环境变量和传入参数算出真正执行的命令(具体在【程序】部分讲)。</li>
</ul>


<p>下面是核心脚本的内容：</p>

<pre><code>. "${SPARK_HOME}"/bin/load-spark-env.sh 
    # 把load-spark-env.sh展开
    . "${user_conf_dir}/spark-env.sh"

    ASSEMBLY_DIR1="${SPARK_HOME}/assembly/target/scala-2.10"  # 通过ASSEMBLY路径来判断SPARK_SCALA_VERSION，编译打包成tar的不需要这个变量
    export SPARK_SCALA_VERSION="2.10"

RUNNER="${JAVA_HOME}/bin/java"

SPARK_ASSEMBLY_JAR=
if [ -f "${SPARK_HOME}/RELEASE" ]; then
  ASSEMBLY_DIR="${SPARK_HOME}/lib"
else
  ASSEMBLY_DIR="${SPARK_HOME}/assembly/target/scala-$SPARK_SCALA_VERSION"
fi
ASSEMBLY_JARS="$(ls -1 "$ASSEMBLY_DIR" | grep "^spark-assembly.*hadoop.*\.jar$" || true)"
SPARK_ASSEMBLY_JAR="${ASSEMBLY_DIR}/${ASSEMBLY_JARS}"
LAUNCH_CLASSPATH="$SPARK_ASSEMBLY_JAR"

export _SPARK_ASSEMBLY="$SPARK_ASSEMBLY_JAR"

CMD=()
while IFS= read -d '' -r ARG; do
  CMD+=("$ARG")
done &lt; &lt;("$RUNNER" -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@")
exec "${CMD[@]}"
</code></pre>

<p>大部分内容都是准备环境变量，就最后几行代码比较复杂。这里设置DEBUG在脚本 <code>while</code> 循环打印每个输出的值看下输出的是什么。</p>

<pre><code># 修改后的效果
CMD=()
while IFS= read -d '' -r ARG; do
  echo "[DEBUG] $ARG"
  CMD+=("$ARG")
done &lt; &lt;(set -x; "$RUNNER" -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@")
echo "${CMD[@]}"
exec "${CMD[@]}"
</code></pre>

<p>启动 bin/spark-shell（最终会调用 bin/spark-class，上面已经讲过脚本之间的关系），查看输出的调试信息：</p>

<pre><code>[hadoop@cu2 spark-1.6.0-bin-2.6.3]$ bin/spark-shell 
++ /opt/jdk1.8.0/bin/java -cp /home/hadoop/spark-1.6.0-bin-2.6.3/lib/spark-assembly-1.6.0-hadoop2.6.3-ext-2.1.jar org.apache.spark.launcher.Main org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name 'Spark shell'
[DEBUG] /opt/jdk1.8.0/bin/java
[DEBUG] -cp
[DEBUG] /home/hadoop/spark/lib/mysql-connector-java-5.1.34.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/conf/:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/spark-assembly-1.6.0-hadoop2.6.3-ext-2.1.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-rdbms-3.2.9.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-core-3.2.10.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-api-jdo-3.2.6.jar:/home/hadoop/hadoop/etc/hadoop/
[DEBUG] -Dscala.usejavacp=true
[DEBUG] -Xms512m
[DEBUG] -Xmx512m
[DEBUG] org.apache.spark.deploy.SparkSubmit
[DEBUG] --class
[DEBUG] org.apache.spark.repl.Main
[DEBUG] --name
[DEBUG] Spark shell
[DEBUG] spark-shell
/opt/jdk1.8.0/bin/java -cp /home/hadoop/spark/lib/mysql-connector-java-5.1.34.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/conf/:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/spark-assembly-1.6.0-hadoop2.6.3-ext-2.1.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-rdbms-3.2.9.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-core-3.2.10.jar:/home/hadoop/spark-1.6.0-bin-2.6.3/lib/datanucleus-api-jdo-3.2.6.jar:/home/hadoop/hadoop/etc/hadoop/ -Dscala.usejavacp=true -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name 'Spark shell' spark-shell
...
</code></pre>

<p>从上面的调试信息可以看出：</p>

<ul>
<li><code>org.apache.spark.launcher.Main</code> 把传入参数整理后重新输出</li>
<li>脚本把java输出内容保存到 <code>CMD[@]</code> 数组中</li>
<li>最后使用exec来执行。</li>
</ul>


<p>根据上面 bin/spark-class 产生的启动命令可以直接在idea里面运行，效果与直接运行 bin/spark-shell 一样：</p>

<p><img src="/images/blogs/rrc-spark/idea-spark-shell.png" alt="" /></p>

<p><strong>注意：</strong> 这里的 spark-shell 是一个特殊的字符串，代码中会对其进行特殊处理不额外加载jar。类似的字符串还有： pyspark-shell, sparkr-shell, spark-internal（参看SparkSubmit），如果调用类就在SPARK_CLASSPATH可以使用它们减少不必要的网络传输。</p>

<h4>Launcher模块</h4>

<p>发现 shell 和 launcher的java代码 功能逻辑非常类似。比如说获取java程序路径的代码：</p>

<pre><code>List&lt;String&gt; buildJavaCommand(String extraClassPath) throws IOException {
  ...
  if (javaHome != null) {
      cmd.add(join(File.separator, javaHome, "bin", "java"));
  } else if ((envJavaHome = System.getenv("JAVA_HOME")) != null) {
    cmd.add(join(File.separator, envJavaHome, "bin", "java"));
  } else {
    cmd.add(join(File.separator, System.getProperty("java.home"), "bin", "java"));
  }
  ...
}
</code></pre>

<p>在shell脚本里面的处理是：</p>

<pre><code># Find the java binary
if [ -n "${JAVA_HOME}" ]; then
  RUNNER="${JAVA_HOME}/bin/java"
else
  if [ `command -v java` ]; then
    RUNNER="java"
  else
    echo "JAVA_HOME is not set" &gt;&amp;2
    exit 1
  fi
fi
</code></pre>

<p>对比两者，其实是用脚本更加直观。但是使用java编写一个模块更便于管理和扩展，稍微调整下就能复用代码。比如说要添加windows的cmd脚本、又或者为了兼容多个操作系统/多语言(python，r 等)。所以提取一个公共的 <strong>Launcher模块</strong> 出来其实是个挺不错的选择。同时对于不是很熟悉shell的程序员来说也更方便了解系统运作。</p>

<p><strong>Launcher模块</strong> 按功能可以分为 CommandBuilder 和 SparkLauncher 两个部分。</p>

<ol>
<li><p>CommandBuilder</p></li>
<li><p>SparkSubmitCommandBuilder: 解析用户输入的参数并输出命令给脚本使用</p></li>
<li>SparkClassCommandBuilder: 主要为后台进程产生启动命令（sbin目录下面的脚本）。</li>
</ol>


<p>1.1 公共类</p>

<ul>
<li>Main ： 统一入口</li>
<li>AbstractCommandBuilder : 提供构造命令的公共基类

<ul>
<li>buildJavaCommand

<ul>
<li>buildClassPath

<ul>
<li>SPARK_CLASSPATH</li>
<li>extraClassPath</li>
<li>getConfDir : 等于环境变量 $SPARK_CONF_DIR 或者 $SPARK_HOME/conf 的值</li>
<li>classes

<ul>
<li>SPARK_PREPEND_CLASSES</li>
<li>SPARK_TESTING</li>
</ul>
</li>
<li>findAssembly : 获取 spark-assembly-1.6.0-hadoop2.6.3.jar 的路径，lib 或者 assembly/target/scala-$SPARK_SCALA_VERSION 路径下

<ul>
<li>_SPARK_ASSEMBLY</li>
</ul>
</li>
<li>datanucleus-* : 从 lib / lib_managed/jars 目录下获取</li>
<li>HADOOP_CONF_DIR</li>
<li>YARN_CONF_DIR</li>
<li>SPARK_DIST_CLASSPATH</li>
</ul>
</li>
</ul>
</li>
<li>getEffectiveConfig : 获取 spark-defaults.conf 的内容</li>
</ul>
</li>
</ul>


<p>1.2 SparkSubmitCommandBuilder</p>

<p>主要的类以及参数：</p>

<ul>
<li>SparkSubmitCommandBuilder

<ul>
<li>构造函数调用OptionParser解析参数，解析handle有处理specialClasses！</li>
<li>buildSparkSubmitCommand

<ul>
<li>getEffectiveConfig</li>
<li>extraClassPath : spark.driver.extraClassPath</li>
<li>SPARK_SUBMIT_OPTS</li>
<li>SPARK_JAVA_OPTS</li>
<li>client模式下加载配置

<ul>
<li>spark.driver.memory / SPARK_DRIVER_MEMORY / SPARK_MEM / DEFAULT_MEM(1g)</li>
<li>DRIVER_EXTRA_JAVA_OPTIONS</li>
<li>DRIVER_EXTRA_LIBRARY_PATH</li>
</ul>
</li>
<li>buildSparkSubmitArgs</li>
</ul>
</li>
</ul>
</li>
<li>SparkSubmitOptionParser(子类需要实现handle方法)</li>
<li>SparkSubmitCommandBuilder$OptionParser 命令参数

<ul>
<li><code>bin/spark-submit -h</code> 查看可以<strong>设置的参数</strong></li>
<li>直接查看<a href="http://spark.apache.org/docs/latest/submitting-applications.html">官网文档</a></li>
</ul>
</li>
</ul>


<p>1.3 SparkClassCommandBuilder</p>

<p>主要CommandBuilder的功能上面已经都覆盖了，SparkClassCommandBuilder主要关注命令行可以设置哪些环境变量：</p>

<ul>
<li>org.apache.spark.deploy.master.Master

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_MASTER_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.deploy.worker.Worker

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_WORKER_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.deploy.history.HistoryServer

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_HISTORY_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.executor.CoarseGrainedExecutorBackend

<ul>
<li>SPARK_JAVA_OPTS</li>
<li>SPARK_EXECUTOR_OPTS</li>
<li>SPARK_EXECUTOR_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.executor.MesosExecutorBackend

<ul>
<li>SPARK_EXECUTOR_OPTS</li>
<li>SPARK_EXECUTOR_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.deploy.ExternalShuffleService / org.apache.spark.deploy.mesos.MesosExternalShuffleService

<ul>
<li>SPARK_DAEMON_JAVA_OPTS</li>
<li>SPARK_SHUFFLE_OPTS</li>
<li>SPARK_DAEMON_MEMORY</li>
</ul>
</li>
<li>org.apache.spark.tools.

<ul>
<li>extraClassPath : spark-tools_.*.jar</li>
<li>SPARK_JAVA_OPTS</li>
<li>DEFAULT_MEM(1g)</li>
</ul>
</li>
<li>other

<ul>
<li>SPARK_JAVA_OPTS</li>
<li>SPARK_DRIVER_MEMORY</li>
</ul>
</li>
</ul>


<h4>SparkLauncher</h4>

<p>SparkLauncher提供了在程序中提交任务的方式。通过Driver端的支持获取程序执行动态（通过socket与Driver交互），为实现后端管理应用提供一种可行的方式。</p>

<p>SparkLauncher提交任务其中一部分还是使用spark-submit脚本，绕一圈又回到上面的参数解析生成命令然后exec执行。另外SparkLauncher通过启动 SocketServer(LauncherServer)接收来自Driver(LauncherBackend)任务执行情况的最新状态。</p>

<p><img src="/images/blogs/rrc-spark/spark-launcher.jpg" alt="" /></p>

<p>代码包括：</p>

<ul>
<li>SparkLauncher 主要是startApplication。其他都是解析设置参数，相当于把shell的工作用java重写了一遍</li>
<li>LauncherServer 服务SocketServer类</li>
<li>LauncherServer$ServerConnection 状态处理类</li>
<li>LauncherConnection 通信基类：接收、发送消息</li>
<li>LauncherProtocol 通信协议</li>
<li>ChildProcAppHandle : SparkAppHandle 接收到Driver的状态后，请求分发类</li>
</ul>


<p>具体功能的流转请下载代码 <a href="https://github.com/winse/spark-examples/blob/master/src/main/scala/com/github/winse/spark/HelloWorldLauncher.scala">HelloWorldLauncher.scala</a> ，然后本地调试一步步的追踪学习。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读读书]Apache Spark源码剖析-序]]></title>
    <link href="http://winseliu.com/blog/2016/05/07/rrc-apache-spark-source-inside-preface/"/>
    <updated>2016-05-07T23:58:57+08:00</updated>
    <id>http://winseliu.com/blog/2016/05/07/rrc-apache-spark-source-inside-preface</id>
    <content type="html"><![CDATA[<p><a href="http://dongxicheng.org/mapreduce-nextgen/how-to-read-hadoop-code-effectively/">如何高效的阅读hadoop源代码？</a> 先看看这篇。</p>

<p>今天去广州图书馆办了证，借了几本关于大数据的书。老实说，国家提供的便民基础设施应该发挥她的价值，国家建那么多公共设施，还有很多人在后台让这些服务运作起来。借书是一种最高性价比学习的方式，第一：不能乱写乱画必须做笔记或者背下来，把最有价值的东西汇集；第二：有时间限制，好书逼着我们持续的去读；第三：自然是读到烂书也不用花钱，有价值的书必然也是最多人看的，看到翻的很旧的新书你就借了吧。</p>

<p>其中一个《Apache Spark源码剖析-徐鹏》，大致翻了一下，老实说作者很牛逼啊，从那么多的代码里面挑出和主题相关的，不比鸡蛋里面挑石头容易，跟着作者的思路去读应该不错。打算每天读点代码，同时把看书和看代码也记录下来，每天一小结，同时希望对别人有些参考作用。</p>

<p>Spark更新的很快，书本介绍的是 spark-1.0 ，不过书中介绍的主要是思路，我们这里选择比较新的版本 1.6.0 来读（生产用的是1.6）。</p>

<p><strong> 说到思路，如果你对Redis也感兴趣，强烈推荐读读 《Redis设计与实现-黄建宏》 </strong></p>

<h2>使用环境说明</h2>

<p>和作者不同，我选择直接在windows来读/调试代码，为了读个代码还得整一套linux的开发环境挺累的（原来也试过整linux开发环境后来放弃了），Windows 积累的经验已经可以让我自由调试和看代码了。</p>

<p>吐槽下sbt，很讨厌这玩意又慢还用ivy，我X，大数据不都用 maven 嘛，难道我还得为 spark 整一套完全一样的jar本地缓冲？不过还好 spark-1.6 已经是用 maven 来管理了。</p>

<ul>
<li>win10 + cygwin</li>
<li>jdk8_x64（内存可以调到大于1G）</li>
<li>maven3</li>
<li>scala_2.10</li>
<li>spark_1.6.0</li>
<li>hive_1.2.1</li>
<li>hadoop_2.6.3</li>
<li>JetBrains idea 看代码确实不错</li>
</ul>


<h2>Spark开发环境搭建 - 对应书本的[附录A Spark源码调试]部分</h2>

<h4>配置 idea-scala</h4>

<h6>优化idea启动参数</h6>

<p>安装 <strong>最新版idea</strong> (当前最新版本是15.0.5)。在程序安装的 bin 目录下，有x64配置文件 idea64.exe.vmoptions ，在配置文件开头添加jdk8内存配置：</p>

<pre><code>-server
-Xms1g
-Xmx2g
-XX:MetaspaceSize=256m
-XX:MaxMetaspaceSize=256m
</code></pre>

<p>由于机器 eclipse 原来使用的 jdk_x86，为了兼容，单独编写 idea64.exe 的启动脚本 <strong> idea.bat </strong>：</p>

<pre><code>set JAVA_HOME=D:\Java\jdk1.8.0_40
D:
cd "D:\Program Files\JetBrains\IntelliJ IDEA Community Edition 15.0.5\bin"
start idea64.exe"

exit
</code></pre>

<p><strong> [IDEA的快键配置]：IDEA 适配 Eclipse 的快键集，通过 <code>Settings -&gt; Keymap -&gt; Keymaps</code> 配置。 </strong></p>

<h6>安装scala插件</h6>

<ol>
<li>第一种方式：当然最好就是通过plugins的搜索框就能安装，但在中国这得看运气。</li>
<li><p>第二种方式：首先下载好插件，然后选择从硬盘安装插件。</p></li>
<li><p>从网络安装</p></li>
</ol>


<p>打开 plugins 管理页面：（也可以通过 File -> Settings&hellip; -> Plugins 打开）</p>

<p><img src="/images/blogs/rrc-spark/idea-start-configure.png" alt="" /></p>

<p>弹出的 Plugins 对话框显示了当前已经安装的插件：</p>

<p><img src="/images/blogs/rrc-spark/idea-plugins-list.png" alt="" /></p>

<p>在 Plugins 对话框页面选择 [<strong>Browse repositories&hellip;</strong>] 按钮，再在弹出的对话框中查找 <strong>Scala</strong> 的插件：</p>

<p><img src="/images/blogs/rrc-spark/idea-browse-plugins.png" alt="" /></p>

<p>选择安装 Scala ，当然你也可以同时安装上 SBT 。</p>

<ul>
<li>从硬盘安装</li>
</ul>


<p>运气好就算可以直接从网络安装，但是下载过程其实也挺慢的。</p>

<p>我们还可以先自己下载好插件再安装（或者从其他同学获取、迅雷分分钟下完）。首先需要查看自己 idea 的版本，再在 <a href="https://plugins.jetbrains.com/?idea_ce">https://plugins.jetbrains.com/?idea_ce</a> 查找下载符合自己版本的 <a href="https://plugins.jetbrains.com/plugin/1347?pr=idea_ce">scala 插件</a>，最后通过 [<strong>Install plugin from disk&hellip;</strong>] 安装，然后重启IDEA即可。</p>

<p><img src="/images/blogs/rrc-spark/idea-version.png" alt="" />
<img src="/images/blogs/rrc-spark/download-scala-plugin.png" alt="" />
<img src="/images/blogs/rrc-spark/idea-scala-from-disk.png" alt="" /></p>

<h4>下载 spark 源码，并导入idea</h4>

<ol>
<li>下载源码，检出 1.6.0 版本</li>
</ol>


<pre><code>$ git clone https://github.com/apache/spark.git
$ git checkout v1.6.0
</code></pre>

<p>如果你只想看 1.6.0 的内容，可以直接在clone命令添加参数指定版本：</p>

<pre><code>$ git clone https://github.com/apache/spark.git -b v1.6.0
</code></pre>

<ol>
<li>导入idea</li>
</ol>


<p>导入之前先要生成arvo的java类(这里直接package编译一下)：</p>

<pre><code>E:\git\spark\external\flume-sink&gt;mvn package -DskipTests
</code></pre>

<p>由于我使用 hadoop-2.6.3 ，并且导入过程中不能修改环境变量，直接修改 pom.xml 里面 hadoop.version 属性的值。</p>

<p><img src="/images/blogs/rrc-spark/spark-hadoop-version.png" alt="" /></p>

<p>启动IDEA，使用 [<strong>Import Project</strong>] 导入源代码; 然后选择 <code>E:/git/spark</code>（刚刚下载的源码位置）; 然后选择导入maven项目; 在 profile 页把必要的都选上（当然也可以后期通过 <code>Maven Projects</code> 面板来修改）:</p>

<p><img src="/images/blogs/rrc-spark/spark-import-profile.png" alt="" /></p>

<p>导入完成后，依赖关系maven已经处理好了，直接就能用了。也可以 Make Projects 再编译一次，并把运行application的make去掉，免得浪费编译时间）。</p>

<p><strong> 注意：mvn idea:idea 其实不咋的，生成的配置不兼容。最好不要用！！ </strong></p>

<ol>
<li>调试/测试</li>
</ol>


<p>在调试运行之前，先了解下并解决 idea maven-provided 的问题：</p>

<p>在idea里面直接运行 src/main/java 下面的类会被当做在生产环境运行，所以idea不会把这些 provided的依赖 加入到运行的classpath。</p>

<ul>
<li><a href="https://youtrack.jetbrains.com/issue/IDEA-54595">https://youtrack.jetbrains.com/issue/IDEA-54595</a></li>
<li><a href="http://stackoverflow.com/questions/30453269/maven-provided-dependency-will-cause-noclassdeffounderror-in-intellij">http://stackoverflow.com/questions/30453269/maven-provided-dependency-will-cause-noclassdeffounderror-in-intellij</a></li>
</ul>


<p><img src="/images/blogs/rrc-spark/idea-maven-provided.png" alt="" /></p>

<p>IDEA运行时是从 <code>examples/spark-examples_2.10.iml</code> 文件中读取classpath的配置，所以我们直接把 <code>spark-examples_2.10.iml</code> 的 <code>scope="PROVIDED"</code> 全部删掉即可。</p>

<pre><code># 一次全部删掉！
winse@Lenovo-PC ~/git/spark
$ find . -name "*.iml"  | xargs -I{} sed -i 's/scope="PROVIDED"//' {}
</code></pre>

<p>首先右键 [<strong>Run LogQuery</strong>] 运行（由于缺少master的配置会报错的），主要用于生成启动的 <code>LogQuery Configuration</code>：</p>

<p><img src="/images/blogs/rrc-spark/spark-logquery-firststart.png" alt="" /></p>

<p>然后选择上图中下拉选项的 [<strong>Edit Configurations&hellip;</strong>] ，在弹出配置对话框为中为 <code>LogQuery</code> 添加 <strong>VM options</strong> 配置: <code>-Dspark.master=local</code> ，接下来我们就可以打断点，Debug调试了。</p>

<p><img src="/images/blogs/rrc-spark/spark-logquery-config.png" alt="" /></p>

<p>运行结果如下：</p>

<p><img src="/images/blogs/rrc-spark/spark-logquery-result.png" alt="" /></p>

<p>遇到IDEA导入maven依赖有问题的，可以参考下 <a href="http://stackoverflow.com/questions/11454822/import-maven-dependencies-in-intellij-idea">Import Maven dependencies in IntelliJ IDEA</a> 。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【linux 101 Hacks】读后感]]></title>
    <link href="http://winseliu.com/blog/2015/09/13/review-linux-101-hacks/"/>
    <updated>2015-09-13T13:12:53+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/13/review-linux-101-hacks</id>
    <content type="html"><![CDATA[<p>本书讲了linux维护和管理过程中常用的命令。</p>

<p>分12个章节，分别将了目录切换、日期、SSH远程登录、常用linux命令、PS1-4操作提示符、解压缩、命令历史记录、系统管理、容器服务器Apache、脚本环境变量、性能监控等。</p>

<p>介绍的命令有：cd, dirs, pushd, popd, cdpath, <strong>alias</strong>, mkdir, eval, date, hwclock, ssh, grep, find, 输出重定向, join, tr, xargs, sort, uniq, cut, stat, diff, ac, ps1, ps2, ps3, ps4, PROMPT_COMMAND, zip, unzip, tar, gzip, bzip2, HISTTIMEFORMAT, HISTSIZE, HISTIGNORE, fdisk, mke2fsk, mount, tune2fs, useradd, adduser, passwd, groupadd, id, ssh-copy-id, ssh-agent, crontab, apachectl, httpd, <strong>.bash_rc</strong>, .bash_profile, 单引号, 双引号, free, top, ps, df, kill, du, lsof, sar, vmstat, netstat, sysctl, nice, renice等等。</p>

<p>下面结合工作中的一些实践，谈一谈</p>

<h2>技巧一：登录服务器</h2>

<p>不管是正式环境还是云端的测试环境，一般提供给我们访问的只有一个入口（也就是常说的跳板机），登录跳板机后然后才能连接其他服务器。常用的工具有【SecureCRT】和【Xshell】，它们的使用方式基本相同。</p>

<p>最佳实践：连接跳板机的同时，建立自己机器和内网机器之间的隧道，即可以方便浏览器的访问，同时也可以使用sftp直接传输文件到内网机器。</p>

<p><img src="/images/blogs/linux-101-hacks-review-securecrt-config.png" alt="" /></p>

<p><img src="/images/blogs/linux-101-hacks-review-securecrt-web.png" alt="" /></p>

<h2>技巧二：ssh-copy-id【hack 72】</h2>

<p>想不通，现在的教程都使用【复制-添加-修改权限】公钥的方式来进行无密钥登录配置。</p>

<pre><code>[hadoop@hadoop-master1 ~]$ scp .ssh/id_rsa.pub 172.17.0.3:~/master_id_rsa.pub
hadoop@172.17.0.3's password: 
id_rsa.pub                                                                                                                     100%  403     0.4KB/s   00:00    
[hadoop@hadoop-master1 ~]$ ssh 172.17.0.3
hadoop@172.17.0.3's password: 
Last login: Sun Sep 13 11:41:17 2015 from 172.17.0.2
[hadoop@hadoop-slaver1 ~]$ cat master_id_rsa.pub &gt;&gt; .ssh/authorized_keys 
[hadoop@hadoop-slaver1 ~]$ ll -d .ssh
drwx------. 2 hadoop hadoop 4096 Mar 10  2015 .ssh
[hadoop@hadoop-slaver1 ~]$ ll .ssh/authorized_keys 
-rw-------. 1 hadoop hadoop 403 Sep 13 11:58 .ssh/authorized_keys
</code></pre>

<p>处理一个ssh无密钥登录搞N多的步骤，还不一定能成功！其实使用ssh-copy-id的命令就行了，不知道各类书籍上面都使用老旧的方法，都是抄来的吗？！</p>

<pre><code>[hadoop@hadoop-slaver1 ~]$ ssh-copy-id -i .ssh/id_rsa.pub 172.17.0.2
The authenticity of host '172.17.0.2 (172.17.0.2)' can't be established.
RSA key fingerprint is aa:41:79:6d:9d:c2:ec:f1:29:71:43:24:39:09:58:b6.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '172.17.0.2' (RSA) to the list of known hosts.
hadoop@172.17.0.2's password: 
Now try logging into the machine, with "ssh '172.17.0.2'", and check in:

  .ssh/authorized_keys

to make sure we haven't added extra keys that you weren't expecting.
</code></pre>

<h2>技巧三：查看机器</h2>

<p>碰到不认识的人，我们都会上下打量。机器也一样，首先要了解机器，才能充分的发挥自己的性能。存储不够要么删点，要么加磁盘等等。</p>

<pre><code>uname -a
cat /etc/redhat-release 
ifconfig

date

df -h , df -Tha
free -m 
uptime
top
ps aux , ps auxf
netstat -atp
du -h --max-depth=1
lsof -i:[PORT]

cat /etc/hosts
</code></pre>

<h2>技巧四：管道</h2>

<p>一个命令的结果直接输出给另一个命令。就像水从一个结头通过管子直接流向下一个结头一样。中间不需要落地，直接立即用于下一个命令，直到结果输出。</p>

<pre><code>cat /etc/hosts | grep 'hadoop' | awk '{print $2}' | while read h ; do echo $h ; done
</code></pre>

<p>shell的命令那么多，简单功能的材料都准备好了，就像堆积木一样，叠加后总能实现你想得到的效果。</p>

<p>在进行一次性文件拷贝时，如果文件数量过多，可以先打包然后传到远程机器再解压：</p>

<pre><code>tar zc nginx | ssh bigdata1 'tar zx'
</code></pre>

<h2>技巧N：查看帮助</h2>

<p>写java的没看过开源项目不要说自己会java，写shell没用过man不要说自己会shell！</p>

<pre><code>man [CMD]
info [CMD]
[CMD] help
[CMD] -h
[CMD] -help
</code></pre>

<p>总有一款适合你，带着实践和问题的目的去学/写，能更好的把握它。（shell的命令太多，不要寄希望于看一个宝典就能写好！实践出真知，真正用到的才是实用的）</p>

<h2>技巧N-1：调试Shell脚本</h2>

<p>Shell脚本/命令在执行前会对变量进行解析、处理。查看最终执行的命令，能让我们了解到脚本不正确的地方，然后及时进行更正。</p>

<pre><code>set命令的参数说明
-v      Print shell input lines as they are read.
-x      After  expanding each simple command, for command, case command, select command, or arithmetic for command, 
display the expanded value of PS4, followed by the command and its expanded arguments or associated word list.

[hadoop@hadoop-master2 ~]$ set -x
[hadoop@hadoop-master2 1]$ cmd=*
+ cmd='*'
[hadoop@hadoop-master2 1]$ echo $cmd
+ echo 2 file
2 file
[hadoop@hadoop-master2 1]$ echo "$cmd" # 双引号
+ echo '*'
*
[hadoop@hadoop-master2 1]# echo '$cmd' # 单引号
+ echo '$cmd'
$cmd
</code></pre>

<p>调试脚本</p>

<pre><code>[root@hadoop-master2 1]# vi run.sh
#!/bin/sh

bin=$(dir=`dirname $0`; cd $dir ; pwd)

cd $bin
ls -l

[root@hadoop-master2 1]# sh -x run.sh 
+++ dirname run.sh
++ dir=.
++ cd .
++ pwd
+ bin=/tmp/1
+ cd /tmp/1
+ ls -l
total 8
drwxrwxr-x 2 hadoop hadoop 4096 Sep 13 20:33 2
-rw-rw-r-- 1 hadoop hadoop    0 Sep 13 20:33 file
-rw-r--r-- 1 root   root     66 Sep 13 21:12 run.sh
</code></pre>

<h2>技巧N-2：历史history</h2>

<p>历史如足迹。如果你要学习前辈的经验，理着他的足迹，一步步的走！</p>

<p>很多书上说的，<code>CTRL+R, !!, !-1, CTRL+P, ![CMD], !!:$, !^, ![CMD]:2, ![CMD]:$</code>用于获取以后执行的命令或者参数，多半好看不实用。会写的也就前面1-2个命令重复用一下，上下方向键就可以了，不会写的用history查看全部慢慢学更实际点。</p>

<pre><code>历史记录执行时间
export HISTTIMEFORMAT='%F %T '
输出最近10条历史
alias hist10='history 10'

持久化保存的历史记录数
vi ~/.bash_profile
HISTSIZE=450
HISTFILESIZE=450
# HISTFILE

忽略连续重复的命令
export HISTCONTROL=ignoredups

忽略重复的命令
export HISTCONTROL=erasedups

忽略指定的命令
export HISTIGNORE='pwd:ls'
</code></pre>

<h2>技巧N-3：shell之grep awk sed vi</h2>

<p>这些就不是看看man就能上手的，细嚼慢咽找几本书翻翻！！</p>

<p>推荐两本书： [sed与awk(第二版)], [Shell脚本学习指南]</p>

<h2>技巧N-4：批量处理之神：expect/for/while</h2>

<p>传入用户（与ssh的用户一致）密码，进行SSH无密钥认证：</p>

<pre><code>[root@hadoop-master2 1]# vi ssh-copy-id.expect
#!/usr/bin/expect  

## Usage $0 [user@]host password

set host [lrange $argv 0 0];
set password [lrange $argv 1 1] ;

set timeout 30;

spawn ssh-copy-id $host ;

expect {
  "(yes/no)?" { send yes\n; exp_continue; }
  "password:" { send $password\n; exp_continue; }
}

exec sleep 1;

批量处理
[root@hadoop-master2 1]# for h in `cat /etc/hosts | grep hadoop | awk '{print $2}' ` ; do ./ssh-copy-id.expect $h root-password ; done
</code></pre>

<p>传入新用户名称和密码，新建用户：</p>

<pre><code>[root@hadoop-master2 1]# vi passwd.expect
#!/usr/bin/expect  

## Usage $0 host username password

set host [lrange $argv 0 0];
set username [lrange $argv 1 1];
set password [lrange $argv 2 2] ;

set timeout 30;

##

spawn ssh $host useradd $username ;

exec sleep 1;

##

spawn ssh $host passwd $username ;

## password and repasswd all use this
expect {
  "password:" { send $password\n; exp_continue; }
}

exec sleep 1;

批量处理
[root@hadoop-master2 1]# for h in `cat /etc/hosts | grep hadoop | awk '{print $2}' ` ; do ./passwd.expect $h hadoop hadoop-password ; done
</code></pre>

<h2>最后</h2>

<p>当然还有很多命令，xargs, if等需要在实践中慢慢积累，shell博大精深继续码字！cdpath眼前一亮，alias还可以这么用！！</p>

<p>在linux把xml转成properties键值对形式的命令，觉得也挺有意思的：</p>

<pre><code>[hadoop@hadoop-master2 ~]$ vi format.xslt
&lt;xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform"&gt;
&lt;xsl:output method="text" encoding="iso-8859-1"/&gt;

&lt;xsl:strip-space elements="*" /&gt;

&lt;xsl:template match="/*/child::*"&gt;
&lt;xsl:for-each select="child::*"&gt;
&lt;xsl:if test="position() != last()"&gt;&lt;xsl:value-of select="normalize-space(.)"/&gt;=&lt;/xsl:if&gt;
&lt;xsl:if test="position() = last()"&gt;&lt;xsl:value-of select="normalize-space(.)"/&gt; &lt;xsl:text&gt;&amp;#xa;&lt;/xsl:text&gt; &lt;/xsl:if&gt;
&lt;/xsl:for-each&gt;
&lt;/xsl:template&gt;

&lt;/xsl:stylesheet&gt;

[hadoop@hadoop-master2 ~]$ xsltproc format.xslt ~/hadoop-2.2.0/etc/hadoop/yarn-site.xml
yarn.nodemanager.aux-services=mapreduce_shuffle
yarn.nodemanager.aux-services.mapreduce.shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
yarn.resourcemanager.address=hadoop-master1:8032
yarn.resourcemanager.scheduler.address=hadoop-master1:8030
yarn.resourcemanager.resource-tracker.address=hadoop-master1:8031
yarn.resourcemanager.admin.address=hadoop-master1:8033
yarn.resourcemanager.webapp.address=hadoop-master1:8088
yarn.nodemanager.resource.memory-mb=51200=yarn-default.xml
yarn.scheduler.minimum-allocation-mb=1024=yarn-default.xml
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【笔记】Beginning Scala（1）]]></title>
    <link href="http://winseliu.com/blog/2014/09/08/note-beginning-scala-part1/"/>
    <updated>2014-09-08T07:36:57+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/08/note-beginning-scala-part1</id>
    <content type="html"><![CDATA[<p>Scala借鉴了python、ruby等函数式语言。从java转过来还是需要一个适应阶段，与groovy比似乎困难多了不少。一年前好奇接触过，看了一些官网的入门教程，觉得这就是一个异类，后面就放下了。</p>

<p>直到再次弄hadoop，接触spark。经过一个时间的过渡期后，发现Scala确实能处理java的一些繁琐问题，为我们的双手减负，写出更简洁更优雅的代码，或者说更”易懂“。</p>

<p><strong>这篇是第一章（About Scala and How to Install It）和第二章（Scala Syntax, Scripts, and Your First Scala Programs）的笔记。</strong></p>

<p>作者寄语：</p>

<blockquote><p>My Path was hard, and I hope yours will be easier.</p></blockquote>

<h2>历史与安装</h2>

<p>随着HotSpot对JVM的改进，JDK1.3的程序与C++写的程序一样快。Java程序可以运行几个星期、几个月、甚至一年都不用重启。</p>

<p>好的Java代码与C/C++的代码一样快，甚至更快。在同样功能下，经过深度调优的C/C++程序会比Java程序更高效，与C/C++相比Java程序需要更多的内存，但对于一个适度复杂的项目（非系统内核级别），JVM程序将比C/C++表现的更优异。</p>

<p>这么多年来，Java在语言级别还不成熟。Java语法停滞不前，Java上的web框架越来越笨重。处理XML，或者其他一些简单概念的实现，如字段生成前台的HTML表单，需要越来越多的代码。对Java越来越失望。
Java5增加了枚举和泛型，对语言而言这是一个可喜的消息，但编码方面我们不得不使用IDE来完成Java代码编写。</p>

<p>“写Scala”的Martin Odersky曾编写了java编译器和泛型功能。Scala(2001, first version in 2003)，语法表达能力如ruby，但同时有Java的强类型和高性能。</p>

<p>Scala即快又简洁，同时类型安全。Scala运行效率也很高，最终编译成Java字节码跑在JVM上，又能与Java代码互相调用。</p>

<blockquote><p>But most importantly, Scala taught me  to program and reason about programming differently. I stopped thinking in terms of allocating buffers, structs, and objects, and of changing those pieces of memory. Instead, I learned to think about most of my programs as transforming input to output. This change in thinking has lead to lower defect rates, more modular code, and more testable code. Scala has also given me the tools to write smaller, more modular units of code and asse mble them together into a whole that is maintainable, yet far more complex than anything that I could write in Java or Ruby for that matter.</p></blockquote>

<p>下载安装JDK6+配置PATH, <a href="http://scala-lang.org/download/2.10.4.html">Scala 2.10+</a>下载zip版本的，然后解压就行了。</p>

<pre><code>winse@Lenovo-PC /cygdrive/d/scala/bin
$ ls -1
fsc
fsc.bat
scala
scala.bat
scalac
scalac.bat
scaladoc
scaladoc.bat
scalap
scalap.bat

winse@Lenovo-PC /cygdrive/d/scala/bin
$ scala
Welcome to Scala version 2.10.4 (Java HotSpot(TM) Client VM, Java 1.7.0_02).
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt; def fact(n:Int)=1 to n reduceLeft(_*_) // n!
fact: (n: Int)Int

scala&gt; fact(5)
res0: Int = 120
</code></pre>

<h2>语法结构，第一个Scala程序</h2>

<p>运行程序的三种方式：</p>

<ul>
<li>命令行交互式的REPL（read-eval-print loop)</li>
<li>shell/cmd脚本</li>
<li>编译打包成jar后运行，跟Java一样</li>
</ul>


<h3>REPL</h3>

<p>进入到Scala的bin目录下，双击scala.bat打开。</p>

<pre><code>scala&gt; 1+1
res0: Int = 2

scala&gt; res0*8
res1: Int = 16

scala&gt; val x="hello world"
x: String = hello world

scala&gt; var xl=x.length
xl: Int = 11

scala&gt; import java.util._
import java.util._

scala&gt; val d = new Date
d: java.util.Date = Mon Sep 08 09:17:08 CST 2014
</code></pre>

<h3>脚本</h3>

<p>脚本中无需显示的定义main方法，当你运行脚本时，Scala把整个文件的内容添加到类的main方法中，编译代码，然后运行生成的main方法。你只需在脚本文件中编写scala代码即可。</p>

<pre><code>winse@Lenovo-PC ~
$ scala hello.scala
hello world

winse@Lenovo-PC ~
$ cat hello.scala
println("hello world")
</code></pre>

<h3>编译后运行</h3>

<p>运行方式和javac类似，会生成对应类的字节码class文件。</p>

<pre><code>winse@Lenovo-PC ~/scala-hello
$ scalac hello.scala

winse@Lenovo-PC ~/scala-hello
$ ll
total 13
-rwxr-xr-x  1 winse None 2067 Sep  8 09:27 hello$.class
-rwxr-xr-x  1 winse None  704 Sep  8 09:27 hello$delayedInit$body.class
-rwxr-xr-x  1 winse None  921 Sep  8 09:27 hello.class
-rw-r--r--+ 1 winse None   58 Sep  8 09:26 hello.scala

winse@Lenovo-PC ~/scala-hello
$ cat hello.scala
object hello extends App {

  println("hello world")

}

winse@Lenovo-PC ~/scala-hello
$ scala hello
hello world
</code></pre>

<p>编译器的启动是很耗时的操作，你可以使用fsc（fast Scala Compiler），fsc是单独运行在后台的编译进程。</p>

<p>如果你原有的项目中使用Ant或Maven，scala有对应的插件，可以很容易把Scala集成到项目中。</p>

<h3>First Scala Programs</h3>

<p>在Scala，你可以编写像ruby和python脚本语言代码。如输出“hello world”的println方法，封装了System.out.println()。因为太常用了，println被定义在Scala的Predef（预定义成员）中，每个程序都会自动加载，就像java.lang会自动引入到每个java程序一样。</p>

<pre><code>println("hello world")

for {i&lt;- 1 to 10}
  println(i)

for {i&lt;- 1 to 10
     j&lt;- 1 to 10}
  println(i*j)
</code></pre>

<p>99乘法表：</p>

<pre><code>scala&gt; for(i&lt;- 1 to 9){
     | for(j&lt;- 1 to i)
     | printf("%s*%s=%2s\t",j,i,i*j);
     |
     | println()
     | }
1*1= 1
1*2= 2  2*2= 4
1*3= 3  2*3= 6  3*3= 9
1*4= 4  2*4= 8  3*4=12  4*4=16
1*5= 5  2*5=10  3*5=15  4*5=20  5*5=25
1*6= 6  2*6=12  3*6=18  4*6=24  5*6=30  6*6=36
1*7= 7  2*7=14  3*7=21  4*7=28  5*7=35  6*7=42  7*7=49
1*8= 8  2*8=16  3*8=24  4*8=32  5*8=40  6*8=48  7*8=56  8*8=64
1*9= 9  2*9=18  3*9=27  4*9=36  5*9=45  6*9=54  7*9=63  8*9=72  9*9=81
</code></pre>

<p>编写复杂点的程序，可以使用<a href="http://scala-ide.org/">Scala-IDE</a>。</p>

<pre><code>import scala.io._   // like java import scala.io.*

def toInt(in: String): Option[Int] =
    try {
      Some(Integer.parseInt(in.trim))
    } catch {
      case e: NumberFormatException =&gt; None
    }

def sum(in: Seq[String]) = {
    val ints = in.flatMap(s =&gt; toInt(s))
    ints.foldLeft(0)((a, b) =&gt; a + b)
}

println("Enter some numbers and press CTRL+C")

val input = Source.fromInputStream(System.in)
val lines = input.getLines.toSeq

println("Sum " + sum(lines))
</code></pre>

<p>Option是包含一个或零个对象的容器。如果不包含元素，返回的是单例的None。如果包括一个元素，就是新的Some(theElement)的实例。Option是Scala中避免空指针异常（null pointer）和显示进行null检查的处理一种方式。如果是None，一个业务逻辑将应用到0个元素，是Some就应用到一个元素上。</p>

<p>方法没有显示的return语句，默认就是方法“最后”（逻辑上最后执行的）一个语句的返回值。</p>

<p>sum方法的参数Seq是一个trait（类似java interface），是Array，List以机构其他顺序集合的父trait。trait拥有java interface的所有特性，同时traits可以包括方法的实现。你可以混合很多的traits成一个类。Traits除了不能定义有参构造函数外，其他和类一样。trait使得“多重继承”简单化，无需担忧<strong> the diamond problem</strong>（有点类似近亲结婚 ^ v ^）。</p>

<p>如：当BC都实现了M方法，D不知道用谁的M，会有歧义！！</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Diamond_inheritance.svg/220px-Diamond_inheritance.svg.png" alt="" /></p>

<p>在Scala中，定义参数分为val和var，val类似于java final，var类似于java的变量定义。对于不变化的变量，定义为val可以减少代码错误几率，进行防御性的编程。</p>

<p>接下来运行程序， 输入一些数字后按CTRL+C结束，就会输出计算的和。</p>

<pre><code>E:\local\home\Administrator\scala-hello&gt;scala Sum.scala
Enter some numbers and press CTRL+C
12
23
34
45
Sum 114
终止批处理操作吗(Y/N)?
</code></pre>

<h3>基本的语法Basic Syntax</h3>

<p>Scala的全部语法和语言的定义可以查看<a href="http://www.scala-lang.org/docu/files/ScalaReference.pdf">Scala Language Specification</a></p>

<h4>数字、字符串和XML常量</h4>

<ul>
<li><p>; 行结束符可以忽略</p></li>
<li><p>和Java一样的常量定义</p>

<blockquote><p>Integer: 1882, -1
Boolean: true, false
Double: 1.0, 1d, 1e3
Long: 42L
Float: 78.9f
Characters: &lsquo;4&rsquo;, &lsquo;?&rsquo;, &lsquo;z&rsquo;
Strings: &ldquo;Hello World&rdquo;</p></blockquote></li>
<li><p>Scala支持多行的字符串</p>

<blockquote><p>&ldquo;&rdquo;&ldquo;Hello
Multiline
World&rdquo;&ldquo;&rdquo;</p></blockquote></li>
<li><p>Scala支持XML常量，包括内嵌的Scala代码</p>

<blockquote><p>
<b>Foll</b>
<ul>{(1 to 3).map(i => <li>{i}</li>)}</ul>
</p></blockquote></li>
</ul>


<h4>包package和import</h4>

<p>package定义在源代码非注释的第一行。和java一样。</p>

<p>import则比java的更加灵活。基本的用法使用：</p>

<pre><code>import scala.xml._
</code></pre>

<p>scala中的import可以基于前面的imports语句。如再导入<code>scala.xml.transform</code>：</p>

<pre><code>import transform._
</code></pre>

<p>也可以导入一个具体的class和object：</p>

<pre><code>import scala.collection.mutable.HashMap
</code></pre>

<p>一次性倒入一个package下的几个class或object：</p>

<pre><code>import scala.collection.immutable.{TreeMap, TreeSet}
</code></pre>

<p>甚至可以给原有的class或object定义一个别名。</p>

<pre><code>import scala.util.parsing.json.{JSON =&gt; JsonParser}
</code></pre>

<p>import可以定义在任何代码块中，并且只会在当前作用域内有效。还可以引入objects的method，相当于java的import static。</p>

<pre><code>class Frog {
    import scala.xml._
    def n: NodeSeq = NodeSeq.Empty
}

object Moose {
    def bark = "woof"
}

import Moose._
bark
</code></pre>

<h4>Class, Trait和Object</h4>

<p>Scala的对象语法和规则比Java的更加复杂。</p>

<p>Scala去掉了一个文件中只能定义一个public类的限制。你想在一个文件里面放n个类都可以，同时文件的名称也没有限制（Java文件名需要和public的类同名）。</p>

<p>Scala中默认访问级别是public的。</p>

<pre><code>// scala
class Foo

// java
public class Foo {
}
</code></pre>

<p>如果构造函数、方法没有参数，可以省略参数列表（即不需要输入括号）。</p>

<pre><code>new Foo

new Foo()

class Bar(name: String)

new Bar("Working...")

class Baz(name: String) {
    // constructor code is inline
    if(name == null) throw new Exception("Name is null")
}
</code></pre>

<p>Scala的trait，和java中的interface类似。同时trait可以包括具体实现的方法，这是一个非常方便的特性，你不必在定义复杂的类继承关系来实现代码的重用，在Scala中，把代码写在trait中即可。Scala traits类似于Ruby mixins</p>

<pre><code>trait Dog

class Fizz2(name: String) extends Bar(name) with Dog

trait Cat {
    def meow(): String
}

trait FuzzyCat extends Cat {
    override def meow(): String = "Meeeeeeeeeow"
}

trait OtherThing {
    def hello() = 4
}

class Yep extends FuzzyCat with OtherThing

(new Yep).meow()
(new Yep).hello()
</code></pre>

<p>Scala中不支持static关键字，可以使用<code>object</code>单例对象来实现类似的功能。当object对象第一次访问才会被初始化，在对应的访问域内仅有一个该实例。Scala object还有一个优势，由于是类的实例，所以可以作为方法参数进行传递。</p>

<pre><code>object Simple

object OneMethod {
    def myMethod() = "Only One"
}

object Dude extends Yep

object Dude2 extends Yep {
    override def meow() = "Dude looks like a cat"
}

object OtherDude extends Yep {
    def twoMeows(otherparam: Yep) = meow + ", " + otherparam.meow
}

OtherDude.meow // Meeeeeeeeeow
OtherDude.twoMeows(Dude) // Meeeeeeeeeow, Meeeeeeeeeow
OtherDude.twoMeows(Dude2) // Meeeeeeeeeow, Dude looks like a cat
</code></pre>

<p>如果object嵌套定义在class, trait, object内部的时刻，在其作用域下每个<strong>实例</strong>会创建一个object的单例。</p>

<pre><code>class HasYep {
    object myYep extends Yep {
    override def meow = "Moof"
  }
}

(new HasYep).myYep.meow // 每个HasYep实例会有一个单独的myYep
</code></pre>

<p>同样Classes，Objects，traits也可以嵌套在classes，objects，traits。</p>

<pre><code>class HasClass {
    private class MyDude extends FuzzyCat
    def makeOne(): FuzzyCat = new MyDude
}
</code></pre>

<h4>类继承Class Hierarchy</h4>

<p>除了方法（method），其他一切都是对象(an instance of a class)。Java的primitives类型在Scala也被当做对象，如int(Int)。当两个Ints相加时，Scala编译器会对字节码进行优化最终和java的两个ints相加时一样的。如果使用了Int的方法hashCode和toString，当primitive类型被用于需要引用类型时(expects an Any)，Scala编译器会对其进行装箱，如把Int值加入到HashMap。</p>

<p>为了保持命名的规范化，即所有类的第一个单词都是大写的。在Scala中的原始类型对应为Int,Long,Double,Float,Boolean,Char,Short,Byte，他们都是AnyVal的子类。java的void对应Unit， 同样是AnyVal的子类。你也可以使用<code>()</code>来显示的返回Unit类型实例。</p>

<pre><code>val v = ()

List(v) // List[Unit] = List(())
</code></pre>

<p><code>Nothing</code>是很酷，任何方法返回Nothing，表示它不是正常返回，肯定是抛出了异常。<code>None</code>是一个<code>Option[Nothing]</code>的实例，它的get方法会返回<code>Nothing</code>，也就是说get方法会抛出异常，而不是返回底层的值类型null。</p>

<p>Any是Scala中所有类的基类，想Object在Java中的地位。但是，Nothing/primitives等等，所以需要在Object下面定义Scala的根基类。</p>

<p>AnyVal是Scala中primitives对象的包装类的基类。
AnyRef与Java中的Object类似。<code>eq</code>,<code>ne</code>,<code>==</code>,<code>!=</code>这些方法的含义不同。<code>==</code>编译后最终调用java的equals方法，如果需要进行对象引用的比较，使用<code>eq</code>进行处理。</p>

<h4>方法声明</h4>

<p>类型推测很强大也很有用，但是需要小心使用，当类型返回类型不明确时，需要显示进行声明。</p>

<pre><code>def myMethod(): String = "Moof"

def myOtherMethod() = "Moof" // not have to explicity declare the return type

def foo(a: Int): String = a.toString

def f2(a: Int, b: Boolean): String = if(b) b.toString else "false"

def list[T](p: T): List[T] = p :: Nil

list(1)
list("Hello")

// 可变参数， Seq[Int]
def largest(as: Int*): Int = as.reduceLeft((a,b) =&gt; a max b)

largest(1)
largest(2, 3, 99)
largest(33, 22, 33, 22)

def mkString[T](as: T*): String = as.foldLeft("")(_ + _.toString)

def sum[T &lt;: Number](as: T*): Double = as.foldLeft(0d)(_ + _.doubleValue)
</code></pre>

<p>方法可以定义在<strong>任何方法块</strong>中，除了最外层即classes，traits，objects定义的地方。方法中可以使用当前作用域类的所有的成员。</p>

<pre><code>def readLines(br: BufferedReader) = {
    var ret: List[String] = Nil

    def readAll(): Unit = br.readLine match { 
        case null =&gt;
        case s =&gt; ret ::= s; readAll()
    }

    readAll()
    ret.reverse
}
</code></pre>

<p>方法重写和java的不一样，被重写的方法必须带上override的修饰符。重写抽象的方法可以不带override的修饰符。</p>

<pre><code>abstract class Base {
    def thing: String
}

class One extends Base {
    def thing = "Moof"
}
</code></pre>

<p>不带参数的方法和变量可以使用<strong>相同的方式访问</strong>，重写父类方法时可以使用val代替def。</p>

<pre><code>class Two extends One {
    override val thing = (new java.util.Date).toString
}

class Three extends One {
    override lazy val thing = super.thing + (new java.util.Date).toString
}
</code></pre>

<h4>变量声明</h4>

<p>和声明方法类似，不过关键字使用val, var, lazy val。var
可以在设置值以后再次进行修改，类似于java中的变量。val在运行到该作用域时就初始化。lazy val仅在访问的时刻进行计算一次。</p>

<pre><code>var y: String = "Moof"
val x: String = "Moof"
lazy val lz: String = someLongQuery()
</code></pre>

<p>在编程时，不推荐使用var变量除非一定要用变量。Given that mutability leads to unexpected defects, minimizing mutability in code minimizes mutability-related defects.</p>

<p>Scala类型推测对变量一样有效，在参数类型明确的情况下，定义参数时可以不用指定类型。</p>

<pre><code>var y2 = "Moof"
val x2 = "Moof"
</code></pre>

<p>Scala支持同时接受多个参数值。 If a code block or method returns a Tuple, the Tuple can be assigned to a val variable.</p>

<pre><code>val (i1: Int, s1: String) = Pair(33, "Moof")
val (i2, s2) = Pair(43, "Moof")
</code></pre>

<p>运行的效果如下：</p>

<pre><code>scala&gt; val (i2,s2)=Pair(43,"W")
i2: Int = 43
s2: String = W

scala&gt; i2
res0: Int = 43
</code></pre>

<h4>代码块</h4>

<p>方法和参数定义都可以定义在单行。</p>

<pre><code>def meth9 = "hello world"
</code></pre>

<p>或者定义在大括号包围的代码块中。代码块可以去嵌套。代码块的返回值是最后一个行的运行结果。</p>

<pre><code>def meth3(): String = {"Moof"}
def meth4(): String = {
    val d = new java.util.Date()
    d.toString()
}
</code></pre>

<p>参数定义同样可以使用代码块，适合于有少量计算的赋值操作。</p>

<pre><code>val x3: String = {
    val d = new java.util.Date()
    d.toString()
}
</code></pre>

<h4>Call-by-Name</h4>

<p>在java中，所有方法是按call-by-reference或者call-by-value（原始类型）调用。也就是说，在调用栈中的参数的值或者引用（AnyRef）会传递给调用者。</p>

<p>Scala提供另一种传递参数给方法（函数）的方式：call-by-name，可以把方法块传给调用者。 Each time the callee accesses the parameter, the code block is executed and the value is calculated.</p>

<p>Call-by-name容许我们把耗时的操作（但可能不会用到的）当做参数。For example, in a call to the logger you can use call-by-name, and the express to print is only calculated if it’s going to be logged。Call-by-name同样容许我们创建（如while/doWhile）自定义的控制结构。</p>

<pre><code>def nano() ={
    println("Getting nano")
    System.nanoTime
}

def delayed(t: =&gt; Long) = {
    println("In delayed method")
    println("Param: " + t)
    t
}

scala&gt; delayed(nano())
In delayed method
Getting nano
Param: 198642874346225
Getting nano
res1: Long = 198642875202814

def notDelayed(t: Long) = {
    println("In not delayed method")
    println("Param: " + t)
    t
}

scala&gt; notDelayed(nano)
Getting nano
In not delayed method
Param: 199944029171474
res5: Long = 199944029171474
</code></pre>

<p>注意println输出的位置和次数。</p>

<h4>方法调用</h4>

<pre><code>instance.method()
instance.method

instance.method(param)
instance method param
</code></pre>

<p>方法没有参数时可以省略括号。当只有可以参数时，可以省去点和括号。</p>

<p>实际运行效果：</p>

<pre><code>scala&gt; "abc" toUpperCase
warning: there were 1 feature warning(s); re-run with -feature for details
res0: String = ABC

scala&gt; "abc".toUpperCase
res1: String = ABC

scala&gt; "abc".charAt 1
&lt;console&gt;:1: error: ';' expected but integer literal found.
       "abc".charAt 1
                    ^

scala&gt; "abc" charAt 1
res2: Char = b

scala&gt; "abc" concat "efg"
res3: String = abcefg
</code></pre>

<p>Scala允许方法名中包括+/-/*/?， Scala’s dotless method notation creates a syntactically neutral way of invoking methods that are hard-coded operators in Java.</p>

<pre><code>scala&gt; 2.1.*(4.3)
res4: Double = 9.03

scala&gt; 2.1 * 4.3
res5: Double = 9.03
</code></pre>

<p>多参数的方法调用和java一样。</p>

<pre><code>instance.method(p1, p2)
</code></pre>

<p>Scala中的泛型方法，编译器可以进行类型推断。当然你也可以显示的指定类型。</p>

<pre><code>instance.method[TypeParam](p1, p2)
</code></pre>

<h4>Functions, apply, update, and Compiler Magic</h4>

<p>Scala是一门函数语言，也意味着你可以传递函数，可以把函数作为返回值在函数和方法中返回。</p>

<p>函数是一个带有参数和返回值的代码块。
在JVM中是不容许传递代码块的。Scala中使用特定接口的匿名内部类作为函数内部实现。当传递一个函数时，其实就是传递一个特定接口(trait)的对象。</p>

<p>定义函数的trait使用一个参数和一个返回值:</p>

<pre><code>Function1[A, B]
</code></pre>

<p>其中A是参数类型，B是返回值类型。</p>

<p>所有的函数接口都有一个apply的方法，用于函数的调用。</p>

<pre><code>Function1.apply(p: A): B
</code></pre>

<p>Thus, you can define a method that takes a function and invokes the function with the parameter 42:</p>

<pre><code>def answer(f: Function1[Int, String]) = f.apply(42)
</code></pre>

<p>如果（只要）对象包括apply方法，可以省略apply，直接把参数跟在函数名后面。</p>

<pre><code>def answer(f: Function1[Int, String]) = f(42)
</code></pre>

<p>Scala提供的语法糖，在编译时f(42)会编译成f.apply(42)。这样使用可以让代码更简洁漂亮，同时看起来更像函数调用的写法。</p>

<p>更多的语法糖：</p>

<pre><code>Function1[Int, String]
Int =&gt; String

def answer(f: Int =&gt; String) = f(42)
</code></pre>

<p>这种语法糖适用于所有包括apply方法对象。</p>

<pre><code>scala&gt; class Ap {
     | def apply(in: Int) = in.toString
     | }
defined class Ap

scala&gt; new Ap()(44)
res0: String = 44

scala&gt; new Ap(44)
&lt;console&gt;:9: error: too many arguments for constructor Ap: ()Ap
              new Ap(44)
              ^

scala&gt; val a = new Ap
a: Ap = Ap@18258b2

scala&gt; a(44)
res2: String = 44
</code></pre>

<p>如果类包括update方法，编译解析赋值操作时，会调用两个参数的update方法。</p>

<pre><code>scala&gt; class Up {
     | def update(k: Int, v: String) = println("Hey: " + k + " " + v)
     | }
defined class Up

scala&gt; val u = new Up
u: Up = Up@7bfd80

scala&gt; u(33) = "hello"
Hey: 33 hello

scala&gt; class Update {
     | def update(what: String) = println("Singler: " + what)
     | def update(a: Int, b: Int, what: String) = println("2d update")
     | }
defined class Update

scala&gt; val u = new Update
u: Update = Update@4bd4d2

scala&gt; u() = "Foo"
Singler: Foo

scala&gt; u(3,4) = "Howdy"
2d update
</code></pre>

<p>Scala中Array和HashMap使用update的方式进行设值。使用这种方式我们可以编写和Scala类似特性的库。</p>

<p>Scala的这些特性可以让我们编写更易理解的代码。同时理解Scala的这些语法糖，能更好的与java类库一起协作。</p>

<h4>Case Classes</h4>

<p>Scala has a mechanism for creating classes that have the common stuff filled in. Most of the time, when I define a class, I have to write the toString, hashCode, and equals methods.  These methods are boilerplate. Scala provides the case class mechanism for filling in these blanks as well as support for pattern matching.</p>

<p>A case class provides the same facilities as a normal class, but the compiler generates toString,  hashCode, and  equals methods (which you can override).</p>

<p>Case classes can be instantiated without the use of the  new statement. By default, all the parameters in the case class’s constructor become properties on the case class. Here’s how to create a case class:</p>

<pre><code>scala&gt; case class Stuff(name: String, age: Int)
defined class Stuff

scala&gt; val s = Stuff("David", 45)
s: Stuff = Stuff(David,45)

scala&gt; s.toString
res0: String = Stuff(David,45)

scala&gt; s == Stuff("David", 45) // == 相当于java中的equals
res1: Boolean = true

scala&gt; s == Stuff("David", 42)
res2: Boolean = false

scala&gt; s.name
res4: String = David

scala&gt; s.age
res5: Int = 45
</code></pre>

<p>手写case class功能的类：</p>

<pre><code>class Stuff(val name: String, val age: Int) {
    override def toString = "Stuff(" + name + "," + age + ")"
    override def hashCode = name.hashCode + age
    override def equals(other: AnyRef) = other match {
        case s: Stuff =&gt; this.name == s.name &amp;&amp; this.age = s.age
        case _ =&gt; false
    }
}

object Stuff {
    def apply(name: String, age: Int) = new Stuff(name, age)
    def unapply(s: Stuff) = Some((s.name, s.age))
}
</code></pre>

<h4>Basic Pattern Matching</h4>

<p>模式匹配（Pattern matching）可以使用很少的代码编写非常复杂的判断。Scala Pattern matching和Java switch语句类似， but you can test against almost anything, and you can even assign pieces of the matched value to variables. Like everything in Scala, pattern matching is an expression, so it result s in a value that may be assigned or returned. The most basic pattern matching is like Java’s switch, except there is no  break in each case as the cases do not fall through to each other.</p>

<pre><code>44 match {
    case 44 =&gt; true
    case _ =&gt; false
}
</code></pre>

<p>可以对String进行match操作，类似于C#。</p>

<pre><code>"David" match {
    case "David" =&gt; 45
    case "Elwood" =&gt; 77
    case _ =&gt; 0
}
</code></pre>

<p>可以多case classes进行模式匹配（pattern match）操作。Case classes提供了非常适合与pattern-matching的语法。下面的例子，用于匹配Stuff的name==David以及age==45的对象。</p>

<pre><code>Stuff("David", 45) match {
    case Stuff("David", 45) =&gt; true
    case _ =&gt; false
}
</code></pre>

<p>仅匹配名字：</p>

<pre><code>Stuff("David", 45) match {
    case Stuff("David", _) =&gt; "David"
    case _ =&gt; "Other"
}
</code></pre>

<p>还可以把值提取出来，如把age的值赋给howOld变量：</p>

<pre><code>Stuff("David", 45) match {
    case Stuff("David", howOld) =&gt; "David, age: " + howOld
    case _ =&gt; "Other"
}
</code></pre>

<p>还可以在pattern和=>之间添加条件。如年龄小于30的返回young David，其他的结果为old David。</p>

<pre><code>Stuff("David", 45) match {
    case Stuff("David", age) if age &lt; 30 =&gt; "young David"
    case Stuff("David", _) =&gt; "old David"
    case _ =&gt; "Other"
}
</code></pre>

<p>Pattern matching还可以根据类型进行匹配：</p>

<pre><code>x match {
    case d: java.util.Date =&gt; "The date in milliseconds is " + d.getTime
    case u: java.net.URL =&gt; "The URL path: " + u.getPath
    case s: String =&gt; "String: " + s
    case _ =&gt; "Something else"
}
</code></pre>

<p>如果使用Java代码的话，需要多很多的转换！！</p>

<pre><code>if(x instanceof Date) return "The date in milliseconds is " + ((Date)x).getTime();
if(x instanceof URL) return "The URL path: " + ((URL)x).getPath();
if(x instanceof String) return "String: " + ((String)x);
return "Something else"
</code></pre>

<h4>if/else and while</h4>

<p>while在Scala中比较少用。if/else使用频率高一些，比java的三目赋值操作符（?:）使用频率更高。if和while表达式总是返回Unit（相当于Java的Void）。if/else的返回值更具各个部分表单时类型确定。</p>

<pre><code>if(exp) println("yes")

// multiline
if(exp) {
    println("Line one")
    println("Line two")
}

val i: Int = if(exp) 1 else 3

val i: Int = if(exp) 1 
else {
    val j = System.currentTimeMillis
    (j % 100L).toInt
}
</code></pre>

<p>while executes its code block as long  as its expression evaluates to  true, just like Java. In practice, using recursion, a method calling itself, provides more readab le code and enforces the concept of transforming input to output rather than changing, mutating, variables. Recursive methods can be as efficient as a while loop.</p>

<pre><code>while (exp) println("Working...")
while (exp) {
    println("Working...")
}
</code></pre>

<h4>for</h4>

<pre><code>for { i &lt;- 1 to 3} println(i)

for { i &lt;- 1 to 3
        j &lt;- 1 to 3
    } println(i*j)

def isOdd(in: Int) = in % 2 == 1
for {i &lt;- 1 to 5 if ifOdd(i)} println(i)

for {i &lt;- 1 to 5
        j &lt;- 1 to 5 if isOdd(i*j)} println(i*j)

val lst = (1 to 18 by 3).toList
for {i &lt;- lst if isOdd(i)} yield i

for {i &lt;- lst; j &lt;- lst if isOdd(i*j)} yield i*j
</code></pre>

<p>将在第三章-集合中更详细的讲解for使用方法。</p>

<h4>throw, try/catch/finally, and synchronized</h4>

<p>try/finally的写法和java类似：</p>

<pre><code>throw new Exception("Working...")

try{
    throw new Exception("Working...")
} finally {
    println("This will always be printed")
}
</code></pre>

<p>try/catch的语法不大一样，catch对异常进行了封装，首先它是一个表达式其返回值是一个值；使用case（pattern matched）来匹配异常类型。</p>

<pre><code>try {
    file.write(stuff)
} catch {
    case e: java.io.IOException =&gt; // handle IO Exception
    case n: NullPointerException =&gt; // handle null Exception
}

try { Integer.parseInt("dog") } catch { case _ =&gt; 0 } //0
try { Integer.parseInt("44") } catch { case _ =&gt; 0 } //44
</code></pre>

<p>基于对象的同步操作，每个类都自带了synchronized方法。</p>

<pre><code>obj.synchronized {
    // do something that needs to be serialized
}
</code></pre>

<p>不像java有synchronized方法修饰符。在Scala中同步方法定义使用：</p>

<pre><code>def foo(): Int = synchronized {
    42
}
</code></pre>

<h4>Comments</h4>

<p>注释基本上类C的语言都一样，单行<code>//</code>、多上<code>/* ... */</code>。</p>

<p>在Scala中还可以嵌套的注释。</p>

<pre><code>/*
  This is an outer comment
  /* And this comment
     is nested
  */
  Outer comment
*/
</code></pre>

<h4>Scala vs Java vs Ruby</h4>

<p><strong>类和实例</strong></p>

<p>java有原始类型。Scala中操作都是方法调用，所有东西都是对象，无需为了原始类型而进行额外的判断/处理。</p>

<pre><code>1.hashCode
2.toString
</code></pre>

<p>我们可以定义一个方法，传递函数（从一个Int到另一个Int转换操作）。</p>

<pre><code>def with42(in: Int =&gt; Int) = in(42)
with42( 33 + )
</code></pre>

<p>在语言级别，如果所有东西都是统一的，在进行编程设计时就会很方便和简单。同时Scala编译时会针对JVM原始类型进行优化，使得scala的代码在效率上非常接近Java。</p>

<p><strong>Traits, Interfaces, and Mixins</strong></p>

<p>在java中除了Object对象，其他对象都有一个唯一的父类。Java类可以实现一个或者多个接口（定义实现类必须实现方法的约定）。这是依赖注入和测试mocks，以及其他抽象模式的基础。</p>

<p>Scala使用traits， Traits提供了Java接口拥有的所有特性。同时Traits可以包括方法的实现以及参数的定义。方法实现一次，把所有继承traits的方法混入子类中。</p>

<p><strong>Object, Static, and Singletons</strong></p>

<p>在Java中，可以定义类的（静态）方法和属性，提供了访问方法的唯一入口，同时不需要实例化对象。类（静态）属性提供了在JVM中全局共享数据的方式。
Scala提供了类似的机制：Objects。Objects是单例模式的实现。在类加载的时刻实例化该对象。这种方式同样可以共享全局状态。而且，objects也是Scala完全的面向对象的一种体现，objects是一个类的实例，而不是某种类级别的常量（some class-level constant）。可以把objects作为参数来进行传递。</p>

<p><strong>Functions, Anonymous Inner Class, and  Lambdas/Procs</strong></p>

<p>The Java construct to pass units of computation as parameters to methods is anonymous inner class. 匿名内部类在Swing UI库非常的常见。在Swing中，许多UI事件处理的接口定义1-2个方法，在编写程序时，实现事件接口的内部类能访问外部类的私有成员数据。</p>

<p>Scala functions对应的就是匿名内部类。Scala functions实现了统一的接口，调用函数时执行接口的apply方法。和Java匿名内部类相比，Scala创建函数的语法更加简洁和优雅。同时，访问本地参数的规则也更加灵活。在Java匿名内部类只能访问final的参数，而Scala functions能访问和修改vars参数。</p>

<p>Scala和Ruby的面向对象模型和函数式编程很相似。同时Scala在访问类库和静态类型方面和Java很类似。Scala博采众长，把Java和Ruby的优点都囊括了。</p>

<h3>总结</h3>

<p>这一章首相讲了安装和运行Scala程序，然后围绕Scala编程的语法结构来展开。下一章讲解Scala的数据类型，使用很少的代码编写功能健壮的程序，同时编码量的减少也能有效的控制bugs的数量。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读读书]Redis入门指南]]></title>
    <link href="http://winseliu.com/blog/2014/07/27/start-redis/"/>
    <updated>2014-07-27T01:20:44+08:00</updated>
    <id>http://winseliu.com/blog/2014/07/27/start-redis</id>
    <content type="html"><![CDATA[<p>《Redis入门指南》的基本使用笔记，<a href="/blog/categories/redis/">jemalloc/tcmalloc功能和redis3集群的安装参考</a>。</p>

<h2>第一章 简介</h2>

<ul>
<li>讲了redis的产生的缘由</li>
<li>Salvtore Sanfilippo/Pieter Noordhuis被招到VMware专门负责redis</li>
<li>redis的源码可以从github下载编译。</li>
</ul>


<p>redis相比keyvalue，提供了更加丰富的值类型：字符串/散列/列表/集合/有序集合，数据提供多种持久化(RDB/AOF)的方式。</p>

<p>在一台普通的笔记本电脑上，Redis可以在一秒内读写超过十万个键值。</p>

<p>功能丰富，提供TTL，可以做(阻塞)队列、缓冲系统、发布/订阅消息模式。redis是单线程模型，相比memcached的多线程，可以启动多个redis实例。</p>

<h2>第二章 准备</h2>

<p>默认的生产环境使用linux，windows操作系统下也有对应的版本但是版本比较旧。
在linux下，下载完成后直接<code>make</code>就可以使用src目录下生成的命令了，<code>make install</code>会把命令拷贝到/usr/local/bin目录下。同时有介绍iOS和Windows下怎么安装redis。</p>

<h3>启动Redis2.8.3</h3>

<pre><code>src/redis-server # default port 6379
src/redis-server --port 6380
</code></pre>

<p>初始化脚本启动Redis</p>

<pre><code>    #!/bin/sh
    #
    # Simple Redis init.d script conceived to work on linux systems
    # as it does use of the /proc filesystem.

    REDISPORT=6379
    EXEC=/usr/local/bin/redis-server
    CLIEXEC=/usr/local/bin/redis-cli

    PIDFILE=/var/run/redis_${REDISPORT}.pid
    CONF=/etc/redis/${REDISPORT}.conf

    case "$1" in
    start)
        if [ -f $PIDFILE ]
        then
            echo "$PIDFILE exists, process is already running or crashed"
        else
            echo "Starting Redis server..."
            $EXEC $CONF
        fi
        ::
    stop)
        if [ ! -f $PIDFILE ]
        then
            echo "$PIDFILE does not exists, process is not running"
        else
            PID=$(cat $PIDFILE)
            echo "Stopping..."
            $CLIEXEC -p $REDISPORT shutdown
            while [ -x /proc/$PID ]
            do 
                echo "Waiting for Redis to shutdown..."
                sleep 1
            done
            echo "Redis stopped"
        fi
        ::
    *)
        echo "Please use start or stop as first argument"
        ::
    esac
</code></pre>

<h3>停止Redis</h3>

<p>不要直接强制终止程序(<code>kill -9</code>)。使用redis提供的shutdown来停，会等所有操作都flush到磁盘后再关闭。保证数据不会丢失。
当然也可以使用SIGTERM信号来处理，使用<code>kill PID</code>命令，Redis妥善的处理与发送shutdown命令效果一样。</p>

<pre><code>src/redis-cli shutdown
</code></pre>

<h3>命令行客户端(cli Command-Line-Interface)</h3>

<pre><code>redis-cli -h IP -p PORT

[hadoop@master1 src]$ ./redis-cli PING
PONG

[hadoop@master1 src]$ ./redis-cli
127.0.0.1:6379&gt; PING
PONG
127.0.0.1:6379&gt; echo hi
"hi"
</code></pre>

<p>各种返回值</p>

<pre><code>127.0.0.1:6379&gt; errorcommand
(error) ERR unknown command 'errorcommand'
127.0.0.1:6379&gt; incr foo
(integer) 1
127.0.0.1:6379&gt; get foo
"1"
127.0.0.1:6379&gt; get noexists
(nil)
127.0.0.1:6379&gt; keys *
1) "foo"
</code></pre>

<h3>配置</h3>

<pre><code>redis-server CONFPATH --loglevel warning
</code></pre>

<p>也可以通过客户端设置值</p>

<pre><code>127.0.0.1:6379&gt; config set loglevel warning
OK
127.0.0.1:6379&gt; config get loglevel
1) "loglevel"
2) "warning"
</code></pre>

<h3>多数据库</h3>

<p>默认启动的程序启用了16个库（0-15，<code>databases 16</code>），客户端与Redis建立连接后，会自动选择0号数据库，不过可以通过SELECT命令更换数据库:</p>

<pre><code>127.0.0.1:6379&gt; select 1
OK
127.0.0.1:6379[1]&gt; get foo
(nil)
127.0.0.1:6379[1]&gt; set foo 1
OK
127.0.0.1:6379[1]&gt; get foo
"1"
</code></pre>

<p>redis不支持为每个数据库设置不同的访问密码，一个客户端要么可以访问全部数据库，要么连一个数据库也没有权限访问。最重要的一点是多个数据库并不是完全的隔离，比如flushall命令可以清空Redis实例中所有的数据库中的数据。所以这些数据库更像是一个命名空间，而不是适合存储不同应用的数据。</p>

<p>但是可以使用0号数据库存储A应用的生产数据而使用1号数据库存储A应用的测试数据，不同的应用应该使用不同的Redis实例存储数据。由于Redis非常轻量级，一个空Redis实例占用内存只有1M左右，所以不用担心多个Redis实例会额外占用很多内存。</p>

<h2>第三章 入门</h2>

<h3>热身</h3>

<p>获取符合规则的键名（glob风格 ?/*/\X/[]） : <code>KEYS pattern</code></p>

<pre><code>127.0.0.1:6379[1]&gt; KEYS *
1) "foq"
2) "foo"
3) "fop"
127.0.0.1:6379[1]&gt; keys fo[a-p]
1) "foo"
2) "fop"

127.0.0.1:6379[1]&gt; exists foa
(integer) 0 #不存在
127.0.0.1:6379[1]&gt; exists foo
(integer) 1 #存在

127.0.0.1:6379[1]&gt; del foo
(integer) 1
127.0.0.1:6379[1]&gt; del foa
(integer) 0
127.0.0.1:6379[1]&gt; keys *
1) "fop"
</code></pre>

<p>keys会遍历Redis中的所有键，当数量比较多是会影响性能，不建议在生产环境使用。</p>

<p>del可以删除多个键值，返回值为删除的个数。del命令的参数不支持通配符，但可以通过linux的实现批量删除<code>redis-cli DEL $(redis-cli KEYS "user:*")</code>（有长度限制）来达到效果，效果比xargs效果更好。</p>

<p>获取keyvalue值的类型</p>

<pre><code>127.0.0.1:6379&gt; set foo 1
OK
127.0.0.1:6379&gt; lpush foo 1
(error) WRONGTYPE Operation against a key holding the wrong kind of value
127.0.0.1:6379&gt; lpush foa 1
(integer) 1
127.0.0.1:6379&gt; type foo
string
127.0.0.1:6379&gt; type foa
list
</code></pre>

<h3>字符串类型</h3>

<pre><code>set key value
get key

incr key # 对应的值需为数值

set foo 1
incr foo
set foo b
incr foo
# (error) ERR value is not an integer or out of range

# 增加指定的整数

incrby key increment
decr key 
decr key decrement
increbyfloat key increment

append key value
strlen key # 字节数，和java字符串的length不同

mget key [key ...]
mset key value [key value ...]

getbit key offset
setbit key offset value
bitcount key [start] [end]
bitop operation destkey key [key ...] # AND OR XOR NOT

set foo1 bar
set foo2 aar
BITOP OR res foo1 foo2 # 位操作命令可以非常紧凑地存储布尔值
GET res
</code></pre>

<h3>散列值</h3>

<pre><code>hset key field value
hget key field
hmset key field value [field value ...]
hmget key field [field ...]
hgetall key

hexists key field
hsetnx key field value # 当字段不存在时赋值 if not exists

hincrby key field increment

hdel key field [field ...]

hkeys key # 仅key
hvals key # 仅value
hlen key  # 字段数量
</code></pre>

<h3>列表</h3>

<p>双端队列型列表</p>

<pre><code>lpush key value [value ...]
rpush key value [value ...]
lpop key
rpop key
llen key
lrange key start stop # 可以使用负索引，从0开始，包括最右边的元素

lrem key count value 
# 删除列表中前count个值为value的元素，返回的是实际删除的元素个数。
# count为负数是从右边开始删除
# count为0时删除所有值为value的元素

# 获得/设置指定索引的元素值

lindex key index # index为负数是从右边开始
lset key index value

ltrim key start end # 只保留列表指定的片段
linsert key BEFORE/AFTER pivotvalue value

poplpush source destination # 将元素从给一个列表转到另一个列表
</code></pre>

<h3>集合类型</h3>

<pre><code>sadd key member [member ...]
srem key member [member ...]
smembers key # 获取集合中的元素
sismember key member # 判断元素是否在集合中

sdiff key [key ...] # 差集 A-B
sinter key [key ...] # A ∩ B
sunion key [key ...] # A ∪ B

scard key # 获取集合中元素个数

sdiffstore destination key [key ...]
sinterstore destination key [key ...]
sunionstore destination key [key ...]

srandmember key [count] 
# 随机获取集合中的元素，count参数来一次性获取多个元素
# count为负数时，会随机从集合里获得|count|个的元素，这里元素有可能相同。

spop key # 从集合中随机弹出一个元素
</code></pre>

<h3>有序集合</h3>

<p>列表类型是通过链表实现的，获取靠近两端的数据速度极快，而当元素增多后，访问中间数据的速度会较慢，所以它更加适合实现和“新鲜事”或“日志”这样很少访问中间元素的应用。有序集合类型是使用散列和跳跃表（Skip list）实现的，所以即使读取位于中间的数据也很快（时间复杂度是O(log(N))）。列表中不能简单地调整某个元素的位置，但是有序集合可以（通过更改这个元素的分数）。有序集合要比列表类型更耗费内存。</p>

<pre><code>zadd key score member [score member ...]
# 如果该元素已经存在则会用新的分数替换原有的分数。zadd命令的返回值是新加入到集合中的元素个数（不包含之前已经存在的元素）。
# 其中+inf和-inf分别表示正无穷和负无穷

zscore key member

zrange key start stop [withscores] # 获取排名在某个范围的元素列表
zrevrange key start stop [withscores] 
# 负数代表从后向前查找（-1表示最后一个元素），O(logn+m)

zrangebyscore key min max [withscores] [limit offset  count]

# 命令按照元素分数从小到大的顺序返回分数的min和max之间（包含min和max）的元素。
# 如果希望分数范围不包含端点值，可以在分数前加上"("符号。例如，希望返回80分到100分的数据，可以含80分，但不包含100分。则稍微修改一下上面的命令即可：
zrangebyscore scoreboard 80 (100
zrangebyscore scoreboard (80 +inf
# 本命令中LIMIT offset count与SQL中的用法基本相同。获取分数低于或等于100的前3个人
zrevrangebyscore scoreboard 100 0 limit 0 3

zincrby key increment memeber # 增加某个元素的分数

zcard key # 获取集合中元素的数量
zcount key min max # 获得指定分数范围内的元素个数
zrem key member [memeber ...] # 删除一个或多个元素，返回成功删除的元素数量

# 按照排名范围删除元素, 并返回删除的元素数量
zremrangebyrank key start stop
# 按照分数范围删除元素
zremrangebyscore key min max

zrank key member
zrevrank key memeber

zinterstore destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [aggregate sum|min|max]
zunionstore ...
</code></pre>

<h2>第四章 进阶</h2>

<h3>事务</h3>

<pre><code>multi
sadd "user:1:following" 2
sadd "user:2:followers" 1
exec
</code></pre>

<p>脚本语法有错，命令不能执行。但是当数据类型等逻辑运行错误时，事务里面的命令会被redis接受并执行。</p>

<p>如果事务里的一条命令出现错误，事务里的其他命令依然会继续执行（包括出错到最后的命令）。对应的返回值会返回错误信息。</p>

<pre><code>127.0.0.1:6379&gt; multi
OK
127.0.0.1:6379&gt; set key 1
QUEUED
127.0.0.1:6379&gt; sadd key 2
QUEUED
127.0.0.1:6379&gt; set key 3
QUEUED
127.0.0.1:6379&gt; exec
1) OK
2) (error) WRONGTYPE Operation against a key holding the wrong kind of value
3) OK
127.0.0.1:6379&gt; get key
"3"
</code></pre>

<p>redis的事务没有回滚的功能，出现错误事务时必须自己负责收拾剩下的摊子（将数据库复原事务执行前的状态等）。不过由于redis不支持回滚功能，也使得redis在事务上可以保持简洁和快速。其中语法错误完全可以再开发时找出并解决。另外如果能够很好的规划数据库（保证键名规范等）的使用，是不会出现命令与数据类型不匹配这样的错误的。</p>

<p><strong>watch命令</strong></p>

<p>在一个事务中只有当所有命令都依次执行完后才能得到每个结果的返回值。可是有些情况下需要先获得一条命令的返回值，然后再根据这个值执行下一条命令。
如increment的操作，在增加1的是时刻没法保证数据还是原来的数据。为了解决这个问题，可以在GET获取值后保证该键值不会被其他客户端修改，知道函数执行完成后才允许其他客户端修改该键值，这样也可以防止竞态条件。watch命令可以监控一个或多个键，一旦其中一个键被修改（或删除），之后的事务就不会被执行。监控一直持续到exec命令。</p>

<pre><code>127.0.0.1:6379&gt; watch key
OK
127.0.0.1:6379&gt; set key 2
OK
127.0.0.1:6379&gt; multi
OK
127.0.0.1:6379&gt; set key 3
QUEUED
127.0.0.1:6379&gt; exec
(nil)
127.0.0.1:6379&gt; get key
"2"
</code></pre>

<p>执行exec命令会取消对所有键的监控，如果不想执行事务中的命令也可以使用unwatch命令来取消监控。</p>

<h3>生存时间TTL</h3>

<pre><code>expire key seconds

ttl key

127.0.0.1:6379&gt; get key
"2"
127.0.0.1:6379&gt; ttl key
(integer) -1
127.0.0.1:6379&gt; expire key 10
(integer) 1
127.0.0.1:6379&gt; ttl key
(integer) 6
127.0.0.1:6379&gt; ttl key
(integer) 1
127.0.0.1:6379&gt; ttl key
(integer) -2

pexpire milliseconds #时间的单位为毫秒
expireat UTC
pexpireat 毫秒（UTC*1000）
</code></pre>

<p>除了persist命令之外，使用set和getset命令为键赋值也会同时清除键的生存时间。使用expire命令会重新设置键的生存时间。其他对键值进行操作的命令（如incr、lpush、hset、zrem）均不会影响键的生存时间。</p>

<p>提示： 如果使用watch命令监测一个拥有生存时间的键，该键时间到期自动删除并不会被watch命令认为该键被改变。</p>

<h3>缓冲</h3>

<p>expire + maxmemory maxmemory-policy(LRU)</p>

<h3>排序</h3>

<p>可以使用multi, zintestore, zrange, del, exec来实现，但太麻烦！<a href="https://gist.github.com/winse/30f9db38a4c41aaf5f9d">实际操作日志</a>。</p>

<p>sort命令，可用于集合、列表类型和有序集合类型</p>

<pre><code>sort key [ALPHA] [BY PREFIXKYE:*-&gt;property] [DESC] [LIMIT offset count] 

127.0.0.1:6379&gt; lpush mylist 7 1 3 9 0
(integer) 5
127.0.0.1:6379&gt; sort mylist
1) "0"
2) "1"
3) "3"
4) "7"
5) "9"
</code></pre>

<p>针对有序集合排序时会忽略元素的分数，只针对元素自身的值进行排序。
集合类型中所有元素是无序的，但经常被用于存储对象的ID，很多情况下都是整数。所以redis多这种情况进行了特殊的优化，元素的顺序是有序的。</p>

<pre><code>127.0.0.1:6379&gt; sadd myset 5 2 6 1 8 1 9 0
(integer) 7
127.0.0.1:6379&gt; smembers myset
1) "0"
2) "1"
3) "2"
4) "5"
5) "6"
6) "8"
7) "9"
</code></pre>

<p>除了直接对元素排序排序外，还可以通过BY操作来获取关联值来进行排序。BY参数的语法为“BY参考键”，其中参考键可以使字符串类型或者是散列类型键的某个字段（表示为键名->字段名）。如果提供了BY参数，sort命令将不再依据元素自身的值进行排序，而是对每个元素使用元素的值替换参考键中的第一个<code>*</code>并获取取值，然后依据该值对元素排序。</p>

<pre><code>sort tag:ruby:posts BY post:*-&gt;time desc
sort sortbylist BY itemsore:* desc
</code></pre>

<p>当参考键不包括<code>*</code>时（即常量键名，与元素值无关）。SORT命令将不会执行排序操作，因为redis认为这种情况没有意义（因为所有要比较的值都一样）。没有执行排序操作，在不需要排序但需要借组sort命令获得与元素相关联的数据时，常量键名是很有用的！</p>

<p>如果几个元素的参考键值相同，则SORT命令会在比较元素本身的值来决定元素的顺序。
当某个元素的参考键不存在时，会默认参考键的值为0。
参考键虽然支持散列类型，但是<code>*</code>只能在<code>-&gt;</code>符号前面（即键名部分）才有用，在<code>-&gt;</code>后（即字段名部分）会被当成字段名本身名本身而不会作为占位符被元素的值替换，即常量键名。但是实际运行时会发现一个有趣的结果。</p>

<pre><code>sort sortbylist BY somekey-&gt;somefield:* 
</code></pre>

<p>上面提到了当参考键名是常量键名时SORT命令将不会执行排序操作，然而上例中却是进行了排序，而且只是对元素本身进行排序。这是因为Redis判断参考键名是不是常量键名的方式是判断参考键名中是否包含<code>*</code>，而<code>somekey-&gt;somefield:*</code>中包含<code>*</code>所以不是常量键名。所以在排序的时刻Redis对每个元素都会读取键somekey中的<code>somefield:*</code>字段（<code>*</code>不会被替换）。无论能否获得其值，每个元素的参考键值是相同的，所以redis被按照元素本身的大小排序。</p>

<p>GET参考不影响排序，它的作用是使SORT命令的返回结果不在是元素自身的值。而是GET参数中指定的键值。GET参数的规则和BY参数一样，GET参数也支持字符串类型和散列类型的值，并使用<code>*</code>作为占位符。要实现在排序后直接返回ID对应的违章标题，可以这样写：</p>

<pre><code>127.0.0.1:6379&gt; lpush tag:ruby:posts 1 2 3
(integer) 3
127.0.0.1:6379&gt; hmset post:1 time 140801 name HelloWorld
OK
127.0.0.1:6379&gt; hmset post:2 time 140802 name HelloWorld2
OK
127.0.0.1:6379&gt; hmset post:3 time 140803 name HelloWorld3
OK
127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc
1) "3"
2) "2"
3) "1"
127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time DESC GET post:*-&gt;name
1) "HelloWorld3"
2) "HelloWorld2"
3) "HelloWorld"
</code></pre>

<p>一个sort命令中可以有多个GET参数（而BY参数只能有一个），所以还可以这样用：</p>

<pre><code>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc GET post:*-&gt;name GET post:*-&gt;time
1) "HelloWorld3"
2) "140803"
3) "HelloWorld2"
4) "140802"
5) "HelloWorld"
6) "140801"
</code></pre>

<p>如果还需要返回文章ID，可以使用<code>GET #</code>获得，也就是返回元素本身的值。</p>

<pre><code>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc GET post:*-&gt;name GET post:*-&gt;time GET #
1) "HelloWorld3"
2) "140803"
3) "3"
4) "HelloWorld2"
5) "140802"
6) "2"
7) "HelloWorld"
8) "140801"
9) "1"
</code></pre>

<p>默认情况下SORT会直接返回排序结果，如果希望保存排序结果，可以使用STORE参数。保存后的键的类型为列表类型，如果键已经存在则会覆盖它，加上STORE参数后的SORT命令的返回值的结果的个数。</p>

<pre><code>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc GET post:*-&gt;name GET post:*-&gt;time GET # STORE tag.ruby.posts.sort
(integer) 9
127.0.0.1:6379&gt; lrange tag.ruby.posts.sort 0 -1
1) "HelloWorld3"
2) "140803"
3) "3"
4) "HelloWorld2"
5) "140802"
6) "2"
7) "HelloWorld"
8) "140801"
9) "1"
</code></pre>

<p>SORT命令的时间复杂度是O(n+mlogm)，其中n表示要排序的列表（集合或有序集合）中的元素个数，m表示要返回的元素个数。当n较大时SORT命令的性能相对较低，并且redis在排序前会建立一个长度为n的容器来存储排序的元素（当键类型为有序集合且参考键为常量键名时容器大小为m而不是n），虽然是一个临时的过程，但如果同时进行较多的大数据量排序操作则会严重影响性能。</p>

<h3>消息通知</h3>

<p>producer/consumer，松耦合，易于扩展，而且可以分布在不同的服务器中！</p>

<pre><code>BLPOP key [key ...] timeout
BRPOP key [key ...] timeoutseconds
# 超时时间设置为0时，表示不限制等待的时间，即如果没有新元素加入列表就会永远阻塞下去。
</code></pre>

<p>BRPOP可以同时接收多个键，同时检测多个键，如果所有键都没有元素则阻塞，其中有一个键有元素则会从该键中弹出元素。如果存在键都有元素则从左到右的顺序取第一个键中的一个元素。借此特性可以实现优先级的队列任务。</p>

<p>publish/subscribe模式，发布/订阅模式同样可以实现进程间的消息传递。</p>

<pre><code>PUBLISH channel.1 hi
SUBSCRIBE channel.1
</code></pre>

<p>执行SUBSCRIBE命令后，客户端会进入订阅状态，处于此状态下客户端不能使用SUBSCRIBE/UNSUBSCRIBE/PSUBSCRIBE（支持glob风格通配符格式）/PUNSUBSCRIBE这4个属于发布/订阅模式的命令之外的命令，否则会报错。</p>

<p>消息类型： subscribe/message/unsubscribe</p>

<pre><code>psubscribe channel.?*
</code></pre>

<h3>管道pipelining</h3>

<p>在执行多个命令时每条命令都需要等待上一条命令执行完才能执行，即使命令不需要上一条命令的执行结果。通过管道可以一次性发送多条命令并在执行完后一次性将结果返回，当一组命令中每条命令都不依赖与之前命令的执行结果就可以将这一组命令一起通过管道发出。管道通过减少客户端与redis的通信次数来实现降低往返时延。（</p>

<h3>节省空间</h3>

<ul>
<li>精简键名和键值 <code>VIP&lt;-very.important.person</code></li>
<li>内部编码优化（存储和效率的取舍）</li>
</ul>


<p>如果想查看一个键的内部编码方式可以使用<code>OBJECT ENCODING foo</code></p>

<h2>第五章 实践</h2>

<ul>
<li>php用户登录，忘记密码邮件发送队列</li>
<li>ruby自动完成</li>
<li>python在线好友</li>
<li>nodejs的IP段地址查询</li>
</ul>


<h2>第六章 脚本</h2>

<p>代码块多次请求，以及事务竞态等问题，需要用到WATCH，多次请求在网络传输上浪费很多时间。redis的脚本类似于数据库的function，在服务端执行。这种方式不仅代码简单、没有竞态条件（redis的命令都是原子的），而且减少了通过网络发送和接收命令的传输开销。</p>

<p>从2.6开始，允许开发者使用Lua语言编写脚本传到redis中执行。在Lua脚本中可以调用大部分的redis命令。减少网络传输时延，原子操作，复用（发送的脚本永久存储在redis中，其他客户端可以复用）。</p>

<p><strong>访问频率</strong></p>

<pre><code>localtimes=redis.call('incr', KEYS[1])
if times==1 then
redis.call('expire', KEYS[1], ARGV[1])
end

if times&gt;tonumber(ARGV[2]) then
return 0
end

return 1
# redis-cli --eval ratelimiting.lua rate.limiting:127.0.0.1 , 10 3 逗号前的是键，后面的是参数
</code></pre>

<h3>lua语法（和shell脚本有点像，更简洁）</h3>

<pre><code>本地变量 local x=10
注释 --xxx
多行注释 --[[xxxx]]
赋值 local a,b=1,2 # a=1, b=2
   local a={1,2,3}
   a[1]=5
数字操作符的操作数如果是字符串会自动转成数字
tonumber
tostring
只要操作数不是nil或者false，逻辑操作符就认为操作数为真，否则为假！
用..来实现字符串连接
取长度 print(#"hello") -- 5
</code></pre>

<h3>使用脚本</h3>

<pre><code>EVAL script numkeys key [key ...] arg [arg ...]

redis&gt; eval "return redis.call('SET', KEYS[1], ARGV[1])" 1 foo bar

EVALSHA sha1 numkeys key [key ...] arg [arg ...]
</code></pre>

<p>同时获取多个散列类型键的键值</p>

<pre><code>local result={}
for i,v in ipairs(KEYS) do
result[i]=redis.call("HGETALL", v)
end
return result
</code></pre>

<p>获取并删除有序集合中分数最小的元素</p>

<pre><code>local element=redis.call("ZRANGE", KEY[1], 0, 0)[1]
if element the
redis.call('ZREM', KEYS[1], element)
end
return element
</code></pre>

<p>处理JSON</p>

<pre><code>local sum=0
local users=redis.call('mget', unpack(KEYS))
for _,user in ipairs(users) do 
local courses=cjson.decode(user).course
for _,score in pairs(courses) do
sum=sum+score
end
end
return sum
</code></pre>

<p>redis脚本禁用使用lua标准库中与文件或系统调用相关的函数，在脚本中只允许对redis的数据进行处理。并且redis还通过禁用脚本的全局变量的方式保证每个脚本都是相对隔离的们不会互相干扰。
使用沙盒不仅是为了保证服务器的安全性，而且还确保了脚本的执行结果值和脚本本身和执行时传递的参数有关，不依赖外界条件（如系统时间、系统中某个文件的内存。。）。这是因为在执行复制和AOF持久化操作时记录的是脚本的内容而不是脚本调用的命令，所以必须保证在脚本内容和参数一样的前提下脚本的执行进行特殊的处理。</p>

<pre><code>script load 'return 1'
script exists sha1
script flush #清空脚本缓冲

script kill
script nosave
</code></pre>

<p>为了限制某个脚本执行时间过长导致redis无法提供服务（如死循环），redis提供了lua-time-limit参数限制脚本的最长运行时间，默认5s。</p>

<h2>第七章 管理</h2>

<ul>
<li>持久化 rdb/AOF
```
save 900 1
save 300 10
save 60 10000
SAVE
BGSAVE
appendonly yes
appendfilename appendonly.aof
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
BGREWRITEAOF

<h1>appendfsync always</h1>

appendfsync everysec

<h1>appendfsync no</h1>

<p>```</p></li>
<li>复制
<code>
redis-server --port 6380 --slaveof 127.0.0.1 6379
SLAVEOF 127.0.0.1 6379
SLAVEOF NO ONE
</code></li>
<li>读写分离</li>
<li>耗时日志查询
<code>
SLOWLOG GET # slowlog-log-slower-than slowlog-max-len
MONITOR
</code></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
