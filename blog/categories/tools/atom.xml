<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tools | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/tools/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2016-06-30T11:48:50+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[RPM打包]]></title>
    <link href="http://winseliu.com/blog/2016/04/04/rpm-build-your-package/"/>
    <updated>2016-04-04T16:07:21+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/04/rpm-build-your-package</id>
    <content type="html"><![CDATA[<h2>资料</h2>

<ul>
<li><a href="http://www.rpm.org/max-rpm-snapshot/rpmbuild.8.html">http://www.rpm.org/max-rpm-snapshot/rpmbuild.8.html</a></li>
<li><a href="https://fedoraproject.org/wiki/How_to_create_an_RPM_package/zh-cn">https://fedoraproject.org/wiki/How_to_create_an_RPM_package/zh-cn</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/management/package/rpm/part1/index.html">用 RPM 打包软件-打包教程</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/management/package/rpm/part3/index.html">用 RPM 打包软件-高级部分：安装前后控制</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/l-rpm/index.html">RPM 打包技术与典型 SPEC 文件分析-各变量含义</a></li>
<li><a href="http://hlee.iteye.com/blog/343499">http://hlee.iteye.com/blog/343499</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/management/package/rpm/part1/indent-2.spec">案例</a></li>
<li><p><a href="https://github.com/apache/zookeeper/tree/release-3.4.8/src/packages">zookeeper打包案例</a></p></li>
<li><p><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-checkinstall/index.html">http://www.ibm.com/developerworks/cn/linux/l-cn-checkinstall/index.html</a></p></li>
</ul>


<h2>实践</h2>

<ul>
<li>系统配置准备</li>
</ul>


<pre><code># 新建一个docker实例，来测试、学习
[root@cu1 ~]# docker run -ti centos:centos6 /bin/bash

[root@bdc25400cc63 mywget]# cat /etc/redhat-release 
CentOS release 6.6 (Final)

# 安装编译环境所需的软件
yum install which tree lrzsz tar gcc rpm-build
# wget编译的依赖
yum install -y gnutls gnutls-devel
</code></pre>

<ul>
<li>步骤</li>
</ul>


<pre><code>[root@bdc25400cc63 home]# mkdir mywget 
[root@bdc25400cc63 home]# cd mywget/
[root@bdc25400cc63 mywget]# mkdir BUILD RPMS SOURCES SPECS SRPMS
[root@bdc25400cc63 mywget]# cd SOURCES/
[root@bdc25400cc63 SOURCES]# mv /home/wget-1.17.tar.gz .
[root@bdc25400cc63 SOURCES]# ls
wget-1.17.tar.gz
[root@bdc25400cc63 SOURCES]# cd ..

[root@bdc25400cc63 mywget]# rpmbuild --showrc
[test@bdc25400cc63 mywget]$ rpm --eval "%{_topdir}"

[test@bdc25400cc63 mywget]$ grep -i _topdir /usr/lib/rpm/rpmrc /usr/lib/rpm/redhat/rpmrc /usr/lib/rpm/macros /usr/lib/rpm/redhat/macros  | less
/usr/lib/rpm/macros:%_builddir          %{_topdir}/BUILD
/usr/lib/rpm/macros:%_rpmdir            %{_topdir}/RPMS
/usr/lib/rpm/macros:%_sourcedir         %{_topdir}/SOURCES
/usr/lib/rpm/macros:%_specdir           %{_topdir}/SPECS
/usr/lib/rpm/macros:%_srcrpmdir         %{_topdir}/SRPMS
/usr/lib/rpm/macros:%_buildrootdir              %{_topdir}/BUILDROOT
/usr/lib/rpm/macros:%_topdir            %{getenv:HOME}/rpmbuild

[test@bdc25400cc63 mywget]$ cat ~/.rpmmacros 
%_topdir /home/mywget/rpm

# 2016-5-12 15:28:35
# spec里面有define和global，应该是这个导致的！用global应该即可以了？

[root@bdc25400cc63 mywget]# vi SPECS/wget.spec
  # this is a sample spec file for wget

  %define _topdir /home/mywget
  %define name    wget
  %define release 2
  %define version 1.17
  # 定义 _buildrootdir 不起作用，不知道为啥??? 在 .rpmmacros 定义了 %_topdir，root转到 /home/mywget/rpm/BUILDROOT 了。

  %define _unpackaged_files_terminate_build 0

  Summary:   GNU wget
  License:   GPL
  Name:      %{name}
  Version:   %{version}
  Release:   %{release}
  Source:    %{name}-%{version}.tar.gz
  Prefix:    /usr/local/wget
  Group:     Development/Tools

  %description
  The GNU wget program downloads files from the Internet using the command-line.

  %prep
  %setup -q

  %build
  ./configure
  make

  %install
  make install prefix=$RPM_BUILD_ROOT/usr/local/wget # or use DESTDIR=$RPM_BUILD_ROOT

  %post
  echo "hello world"

  %preun
  echo "bye"

  %clean
  rm -rf $RPM_BUILD_ROOT

  %files
  %defattr(-, root, root)
  /usr/local/wget/bin/wget

[root@bdc25400cc63 mywget]# rpmbuild -vv -bb --clean SPECS/wget.spec 

[root@bdc25400cc63 mywget]# tree .
.
├── BUILD
├── RPMS
│   └── x86_64
│       ├── wget-1.17-2.x86_64.rpm
│       └── wget-debuginfo-1.17-2.x86_64.rpm
├── SOURCES
│   └── wget-1.17.tar.gz
├── SPECS
│   └── wget.spec
└── SRPMS

6 directories, 4 files

[root@bdc25400cc63 mywget]# rpm -qpl RPMS/x86_64/wget-1.17-2.x86_64.rpm  
/usr/local/wget/bin/wget
</code></pre>

<p>接下来就可以直接拿到这个包到其他机器上安装了，如果自己建立了本地库，使用createrepo更新下，就可以使用yum安装最新打的包了。</p>

<p>注： <code>%pre</code> , <code>%post</code> 和 <code>%preun</code> , <code>%postun</code> 可以在安装前后执行一些脚本。</p>

<pre><code>[root@cu2 ganglia-build]# mkdir BUILD RPMS SOURCES SPECS SRPMS
[root@cu2 ganglia-build]# cd SOURCES/
[root@cu2 SOURCES]# ll
total 1272
-rw-r--r-- 1 root root 1302320 Jan 20 09:35 ganglia-3.7.2.tar.gz
[root@cu2 SOURCES]# cd ..

[root@cu2 ganglia-build]# ll
total 20
drwxr-xr-x 2 root root 4096 Jun 15 10:25 BUILD
drwxr-xr-x 2 root root 4096 Jun 15 10:25 RPMS
drwxr-xr-x 2 root root 4096 Jun 15 10:25 SOURCES
drwxr-xr-x 2 root root 4096 Jun 15 10:25 SPECS
drwxr-xr-x 2 root root 4096 Jun 15 10:25 SRPMS

[root@cu2 ganglia-build]# cd SPECS/
[root@cu2 SPECS]# vi gmetad.spec

[root@cu2 ganglia-build]# rpmbuild --clean -v -ba SPECS/gmetad.spec 

[root@cu2 ganglia-build]# rpm -qpl RPMS/x86_64/ganglia-3.7.2-1.el6.x86_64.rpm 
</code></pre>

<h2>重新打包已有rpm</h2>

<p>下载源码包，再修改内容，最后使用rpm-build重新打包。</p>

<p>这里以puppetserver为例，使用jdk7即可但官网打包的依赖是jdk8，这里修改依赖然后重新打包：</p>

<pre><code>[root@cu2 rpmbuild]# rpm -ivh puppetserver-2.3.1-1.el6.src.rpm 
warning: puppetserver-2.3.1-1.el6.src.rpm: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY
   1:puppetserver           warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
########################################### [100%]
warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
[root@cu2 rpmbuild]# ll
total 32904
-rw-r--r-- 1 root root 33681889 May 10 17:44 puppetserver-2.3.1-1.el6.src.rpm
drwxr-xr-x 2 root root     4096 May 10 17:55 SOURCES
drwxr-xr-x 2 root root     4096 May 10 17:55 SPECS

#-- 注释掉jdk8的部分
[root@cu2 rpmbuild]# grep -3 jdk SPECS/puppetserver.spec 

# java 1.8.0 is available starting in fedora 20 and el 6
#%if 0%{?fedora} &gt;= 20 || 0%{?rhel} &gt;= 6
#%global open_jdk          java-1.8.0-openjdk-headless
#%else
%global open_jdk          java-1.7.0-openjdk
#%endif

[root@cu2 rpmbuild]# yum install -y ruby
[root@cu2 rpmbuild]# rpmbuild -v -bb --clean SPECS/puppetserver.spec 

[root@cu2 rpmbuild]# yum deplist RPMS/noarch/puppetserver-2.3.1-1.el6.noarch.rpm 
Loaded plugins: fastestmirror, priorities
Finding dependencies: 
Loading mirror speeds from cached hostfile
 * base: centos.ustc.edu.cn
 * centosplus: centos.ustc.edu.cn
 * epel: mirror01.idc.hinet.net
 * extras: centos.ustc.edu.cn
 * updates: centos.ustc.edu.cn
193 packages excluded due to repository priority protections
package: puppetserver.noarch 2.3.1-1.el6
  dependency: chkconfig
   provider: chkconfig.x86_64 1.3.49.3-5.el6
   provider: chkconfig.x86_64 1.3.49.3-5.el6_7.2
  dependency: /bin/bash
   provider: bash.x86_64 4.1.2-33.el6
   provider: bash.x86_64 4.1.2-33.el6_7.1
  dependency: java-1.7.0-openjdk
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.79-2.5.5.4.el6
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.101-2.6.6.1.el6_7
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.85-2.6.1.3.el6_6
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.85-2.6.1.3.el6_7
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.91-2.6.2.2.el6_7
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.95-2.6.4.0.el6_7
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.99-2.6.5.0.el6_7
  dependency: puppet-agent &gt;= 1.4.0
   provider: puppet-agent.x86_64 1.4.1-1.el6
  dependency: net-tools
   provider: net-tools.x86_64 1.60-110.el6_2
  dependency: /usr/bin/env
   provider: coreutils.x86_64 8.4-37.el6
   provider: coreutils.x86_64 8.4-37.el6_7.3
  dependency: /bin/sh
   provider: bash.x86_64 4.1.2-33.el6
   provider: bash.x86_64 4.1.2-33.el6_7.1
  dependency: config(puppetserver) = 2.3.1-1.el6
   provider: puppetserver.noarch 2.3.1-1.el6 
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装配置OpenVPN]]></title>
    <link href="http://winseliu.com/blog/2016/03/11/install-and-config-openvpn/"/>
    <updated>2016-03-11T09:46:49+08:00</updated>
    <id>http://winseliu.com/blog/2016/03/11/install-and-config-openvpn</id>
    <content type="html"><![CDATA[<p>由于测试环境搭建不在同一个网络，平时查看hadoop集群状态、提交任务都可以通过hadoop-master的外网来操作。但是要读写kafka，需要直接连通所有的节点，全部映射端口太麻烦。一开始想到了VLAN(虚拟局域网），远远超出能力范围。最后通过搭架VPN来实现与测试环境的透明访问。</p>

<h2>使用集成版本</h2>

<p>参考 <a href="https://linux.cn/article-4733-1.html">https://linux.cn/article-4733-1.html</a></p>

<pre><code># download https://openvpn.net/index.php/access-server/download-openvpn-as-sw.html

# 安装
[root@cu2 ~]# rpm -ivh openvpn-as-2.0.25-CentOS6.x86_64.rpm 
Preparing...                ########################################### [100%]
   1:openvpn-as             ########################################### [100%]
The Access Server has been successfully installed in /usr/local/openvpn_as
Configuration log file has been written to /usr/local/openvpn_as/init.log
Please enter "passwd openvpn" to set the initial
administrative password, then login as "openvpn" to continue
configuration here: https://192.168.0.214:943/admin
To reconfigure manually, use the /usr/local/openvpn_as/bin/ovpn-init tool.

Access Server web UIs are available here:
Admin  UI: https://192.168.0.214:943/admin
Client UI: https://192.168.0.214:943/

[root@cu2 ~]# passwd openvpn

然后通过web admin进行配置。如主机的信息、hostname以及监听绑定的IP
</code></pre>

<p>配置好以后，本地通过网页下载client程序安装。连接配置后：</p>

<pre><code>
C:\Users\winse&gt;tracert  cu3

通过最多 30 个跃点跟踪
到 cu3 [192.168.0.148] 的路由:

  1     2 ms     2 ms     2 ms  172.27.232.1
  2     2 ms     2 ms     2 ms  cu3 [192.168.0.148]

跟踪完成。


C:\Users\winse&gt;route print
===========================================================================
IPv4 路由表
===========================================================================
活动路由:
网络目标        网络掩码          网关       接口   跃点数
          0.0.0.0          0.0.0.0      192.168.1.1    192.168.1.102     20
          0.0.0.0        128.0.0.0     172.27.232.1     172.27.232.2     20
...
</code></pre>

<p><a href="http://designmylife.blog.163.com/blog/static/2067142542013527101659960/">http://designmylife.blog.163.com/blog/static/2067142542013527101659960/</a></p>

<p>路由匹配按最大(最亲)方式匹配。上面路由会先匹配mask为 <code>128.0.0.0</code> 的路由。最终把所有的流量经由VPN出去。</p>

<p>通过 <strong>Access Server</strong> 安装简单，配置通过网页来弄，和网上资料的都匹配不上，还有用户数量的限制，囧。</p>

<h2>编译源码安装</h2>

<ul>
<li>服务端安装配置</li>
</ul>


<pre><code>[root@cu2 openvpn-2.3.10]# yum install libpam*
[root@cu2 openvpn-2.3.10]# yum install pam-devel.x86_64

[root@cu2 ~]# rz
rz waiting to receive.
Starting zmodem transfer.  Press Ctrl+C to cancel.
Transferring lzo-2.06.tar.gz...
  100%     569 KB     569 KB/sec    00:00:01       0 Errors  

[root@cu2 ~]# tar zxvf lzo-2.06.tar.gz 
[root@cu2 ~]# cd lzo-2.06
[root@cu2 lzo-2.06]# ./configure 
[root@cu2 lzo-2.06]# make &amp;&amp;  make install

[root@cu2 openvpn-2.3.10]# ./configure --prefix=/usr/local/openvpn 
[root@cu2 openvpn-2.3.10]# make &amp;&amp; make install

[root@cu2 openvpn-2.3.10]# /usr/local/openvpn/sbin/openvpn --version
OpenVPN 2.3.10 x86_64-unknown-linux-gnu [SSL (OpenSSL)] [EPOLL] [MH] [IPv6] built on Mar  9 2016

https://github.com/OpenVPN/easy-rsa/releases

[root@cu2 EasyRSA-3.0.1]# ./easyrsa  help

[root@cu2 EasyRSA-3.0.1]# ./easyrsa init-pki
[root@cu2 EasyRSA-3.0.1]#  ./easyrsa build-ca

[root@cu2 EasyRSA-3.0.1]# ./easyrsa gen-req openvpn nopass
[root@cu2 EasyRSA-3.0.1]# ./easyrsa sign client openvpn

[root@cu2 EasyRSA-3.0.1]# ./easyrsa gen-req eshore-cu nopass
[root@cu2 EasyRSA-3.0.1]# ./easyrsa sign server eshore-cu

[root@cu2 EasyRSA-3.0.1]# tree pki/
pki/
├── ca.crt
├── certs_by_serial
│   ├── 01.pem
│   └── 02.pem
├── index.txt
├── index.txt.attr
├── index.txt.attr.old
├── index.txt.old
├── issued
│   ├── eshore-cu.crt
│   └── openvpn.crt
├── private
│   ├── ca.key
│   ├── eshore-cu.key
│   └── openvpn.key
├── reqs
│   ├── eshore-cu.req
│   └── openvpn.req
├── serial
└── serial.old

[root@cu2 EasyRSA-3.0.1]#  ./easyrsa gen-dh
[root@cu2 EasyRSA-3.0.1]# cd pki
[root@cu2 pki]# cp ca.crt dh.pem issued/eshore-cu.crt private/eshore-cu.key /etc/openvpn/ 

[root@cu2 openvpn-2.3.10]# cp sample/sample-config-files/server.conf /etc/openvpn/

    proto tcp
    cert eshore-cu.crt
    key eshore-cu.key 
    dh dh.pem
    # 在客户端额外添加这条路由到VPN
    push "route 192.168.0.0 255.255.255.0"
    # 和AS一样，会添加0.0.0.0到VPN的路由。默认走VPN
    ;push "redirect-gateway def1 bypass-dhcp"
    user nobody
    group nobody

[root@cu2 pki]# cd /etc/openvpn/
[root@cu2 openvpn]# /usr/local/openvpn/sbin/openvpn --config /etc/openvpn/server.conf 
[root@cu2 openvpn]# /usr/local/openvpn/sbin/openvpn --daemon --config server.conf 
</code></pre>

<p></p>

<ul>
<li>安装客户端：</li>
</ul>


<p><a href="https://openvpn.net/index.php/open-source/downloads.html">https://openvpn.net/index.php/open-source/downloads.html</a> 下载安装对应的版本。</p>

<p>拷贝sample-config/client.ovpn和服务端的ca.crt、openvpn.crt、openvpn.key到config目录下面。</p>

<p>修改client.ovpn:</p>

<pre><code>proto tcp
remote webcu2 1194
cert openvpn.crt
key openvpn.key
</code></pre>

<p>然后启动 <strong>OpenVPN GUI</strong> ，右键connect就行了。</p>

<pre><code>$ route print
...
IPv4 路由表
===========================================================================
活动路由:
网络目标        网络掩码          网关       接口   跃点数
          0.0.0.0          0.0.0.0      192.168.1.1    192.168.1.102     20
         10.8.0.1  255.255.255.255         10.8.0.5         10.8.0.6     20
         10.8.0.4  255.255.255.252            在链路上          10.8.0.6    276
         10.8.0.6  255.255.255.255            在链路上          10.8.0.6    276
         10.8.0.7  255.255.255.255            在链路上          10.8.0.6    276
      192.168.0.0    255.255.255.0         10.8.0.5         10.8.0.6     20
...
</code></pre>

<h2>问题</h2>

<ul>
<li>连接到VPN服务端的机器是没有问题，但是不能访问该机器的应用（端口不同）</li>
</ul>


<p>被防火墙限制了，在服务端把10.8.0.0/24加入到防火墙允许。</p>

<pre><code>iptables -A INPUT -s 10.8.0.0/24 -j ACCEPT 
</code></pre>

<ul>
<li>不能访问服务端其他机器</li>
</ul>


<p>在iptables上增加转发</p>

<pre><code>iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE
</code></pre>

<p>查看iptables规则：</p>

<pre><code>iptables -nL -t nat
</code></pre>

<p>测试下:</p>

<pre><code>$ ping cu3

正在 Ping cu3 [192.168.0.148] 具有 32 字节的数据:
来自 192.168.0.148 的回复: 字节=32 时间=5ms TTL=63
来自 192.168.0.148 的回复: 字节=32 时间=5ms TTL=63
</code></pre>

<p>其他（参数，未实践，记录下来）</p>

<blockquote><p>必须在服务器端的内网网关上将到10.8.0.0/24网段的路由指向到openvpn服务器，不然从服务器端内网其他机器根本找不到去往10.8.0.0/24网段的路由。这里又分两种情况，一种是服务端有内网网关设备的（按如上说法即可）；一种是服务端内网没有网关设备，即服务器通过交换机相连，相互通讯靠广播的情况。我的就是这种情况。需要在想访问的server上增加到10.8.0.0/24的路由，如下</p>

<p>route add -net 10.8.0.0/24 gw 192.168.1.211    #1.211为openvpn服务器的内网IP</p>

<p>Make sure that you&rsquo;ve enabled IP and TUN/TAP forwarding on the OpenVPN server machine.
确定开启了转发功能，然后在openvpn服务器Iptables添加如下两条规则</p>

<p>iptables -A FORWARD -i tun0 -s 10.8.0.0/24 -j ACCEPT    #简单说，允许数据从客户端到后端server
iptables -A FORWARD -i em2 -d 10.8.0.0/24 -j ACCEPT    #允许数据从后端server到客户端</p></blockquote>

<h2>参考</h2>

<ul>
<li><a href="https://openvpn.net/index.php/open-source/documentation/howto.html">https://openvpn.net/index.php/open-source/documentation/howto.html</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_86fbdd650101a0ax.html">http://blog.sina.com.cn/s/blog_86fbdd650101a0ax.html</a></li>
<li><a href="http://www.linuxquestions.org/questions/linux-networking-3/openvpn-conencts-but-can%27t-ping-servers-on-the-other-network-660610/">http://www.linuxquestions.org/questions/linux-networking-3/openvpn-conencts-but-can%27t-ping-servers-on-the-other-network-660610/</a></li>
<li><a href="http://www.ilanni.com/?p=9877">http://www.ilanni.com/?p=9877</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_86fbdd650101a0ax.html">http://blog.sina.com.cn/s/blog_86fbdd650101a0ax.html</a></li>
<li><a href="http://kaifly.blog.51cto.com/3209616/1367591">http://kaifly.blog.51cto.com/3209616/1367591</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rsync与scp优势]]></title>
    <link href="http://winseliu.com/blog/2016/03/07/rsync-vs-scp/"/>
    <updated>2016-03-07T17:05:45+08:00</updated>
    <id>http://winseliu.com/blog/2016/03/07/rsync-vs-scp</id>
    <content type="html"><![CDATA[<p>今天在做flume写kafka数据时，数据从其他目录cp拷贝过来，flume采集程序报错 <strong>程序采集的时刻文件发生了改变</strong>。</p>

<pre><code>07 Mar 2016 16:46:05,535 ERROR [pool-3-thread-1] (org.apache.flume.source.SpoolDirectorySource$SpoolDirectoryRunnable.run:256)  - FATAL: Spool Directory source s1: { spoolDir: /home/hadoop/flume/data/ }: Uncaught exception in SpoolDirectorySource thread. Restart or reconfigure Flume to continue processing.
java.lang.IllegalStateException: File has changed size since being read: /home/hadoop/flume/data/hbase-hadoop-master-cu2.log
        at org.apache.flume.client.avro.ReliableSpoolingFileEventReader.retireCurrentFile(ReliableSpoolingFileEventReader.java:326)
        at org.apache.flume.client.avro.ReliableSpoolingFileEventReader.readEvents(ReliableSpoolingFileEventReader.java:259)
</code></pre>

<p>联想到scp和rsync，好像rsync是有重命名这样的步骤的。网上也有很多对比这个两个工具的资料。</p>

<ul>
<li><a href="http://stackoverflow.com/questions/20244585/how-does-scp-differ-from-rsync">http://stackoverflow.com/questions/20244585/how-does-scp-differ-from-rsync</a></li>
<li><p><a href="http://superuser.com/questions/193952/why-is-rsync-avz-faster-than-scp-r">http://superuser.com/questions/193952/why-is-rsync-avz-faster-than-scp-r</a></p></li>
<li><p>rsync可以增量复制，并且只复制内容不同的部分</p></li>
<li>rsync可以压缩，通过有断点续传 <code>-P</code></li>
<li>rsync有各种参数： exclude等</li>
<li>SCP也可以增加压缩参数： <code>scp -C -o 'CompressionLevel 9' -o 'IPQoS throughput'  -c arcfour machine:file .</code></li>
<li>rsync会先写临时文件，复制完成后再重命名！</li>
</ul>


<p>这里只关注最后一点，对于按照名称来采集的程序非常关键！下面使用inotify监控目录的操作，在进行scp和rsync时发生的操作：</p>

<pre><code>[hadoop@cu2 test]$ scp -r source target/
[hadoop@cu2 test]$ rm target/source/1234
[hadoop@cu2 test]$ rsync -vaz source target/
sending incremental file list
source/
source/1234

sent 141 bytes  received 35 bytes  352.00 bytes/sec
total size is 34  speedup is 0.19
</code></pre>

<p>对应的inotify的输出为：</p>

<pre><code>[hadoop@cu2 test]$ inotifywait -m target/source/
Setting up watches.
Watches established.
target/source/ CREATE 1234
target/source/ OPEN 1234
target/source/ MODIFY 1234
target/source/ CLOSE_WRITE,CLOSE 1234

target/source/ DELETE 1234

target/source/ ATTRIB,ISDIR 
target/source/ CREATE .1234.ARUg56
target/source/ OPEN .1234.ARUg56
target/source/ ATTRIB .1234.ARUg56
target/source/ MODIFY .1234.ARUg56
target/source/ CLOSE_WRITE,CLOSE .1234.ARUg56
target/source/ ATTRIB .1234.ARUg56
target/source/ MOVED_FROM .1234.ARUg56
target/source/ MOVED_TO 1234
</code></pre>

<p>rsync会先写把内容复制到一个临时文件，复制完成后，再重命名为正式的名称。</p>

<p><strong>在生产环境尽量使用rsync来进行文件(夹)的复制/同步操作，即快键有安全。</strong></p>

<p>当然还有奇葩的快速删除海量文件夹的方式也用的是rsync：</p>

<pre><code>rsync --delete-before -d /data/blank/ /var/spool/clientmqueue/ 

rsync --delete-before -a -H -v --progress --stats /tmp/test/ log/
</code></pre>

<ul>
<li><a href="http://logo32.iteye.com/blog/1564727">http://logo32.iteye.com/blog/1564727</a></li>
<li><a href="http://www.ha97.com/4107.html">http://www.ha97.com/4107.html</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pdsh]]></title>
    <link href="http://winseliu.com/blog/2016/01/25/pdsh-simple-usage/"/>
    <updated>2016-01-25T19:50:35+08:00</updated>
    <id>http://winseliu.com/blog/2016/01/25/pdsh-simple-usage</id>
    <content type="html"><![CDATA[<p>弄hadoop总是需要折腾不少机器，单单执行 <code>rsync</code> 就挺折腾人的，有时还要排除部分机器来查看一堆机器使用内存情况，等等。以前都使用 <code>expect</code> 结合 <code>for in</code> 来实现，总归简单用着也觉得还行。</p>

<p>但是最近，升级hadoop、tez、安装ganglia被折腾的不行。复制 <code>for</code> 语句到累，原来看过 <code>pdsh</code> 的介绍，不过原来就部署4-5台机器，最近查找Ganglia安装问题的博文里面再次 <code>pdsh</code> ，觉得非常亲切和简洁。再次安装使用也就有了本文。</p>

<h2>安装</h2>

<pre><code>[root@bigdatamgr1 pdsh-2.29]# umask 0022
[root@bigdatamgr1 pdsh-2.29]# ./configure -h
[root@bigdatamgr1 pdsh-2.29]# ./configure --with-dshgroups  --with-exec --with-ssh 
[root@bigdatamgr1 pdsh-2.29]# make &amp;&amp; make install
</code></pre>

<p>挺多选项的，用 <code>disgroups</code> 加上 <code>ssh</code> 差不多够用了，以后不够用的时刻再慢慢研究这些选项。</p>

<h2>简单使用</h2>

<p>使用pdsh管理机器的前提是已经建立了到目标机器的SSH无密钥登录，而建立这N台机器的无秘钥登录还是少不了 <code>expect</code> (当然你愿意一个个输入yes和密码也是OK的)！</p>

<ul>
<li>加载的模块</li>
</ul>


<pre><code># 查看，安装的ssh/exec
[eshore@bigdatamgr1 ~]$ pdsh -L

# 设置默认使用的模块
[eshore@bigdatamgr1 ~]$ export PDSH_RCMD_TYPE=exec
[eshore@bigdatamgr1 ~]$ pdsh -w bigdata[1-2] ssh %h hostname
bigdata2: bigdata2
bigdata1: bigdata1

# 命令行指定模块
[eshore@bigdatamgr1 ~]$ pdsh -R ssh -w bigdata1,bigdata2 hostname
bigdata2: bigdata2
bigdata1: bigdata1

# 一个个的指定
[eshore@bigdatamgr1 ~]$ pdsh -w ssh:bigdata1,ssh:bigdata2 hostname
bigdata2: bigdata2
bigdata1: bigdata1
[eshore@bigdatamgr1 ~]$ pdsh -w ssh:bigdata[1,2] hostname
bigdata2: bigdata2
bigdata1: bigdata1
</code></pre>

<ul>
<li>主机加载</li>
</ul>


<pre><code>[eshore@bigdatamgr1 ~]$ pdsh -w bigdata[1-2,5,6-8] -X nodes hostname
bigdata5: bigdata5
bigdata6: bigdata6
bigdata2: bigdata2
bigdata8: bigdata8
bigdata7: bigdata7
</code></pre>

<p>pdsh除了使用 <code>-w</code> 来指定主机列表，还可以通过文件来指定，如编译时的 <code>--with-machines</code> ，同时可以通过读取默认的位置的文件来获取。在编译pdsh时可以通过 <code>--with-dshgroups</code> 参数来激活此选项，默认可以将一组主机列表写入一个文件中并放到本地主机的 <code>~/.dsh/group</code> 或 <code>/etc/dsh/group</code> 目录下，这样就可以通过 <code>-g</code> 参数调用了。同时 <code>-X groupname</code> 可以用来排除主机列表中属于groupname组的主机（下面会提到group分组）。</p>

<pre><code>[eshore@bigdatamgr1 ~]$ export PDSH_RCMD_TYPE=ssh

[eshore@bigdatamgr1 ~]$ mkdir -p .dsh/group
[eshore@bigdatamgr1 ~]$ cd .dsh/group/
[eshore@bigdatamgr1 group]$ vi nodes
bigdata1
bigdata3

[eshore@bigdatamgr1 ~]$ pdsh -g nodes hostname
bigdata3: bigdata3
bigdata1: bigdata1

[eshore@bigdatamgr1 ~]$ pdsh -w bigdata[1-8] -X nodes hostname
bigdata2: bigdata2
bigdata8: bigdata8
bigdata5: bigdata5
bigdata6: bigdata6
bigdata4: bigdata4
bigdata7: bigdata7
</code></pre>

<p><code>-w</code> 参数也可以用来读取特定文件中的主机列表，同时结合其他规则和进行过滤（具体查看man帮助）。<code>-x</code> 在主机列表基础上进行过滤（提供多一种的方式来实现过滤）。</p>

<pre><code>[eshore@bigdatamgr1 ~]$ cat slaves | head -2
bigdata1
bigdata2

[eshore@bigdatamgr1 ~]$ pdsh -w ^slaves hostname | head -5
bigdata8: bigdata8
bigdata6: bigdata6
bigdata5: bigdata5
bigdata2: bigdata2
bigdata3: bigdata3

[eshore@bigdatamgr1 ~]$ pdsh -w ^slaves,-bigdata[2-8]
pdsh&gt; hostname
bigdata1: bigdata1
pdsh&gt; 
pdsh&gt; exit
[eshore@bigdatamgr1 ~]$ pdsh -w ^slaves,-/bigdata.?/
pdsh@bigdatamgr1: no remote hosts specified

[eshore@bigdatamgr1 ~]$ pdsh -w ^slaves -x bigdata[1-7] hostname
bigdata8: bigdata8
</code></pre>

<ul>
<li>输出格式化</li>
</ul>


<p>当一台主机的输出多余一行时，pdsh输出的内容看起来并不和谐。使用dshbak格式化</p>

<pre><code>[eshore@bigdatamgr1 ~]$ pdsh -w bigdata[1-2] free -m  | dshbak -c
----------------
bigdata1
----------------
             total       used       free     shared    buffers     cached
Mem:         64405      59207       5198          0        429      31356
-/+ buffers/cache:      27420      36985
Swap:        65535         57      65478
----------------
bigdata2
----------------
             total       used       free     shared    buffers     cached
Mem:         64405      58192       6213          0        505      29847
-/+ buffers/cache:      27838      36566
Swap:        65535         58      65477
</code></pre>

<h2>批量SSH无密钥登录</h2>

<pre><code>[hadoop@hadoop-master4 ~]$ cat ssh-copy-id.expect 
#!/usr/bin/expect  

## Usage $0 [user@]host password

set host [lrange $argv 0 0];
set password [lrange $argv 1 1] ;

set timeout 30;

spawn ssh-copy-id $host ;

expect {
  "(yes/no)?" { send yes\n; exp_continue; }
  "password:" { send $password\n; exp_continue; }
}

exec sleep 1;

[hadoop@hadoop-master4 ~]$ pdsh -w ^slaves ./ssh-copy-id.expect %h 'PASSWD'

# 验证是否全部成功
[hadoop@hadoop-master4 ~]# pdsh -w ^slaves -x hadoop-slaver[1-16] -R ssh hostname
</code></pre>

<h2>参考</h2>

<pre><code>pdsh -w ssh:user00[1-10] "date"
此命令用于在user001到user0010上执行date命令。
pdsh -w ssh:user0[10-31],/1$/ "uptime"
此命令在选择远程主机时使用了正则表达式，表示在user010到user031中选择以1结尾的主机名，即在user011、user021、user031上执行uptime命令

-l  指定在远程主机上使用的用户名称。例如：
pdsh -R ssh -l opsuser -w user00[1-9] "date"

-f  设置同时连接到远程主机的个数
</code></pre>

<ul>
<li><a href="http://ixdba.blog.51cto.com/2895551/1550184">并行分布式运维工具pdsh</a></li>
</ul>


<pre><code>Some quick tips on how to get started using pdsh:
Set up your environment:
export PDSH_SSH_ARGS_APPEND=”-o ConnectTimeout=5 -o CheckHostIP=no -o StrictHostKeyChecking=no” (Add this to your .bashrc to save time.)
</code></pre>

<ul>
<li><a href="https://radfest.wordpress.com/2012/05/24/parallel-remote-shelling-via-pdsh/">Parallel remote &ldquo;shelling&rdquo; via pdsh</a></li>
<li><a href="http://kumu-linux.github.io/blog/2013/06/19/pdsh/">Pdsh使用方法</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装配置Ganglia(2)]]></title>
    <link href="http://winseliu.com/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/"/>
    <updated>2016-01-23T17:47:28+08:00</updated>
    <id>http://winseliu.com/blog/2016/01/23/install-and-config-ganglia-on-redhat-2</id>
    <content type="html"><![CDATA[<p>前一篇介绍了全部手工安装Ganglia的文章，当时安装测试的环境比较简单。按照网上的步骤安装好，看到图了以为就懂了。Ganglia的基本多播/单播的概念都没弄懂。</p>

<p>这次有机会把Ganglia安装到正式环境，由于网络复杂一些，遇到新的问题。也更进一步的了解了Ganglia。</p>

<p>后端Gmetad(ganglia meta daemon)和Gmond(ganglia monitoring daemon)是Ganglia的两个组件。</p>

<p>Gmetad负责收集各个cluster的数据，并更新到rrd数据库中；Gmond把本机的数据UDP广播（或者单播给某台机），同时收集集群节点的数据供Gmetad读取。Gmetad并不用于监控数据的汇总，是对已经采集好的全部数据处理并存储到rrdtool数据库。</p>

<h2>搭建yum环境</h2>

<p>由于正式环境没有提供外网环境，所以需要把安装光盘拷贝到机器，作为yum的本地源。</p>

<pre><code>mount -t iso9660 -o loop rhel-server-6.4-x86_64-dvd\[ED2000.COM\].iso iso/
ln -s iso rhel6.4

vi /etc/yum.repos.d/rhel.repo 
[os]
name = Linux OS Packages
baseurl = file:///opt/rhel6.4
enabled=1
gpgcheck = 0
</code></pre>

<p>再极端点，yum程序都没有安装。到 Packages 目录用 rpm 安装 <code>yum*</code> 。</p>

<p>安装httpd后，把 rhel6.4 源建一个软链接到 <code>/var/www/html/rhel6.4</code> ，其他机器就可以使用该源来进行安装软件了。</p>

<pre><code>cat /etc/yum.repos.d/rhel.repo
[http]
name=LOCAL YUM server
baseurl = http://cu-omc1/rhel6.4
enabled=1
gpgcheck=0
</code></pre>

<p>注意：如果用CentOS的ISO会有两个光盘，两个地址用逗号分隔全部加到baseurl（http方式也一样）：</p>

<pre><code>[centos-local]
name=Centos Local
baseurl=file:///mnt/cdrom,file:///mnt/cdrom2 
failovermethod=priority
enabled=1
gpgcheck=0
</code></pre>

<h2>使用yum安装依赖</h2>

<pre><code>yum install -y gcc gd httpd php php-devel php-mysql php-pear php-common php-gd php-mbstring php-cli 

yum install -y rrdtool 

yum install -y apr*

# 编译Ganglia时加 --with-libpcre=no 可以不安装pcre
yum install -y pcre*

# yum install -y zlib-devel
</code></pre>

<h2>(仅)编译安装Ganglia</h2>

<p>下载下面的软件(yum没有这些软件)：</p>

<ul>
<li><a href="http://rpm.pbone.net/index.php3/stat/4/idpl/15992683/dir/scientific_linux_6/com/rrdtool-devel-1.3.8-6.el6.x86_64.rpm.html">rrdtool-devel-1.3.8-6.el6.x86_64.rpm</a></li>
<li><a href="http://download.savannah.gnu.org/releases/confuse/">confuse-2.7.tar.gz</a></li>
<li><a href="http://sourceforge.net/projects/ganglia/files/ganglia%20monitoring%20core/">ganglia</a></li>
<li><a href="http://sourceforge.net/projects/ganglia/files/ganglia-web/">ganglia-web</a></li>
</ul>


<p>安装：</p>

<pre><code>umask 0022 # 临时修改下，不然后面会遇到权限问题

rpm -ivh rrdtool-devel-1.3.8-6.el6.x86_64.rpm 

# 如果yum可以安装的话：yum install -y libconfuse*
tar zxf confuse-2.7.tar.gz
cd confuse-2.7
./configure CFLAGS=-fPIC --disable-nls
make &amp;&amp; make install

tar zxf ganglia-3.7.2.tar.gz 
cd ganglia-3.7.2
./configure --with-gmetad --enable-gexec --enable-status --prefix=/usr/local/ganglia
# 可选项，用于指定默认配置位置 `-sysconfdir=/etc/ganglia`

make &amp;&amp; make install

cp gmetad/gmetad.init /etc/init.d/gmetad
chkconfig gmetad on
# 查看gmetad的情况
chkconfig --list | grep gm

df -h # 把rrds目录放到最大的分区，再做个链接到data目录下
mkdir -p /data/ganglia/rrds
chown nobody:nobody /data/ganglia/rrds
ln -s /usr/local/ganglia/sbin/gmetad /usr/sbin/gmetad

gmetad -h # 查看默认的config位置。下面步骤AB 二选一 根据是否配置 sysconfdir 选项
# 步骤A
# cp gmetad/gmetad.conf /etc/ganglia/
# 步骤B
vi /etc/init.d/gmetad 
  /usr/local/ganglia/etc/gmetad.conf #修改原来的默认配置路径

cd ganglia-3.7.2/gmond/
ln -s /usr/local/ganglia/sbin/gmond /usr/sbin/gmond
cp gmond.init /etc/init.d/gmond
chkconfig gmond on
chkconfig --list gmond

gmond -h # 查看默认的config位置。
./gmond -t &gt;/usr/local/ganglia/etc/gmond.conf
vi /etc/init.d/gmond 
  /usr/local/ganglia/etc/gmond.conf #修改原来的默认配置路径
</code></pre>

<h2>配置</h2>

<ul>
<li>Ganglia配置</li>
</ul>


<pre><code>vi /usr/local/ganglia/etc/gmetad.conf
  datasource "HADOOP" hadoop-master1
  datasource "CU" cu-ud1
  rrd_rootdir "/data/ganglia/rrds"
  gridname "bigdata"

vi /usr/local/ganglia/etc/gmond.conf
  cluster {
   name = "CU"

  udp_send_channel {
   bind_hostname = yes
</code></pre>

<p><a href="http://ixdba.blog.51cto.com/2895551/1149003">http://ixdba.blog.51cto.com/2895551/1149003</a></p>

<p>Ganglia的收集数据工作可以工作在单播（unicast)或多播(multicast)模式下，默认为多播模式。</p>

<ul>
<li>单播：发送自己 <strong>收集</strong> 到的监控数据到特定的一台或几台机器上，可以跨网段</li>
<li>多播：发送自己收集到的监控数据到同一网段内所有的机器上，同时收集同一网段内的所有机器发送过来的监控数据。因为是以广播包的形式发送，因此需要同一网段内。但同一网段内，又可以定义不同的发送通道。</li>
</ul>


<p>主机多网卡(多IP)情况下需要绑定到特定的IP，设置bind_hostname来设置要绑定的IP地址。单IP情况下可以不需要考虑。</p>

<p>多播情况下只能在单一网段进行，如果集群存在多个网段，可以分拆成多个子集群（data_source)，或者使用单播来进行配置。期望配置简单点的话，配置多个 data_source 。</p>

<ul>
<li><code>data_source "cluster-db" node1 node2</code>  定义集群名称，以及获取集群监控数据的节点。由于采用multicast模式，每台gmond节点都有本集群内节点服务器的所有监控数据，因此不必把所有节点都列出来。node1 node2是or的关系，如果node1无法下载，则才会尝试去node2下载，所以它们应该都是同一个集群的节点，保存着同样的数据。</li>
<li><code>cluster.name</code> 本节点属于哪个cluster，需要与data_source对应。</li>
<li><code>host.location</code> 类似于hostname的作用。</li>
<li><code>udp_send_channel.mcast_join/host</code> 多播地址，工作在239.2.11.71通道下。如果使用单播模式，则要写host=node1，单播模式下可以配置多个upd_send_channel</li>
<li><code>udp_recv_channel.mcast_join</code></li>
</ul>


<p><strong>参考思路</strong> (未具体实践)：多网段情况可以用单播解决，要是单网段要配置多个data_source(集群)那就换个多播的端口吧！</p>

<h2>启动以及测试</h2>

<pre><code>service httpd restart
service gmetad start
service gmond start

[root@cu-omc1 ganglia]# netstat -anp | grep gm
tcp        0      0 0.0.0.0:8649                0.0.0.0:*                   LISTEN      916/gmond           
tcp        0      0 0.0.0.0:8651                0.0.0.0:*                   LISTEN      12776/gmetad        
tcp        0      0 0.0.0.0:8652                0.0.0.0:*                   LISTEN      12776/gmetad        
udp        0      0 239.2.11.71:8649            0.0.0.0:*                               916/gmond           
udp        0      0 192.168.31.11:60126         239.2.11.71:8649            ESTABLISHED 916/gmond           
unix  2      [ ]         DGRAM                    1331526917 12776/gmetad        
[root@cu-omc1 ganglia]# bin/gstat -a
CLUSTER INFORMATION
       Name: CU
      Hosts: 0
Gexec Hosts: 0
 Dead Hosts: 0
  Localtime: Wed Jun 15 20:17:36 2016

There are no hosts up at this time



netstat -anp | grep -E "gmond|gmetad"

# 启动如果有问题，使用调试模式启动查找问题
/usr/sbin/gmetad -d 10

/usr/local/ganglia/bin/gstat -a
/usr/local/ganglia/bin/gstat -a -i hadoop-master1

telnet localhost 8649
telnet localhost 8651
</code></pre>

<p>问题：多播地址绑定失败</p>

<p>如果telnet8649没有数据，查看下route是否有 [hostname对应的IP] 到 [239.2.11.71] 的路由！(多网卡多IP的时刻，可能default的路由并非主机名对应IP的地址)</p>

<blockquote><p><a href="http://llydmissile.blog.51cto.com/7784666/1411239">http://llydmissile.blog.51cto.com/7784666/1411239</a>
<a href="http://www.cnblogs.com/Cherise/p/4350581.html">http://www.cnblogs.com/Cherise/p/4350581.html</a></p>

<p>测试过程中可能会出现以下错误：Error creating multicast server mcast_join=239.2.11.71 port=8649 mcast_if=NULL family=&lsquo;inet4&rsquo;. Will try again&hellip;，系统不支持多播，需要将多播ip地址加入路由表，使用route add -host 239.2.11.71 dev eth0命令即可，将该命令加入/etc/rc.d/rc.local文件中，一劳永逸</p></blockquote>

<pre><code>[root@hadoop-master4 ~]# gmond -d 10
loaded module: core_metrics
loaded module: cpu_module
loaded module: disk_module
loaded module: load_module
loaded module: mem_module
loaded module: net_module
loaded module: proc_module
loaded module: sys_module
udp_recv_channel mcast_join=239.2.11.71 mcast_if=NULL port=8649 bind=239.2.11.71 buffer=0
Error creating multicast server mcast_join=239.2.11.71 port=8649 mcast_if=NULL family='inet4'.  Will try again...
</code></pre>

<p>环境的default route被清理掉了(或者是由于网关和本机不在同一网段)。需要手动添加一条到网卡的route。</p>

<pre><code>[root@hadoop-master4 ~]# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.32.0    *               255.255.255.0   U     0      0        0 bond0
192.168.31.0    192.168.32.254  255.255.255.0   UG    0      0        0 bond0
link-local      *               255.255.0.0     U     1006   0        0 bond0
[root@hadoop-master4 ~]# route add -host 239.2.11.71 dev bond0
[root@hadoop-master4 ~]# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
239.2.11.71     *               255.255.255.255 UH    0      0        0 bond0
192.168.32.0    *               255.255.255.0   U     0      0        0 bond0
192.168.31.0    192.168.32.254  255.255.255.0   UG    0      0        0 bond0
link-local      *               255.255.0.0     U     1006   0        0 bond0
</code></pre>

<h2>安装GWeb</h2>

<pre><code>cd ~/ganglia-web-3.7.1
vi Makefile # 一次性配置好，不再需要去修改conf_default.php
    GDESTDIR = /var/www/html/ganglia
    GCONFDIR = /usr/local/ganglia/etc/
    GWEB_STATEDIR = /var/www/html/ganglia
    # Gmetad rootdir (parent location of rrd folder)
    GMETAD_ROOTDIR = /data/ganglia
    APACHE_USER = apache
make install

# 注意：内网还是需要改下 conf_default.php 一堆jquery的js。
# 如果Web不能访问，查看下防火墙以及SELinux
</code></pre>

<ul>
<li>httpd登录密码配置</li>
</ul>


<pre><code>htpasswd -c /var/www/html/ganglia/etc/htpasswd.users gangliaadmin 

vi /etc/httpd/conf/httpd.conf 

    &lt;Directory "/var/www/html/ganglia"&gt;
    #  SSLRequireSSL
       Options None
       AllowOverride None
       &lt;IfVersion &gt;= 2.3&gt;
          &lt;RequireAll&gt;
             Require all granted
    #        Require host 127.0.0.1

             AuthName "Ganglia Access"
             AuthType Basic
             AuthUserFile /var/www/html/ganglia/etc/htpasswd.users
             Require valid-user
          &lt;/RequireAll&gt;
       &lt;/IfVersion&gt;
       &lt;IfVersion &lt; 2.3&gt;
          Order allow,deny
          Allow from all
    #     Order deny,allow
    #     Deny from all
    #     Allow from 127.0.0.1

          AuthName "Ganglia Access"
          AuthType Basic
          AuthUserFile /var/www/html/ganglia/etc/htpasswd.users
          Require valid-user
       &lt;/IfVersion&gt;
    &lt;/Directory&gt;

service httpd restart
</code></pre>

<p>如果在nginx做权限控制，一样很简单：</p>

<pre><code>location /ganglia {
        proxy_pass http://localhost/ganglia;
        auth_basic "Ganglia Access";
        auth_basic_user_file "/var/www/html/ganglia/etc/htpasswd.users";
}
</code></pre>

<h2>集群配置</h2>

<pre><code>cd /usr/local 
# for h in cu-ud{1,2} hadoop-master{1,2} ; do echo $h ; done
for h in cu-ud1 cu-ud2 hadoop-master1 hadoop-master2 ; do 
    cd /usr/local;
    rsync -vaz  ganglia $h:/usr/local/ ;
    ssh $h ln -s /usr/local/ganglia/sbin/gmond /usr/sbin/gmond ;
    scp /etc/init.d/gmond $h:/etc/init.d/ ;
    ssh $h "chkconfig gmond on" ;
    ssh $h "yum install apr* -y" ; 
    ssh $h "service gmond start" ; 
done

# 不同的集群，gmond.conf的cluster.name需要修改

telnet hadoop-master1 8649
netstat -anp | grep gm
</code></pre>

<p>要是集群有变动，添加还好，删除的话，会存在原来的旧数据，页面会提示机器down掉了。可以删除rrds目录下对应集群中节点的数据，然后重庆gmetad/httpd即可。</p>

<h2>参考</h2>

<h3>内容</h3>

<pre><code>防火墙规则设置
iptables -I INPUT 3 -p tcp -m tcp --dport 80 -j ACCEPT
iptables -I INPUT 3 -p udp -m udp --dport 8649 -j ACCEPT

service iptables save
service iptables restart

关闭selinux
vi /etc/selinux/config
SELINUX=disabled
setenforce 0
</code></pre>

<p>实际应用中，需要监控的机器往往在不同的网段内，这个时候，就不能用gmond默认的多播方式（用于同一个网段内）来传送数据，必须使用单播的方法。</p>

<p>gmond可以配置成为一个cluster，这些gmond节点之间相互发送各自的监控数据。所以每个gmond节点上实际上都会有 cluster内的所有节点的监控数据。gmetad只需要去某一个节点获取数据就可以了。</p>

<p>web front-end 一个基于web的监控界面，通常和Gmetad安装在同一个节点上(还需确认是否可以不在一个节点上，因为php的配置文件中ms可配置gmetad的地址及端口)，它从Gmetad取数据，并且读取rrd数据库，生成图片，显示出来。</p>

<p>gmetad周期性的去gmond节点或者gmetad节点poll数据。一个gmetad可以设置多个datasource，每个datasource可以有多个备份，一个失败还可以去其他host取数据。Gmetad只有tcp通道，一方面他向datasource发送请求，另一方面会使用一个tcp端口，发 布自身收集的xml文件，默认使用8651端口。所以gmetad即可以从gmond也可以从其他的gmetad得到xml数据。</p>

<p>对于IO来说，Gmetad默认15秒向gmond取一次xml数据，如果gmond和gmetad都是在同一个节点，这样就相当于本地io请求。同时gmetad请求完xml文件后，还需要对其解析，也就是说按默认设置每15秒需要解析一个10m级别的xml文件，这样cpu的压力就会很大。同时它还有写入RRD数据库，还要处理来自web客户端的解析请求，也会读RRD数据库。这样本身的IO CPU 网络压力就很大，因此这个节点至少应该是个空闲的而且能力比较强的节点。</p>

<ul>
<li>多播模式配置
这个是默认的方式，基本上不需要修改配置文件，且所有节点的配置是一样的。这种模式的好处是所有的节点上的 gmond 都有完备的数据，gmetad 连接其中任意一个就可以获取整个集群的所有监控数据，很方便。
其中可能要修改的是 mcast_if 这个参数，用于指定多播的网络接口。如果有多个网卡，要填写对应的内网接口。</li>
<li>单播模式配置
监控机上的接收 Channel 配置。我们使用 UDP 单播模式，非常简单。我们的集群有部分机器在另一个机房，所以监听了 0.0.0.0，如果整个集群都在一个内网中，建议只 bind 内网地址。如果有防火墙，要打开相关的端口。</li>
<li>最重要的配置项是 data_source: <code>data_source "my-cluster" localhost:8648</code> 如果使用的是默认的 8649 端口，则端口部分可以省略。如果有多个集群，则可以指定多个 data_source，每行一个。</li>
<li>最后是 gridname 配置，用于给整个 Grid 命名</li>
<li><a href="https://github.com/ganglia/gmond_python_modules">https://github.com/ganglia/gmond_python_modules</a></li>
</ul>


<h3>网址</h3>

<ul>
<li><a href="http://yhz.me/blog/Install-Ganglia-On-CentOS.html">在 CentOS 6.5 上安装 Ganglia 3.6.0</a></li>
<li>*<a href="http://ixdba.blog.51cto.com/2895551/1149003">分布式监控系统ganglia配置文档</a></li>
<li><p>*<a href="http://www.3mu.me/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%80%E6%BA%90%E7%9B%91%E6%8E%A7%E8%BD%AF%E4%BB%B6ganglia-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/">企业级开源监控软件Ganglia 安装与配置</a></p></li>
<li><p>*<a href="http://jerrypeng.me/2014/07/04/server-side-java-monitoring-ganglia/">Java 服务端监控方案（二. Ganglia 篇）</a></p></li>
<li><a href="http://jerrypeng.me/2014/07/22/server-side-java-monitoring-nagios/">Java 服务端监控方案（三. Nagios 篇）</a></li>
<li><p><a href="https://github.com/ganglia/ganglia-web/wiki/Nagios-Integration">https://github.com/ganglia/ganglia-web/wiki/Nagios-Integration</a></p></li>
<li><p><a href="https://ganglia.wikimedia.org/latest/">维基百科Ganglia</a></p></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
