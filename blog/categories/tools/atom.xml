<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tools | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/tools/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2016-08-19T17:31:14+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Maven压缩js/css功能实践]]></title>
    <link href="http://winseliu.com/blog/2016/08/19/j2ee-maven-resources-compress/"/>
    <updated>2016-08-19T16:34:28+08:00</updated>
    <id>http://winseliu.com/blog/2016/08/19/j2ee-maven-resources-compress</id>
    <content type="html"><![CDATA[<p>为了节约网络带宽，一般在发布项目时对资源(js/css)文件进行压缩（去掉空行、精简代码等）。但是要做到兼容开发与生产还是的下一番功夫才行。</p>

<pre><code>$ ls -l src/main/webapp/static/assets/js/ | head
total 3120
-rwxrwxr--+ 1 winse None  24804 Aug 10 17:40 bootbox.js
-rwxrwxr--+ 1 winse None  71315 Aug 10 17:40 bootstrap.js
-rwxrwxr--+ 1 winse None  13905 Aug 10 17:40 bootstrap-colorpicker.js
-rwxrwxr--+ 1 winse None  49319 Aug 10 17:40 bootstrap-multiselect.js
...

$ ls -l target/dist/js/ | head
total 1368
-rwxrwx---+ 1 winse None   8943 Aug 19 16:53 bootbox-min.js
-rwxrwx---+ 1 winse None   8057 Aug 19 16:53 bootstrap-colorpicker-min.js
-rwxrwx---+ 1 winse None  38061 Aug 19 16:53 bootstrap-min.js
-rwxrwx---+ 1 winse None  18232 Aug 19 16:53 bootstrap-multiselect-min.js
...
</code></pre>

<p>项目中原本使用dist(压缩)、assets目录放置js/css等资源，在部署的时刻替换dist为assets，有点麻烦。首先想到的用nginx进行url重写，但是需要增加一个服务有点麻烦，能不能直接用spring来实现呢？</p>

<ul>
<li>自定义一个handler类</li>
</ul>


<p>查看 <code>mvn:resources</code> 实现，相当于注册了一个 <code>location -&gt; ResourceHttpRequestHandler</code> 的映射。
我们第一种尝试自动化的方式就是自定义handler类来进行资源的定位。增加 StaticRequestHandler 的处理类，添加配置 location 和 compressLocation，首先去查找压缩文件([NAME]-min.js)，找不到然后再找源文件([NAME].js)位置。</p>

<pre><code>## java
public class StaticRequestHandler extends ResourceHttpRequestHandler {

    private final static Log logger = LogFactory.getLog(ResourceHttpRequestHandler.class);

    private String location;
    private String compressLocation;

    private Resource locationResource;
    private Resource compressLocationResource;

    public void setLocation(String location) {
        this.location = location;
    }

    public void setCompressLocation(String compressLocation) {
        this.compressLocation = compressLocation;
    }

    @Override
    public void afterPropertiesSet() throws Exception {
        super.afterPropertiesSet();

        this.locationResource = getWebApplicationContext().getResource(location);
        super.setLocations(Collections.singletonList(this.locationResource));

        this.compressLocationResource = getWebApplicationContext().getResource(compressLocation);
    }

    @Override
    protected Resource getResource(HttpServletRequest request) {
        String path = (String) request.getAttribute(HandlerMapping.PATH_WITHIN_HANDLER_MAPPING_ATTRIBUTE);
        if (path == null) {
            throw new IllegalStateException("Required request attribute '"
                    + HandlerMapping.PATH_WITHIN_HANDLER_MAPPING_ATTRIBUTE + "' is not set");
        }

        if (!StringUtils.hasText(path) || isInvalidPath(path)) {
            if (logger.isDebugEnabled()) {
                logger.debug("Ignoring invalid resource path [" + path + "]");
            }
            return null;
        }

        Resource res = null;
        if (path.endsWith(".css")) {
            res = findResource(compressLocationResource, path.substring(0, path.length() - 4) + ".min.css");
        } else if (path.endsWith(".js")) {
            res = findResource(compressLocationResource, path.substring(0, path.length() - 3) + ".min.js");
        }

        if (res == null) {
            res = findResource(locationResource, path);
        }

        return res;
    }

    private Resource findResource(Resource location, String path) {
        try {
            if (logger.isDebugEnabled()) {
                logger.debug("Trying relative path [" + path + "] against base location: " + location);
            }
            Resource resource = location.createRelative(path);
            if (resource.exists() &amp;&amp; resource.isReadable()) {
                if (logger.isDebugEnabled()) {
                    logger.debug("Found matching resource: " + resource);
                }
                return resource;
            } else if (logger.isTraceEnabled()) {
                logger.trace("Relative resource doesn't exist or isn't readable: " + resource);
            }
        } catch (IOException ex) {
            logger.debug("Failed to create relative resource - trying next resource location", ex);
        }

        return null;
    }

}

## config
    &lt;!-- 静态资源 --&gt;
    &lt;!-- &lt;mvc:resources mapping="/static/**" location="/static/" /&gt; --&gt;

    &lt;bean class="org.springframework.web.servlet.handler.SimpleUrlHandlerMapping"&gt;
        &lt;property name="mappings"&gt;
            &lt;value&gt;
                /static/assets/**=staticRequestHandler
            &lt;/value&gt;
        &lt;/property&gt;
    &lt;/bean&gt;
    &lt;bean id="staticRequestHandler" class="com.hotel.servlet.resource.StaticRequestHandler"&gt;
        &lt;property name="location" value="/static/assets/" /&gt;
        &lt;property name="compressLocation" value="/static/dist/" /&gt;
    &lt;/bean&gt;
</code></pre>

<p>这种方式解决了请求自动定位 <code>min.js</code> 压缩资源的功能，但是压缩还是不能自动化不能实时的更新，并且调试的时刻还是需要手动的修改配置来切换。</p>

<p>有没有更好的自动化的实现开发环境和生产环境分开呢？</p>

<ul>
<li>Maven打包时压缩然后替换源文件</li>
</ul>


<p>使用 yuicompressor-maven-plugin 插件压缩资源，然后把压缩资源打包放置到assets目录下。</p>

<p>注意： yuicomressor 插件的 nosuffix 配置为 true ! 这样压缩后的文件名和源文件名称才一样。</p>

<pre><code>## spring config
    &lt;!-- 静态资源 --&gt;
    &lt;mvc:resources mapping="/static/**" location="/static/" /&gt;

## maven pom.xml
        &lt;profile&gt;
            &lt;id&gt;release&lt;/id&gt;

            &lt;build&gt;
                &lt;plugins&gt;
                    &lt;!-- http://alchim.sourceforge.net/yuicompressor-maven-plugin/compress-mojo.html --&gt;
                    &lt;plugin&gt;
                        &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
                        &lt;artifactId&gt;yuicompressor-maven-plugin&lt;/artifactId&gt;
                        &lt;version&gt;1.3.2&lt;/version&gt;
                        &lt;executions&gt;
                            &lt;execution&gt;
                                &lt;id&gt;compress_js_css&lt;/id&gt;
                                &lt;phase&gt;process-resources&lt;/phase&gt;
                                &lt;goals&gt;
                                    &lt;goal&gt;compress&lt;/goal&gt;
                                &lt;/goals&gt;
                            &lt;/execution&gt;
                        &lt;/executions&gt;
                        &lt;configuration&gt;
                            &lt;encoding&gt;UTF-8&lt;/encoding&gt;
                            &lt;nosuffix&gt;true&lt;/nosuffix&gt;
                            &lt;skip&gt;false&lt;/skip&gt;

                            &lt;jswarn&gt;false&lt;/jswarn&gt;
                            &lt;nomunge&gt;false&lt;/nomunge&gt;
                            &lt;preserveAllSemiColons&gt;false&lt;/preserveAllSemiColons&gt;

                            &lt;sourceDirectory&gt;src/main/webapp/static/assets&lt;/sourceDirectory&gt;
                            &lt;outputDirectory&gt;${project.build.directory}/dist&lt;/outputDirectory&gt;
                        &lt;/configuration&gt;
                    &lt;/plugin&gt;

                    &lt;plugin&gt;
                        &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt;
                        &lt;version&gt;2.6&lt;/version&gt;
                        &lt;configuration&gt;
                            &lt;webResources&gt;
                                &lt;resource&gt;
                                    &lt;directory&gt;${project.build.directory}/dist&lt;/directory&gt;
                                    &lt;targetPath&gt;static/assets&lt;/targetPath&gt;
                                    &lt;filtering&gt;false&lt;/filtering&gt;
                                &lt;/resource&gt;
                            &lt;/webResources&gt;
                        &lt;/configuration&gt;
                    &lt;/plugin&gt;

                &lt;/plugins&gt;
            &lt;/build&gt;
        &lt;/profile&gt;
</code></pre>

<p>war插件添加了自定义webResources资源，首先把压缩的文件拷贝到对应目录，maven发现文件已经存在，不会再拷贝同名的文件，这样源文件就相当于被替换掉了。</p>

<h2>总结</h2>

<p>使用maven插件压缩打包，完美的解决js/css压缩导致的开发和生产不兼容问题。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用 Naxsi 处理 XSS]]></title>
    <link href="http://winseliu.com/blog/2016/07/19/xss-blocked-by-naxsi/"/>
    <updated>2016-07-19T19:43:13+08:00</updated>
    <id>http://winseliu.com/blog/2016/07/19/xss-blocked-by-naxsi</id>
    <content type="html"><![CDATA[<p>前台安全检查时出现了【检测到目标URL存在跨站漏洞】，就是可以通过url带js来截取用户的信息。</p>

<pre><code>js/jquery/jquery-1.8.2.min.js/&lt;ScRipt&gt;jovoys(6258);&lt;/ScRipt&gt;
</code></pre>

<p>XSS的一些简单介绍：</p>

<ul>
<li><a href="http://anti-hacker.blogspot.com/2008/01/xsscross-site-script.html">淺析XSS(Cross Site Script)漏洞原理</a></li>
<li><a href="http://www.freebuf.com/articles/web/42727.html">XSS的原理分析与解剖（第二篇）</a></li>
</ul>


<p>搜索到使用 <strong>naxsi</strong> 配合 <strong>nginx</strong> 有现成的解决方案，网上的资料很乱，直接看 <a href="https://github.com/nbs-system/naxsi/wiki">官方文档</a> 清晰一些。</p>

<ol>
<li>编译</li>
</ol>


<pre><code>[hadoop@cu2 sources]$ ll
drwxrwxr-x  6 hadoop hadoop      4096 Sep 10  2015 naxsi-0.54
-rw-r--r--  1 hadoop hadoop    192843 Jul 19 18:42 naxsi-0.54.zip
drwxr-xr-x  9 hadoop hadoop      4096 Nov 11  2015 nginx-1.7.10

[hadoop@cu2 sources]$ ll nginx-1.7.10/
total 3180
drwxr-xr-x  6 hadoop hadoop    4096 Nov 11  2015 auto
-rw-r--r--  1 hadoop hadoop  246649 Feb 10  2015 CHANGES
-rw-r--r--  1 hadoop hadoop  375103 Feb 10  2015 CHANGES.ru
drwxr-xr-x  2 hadoop hadoop    4096 Nov 11  2015 conf
-rwxr-xr-x  1 hadoop hadoop    2463 Feb 10  2015 configure
drwxr-xr-x  4 hadoop hadoop    4096 Nov 11  2015 contrib
drwxr-xr-x  2 hadoop hadoop    4096 Nov 11  2015 html
-rw-r--r--  1 hadoop hadoop    1397 Feb 10  2015 LICENSE
-rw-rw-r--  1 hadoop hadoop     342 Jul 19 18:44 Makefile
drwxr-xr-x  2 hadoop hadoop    4096 Nov 11  2015 man
drwxrwxr-x  4 hadoop hadoop    4096 Jul 19 18:45 objs
-rw-r--r--  1 hadoop hadoop 2009464 Nov 11  2015 pcre-8.36.tar.gz
-rw-r--r--  1 hadoop hadoop      49 Feb 10  2015 README
drwxr-xr-x 10 hadoop hadoop    4096 Nov 11  2015 src
-rw-r--r--  1 hadoop hadoop  571091 Nov 11  2015 zlib-1.2.8.tar.gz

[hadoop@cu2 nginx-1.7.10]$ ./configure --add-module=../naxsi-x.xx/naxsi_src/ --prefix=/opt/nginx
[hadoop@cu2 nginx-1.7.10]$ make &amp;&amp; make install
</code></pre>

<ol>
<li>配置</li>
</ol>


<p>需要在 nginx.conf 的http中引入 <strong>naxsi_core.rules</strong> ，在location中加入规则。</p>

<p>先把 naxsi_core.rules 拷贝到 nginx/conf 目录下。</p>

<pre><code>http {
    include       mime.types;
    include       naxsi_core.rules;
    ...
    server {
    ...
        location /omc {

#Enable naxsi
SecRulesEnabled;

#Enable learning mide
#LearningMode;

#Define where blocked requests go
DeniedUrl "/omc/error.jsp";

#CheckRules, determining when naxsi needs to take action
CheckRule "$SQL &gt;= 8" BLOCK;
CheckRule "$RFI &gt;= 8" BLOCK;
CheckRule "$TRAVERSAL &gt;= 4" BLOCK;
CheckRule "$EVADE &gt;= 4" BLOCK;
CheckRule "$XSS &gt;= 8" BLOCK;

#naxsi logs goes there
error_log logs/foo.log;

                proxy_set_header        X-Real-IP $remote_addr;
                proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header        Host $http_host;

                proxy_pass http://localhost:8888/omc;
        }
        ...
</code></pre>

<ol>
<li>启动生效</li>
</ol>


<pre><code>sbin/nginx -p $PWD
</code></pre>

<p><a href="https://github.com/nbs-system/naxsi/wiki/naxsi-setup">https://github.com/nbs-system/naxsi/wiki/naxsi-setup</a>
<a href="https://github.com/nbs-system/naxsi/wiki/checkrules-bnf">https://github.com/nbs-system/naxsi/wiki/checkrules-bnf</a></p>

<p>检查会比较严格，添加后应用可能会报错，需要对 foo.log 中的情况进行确认，对规则进行一些修改。如不需要监控 cookie 里面的内容：</p>

<pre><code>[omc@cu-omc1 nginx]$ vi conf/naxsi_core.rules 
:%s/|$HEADERS_VAR:Cookie//
</code></pre>

<p>还有一些 <code>%[2|3]</code> 的可能也需要改改。</p>

<pre><code>uri=/omc/Frame/Time.do&amp;learning=0&amp;vers=0.54&amp;total_processed=404&amp;total_blocked=10&amp;block=1&amp;zone0=BODY&amp;id0=16&amp;var_name0=
</code></pre>

<p>根据请求的 id 去规则配置里面找具体的描述，然后 uri 和 var_name 查看具体的请求对症下药：去掉规则或者改请求。</p>

<p>如上面请求的 <strong>id0=16</strong> 对应 <strong>#@MainRule &ldquo;msg:empty POST&rdquo; id:16;</strong> 把请求修改成get即可。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Flamegraphs Java Cpu]]></title>
    <link href="http://winseliu.com/blog/2016/05/06/flamegraphs-java-cpu/"/>
    <updated>2016-05-06T21:35:03+08:00</updated>
    <id>http://winseliu.com/blog/2016/05/06/flamegraphs-java-cpu</id>
    <content type="html"><![CDATA[<p>在MacTalk的公众号上读到了agentzh关于火焰图介绍(2016年5月6日07:57 动态追踪技术（中） - Dtrace、SystemTap、火焰图)，挺新奇的，并且应该对于查询热线程还是有作用的。</p>

<p>先了解perf和flamegraphs基础知识：</p>

<ul>
<li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-perf1/">https://www.ibm.com/developerworks/cn/linux/l-cn-perf1/</a></li>
<li><a href="http://www.brendangregg.com/perf.html#FlameGraphs">perf Examples</a></li>
<li><a href="http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">CPU Flame Graphs</a></li>
<li><a href="http://techblog.netflix.com/2015/07/java-in-flames.html">Java in Flames</a></li>
<li><a href="http://isuru-perera.blogspot.hk/2015/07/java-cpu-flame-graphs.html">Java CPU Flame Graphs</a></li>
<li>使用方法<a href="https://randomascii.wordpress.com/2013/03/26/summarizing-xperf-cpu-usage-with-flame-graphs/">xperf - windows perf</a></li>
<li>工具<a href="https://github.com/google/UIforETW/blob/master/bin/xperf_to_collapsedstacks.py">UIforETW</a></li>
</ul>


<p>perf好像有点类似java的btrace，不过perf是操作系统层面的。把操作系统当做服务，客户端通过perf来获取/查询系统的信息。</p>

<h2>安装</h2>

<p>perf包括在linux 2.6.31代码里面，redhat可以通过yum来安装/更新：</p>

<pre><code>[root@hadoop-master2 ~]# yum install perf
...
Installed:
  perf.x86_64 0:2.6.32-573.26.1.el6  

[root@hadoop-master2 ~]# perf stat ls /dev/shm

 Performance counter stats for 'ls /dev/shm':

          0.697115 task-clock                #    0.613 CPUs utilized          
                 0 context-switches          #    0.000 K/sec                  
                 0 cpu-migrations            #    0.000 K/sec                  
               236 page-faults               #    0.339 M/sec                  
   &lt;not supported&gt; cycles                  
   &lt;not supported&gt; stalled-cycles-frontend 
   &lt;not supported&gt; stalled-cycles-backend  
   &lt;not supported&gt; instructions            
   &lt;not supported&gt; branches                
   &lt;not supported&gt; branch-misses           

       0.001137015 seconds time elapsed
</code></pre>

<p>虚拟机可能有一些event不能用，到真正的实体机上面应该是没问题的（网上有同学验证过）。可以通过 <code>perf list</code> 查看支持的event。</p>

<p>补充，实体机效果：</p>

<pre><code>[root@dacs ~]# perf stat ls /dev/shm
...

 Performance counter stats for 'ls /dev/shm':

          1.793297      task-clock (msec)         #    0.677 CPUs utilized          
                 1      context-switches          #    0.558 K/sec                  
                 0      cpu-migrations            #    0.000 K/sec                  
               255      page-faults               #    0.142 M/sec                  
           2765454      cycles                    #    1.542 GHz                     [44.66%]
           1544155      stalled-cycles-frontend   #   55.84% frontend cycles idle    [64.12%]
           1013635      stalled-cycles-backend    #   36.65% backend  cycles idle   
           2692743      instructions              #    0.97  insns per cycle        
                                                  #    0.57  stalled cycles per insn
            603340      branches                  #  336.442 M/sec                  
             12499      branch-misses             #    2.07% of all branches         [98.00%]

       0.002650313 seconds time elapsed
</code></pre>

<p>windows的话直接下载 UIforETW ，运行 UIforETW.exe 就可以用来采样了。把采样产生的etl文件传给xperf_to_collapsedstacks.py，最后用flamegraph.pl画图。</p>

<p>perf的常用命令：</p>

<pre><code># http://www.brendangregg.com/perf.html
perf list

perf stat ./t1 
perf stat -a -A ls

perf top

perf record – e cpu-clock ./t1 
perf report
</code></pre>

<h2>画图</h2>

<ul>
<li><a href="http://isuru-perera.blogspot.hk/2015/07/java-cpu-flame-graphs.html">http://isuru-perera.blogspot.hk/2015/07/java-cpu-flame-graphs.html</a></li>
<li><a href="https://github.com/coderplay/perfj/releases">https://github.com/coderplay/perfj/releases</a></li>
<li><a href="http://techblog.netflix.com/2015/07/java-in-flames.html">http://techblog.netflix.com/2015/07/java-in-flames.html</a></li>
</ul>


<h4>系统火焰图</h4>

<pre><code># http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html
# https://github.com/brendangregg/FlameGraph
# 真实的机器效果还是挺不错的
perf record -F 99 -a -g -- sleep 60
perf script | ~/FlameGraph/stackcollapse-perf.pl &gt;out.perf-folded
~/FlameGraph/flamegraph.pl out.perf-folded &gt;perf.svg
sz perf.svg

# --
# perf script | ./stackcollapse-perf.pl &gt; out.perf-folded
# grep -v cpu_idle out.perf-folded | ./flamegraph.pl &gt; nonidle.svg
# grep ext4 out.perf-folded | ./flamegraph.pl &gt; ext4internals.svg
# egrep 'system_call.*sys_(read|write)' out.perf-folded | ./flamegraph.pl &gt; rw.svg
</code></pre>

<p>安装的虚拟机中操作没采集到有用的。虚拟机和真实机器两个图：</p>

<p><img src="/images/blogs/flames/flames-real.png" alt="" /></p>

<p><img src="/images/blogs/flames/flames-vm.png" alt="" /></p>

<h4>java</h4>

<p>首先需要jdk8_u60+，直接下载最新的jdk就好了。应用启动带上参数 -XX:+PreserveFramePointer ：</p>

<pre><code>[root@hadoop-master2 ~]# java -version
java version "1.8.0_92"
Java(TM) SE Runtime Environment (build 1.8.0_92-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode)
[root@hadoop-master2 ~]# cd /home/hadoop/spark-1.6.0-bin-2.6.3/
[root@hadoop-master2 spark-1.6.0-bin-2.6.3]# export SPARK_SUBMIT_OPTS=-XX:+PreserveFramePointer     
[root@hadoop-master2 spark-1.6.0-bin-2.6.3]# bin/spark-shell --master local   
</code></pre>

<p>这里java进程使用root启动的，如果是普通用户如hadoop，为了采样需要把hadoop用户加入sudoer，在采样时使用 <code>sudo -u hadoop CMD</code>。</p>

<p><a href="http://techblog.netflix.com/2015/07/java-in-flames.html">http://techblog.netflix.com/2015/07/java-in-flames.html</a></p>

<ul>
<li>A 使用perfj采样</li>
</ul>


<p><a href="http://greenteajug.cn/2015/07/02/greenteajug%E6%B4%BB%E5%8A%A8-%E7%AC%AC16%E6%9C%9F-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%88%A9%E5%99%A8-perfj/">性能调优利器——PerfJ</a></p>

<p>直接下载release-1.0的版本，解压后给 bin/perfj 加上执行权限。</p>

<pre><code># 测试的时刻可以把-F 99设置大一点
# java和perfj的用户得一致！！
# https://github.com/coderplay/perfj/wiki/CPU-Flame-Graph

[root@dacs ~]# export JAVA_HOME=/usr/java/jdk1.8.0_92 
[root@dacs ~]# wget http://blog.minzhou.info/perfj/leveldb-benchmark.jar
[root@dacs ~]# $JAVA_HOME/bin/java -cp leveldb-benchmark.jar -XX:+PreserveFramePointer org.iq80.leveldb.benchmark.DbBenchmark --benchmarks=fillrandom --num=100000000

[root@dacs ~]# export JAVA_HOME=/usr/java/jdk1.8.0_92 
[root@dacs ~]# perfj-1.0/bin/perfj record -F 999 -g -p `pgrep -f DbBenchmark` 
perf script | ~/FlameGraph/stackcollapse-perf.pl &gt;out.perf-folded
~/FlameGraph/flamegraph.pl out.perf-folded  --color=java &gt;perf.svg
sz perf.svg
</code></pre>

<p><img src="/images/blogs/flames/flames-java-leveldb.png" alt="" /></p>

<p>还是挺有意思的。</p>

<p><img src="/images/blogs/flames/flames-java-leveldb-vm.png" alt="" /></p>

<p>虚拟机的少了好多信息！一模一样的命令，得出来的东西差好远！！</p>

<pre><code># https://github.com/coderplay/perfj/wiki/Context-Switch-Analysis
# 在vmware虚拟机里面运行啥都看不到！实体机也看不到作者的那些栈信息
[root@dacs ~]# wget http://blog.minzhou.info/perfj/leveldb-benchmark.jar
[root@dacs ~]# export JAVA_HOME=/usr/java/jdk1.8.0_92 
[root@dacs ~]# $JAVA_HOME/bin/javac ContextSwitchTest.java 
[root@dacs ~]# $JAVA_HOME/bin/java -XX:+PreserveFramePointer ContextSwitchTest

[root@dacs ~]# export JAVA_HOME=/usr/java/jdk1.8.0_92 
[root@dacs ~]# perfj-1.0/bin/perfj record  -e sched:sched_switch -F 999 -g -p `pgrep -f ContextSwitchTest` 
[root@dacs ~]# perfj-1.0/bin/perfj report --stdio
</code></pre>

<ul>
<li>B 使用perf-map-agent</li>
</ul>


<pre><code>git clone https://github.com/jrudolph/perf-map-agent.git
cd perf-map-agent/
export JAVA_HOME=/opt/jdk1.8.0_92
cmake .
make

perf record -F 99 -g -p 7661 -- sleep 120
bin/create-java-perf-map.sh 7661

sudo perf script | ~/FlameGraph/stackcollapse-perf.pl &gt;out.perf-folded
cat out.perf-folded | ~/FlameGraph/flamegraph.pl --color=java &gt;perf.svg
sz perf.svg
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RPM打包]]></title>
    <link href="http://winseliu.com/blog/2016/04/04/rpm-build-your-package/"/>
    <updated>2016-04-04T16:07:21+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/04/rpm-build-your-package</id>
    <content type="html"><![CDATA[<h2>资料</h2>

<ul>
<li><a href="http://www.rpm.org/max-rpm-snapshot/rpmbuild.8.html">http://www.rpm.org/max-rpm-snapshot/rpmbuild.8.html</a></li>
<li><a href="https://fedoraproject.org/wiki/How_to_create_an_RPM_package/zh-cn">https://fedoraproject.org/wiki/How_to_create_an_RPM_package/zh-cn</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/management/package/rpm/part1/index.html">用 RPM 打包软件-打包教程</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/management/package/rpm/part3/index.html">用 RPM 打包软件-高级部分：安装前后控制</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/l-rpm/index.html">RPM 打包技术与典型 SPEC 文件分析-各变量含义</a></li>
<li><a href="http://hlee.iteye.com/blog/343499">http://hlee.iteye.com/blog/343499</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/management/package/rpm/part1/indent-2.spec">案例</a></li>
<li><p><a href="https://github.com/apache/zookeeper/tree/release-3.4.8/src/packages">zookeeper打包案例</a></p></li>
<li><p><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-checkinstall/index.html">http://www.ibm.com/developerworks/cn/linux/l-cn-checkinstall/index.html</a></p></li>
</ul>


<h2>实践</h2>

<ul>
<li>系统配置准备</li>
</ul>


<pre><code># 新建一个docker实例，来测试、学习
[root@cu1 ~]# docker run -ti centos:centos6 /bin/bash

[root@bdc25400cc63 mywget]# cat /etc/redhat-release 
CentOS release 6.6 (Final)

# 安装编译环境所需的软件
yum install which tree lrzsz tar gcc rpm-build
# wget编译的依赖
yum install -y gnutls gnutls-devel
</code></pre>

<ul>
<li>步骤</li>
</ul>


<pre><code>[root@bdc25400cc63 home]# mkdir mywget 
[root@bdc25400cc63 home]# cd mywget/
[root@bdc25400cc63 mywget]# mkdir BUILD RPMS SOURCES SPECS SRPMS
[root@bdc25400cc63 mywget]# cd SOURCES/
[root@bdc25400cc63 SOURCES]# mv /home/wget-1.17.tar.gz .
[root@bdc25400cc63 SOURCES]# ls
wget-1.17.tar.gz
[root@bdc25400cc63 SOURCES]# cd ..

[root@bdc25400cc63 mywget]# rpmbuild --showrc
[test@bdc25400cc63 mywget]$ rpm --eval "%{_topdir}"

[test@bdc25400cc63 mywget]$ grep -i _topdir /usr/lib/rpm/rpmrc /usr/lib/rpm/redhat/rpmrc /usr/lib/rpm/macros /usr/lib/rpm/redhat/macros  | less
/usr/lib/rpm/macros:%_builddir          %{_topdir}/BUILD
/usr/lib/rpm/macros:%_rpmdir            %{_topdir}/RPMS
/usr/lib/rpm/macros:%_sourcedir         %{_topdir}/SOURCES
/usr/lib/rpm/macros:%_specdir           %{_topdir}/SPECS
/usr/lib/rpm/macros:%_srcrpmdir         %{_topdir}/SRPMS
/usr/lib/rpm/macros:%_buildrootdir              %{_topdir}/BUILDROOT
/usr/lib/rpm/macros:%_topdir            %{getenv:HOME}/rpmbuild

[test@bdc25400cc63 mywget]$ cat ~/.rpmmacros 
%_topdir /home/mywget/rpm

# 2016-5-12 15:28:35
# spec里面有define和global，应该是这个导致的！用global应该即可以了？

[root@bdc25400cc63 mywget]# vi SPECS/wget.spec
  # this is a sample spec file for wget

  %define _topdir /home/mywget
  %define name    wget
  %define release 2
  %define version 1.17
  # 定义 _buildrootdir 不起作用，不知道为啥??? 在 .rpmmacros 定义了 %_topdir，root转到 /home/mywget/rpm/BUILDROOT 了。

  %define _unpackaged_files_terminate_build 0

  Summary:   GNU wget
  License:   GPL
  Name:      %{name}
  Version:   %{version}
  Release:   %{release}
  Source:    %{name}-%{version}.tar.gz
  Prefix:    /usr/local/wget
  Group:     Development/Tools

  %description
  The GNU wget program downloads files from the Internet using the command-line.

  %prep
  %setup -q

  %build
  ./configure
  make

  %install
  make install prefix=$RPM_BUILD_ROOT/usr/local/wget # or use DESTDIR=$RPM_BUILD_ROOT

  %post
  echo "hello world"

  %preun
  echo "bye"

  %clean
  rm -rf $RPM_BUILD_ROOT

  %files
  %defattr(-, root, root)
  /usr/local/wget/bin/wget

[root@bdc25400cc63 mywget]# rpmbuild -vv -bb --clean SPECS/wget.spec 

[root@bdc25400cc63 mywget]# tree .
.
├── BUILD
├── RPMS
│   └── x86_64
│       ├── wget-1.17-2.x86_64.rpm
│       └── wget-debuginfo-1.17-2.x86_64.rpm
├── SOURCES
│   └── wget-1.17.tar.gz
├── SPECS
│   └── wget.spec
└── SRPMS

6 directories, 4 files

[root@bdc25400cc63 mywget]# rpm -qpl RPMS/x86_64/wget-1.17-2.x86_64.rpm  
/usr/local/wget/bin/wget
</code></pre>

<p>接下来就可以直接拿到这个包到其他机器上安装了，如果自己建立了本地库，使用createrepo更新下，就可以使用yum安装最新打的包了。</p>

<p>注： <code>%pre</code> , <code>%post</code> 和 <code>%preun</code> , <code>%postun</code> 可以在安装前后执行一些脚本。</p>

<pre><code>[root@cu2 ganglia-build]# mkdir BUILD RPMS SOURCES SPECS SRPMS
[root@cu2 ganglia-build]# cd SOURCES/
[root@cu2 SOURCES]# ll
total 1272
-rw-r--r-- 1 root root 1302320 Jan 20 09:35 ganglia-3.7.2.tar.gz
[root@cu2 SOURCES]# cd ..

[root@cu2 ganglia-build]# ll
total 20
drwxr-xr-x 2 root root 4096 Jun 15 10:25 BUILD
drwxr-xr-x 2 root root 4096 Jun 15 10:25 RPMS
drwxr-xr-x 2 root root 4096 Jun 15 10:25 SOURCES
drwxr-xr-x 2 root root 4096 Jun 15 10:25 SPECS
drwxr-xr-x 2 root root 4096 Jun 15 10:25 SRPMS

[root@cu2 ganglia-build]# cd SPECS/
[root@cu2 SPECS]# vi gmetad.spec

[root@cu2 ganglia-build]# rpmbuild --clean -v -ba SPECS/gmetad.spec 

[root@cu2 ganglia-build]# rpm -qpl RPMS/x86_64/ganglia-3.7.2-1.el6.x86_64.rpm 
</code></pre>

<h2>重新打包已有rpm</h2>

<p>下载源码包，再修改内容，最后使用rpm-build重新打包。</p>

<p>这里以puppetserver为例，使用jdk7即可但官网打包的依赖是jdk8，这里修改依赖然后重新打包：</p>

<pre><code>[root@cu2 rpmbuild]# rpm -ivh puppetserver-2.3.1-1.el6.src.rpm 
warning: puppetserver-2.3.1-1.el6.src.rpm: Header V4 RSA/SHA1 Signature, key ID 4bd6ec30: NOKEY
   1:puppetserver           warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
########################################### [100%]
warning: user mockbuild does not exist - using root
warning: group mockbuild does not exist - using root
[root@cu2 rpmbuild]# ll
total 32904
-rw-r--r-- 1 root root 33681889 May 10 17:44 puppetserver-2.3.1-1.el6.src.rpm
drwxr-xr-x 2 root root     4096 May 10 17:55 SOURCES
drwxr-xr-x 2 root root     4096 May 10 17:55 SPECS

#-- 注释掉jdk8的部分
[root@cu2 rpmbuild]# grep -3 jdk SPECS/puppetserver.spec 

# java 1.8.0 is available starting in fedora 20 and el 6
#%if 0%{?fedora} &gt;= 20 || 0%{?rhel} &gt;= 6
#%global open_jdk          java-1.8.0-openjdk-headless
#%else
%global open_jdk          java-1.7.0-openjdk
#%endif

[root@cu2 rpmbuild]# yum install -y ruby
[root@cu2 rpmbuild]# rpmbuild -v -bb --clean SPECS/puppetserver.spec 

[root@cu2 rpmbuild]# yum deplist RPMS/noarch/puppetserver-2.3.1-1.el6.noarch.rpm 
Loaded plugins: fastestmirror, priorities
Finding dependencies: 
Loading mirror speeds from cached hostfile
 * base: centos.ustc.edu.cn
 * centosplus: centos.ustc.edu.cn
 * epel: mirror01.idc.hinet.net
 * extras: centos.ustc.edu.cn
 * updates: centos.ustc.edu.cn
193 packages excluded due to repository priority protections
package: puppetserver.noarch 2.3.1-1.el6
  dependency: chkconfig
   provider: chkconfig.x86_64 1.3.49.3-5.el6
   provider: chkconfig.x86_64 1.3.49.3-5.el6_7.2
  dependency: /bin/bash
   provider: bash.x86_64 4.1.2-33.el6
   provider: bash.x86_64 4.1.2-33.el6_7.1
  dependency: java-1.7.0-openjdk
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.79-2.5.5.4.el6
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.101-2.6.6.1.el6_7
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.85-2.6.1.3.el6_6
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.85-2.6.1.3.el6_7
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.91-2.6.2.2.el6_7
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.95-2.6.4.0.el6_7
   provider: java-1.7.0-openjdk.x86_64 1:1.7.0.99-2.6.5.0.el6_7
  dependency: puppet-agent &gt;= 1.4.0
   provider: puppet-agent.x86_64 1.4.1-1.el6
  dependency: net-tools
   provider: net-tools.x86_64 1.60-110.el6_2
  dependency: /usr/bin/env
   provider: coreutils.x86_64 8.4-37.el6
   provider: coreutils.x86_64 8.4-37.el6_7.3
  dependency: /bin/sh
   provider: bash.x86_64 4.1.2-33.el6
   provider: bash.x86_64 4.1.2-33.el6_7.1
  dependency: config(puppetserver) = 2.3.1-1.el6
   provider: puppetserver.noarch 2.3.1-1.el6 
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装配置OpenVPN]]></title>
    <link href="http://winseliu.com/blog/2016/03/11/install-and-config-openvpn/"/>
    <updated>2016-03-11T09:46:49+08:00</updated>
    <id>http://winseliu.com/blog/2016/03/11/install-and-config-openvpn</id>
    <content type="html"><![CDATA[<p>由于测试环境搭建不在同一个网络，平时查看hadoop集群状态、提交任务都可以通过hadoop-master的外网来操作。但是要读写kafka，需要直接连通所有的节点，全部映射端口太麻烦。一开始想到了VLAN(虚拟局域网），远远超出能力范围。最后通过搭架VPN来实现与测试环境的透明访问。</p>

<h2>使用集成版本</h2>

<p>参考 <a href="https://linux.cn/article-4733-1.html">https://linux.cn/article-4733-1.html</a></p>

<pre><code># download https://openvpn.net/index.php/access-server/download-openvpn-as-sw.html

# 安装
[root@cu2 ~]# rpm -ivh openvpn-as-2.0.25-CentOS6.x86_64.rpm 
Preparing...                ########################################### [100%]
   1:openvpn-as             ########################################### [100%]
The Access Server has been successfully installed in /usr/local/openvpn_as
Configuration log file has been written to /usr/local/openvpn_as/init.log
Please enter "passwd openvpn" to set the initial
administrative password, then login as "openvpn" to continue
configuration here: https://192.168.0.214:943/admin
To reconfigure manually, use the /usr/local/openvpn_as/bin/ovpn-init tool.

Access Server web UIs are available here:
Admin  UI: https://192.168.0.214:943/admin
Client UI: https://192.168.0.214:943/

[root@cu2 ~]# passwd openvpn

然后通过web admin进行配置。如主机的信息、hostname以及监听绑定的IP
</code></pre>

<p>配置好以后，本地通过网页下载client程序安装。连接配置后：</p>

<pre><code>
C:\Users\winse&gt;tracert  cu3

通过最多 30 个跃点跟踪
到 cu3 [192.168.0.148] 的路由:

  1     2 ms     2 ms     2 ms  172.27.232.1
  2     2 ms     2 ms     2 ms  cu3 [192.168.0.148]

跟踪完成。


C:\Users\winse&gt;route print
===========================================================================
IPv4 路由表
===========================================================================
活动路由:
网络目标        网络掩码          网关       接口   跃点数
          0.0.0.0          0.0.0.0      192.168.1.1    192.168.1.102     20
          0.0.0.0        128.0.0.0     172.27.232.1     172.27.232.2     20
...
</code></pre>

<p><a href="http://designmylife.blog.163.com/blog/static/2067142542013527101659960/">http://designmylife.blog.163.com/blog/static/2067142542013527101659960/</a></p>

<p>路由匹配按最大(最亲)方式匹配。上面路由会先匹配mask为 <code>128.0.0.0</code> 的路由。最终把所有的流量经由VPN出去。</p>

<p>通过 <strong>Access Server</strong> 安装简单，配置通过网页来弄，和网上资料的都匹配不上，还有用户数量的限制，囧。</p>

<h2>编译源码安装</h2>

<ul>
<li>服务端安装配置</li>
</ul>


<pre><code>[root@cu2 openvpn-2.3.10]# yum install libpam*
[root@cu2 openvpn-2.3.10]# yum install pam-devel.x86_64

[root@cu2 ~]# rz
rz waiting to receive.
Starting zmodem transfer.  Press Ctrl+C to cancel.
Transferring lzo-2.06.tar.gz...
  100%     569 KB     569 KB/sec    00:00:01       0 Errors  

[root@cu2 ~]# tar zxvf lzo-2.06.tar.gz 
[root@cu2 ~]# cd lzo-2.06
[root@cu2 lzo-2.06]# ./configure 
[root@cu2 lzo-2.06]# make &amp;&amp;  make install

[root@cu2 openvpn-2.3.10]# ./configure --prefix=/usr/local/openvpn 
[root@cu2 openvpn-2.3.10]# make &amp;&amp; make install

[root@cu2 openvpn-2.3.10]# /usr/local/openvpn/sbin/openvpn --version
OpenVPN 2.3.10 x86_64-unknown-linux-gnu [SSL (OpenSSL)] [EPOLL] [MH] [IPv6] built on Mar  9 2016

https://github.com/OpenVPN/easy-rsa/releases

[root@cu2 EasyRSA-3.0.1]# ./easyrsa  help

[root@cu2 EasyRSA-3.0.1]# ./easyrsa init-pki
[root@cu2 EasyRSA-3.0.1]#  ./easyrsa build-ca

[root@cu2 EasyRSA-3.0.1]# ./easyrsa gen-req openvpn nopass
[root@cu2 EasyRSA-3.0.1]# ./easyrsa sign client openvpn

[root@cu2 EasyRSA-3.0.1]# ./easyrsa gen-req eshore-cu nopass
[root@cu2 EasyRSA-3.0.1]# ./easyrsa sign server eshore-cu

[root@cu2 EasyRSA-3.0.1]# tree pki/
pki/
├── ca.crt
├── certs_by_serial
│   ├── 01.pem
│   └── 02.pem
├── index.txt
├── index.txt.attr
├── index.txt.attr.old
├── index.txt.old
├── issued
│   ├── eshore-cu.crt
│   └── openvpn.crt
├── private
│   ├── ca.key
│   ├── eshore-cu.key
│   └── openvpn.key
├── reqs
│   ├── eshore-cu.req
│   └── openvpn.req
├── serial
└── serial.old

[root@cu2 EasyRSA-3.0.1]#  ./easyrsa gen-dh
[root@cu2 EasyRSA-3.0.1]# cd pki
[root@cu2 pki]# cp ca.crt dh.pem issued/eshore-cu.crt private/eshore-cu.key /etc/openvpn/ 

[root@cu2 openvpn-2.3.10]# cp sample/sample-config-files/server.conf /etc/openvpn/

    proto tcp
    cert eshore-cu.crt
    key eshore-cu.key 
    dh dh.pem
    # 在客户端额外添加这条路由到VPN
    push "route 192.168.0.0 255.255.255.0"
    # 和AS一样，会添加0.0.0.0到VPN的路由。默认走VPN
    ;push "redirect-gateway def1 bypass-dhcp"
    user nobody
    group nobody

[root@cu2 pki]# cd /etc/openvpn/
[root@cu2 openvpn]# /usr/local/openvpn/sbin/openvpn --config /etc/openvpn/server.conf 
[root@cu2 openvpn]# /usr/local/openvpn/sbin/openvpn --daemon --config server.conf 
</code></pre>

<p></p>

<ul>
<li>安装客户端：</li>
</ul>


<p><a href="https://openvpn.net/index.php/open-source/downloads.html">https://openvpn.net/index.php/open-source/downloads.html</a> 下载安装对应的版本。</p>

<p>拷贝sample-config/client.ovpn和服务端的ca.crt、openvpn.crt、openvpn.key到config目录下面。</p>

<p>修改client.ovpn:</p>

<pre><code>proto tcp
remote webcu2 1194
cert openvpn.crt
key openvpn.key
</code></pre>

<p>然后启动 <strong>OpenVPN GUI</strong> ，右键connect就行了。</p>

<pre><code>$ route print
...
IPv4 路由表
===========================================================================
活动路由:
网络目标        网络掩码          网关       接口   跃点数
          0.0.0.0          0.0.0.0      192.168.1.1    192.168.1.102     20
         10.8.0.1  255.255.255.255         10.8.0.5         10.8.0.6     20
         10.8.0.4  255.255.255.252            在链路上          10.8.0.6    276
         10.8.0.6  255.255.255.255            在链路上          10.8.0.6    276
         10.8.0.7  255.255.255.255            在链路上          10.8.0.6    276
      192.168.0.0    255.255.255.0         10.8.0.5         10.8.0.6     20
...
</code></pre>

<h2>问题</h2>

<ul>
<li>连接到VPN服务端的机器是没有问题，但是不能访问该机器的应用（端口不同）</li>
</ul>


<p>被防火墙限制了，在服务端把10.8.0.0/24加入到防火墙允许。</p>

<pre><code>iptables -A INPUT -s 10.8.0.0/24 -j ACCEPT 
</code></pre>

<ul>
<li>不能访问服务端其他机器</li>
</ul>


<p>在iptables上增加转发</p>

<pre><code>iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE
</code></pre>

<p>查看iptables规则：</p>

<pre><code>iptables -nL -t nat
</code></pre>

<p>测试下:</p>

<pre><code>$ ping cu3

正在 Ping cu3 [192.168.0.148] 具有 32 字节的数据:
来自 192.168.0.148 的回复: 字节=32 时间=5ms TTL=63
来自 192.168.0.148 的回复: 字节=32 时间=5ms TTL=63
</code></pre>

<p>其他（参数，未实践，记录下来）</p>

<blockquote><p>必须在服务器端的内网网关上将到10.8.0.0/24网段的路由指向到openvpn服务器，不然从服务器端内网其他机器根本找不到去往10.8.0.0/24网段的路由。这里又分两种情况，一种是服务端有内网网关设备的（按如上说法即可）；一种是服务端内网没有网关设备，即服务器通过交换机相连，相互通讯靠广播的情况。我的就是这种情况。需要在想访问的server上增加到10.8.0.0/24的路由，如下</p>

<p>route add -net 10.8.0.0/24 gw 192.168.1.211    #1.211为openvpn服务器的内网IP</p>

<p>Make sure that you&rsquo;ve enabled IP and TUN/TAP forwarding on the OpenVPN server machine.
确定开启了转发功能，然后在openvpn服务器Iptables添加如下两条规则</p>

<p>iptables -A FORWARD -i tun0 -s 10.8.0.0/24 -j ACCEPT    #简单说，允许数据从客户端到后端server
iptables -A FORWARD -i em2 -d 10.8.0.0/24 -j ACCEPT    #允许数据从后端server到客户端</p></blockquote>

<h2>参考</h2>

<ul>
<li><a href="https://openvpn.net/index.php/open-source/documentation/howto.html">https://openvpn.net/index.php/open-source/documentation/howto.html</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_86fbdd650101a0ax.html">http://blog.sina.com.cn/s/blog_86fbdd650101a0ax.html</a></li>
<li><a href="http://www.linuxquestions.org/questions/linux-networking-3/openvpn-conencts-but-can%27t-ping-servers-on-the-other-network-660610/">http://www.linuxquestions.org/questions/linux-networking-3/openvpn-conencts-but-can%27t-ping-servers-on-the-other-network-660610/</a></li>
<li><a href="http://www.ilanni.com/?p=9877">http://www.ilanni.com/?p=9877</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_86fbdd650101a0ax.html">http://blog.sina.com.cn/s/blog_86fbdd650101a0ax.html</a></li>
<li><a href="http://kaifly.blog.51cto.com/3209616/1367591">http://kaifly.blog.51cto.com/3209616/1367591</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
