<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Elasticsearch | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/elasticsearch/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2017-09-21T03:12:41+00:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[elasticsearch5安装Head插件]]></title>
    <link href="http://winseliu.com/blog/2016/12/14/elasticsearch5-head-plugin-config/"/>
    <updated>2016-12-14T12:16:39+00:00</updated>
    <id>http://winseliu.com/blog/2016/12/14/elasticsearch5-head-plugin-config</id>
    <content type="html"><![CDATA[<p>新版本ES5和原来的有写不同，head等一些插件不能直接安装，需要单独起一个程序然后通过ajax的方式获取数据。</p>

<p>下载 elasticsearch-5.1.1 ，修改配置 <strong> network.host: cu-ud1 </strong>，然后启动服务 bin/elasticsearch -d 。</p>

<p>Head插件安装相对比较麻烦。在windows一些插件安装不了，现在能上外网的Linux下载依赖，然后把全部的包复制到生产环境。最后配置并启动head服务。</p>

<ul>
<li><a href="https://github.com/mobz/elasticsearch-head#running-with-built-in-server">https://github.com/mobz/elasticsearch-head#running-with-built-in-server</a></li>
</ul>


<pre><code>[hadoop@cu2 elasticsearch-head]$ npm install

[ud@cu-ud1 ~]$ xz -d node-v6.9.2-linux-x64.tar.xz 
[ud@cu-ud1 ~]$ tar xvf node-v6.9.2-linux-x64.tar 
[ud@cu-ud1 node-v6.9.2-linux-x64]$ vi ~/.bash_profile 
...
NODE_HOME=/home/ud/node-v6.9.2-linux-x64
PATH=$NODE_HOME/bin:$PATH
export PATH

[ud@cu-ud1 ~]$ tar zxvf elasticsearch-head-standalone.tar.gz 

[ud@cu-ud1 elasticsearch-5.1.1]$ vi config/elasticsearch.yml 
...
http.cors.enabled: true
http.cors.allow-origin: "*"

[ud@cu-ud1 elasticsearch-head]$ node_modules/grunt/bin/grunt server
</code></pre>

<p>然后浏览器访问9100端口即可。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用 Flume+kafka+elasticsearch 处理数据]]></title>
    <link href="http://winseliu.com/blog/2016/06/28/flume-kafka-elasticsearch-for-analyse/"/>
    <updated>2016-06-28T01:50:05+00:00</updated>
    <id>http://winseliu.com/blog/2016/06/28/flume-kafka-elasticsearch-for-analyse</id>
    <content type="html"><![CDATA[<p>flume-1.6依赖的kafka、elasticsearch的版本与我这使用程序的版本不一致，部分jar依赖需要替换，flume-elasticsearch-sink源码需要进行一些修改来适配elasticsearch-2.2。</p>

<ul>
<li>flume-1.6.0</li>
<li>kafka_2.11-0.9.0.1(可以与0.8.2客户端通信, flume-kafka-channel-1.6.0不改)</li>
<li>elasticsearch-2.2.0</li>
</ul>


<p>由于版本的差异，需要替换/添加以下jar到 <code>flume/lib</code> 下：</p>

<p>使用 <code>mvn dependecy:copy-dependencies</code> 导出所需依赖的包</p>

<p>jackson一堆，hppc-0.7.1.jar，t-digest-3.0.jar，jsr166e-1.1.0.jar，guava-18.0.jar，lucene一堆，elasticsearch-2.2.0.jar。</p>

<p>远程调试配置：</p>

<p>source由于项目上的一些特殊规则，需要自己编写。通过远程DEBUG来打断点来排查BUG。</p>

<pre><code>[hadoop@cu2 flume]$ vi conf/flume-env.sh
export JAVA_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8092"
</code></pre>

<h2>实战1</h2>

<p>KafkaChannel：考虑到其他功能也需要用到这些数据。</p>

<p>先写一个配置把flume自带功能跑通，这里用 netcat 作为输入运行：</p>

<pre><code>[hadoop@cu2 flumebin]$ cat dta.flume 
dta.sources=s1
dta.channels=c1
dta.sinks=k1

dta.channels.c1.type=org.apache.flume.channel.kafka.KafkaChannel
dta.channels.c1.capacity=10000
dta.channels.c1.transactionCapacity=1000
dta.channels.c1.brokerList=cu5:9093
dta.channels.c1.topic=flume_cmdid_1234
dta.channels.c1.groupId=flume_dta
dta.channels.c1.zookeeperConnect=cu3:2181/kafka_0_9
dta.channels.c1.parseAsFlumeEvent=false

dta.sources.s1.channels=c1
dta.sources.s1.type=netcat
dta.sources.s1.bind=0.0.0.0
dta.sources.s1.port=6666
dta.sources.s1.max-line-length=88888888

dta.sinks.k1.channel=c1
dta.sinks.k1.type=elasticsearch
dta.sinks.k1.hostNames=cu2:9300
dta.sinks.k1.indexName=foo_index
dta.sinks.k1.indexType=idcisp
dta.sinks.k1.clusterName=eshore-cu
dta.sinks.k1.batchSize=500
dta.sinks.k1.ttl=5d
dta.sinks.k1.serializer=com.eshore.zhfx.collector.InfoSecurityLogIndexRequestBuilderFactory
dta.sinks.k1.serializer.idcispUrlBase64=true

[hadoop@cu2 flumebin]$ bin/flume-ng agent --classpath flume-dta-source-2.1.jar  -n dta -c conf -f dta.flume

# 新开一个窗口
[hadoop@cu2 ~]$ nc localhost 6666
</code></pre>

<p>kafka的主题、ES的索引可以不要手动建，当然为了更好的控制ES索引创建可以添加一个索引名的template。</p>

<p>InfoSecurityLogIndexRequestBuilderFactory 实现 ElasticSearchIndexRequestBuilderFactory 把原始记录转换成 ES 的JSON对象。</p>

<pre><code>  private Counter allRecordMetric = MetricManager.getInstance().counter("all_infosecurity");
  private Counter errorRecordMetric = MetricManager.getInstance().counter("error_infosecurity");

  public IndexRequestBuilder createIndexRequest(Client client, String indexPrefix, String indexType, Event event)
      throws IOException {
    allRecordMetric.inc();

    String record = new String(event.getBody(), outputCharset);

    context.put(ElasticSearchSinkConstants.INDEX_NAME, indexPrefix);
    indexNameBuilder.configure(context);
    IndexRequestBuilder indexRequestBuilder = client.prepareIndex(indexNameBuilder.getIndexName(event), indexType);

    try {
      Gson gson = new Gson();
      IdcIspLog log = parseRecord(record);
      BytesArray data = new BytesArray(gson.toJson(log));

      indexRequestBuilder.setSource(data);
      indexRequestBuilder.setRouting(log.commandld);
    } catch (Exception e) {
      LOG.error(e.getMessage(), e);
      errorRecordMetric.inc();

      indexRequestBuilder.setSource(record.getBytes(outputCharset));
      // 保留错误的数据
      indexRequestBuilder.setRouting("error");
    }

    return indexRequestBuilder;
  }
</code></pre>

<h2>实战2</h2>

<p>测试自定义的Source：</p>

<pre><code>dta.sources=s1
dta.channels=c1
dta.sinks=k1

dta.channels.c1.type=memory
dta.channels.c1.capacity=1000000
dta.channels.c1.transactionCapacity=1000000
dta.channels.c1.byteCapacity=7000000000

dta.sources.s1.channels=c1
dta.sources.s1.type=com.eshore.zhfx.collector.CollectSource
dta.sources.s1.spoolDir=/home/hadoop/flume/data/
dta.sources.s1.trackerDir=/tmp/dtaspool

dta.sinks.k1.channel=c1
dta.sinks.k1.type=logger
</code></pre>

<p>CollectSource 实现PollableSource 继承AbstractSource类。参考Flume开发文档: <a href="http://flume.apache.org/FlumeDeveloperGuide.html#source">http://flume.apache.org/FlumeDeveloperGuide.html#source</a>  <code>org.apache.flume.source.SequenceGeneratorSource</code> 类。</p>

<p>方法process主逻辑代码如下：</p>

<pre><code>  public Status process() throws EventDeliveryException {
    Status status = Status.READY;

    try {
      List&lt;Event&gt; events = readEvent(batchSize);
      if (!events.isEmpty()) {
        sourceCounter.addToEventReceivedCount(events.size());
        sourceCounter.incrementAppendBatchReceivedCount();

        getChannelProcessor().processEventBatch(events);
        // 记录文件已经处理的位置
        commit();

        sourceCounter.addToEventAcceptedCount(events.size());
        sourceCounter.incrementAppendBatchAcceptedCount();
      }
    } catch (ChannelException | IOException e) {
      status = Status.BACKOFF;
      Throwables.propagate(e);
    }

    return status;
  }
</code></pre>

<h2>实例：Flume+Kafka+ES</h2>

<p>把两个实例整合起来，把实例1的Source替换下即可。</p>

<h2>附-kafka基本操作</h2>

<pre><code>[hadoop@cu5 kafka_2.11-0.9.0.1]$ bin/kafka-server-start.sh config/server1.properties 

[hadoop@cu5 kafka_2.11-0.9.0.1]$ cat config/server1.properties 
listeners=PLAINTEXT://:9093
log.dirs=/tmp/kafka-logs1
num.partitions=1
zookeeper.connect=cu3,cu4,cu5/kafka_0_9

[hadoop@cu5 kafka_2.11-0.9.0.1]$ bin/kafka-topics.sh --create --zookeeper cu3:2181/kafka_0_9 --replication 1 --partitions 1 --topic flume
Created topic "flume".

[hadoop@cu5 kafka_2.11-0.9.0.1]$ bin/kafka-topics.sh --list --zookeeper cu3:2181/kafka_0_9
flume

[hadoop@cu5 kafka_2.11-0.9.0.1]$ bin/kafka-console-producer.sh --broker-list cu5:9093 --topic flume

[hadoop@cu5 kafka_2.11-0.9.0.1]$ bin/kafka-console-consumer.sh --zookeeper cu3:2181/kafka_0_9 --topic flume --from-beginning

##添加kafka-manager：

启动kafka添加JMX
export JMX_PORT=19999
nohup bin/kafka-server-start.sh config/server.properties &amp;

# https://github.com/yahoo/kafka-manager/tree/1.3.1.8
nohup bin/kafka-manager -Dhttp.port=9090 &amp;
</code></pre>

<h2>附-Flume操作</h2>

<pre><code>https://flume.apache.org/FlumeUserGuide.html#fan-out-flow

#conf/flume-env.sh
export FLUME_JAVA_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8092"
bin/flume-ng agent --classpath "flume-dta-libs/*" -Dflume.root.logger=DEBUG,console  -n dta -c conf -f accesslog.flume

# with ganglia
[ud@cu-ud1 apache-flume-1.6.0-bin]$ bin/flume-ng agent --classpath "/home/ud/collector/common-lib/*"  -Dflume.root.logger=Debug,console -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=239.2.11.71:8649 -n dta -c conf -f accesslog.flume 

# windows
bin\flume-ng.cmd agent -n agent -c conf -f helloworld.flume -property "flume.root.logger=INFO,console"
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch Startguide]]></title>
    <link href="http://winseliu.com/blog/2016/06/15/elasticsearch-startguide/"/>
    <updated>2016-06-15T00:40:28+00:00</updated>
    <id>http://winseliu.com/blog/2016/06/15/elasticsearch-startguide</id>
    <content type="html"><![CDATA[<p>如果有Lucene的使用经历，elasticsearch的入门还是比较简单的。直接解压启动命令就安装好了，然后就是添加一些plugins就OK了。</p>

<h2>安装</h2>

<p>从官网下载 <a href="https://www.elastic.co/downloads/elasticsearch">TAR包</a> ，解压后，运行 elasticsearch 脚本启动服务。</p>

<pre><code># -d 表示 daemonize 后台运行
[hadoop@cu2 elasticsearch-2.2.0]$ bin/elasticsearch -d
</code></pre>

<h2>插件</h2>

<p>大部分插件都是ajax方式的静态页面，可以通过plugin脚本安装，或者直接解压文件到plugins目录下面。</p>

<p>安装已经下载到本地的插件需要加file协议，不然程序会从官网下载。或者直接解压到plugins目录下：</p>

<pre><code>[hadoop@cu2 elasticsearch-2.2.0]$ bin/plugin install file:///home/hadoop/elasticsearch-head-master.zip 
-&gt; Installing from file:/home/hadoop/elasticsearch-head-master.zip...
Trying file:/home/hadoop/elasticsearch-head-master.zip ...
Downloading .........DONE
Verifying file:/home/hadoop/elasticsearch-head-master.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
Installed head into /home/hadoop/elasticsearch-2.2.0/plugins/head
</code></pre>

<p>windows</p>

<pre><code>E:\local\usr\share\elasticsearch-2.3.3\bin&gt;plugin.bat install file:///D:/SOFTWARE/elasticsearch/elasticsearch-plugin/elasticsearch-head-master.zip
</code></pre>

<p>安装好plugin后，打开浏览器查看索引情况： <a href="http://localhost:9200/_plugin/head/">http://localhost:9200/_plugin/head/</a></p>

<h2>插件高阶</h2>

<p>有些插件版本比较旧需要改一改，需要了解新版本的 elasticsearch-plugin 的规范：</p>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/installation.html">https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/installation.html</a></p>

<p>新版本插件主要是需要增加一个描述文件：</p>

<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/breaking_20_plugin_and_packaging_changes.html#_plugins_require_descriptor_file">Plugins require descriptor file</a></p>

<p>遇到想安装的旧版本的plugin，描述文件写法可以参考 <a href="https://github.com/mobz/elasticsearch-head">elasticsearch-head</a> 。</p>

<p>可选插件：</p>

<ul>
<li>paramedic <a href="https://github.com/karmi/elasticsearch-paramedic">https://github.com/karmi/elasticsearch-paramedic</a></li>
<li>head <a href="https://github.com/mobz/elasticsearch-head">https://github.com/mobz/elasticsearch-head</a></li>
<li>kopf <a href="https://github.com/lmenezes/elasticsearch-kopf">https://github.com/lmenezes/elasticsearch-kopf</a></li>
</ul>


<h2>常用URL请求</h2>

<pre><code># https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html
# 创建
$ curl -XPUT 'http://localhost:9200/t_ods_idc_isp_log2/' -d '{
    "settings" : {
        "index" : {
            "number_of_shards" : 3,
            "number_of_replicas" : 0
        }
    }
}'
{"acknowledged":true}

# https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html
# 更新
# mapping.json
{
    "properties": {
        "author": {
            "type": "string"
        },
...
        "year": {
            "type": "long",
            "ignore_malformed": false,
            "index": "analyzed"
        },
        "avaiable": {
            "type": "boolean"
        }
    }
}
$ curl -XPUT 'localhost:9200/t_ods_idc_isp_log2/_mapping/default' -d @mapping.json

$ curl -XPUT 'localhost:9200/t_ods_idc_isp_log2/_mapping/default' -d '
{
  "properties": {
    "fDIID": {
      "type": "string"
    },
...
    "gatherTime": {
      "type": "long"
    }
  }
}
'

# https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html
# 索引
# documents.json
{ "index": {        "_index": "library",        "_type": "book",        "_id": "1"  } }
{   "title": "All Quiet on the Western Front",  "otitle": "Im Westen nichts Neues",     "author": "Erich Maria Remarque",   "year": 1929,   "characters": ["Paul Baumer",   "Albert Kropp",     "Haie Westhus",     "Fredrich Muller",  "Stanislaus Katczinsky",    "Tjaden"],  "tags": ["novel"],  "copies": 1,    "available": true,  "section": 3 }
{   "index": {      "_index": "library",        "_type": "book",        "_id": "2"  } }
{   "title": "Catch-22",    "author": "Joseph Heller",  "year": 1961,   "characters": ["John Yossarian",    "Captain Aardvark",     "Chaplain Tappman",     "Colonel Cathcart",     "Doctor Daneeka"],  "tags": ["novel"],  "copies": 6,    "available": false,     "section": 1 }

$ curl -s -XPOST localhost:9200/_bulk --data-binary @documents.json

# 删除
$ curl -XDELETE 'http://localhost:9200/t_ods_idc_isp_log2/'
{"acknowledged":true}

# https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html
# https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-field-mapping.html
# 状态查看
http://localhost:9200/_cat/health?v
http://localhost:9200/_cat/nodes?v
http://localhost:9200/_cat/indices?v

curl -XGET 'http://localhost:9200/_all/_mapping/book/field/author'
curl -XHEAD -i 'http://localhost:9200/twitter/tweet'
curl localhost:9200/_stats
curl -XGET 'http://localhost:9200/_all/_mapping/[type]'
</code></pre>

<p></p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
