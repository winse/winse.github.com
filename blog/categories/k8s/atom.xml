<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: #k8s | Winse Blog]]></title>
  <link href="http://winse.github.io/blog/categories/k8s/atom.xml" rel="self"/>
  <link href="http://winse.github.io/"/>
  <updated>2024-01-16T12:59:42+08:00</updated>
  <id>http://winse.github.io/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[K8S官方例子NFS]]></title>
    <link href="http://winse.github.io/blog/2022/04/19/k8s-nfs-run-example/"/>
    <updated>2022-04-19T18:34:01+08:00</updated>
    <id>http://winse.github.io/blog/2022/04/19/k8s-nfs-run-example</id>
    <content type="html"><![CDATA[<h2>参考</h2>

<p>源码</p>

<ul>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs">https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs</a></li>
<li><a href="https://github.com/kubernetes/examples/blob/master/staging/volumes/nfs/nfs-server-deployment.yaml">https://github.com/kubernetes/examples/blob/master/staging/volumes/nfs/nfs-server-deployment.yaml</a></li>
</ul>


<p>本地volumn</p>

<ul>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#local">https://kubernetes.io/zh/docs/concepts/storage/volumes/#local</a></li>
</ul>


<h2>运行</h2>

<h3>目录结构</h3>

<pre><code>[ec2-user@k8s ~]$ git clone https://github.com/kubernetes/examples

[ec2-user@k8s ~]$ cd examples/staging/volumes/nfs/
[ec2-user@k8s nfs]$ ls -l
total 48
-rw-rw-r-- 1 ec2-user ec2-user  823 Apr 19 20:34 nfs-busybox-deployment.yaml
drwxrwxr-x 2 ec2-user ec2-user   77 Apr 19 20:34 nfs-data
-rw-rw-r-- 1 ec2-user ec2-user  193 Apr 19 20:34 nfs-pvc.yaml
-rw-rw-r-- 1 ec2-user ec2-user 9379 Apr 19 20:34 nfs-pv.png
-rw-rw-r-- 1 ec2-user ec2-user  234 Apr 19 20:34 nfs-pv.yaml
-rw-rw-r-- 1 ec2-user ec2-user  729 Apr 19 20:34 nfs-server-deployment.yaml
-rw-rw-r-- 1 ec2-user ec2-user  212 Apr 19 20:34 nfs-server-service.yaml
-rw-rw-r-- 1 ec2-user ec2-user  673 Apr 19 20:34 nfs-web-deployment.yaml
-rw-rw-r-- 1 ec2-user ec2-user  120 Apr 19 20:34 nfs-web-service.yaml
drwxrwxr-x 2 ec2-user ec2-user   98 Apr 19 20:34 provisioner
-rw-rw-r-- 1 ec2-user ec2-user 6540 Apr 19 20:34 README.md
[ec2-user@k8s nfs]$ ls -l provisioner/
total 12
-rw-rw-r-- 1 ec2-user ec2-user 291 Apr 19 20:34 nfs-server-azure-pv.yaml
-rw-rw-r-- 1 ec2-user ec2-user 324 Apr 19 20:34 nfs-server-cdk-pv.yaml
-rw-rw-r-- 1 ec2-user ec2-user 215 Apr 19 20:34 nfs-server-gce-pv.yaml
</code></pre>

<ul>
<li>nfs-data 是nfs-server镜像构建Dockerfile相关文件。</li>
<li>nfs-server：

<ul>
<li>nfs-server-deployment.yaml</li>
<li>nfs-server-service.yaml</li>
<li>provisioner</li>
</ul>
</li>
<li>web

<ul>
<li>nfs-pv.yaml</li>
<li>nfs-pvc.yaml</li>
<li>nfs-web-deployment.yaml</li>
<li>nfs-web-service.yaml</li>
<li>nfs-busybox-deployment.yaml</li>
</ul>
</li>
</ul>


<p>按照结构，一步步的来进行配置和运行。</p>

<h3>搭建NFS Server的本地存储卷</h3>

<ul>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#local">https://kubernetes.io/zh/docs/concepts/storage/volumes/#local</a></li>
</ul>


<pre><code>[ec2-user@k8s ~]$ sudo mkdir /nfs
[ec2-user@k8s ~]$ sudo chmod 777 /nfs

[ec2-user@k8s nfs]$ cat server-pv.yml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 10Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
# https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /nfs
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - k8s

[ec2-user@k8s nfs]$ kubectl apply -f server-pv.yml
persistentvolume/nfs-pv created
</code></pre>

<p>创建server pvc：</p>

<pre><code>[ec2-user@k8s nfs]$ kubectl create namespace nfs
namespace/nfs-server created

[ec2-user@k8s nfs]$ cat provisioner/nfs-server-gce-pv.yaml   
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pv-provisioning-demo
  labels:
    demo: nfs-pv-provisioning
spec:
  accessModes: [ "ReadWriteOnce" ]
  resources:
    requests:
      storage: 10Gi
  storageClassName: local-storage

[ec2-user@k8s nfs]$ kubectl apply -f provisioner/nfs-server-gce-pv.yaml -n nfs
persistentvolumeclaim/nfs-pv-provisioning-demo created

[ec2-user@k8s nfs]$ kubectl get pv 
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS    REASON   AGE
nfs-pv   10Gi       RWO            Delete           Bound    nfs/nfs-pv-provisioning-demo   local-storage            21s
[ec2-user@k8s nfs]$ kubectl get pvc -n nfs 
NAME                       STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
nfs-pv-provisioning-demo   Bound    nfs-pv   10Gi       RWO            local-storage   18s
</code></pre>

<h3>启动nfs-server</h3>

<pre><code># 下载镜像要代理的，可以替换image或者下载后改tag

[ec2-user@k8s nfs]$ cat nfs-server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-server
spec:
  replicas: 1
  selector:
    matchLabels:
      role: nfs-server
  template:
    metadata:
      labels:
        role: nfs-server
    spec:
      containers:
      - name: nfs-server
        image: k8s.gcr.io/volume-nfs:0.8
        ports:
          - name: nfs
            containerPort: 2049
          - name: mountd
            containerPort: 20048
          - name: rpcbind
            containerPort: 111
        securityContext:
          privileged: true
        volumeMounts:
          - mountPath: /exports
            name: mypvc
      volumes:
        - name: mypvc
          persistentVolumeClaim:
            claimName: nfs-pv-provisioning-demo
[ec2-user@k8s nfs]$ cat nfs-server-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: nfs-server
spec:
  ports:
    - name: nfs
      port: 2049
    - name: mountd
      port: 20048
    - name: rpcbind
      port: 111
  selector:
    role: nfs-server

[ec2-user@k8s nfs]$ kubectl apply -f nfs-server-deployment.yaml -f nfs-server-service.yaml -n nfs
deployment.apps/nfs-server created
service/nfs-server created

[ec2-user@k8s nfs]$ kubectl get all -n nfs -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP             NODE   NOMINATED NODE   READINESS GATES
pod/nfs-server-64886b598f-57mnx   1/1     Running   0          18s   10.244.0.146   k8s    &lt;none&gt;           &lt;none&gt;

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE   SELECTOR
service/nfs-server   ClusterIP   10.111.29.73   &lt;none&gt;        2049/TCP,20048/TCP,111/TCP   18s   role=nfs-server

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                      SELECTOR
deployment.apps/nfs-server   1/1     1            1           18s   nfs-server   k8s.gcr.io/volume-nfs:0.8   role=nfs-server

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                      SELECTOR
replicaset.apps/nfs-server-64886b598f   1         1         1       18s   nfs-server   k8s.gcr.io/volume-nfs:0.8   pod-template-hash=64886b598f,role=nfs-server
</code></pre>

<p>根据nodeAffinity容器部署在了k8s的机器。</p>

<h3>启动web应用</h3>

<p>创建容器使用的卷：</p>

<pre><code># 不知为何，后面容器挂载卷的时刻识别不了域名... 
# mount的时刻可能是在node节点上执行的，所以解析不了域名。
# 把主机的 nameserver设置为（/etc/resolv.conf） nameserver 10.96.0.10 就可以了。
[ec2-user@k8s nfs]$ cat nfs-pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs
spec:
  capacity:
    storage: 1Mi
  accessModes:
    - ReadWriteMany
  nfs:
    server: nfs-server.nfs.svc.cluster.local
    path: "/"
  mountOptions:
    - nfsvers=4.2

[ec2-user@k8s nfs]$ kubectl apply -f nfs-pv.yaml 
persistentvolume/nfs created
[ec2-user@k8s nfs]$ kubectl get pv nfs
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
nfs    1Mi        RWX            Retain           Available                                   12s


[ec2-user@k8s nfs]$ cat nfs-pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: ""
  resources:
    requests:
      storage: 1Mi
  volumeName: nfs

# kubectl get pv | tail -n+2 | awk '$5 == "Released" {print $1}' | xargs -I{} kubectl patch pv {} --type='merge' -p '{"spec":{"claimRef": null}}
# 
# [ec2-user@k8s nfs]$ kubectl patch pv nfs -p '{"spec":{"claimRef": null}}'                
# persistentvolume/nfs patched
[ec2-user@k8s nfs]$ kubectl apply -f nfs-pvc.yaml
persistentvolumeclaim/nfs created


[ec2-user@k8s nfs]$ kubectl get pv 
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS    REASON   AGE
nfs      1Mi        RWX            Retain           Bound    default/nfs                                                 56s
nfs-pv   10Gi       RWO            Delete           Bound    nfs/nfs-pv-provisioning-demo   local-storage            3m8s
[ec2-user@k8s nfs]$ kubectl get pvc -A 
NAMESPACE   NAME                       STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
nfs         nfs                        Bound    nfs      1Mi        RWX                            22s
nfs         nfs-pv-provisioning-demo   Bound    nfs-pv   10Gi       RWO            local-storage   3m
</code></pre>

<p>部署web应用的容器：</p>

<pre><code>[ec2-user@k8s nfs]$ cat nfs-web-deployment.yaml 
# This pod mounts the nfs volume claim into /usr/share/nginx/html and
# serves a simple web page.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-web
spec:
  replicas: 2
  selector:
    matchLabels:
      role: web-frontend
  template:
    metadata:
      labels:
        role: web-frontend
    spec:
      containers:
      - name: web
        image: nginx
        ports:
          - name: web
            containerPort: 80
        volumeMounts:
            # name must match the volume name below
            - name: nfs
              mountPath: "/usr/share/nginx/html"
      volumes:
      - name: nfs
        persistentVolumeClaim:
          claimName: nfs

[ec2-user@k8s nfs]$ cat nfs-web-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: nfs-web
spec:
  ports:
    - port: 80
  selector:
    role: web-frontend

[ec2-user@k8s nfs]$ kubectl apply -f nfs-web-deployment.yaml -f nfs-web-service.yaml
deployment.apps/nfs-web created
service/nfs-web created

[ec2-user@k8s nfs]$ kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/nfs-web-5554f77845-6rq5p   1/1     Running   0          4m56s
pod/nfs-web-5554f77845-8r885   1/1     Running   0          4m56s

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP   33d
service/nfs-web      ClusterIP   10.108.129.153   &lt;none&gt;        80/TCP    4m56s

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nfs-web   2/2     2            2           4m56s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/nfs-web-5554f77845   2         2         2       4m56s
</code></pre>

<p>测试：deployment创建两个busybox，会在nfs共享目录下创建修改index.html，内容为：时间和主机名。</p>

<pre><code>[ec2-user@k8s nfs]$ kubectl apply -f nfs-busybox-deployment.yaml 
deployment.apps/nfs-busybox created

[ec2-user@k8s nfs]$ curl nfs-web.default.svc.cluster.local 
Wed Apr 20 00:15:20 UTC 2022
nfs-busybox-55c668489c-dxqcx
[ec2-user@k8s nfs]$ curl nfs-web.default.svc.cluster.local
Wed Apr 20 00:16:51 UTC 2022
nfs-busybox-55c668489c-sl7jz
</code></pre>

<h3>清理</h3>

<pre><code>[ec2-user@k8s nfs]$ kubectl delete -f nfs-busybox-deployment.yaml -f nfs-web-deployment.yaml -f nfs-web-service.yaml -f nfs-pvc.yaml -f nfs-pv.yaml 
[ec2-user@k8s nfs]$ kubectl delete ns nfs 
[ec2-user@k8s nfs]$ kubectl delete -f server-pv.yml 
</code></pre>

<h2>问题</h2>

<h3>NFS服务器的域名解析不了</h3>

<p>一开始没有改节点的dns，会出现域名解析不了的情况：</p>

<pre><code>[ec2-user@k8s nfs]$ kubectl describe pod nfs-web-7bc965b94f-k6mrj -n nfs 
...
Events:
  Type     Reason       Age               From               Message
  ----     ------       ----              ----               -------
  Normal   Scheduled    43s               default-scheduler  Successfully assigned nfs/nfs-web-7bc965b94f-k6mrj to worker1
  Warning  FailedMount  1s (x7 over 35s)  kubelet            MountVolume.SetUp failed for volume "nfs" : mount failed: exit status 32
Mounting command: mount
Mounting arguments: -t nfs -o nfsvers=4.2 nfs-server.nfs.svc.cluster.local:/ /var/lib/kubelet/pods/03049217-ded3-4d31-86e8-6a13c0f5b12f/volumes/kubernetes.io~nfs/nfs
Output: mount.nfs: Failed to resolve server nfs-server.nfs.svc.cluster.local: Name or service not known
mount.nfs: Operation already in progress
</code></pre>

<p>容器内是可以通过域名访问的，mount的时刻可能是在node节点上执行的，所以解析不了域名。</p>

<pre><code>[ec2-user@k8s ~]$ kubectl run -ti test --rm --image busybox -n nfs -- sh  
/ # ping nfs-server.nfs.svc.cluster.local
[ec2-user@k8s nfs]$ kubectl run -ti test --rm --image busybox -- sh

/ # cat /etc/resolv.conf 
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local localdomain
options ndots:5
</code></pre>

<p>把所有节点的dns设置为 kube-dns 的 10.96.0.10 就可以了。</p>

<h3>集群出问题</h3>

<pre><code>[ec2-user@k8s nfs]$ kubectl get pods -A 
,,,
ingress-nginx          ingress-nginx-controller-755447bb4d-55hjz    0/1     CrashLoopBackOff       32 (2m19s ago)   17d
kube-system            coredns-64897985d-4d5rx                      0/1     CrashLoopBackOff       40 (4m30s ago)   33d
kube-system            coredns-64897985d-m9p9q                      0/1     CrashLoopBackOff       40 (4m15s ago)   33d
,,,
kubernetes-dashboard   dashboard-metrics-scraper-799d786dbf-rj8rl   0/1     CrashLoopBackOff       33 (116s ago)    17d
kubernetes-dashboard   kubernetes-dashboard-fb8648fd9-22krh         0/1     CrashLoopBackOff       32 (3m35s ago)   17d
</code></pre>

<p>把所有的容器都清理了，然后重启服务器（或者kubelet）</p>

<pre><code>[ec2-user@k8s nfs]$ docker ps -a |  awk '{print $1}' | while read i ; do docker rm -f $i ; done 
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[k8s共享存储使用NFS]]></title>
    <link href="http://winse.github.io/blog/2022/04/14/k8s-nfs/"/>
    <updated>2022-04-14T01:23:14+08:00</updated>
    <id>http://winse.github.io/blog/2022/04/14/k8s-nfs</id>
    <content type="html"><![CDATA[<p>容器中的应用数据得保存下来，使用local/hostPath可以临时用用，还是得有一个共享的存储。</p>

<ul>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#volume-types">https://kubernetes.io/zh/docs/concepts/storage/volumes/#volume-types</a></li>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/_print/#types-of-persistent-volumes">https://kubernetes.io/zh/docs/concepts/storage/_print/#types-of-persistent-volumes</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner">https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/volumes/#hostpath">https://kubernetes.io/docs/concepts/storage/volumes/#hostpath</a></li>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#local">https://kubernetes.io/zh/docs/concepts/storage/volumes/#local</a></li>
<li><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#cephfs">https://kubernetes.io/zh/docs/concepts/storage/volumes/#cephfs</a></li>
</ul>


<p>先使用最简单的NFS分区/卷。</p>

<ul>
<li><a href="http://www.lishuai.fun/2021/08/12/k8s-nfs-pv">http://www.lishuai.fun/2021/08/12/k8s-nfs-pv</a></li>
</ul>


<h2>安装NFS server on aws ec2</h2>

<ul>
<li><a href="https://segmentfault.com/a/1190000024512057">https://segmentfault.com/a/1190000024512057</a></li>
</ul>


<pre><code>#所有node节点安装nfs客户端
#yum -y install nfs-utils
#systemctl start nfs &amp;&amp; systemctl enable nfs


[ec2-user@k8s ~]$ sudo yum install nfs-utils 
Loaded plugins: langpacks, priorities, update-motd
amzn2-core                                                                                                                     | 3.7 kB  00:00:00     
Package 1:nfs-utils-1.3.0-0.54.amzn2.0.2.x86_64 already installed and latest version
Nothing to do

[ec2-user@k8s ~]$ sudo mkdir /backup
[ec2-user@k8s ~]$ sudo chmod -R 755 /backup
[ec2-user@k8s ~]$ sudo chown nfsnobody:nfsnobody /backup

[ec2-user@k8s ~]$ sudo vi /etc/exports
[ec2-user@k8s ~]$ cat /etc/exports    
/backup 192.168.191.0/24(rw,sync,no_root_squash,no_all_squash)

# /k8s-fs *(rw,sync,no_root_squash,no_all_squash)

[ec2-user@k8s ~]$ sudo service nfs-server restart 
Redirecting to /bin/systemctl restart nfs-server.service
[ec2-user@k8s ~]$ 
[ec2-user@k8s ~]$ sudo exportfs
/backup         192.168.191.0/24
[ec2-user@k8s ~]$ sudo exportfs -arv 
exporting 192.168.191.0/24:/backup

[ec2-user@k8s ~]$ rpcinfo -p localhost
   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
    100024    1   udp  56847  status
    100024    1   tcp  60971  status
    100005    1   udp  20048  mountd
    100005    1   tcp  20048  mountd
    100005    2   udp  20048  mountd
    100005    2   tcp  20048  mountd
    100005    3   udp  20048  mountd
    100005    3   tcp  20048  mountd
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    3   tcp   2049  nfs_acl
    100003    3   udp   2049  nfs
    100227    3   udp   2049  nfs_acl
    100021    1   udp  47545  nlockmgr
    100021    3   udp  47545  nlockmgr
    100021    4   udp  47545  nlockmgr
    100021    1   tcp  40703  nlockmgr
    100021    3   tcp  40703  nlockmgr
    100021    4   tcp  40703  nlockmgr
[ec2-user@k8s ~]$ showmount -e 192.168.191.131
Export list for 192.168.191.131:
/backup 192.168.191.0/24
</code></pre>

<p>也可以通过docker来启动nfs server：</p>

<ul>
<li><a href="https://blog.ruanbekker.com/blog/2020/09/20/setup-a-nfs-server-with-docker/">https://blog.ruanbekker.com/blog/2020/09/20/setup-a-nfs-server-with-docker/</a></li>
<li><a href="https://westzq1.github.io/k8s/2019/06/28/nfs-server-on-K8S.html">https://westzq1.github.io/k8s/2019/06/28/nfs-server-on-K8S.html</a></li>
<li><a href="https://github.com/kubernetes/examples/blob/master/staging/volumes/nfs/nfs-server-deployment.yaml">https://github.com/kubernetes/examples/blob/master/staging/volumes/nfs/nfs-server-deployment.yaml</a></li>
</ul>


<pre><code>[ec2-user@k8s ~]$ sudo mkdir -p /data/kubernetes-volumes
[ec2-user@k8s ~]$ docker run --privileged -itd --name nfs -p 2049:2049 -e SHARED_DIRECTORY=/data -v /data/kubernetes-volumes:/data itsthenetwork/nfs-server-alpine:12 
f84b70dcca6bd5abb275fbee50fd161d8befdd709ce6523b3a514f04b7af8677

[ec2-user@k8s ~]$ docker logs f84b70dcca6bd5abb2 
Writing SHARED_DIRECTORY to /etc/exports file
The PERMITTED environment variable is unset or null, defaulting to '*'.
This means any client can mount.
The READ_ONLY environment variable is unset or null, defaulting to 'rw'.
Clients have read/write access.
The SYNC environment variable is unset or null, defaulting to 'async' mode.
Writes will not be immediately written to disk.
Displaying /etc/exports contents:
/data *(rw,fsid=0,async,no_subtree_check,no_auth_nlm,insecure,no_root_squash)

Starting rpcbind...
Displaying rpcbind status...
   program version netid     address                service    owner
    100000    4    tcp6      ::.0.111               -          superuser
    100000    3    tcp6      ::.0.111               -          superuser
    100000    4    udp6      ::.0.111               -          superuser
    100000    3    udp6      ::.0.111               -          superuser
    100000    4    tcp       0.0.0.0.0.111          -          superuser
    100000    3    tcp       0.0.0.0.0.111          -          superuser
    100000    2    tcp       0.0.0.0.0.111          -          superuser
    100000    4    udp       0.0.0.0.0.111          -          superuser
    100000    3    udp       0.0.0.0.0.111          -          superuser
    100000    2    udp       0.0.0.0.0.111          -          superuser
    100000    4    local     /var/run/rpcbind.sock  -          superuser
    100000    3    local     /var/run/rpcbind.sock  -          superuser
Starting NFS in the background...
rpc.nfsd: knfsd is currently down
rpc.nfsd: Writing version string to kernel: -2 -3 +4 +4.1 +4.2
rpc.nfsd: Created AF_INET TCP socket.
rpc.nfsd: Created AF_INET6 TCP socket.
Exporting File System...
exporting *:/data
/data           &lt;world&gt;
Starting Mountd in the background...These
Startup successful.

[ec2-user@k8s ~]$ sudo mount -v -o vers=4,loud 127.0.0.1:/ nfsmnt
mount.nfs: timeout set for Thu Apr 14 08:26:48 2022
mount.nfs: trying text-based options 'vers=4.1,addr=127.0.0.1,clientaddr=127.0.0.1'

[ec2-user@k8s ~]$ df -h | grep nfsmnt
127.0.0.1:/      25G  9.8G   16G  39% /home/ec2-user/nfsmnt

[ec2-user@k8s ~]$ touch nfsmnt/$(hostname).txt
[ec2-user@k8s ~]$ ls -l nfsmnt/
total 0
-rw-rw-r-- 1 ec2-user ec2-user 0 Apr 14 08:25 k8s.txt
[ec2-user@k8s ~]$ ls -l /data/kubernetes-volumes/
total 0
-rw-rw-r-- 1 ec2-user ec2-user 0 Apr 14 08:25 k8s.txt
[ec2-user@k8s ~]$ 

# vi /etc/fstab
# 192.168.0.4:/   /mnt   nfs4    _netdev,auto  0  0

### pod
# kubectl create -f nfs-server.yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nfs-server
spec:
  replicas: 1           # &lt;- no more replicas
  template:
    metadata:
      labels:
        app: nfs-server
    spec:
      nodeSelector:     # &lt;- use selector to fix nfs-server on k8s2.zhangqiaoc.com
        kubernetes.io/hostname: k8s2.zhangqiaoc.com
      containers:
      - name: nfs-server
        image: itsthenetwork/nfs-server-alpine:latest
        volumeMounts:
        - name: nfs-storage
          mountPath: /nfsshare
        env:
        - name: SHARED_DIRECTORY
          value: "/nfsshare"
        ports:
        - name: nfs  
          containerPort: 2049   # &lt;- export port
        securityContext:
          privileged: true      # &lt;- privileged mode is mandentory.
      volumes:
      - name: nfs-storage  
        hostPath:               # &lt;- the folder on the host machine.
          path: /root/fileshare
# kubectl expose deployment nfs-server --type=ClusterIP
# kubectl get svc

# yum install -y nfs-utils
# mkdir /root/nfsmnt
# mount -v 10.101.117.226:/ /root/nfsmnt
</code></pre>

<p>client</p>

<pre><code># 所有work节点安装 nfs-utils rpcbind
[ec2-user@worker1 ~]$ sudo yum install nfs-utils 
Loaded plugins: langpacks, priorities, update-motd
amzn2-core                                                                                                                                        | 3.7 kB  00:00:00     
Package 1:nfs-utils-1.3.0-0.54.amzn2.0.2.x86_64 already installed and latest version
Nothing to do

[ec2-user@worker1 ~]$ sudo systemctl status nfs
● nfs-server.service - NFS server and services
   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; disabled; vendor preset: disabled)
   Active: inactive (dead)
[ec2-user@worker1 ~]$ sudo systemctl status rpcbind
● rpcbind.service - RPC bind service
   Loaded: loaded (/usr/lib/systemd/system/rpcbind.service; enabled; vendor preset: enabled)
   Active: active (running) since Wed 2022-04-13 20:44:34 CST; 1h 51min ago
  Process: 6979 ExecStart=/sbin/rpcbind -w $RPCBIND_ARGS (code=exited, status=0/SUCCESS)
 Main PID: 7025 (rpcbind)
    Tasks: 1
   Memory: 2.1M
   CGroup: /system.slice/rpcbind.service
           └─7025 /sbin/rpcbind -w

Apr 13 20:44:34 worker1 systemd[1]: Starting RPC bind service...
Apr 13 20:44:34 worker1 systemd[1]: Started RPC bind service.


[ec2-user@worker1 ~]$ sudo mkdir -p /data
[ec2-user@worker1 ~]$ sudo chmod 777 /data 
[ec2-user@worker1 ~]$ sudo mount -t nfs 192.168.191.131:/backup /data
[ec2-user@worker1 ~]$ df -h | grep 192.168.191.131
192.168.191.131:/backup   25G  9.6G   16G  39% /data


# vi /etc/fstab
# 172.17.30.22:/backup /data nfs defaults 0 0
</code></pre>

<h2>k8s中使用NFS</h2>

<ul>
<li><a href="http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/">http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/</a></li>
</ul>


<h3>容器直接挂载NFS</h3>

<pre><code>[ec2-user@k8s ~]$ cat nginx-1.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        volumeMounts:
        - name: data
          mountPath: /usr/share/nginx/html
      volumes:
      - name: data
        nfs:
          path: /backup
          server: 192.168.191.131

[ec2-user@k8s ~]$ kubectl apply -f nginx-1.yml

[ec2-user@k8s ~]$ kubectl get all 
NAME                                   READY   STATUS    RESTARTS   AGE
pod/nginx-deployment-67dcb957c-g2h8x   1/1     Running   0          2m50s
pod/nginx-deployment-67dcb957c-gfv28   1/1     Running   0          2m50s
pod/nginx-deployment-67dcb957c-rqwjs   1/1     Running   0          2m50s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   27d

NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-deployment   3/3     3            3           2m50s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-deployment-67dcb957c   3         3         3       2m50s

[ec2-user@k8s ~]$ kubectl exec -ti pod/nginx-deployment-67dcb957c-g2h8x -- bash 
root@nginx-deployment-67dcb957c-g2h8x:/# echo $(hostname) &gt;/usr/share/nginx/html/1.txt

root@nginx-deployment-67dcb957c-g2h8x:/# mount | grep 192
192.168.191.131:/backup on /usr/share/nginx/html type nfs4 (rw,relatime,vers=4.1,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.191.132,local_lock=none,addr=192.168.191.131)
root@nginx-deployment-67dcb957c-g2h8x:/# 


# 服务端查看文件内容
[ec2-user@k8s ~]$ cat /backup/1.txt 
nginx-deployment-67dcb957c-g2h8x


[ec2-user@k8s ~]$ kubectl delete -f nginx-1.yml 
deployment.apps "nginx-deployment" deleted
</code></pre>

<h3>pvc</h3>

<ul>
<li><a href="https://segmentfault.com/a/1190000040785500">https://segmentfault.com/a/1190000040785500</a></li>
<li><a href="http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#%E5%88%9B%E5%BB%BA%E7%B1%BB%E5%9E%8B%E4%B8%BAnfs%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%E5%8D%B7">http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#%E5%88%9B%E5%BB%BA%E7%B1%BB%E5%9E%8B%E4%B8%BAnfs%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%E5%8D%B7</a></li>
</ul>


<pre><code>[ec2-user@k8s ~]$ vi pv-nfs.yaml 
[ec2-user@k8s ~]$ cat pv-nfs.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany 
  nfs:
    path: /backup
    server: 192.168.191.131

[ec2-user@k8s ~]$ kubectl apply -f pv-nfs.yaml 
persistentvolume/pv-nfs created
[ec2-user@k8s ~]$ kubectl get pv 
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv-nfs   10Gi       RWX            Retain           Available                                   5s

[ec2-user@k8s ~]$ vi pvc-nfs.yaml 
[ec2-user@k8s ~]$ cat pvc-nfs.yaml 
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-nfs
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
[ec2-user@k8s ~]$ kubectl apply -f pvc-nfs.yaml 
persistentvolumeclaim/pvc-nfs created
[ec2-user@k8s ~]$ kubectl get pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-nfs   Bound    pv-nfs   10Gi       RWX                           7s
[ec2-user@k8s ~]$ kubectl get pv 
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   REASON   AGE
pv-nfs   10Gi       RWX            Retain           Bound    default/pvc-nfs                           79s

[ec2-user@k8s ~]$ vi dp-pvc.yaml
[ec2-user@k8s ~]$ cat dp-pvc.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: busybox
  labels:
    app: busybox
spec:
  replicas: 1
  selector:
    matchLabels:
      app: busybox
  template:
    metadata:
      labels:
        app: busybox
    spec:
      containers:
      - name: busybox
        image: busybox
        command: ['sh', '-c', 'echo "Hello, Kubernetes!" &amp;&amp; sleep 3600']
        volumeMounts:
        - name: data
          mountPath: /data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: pvc-nfs

[ec2-user@k8s ~]$ kubectl apply -f dp-pvc.yaml 
deployment.apps/busybox created
[ec2-user@k8s ~]$ 


[ec2-user@k8s ~]$ kubectl get all 
NAME                           READY   STATUS    RESTARTS   AGE
pod/busybox-6b99c495c9-qnvlp   1/1     Running   0          47s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   27d

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/busybox   1/1     1            1           47s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/busybox-6b99c495c9   1         1         1       47s

# 查看NFS中原来的数据
[ec2-user@k8s ~]$ kubectl exec -ti busybox-6b99c495c9-qnvlp -- cat /data/1.txt 
nginx-deployment-67dcb957c-g2h8x
</code></pre>

<p>测一下subPathExpr：</p>

<pre><code>[ec2-user@k8s ~]$ kubectl delete -f dp-pvc.yaml 
deployment.apps "busybox" deleted
[ec2-user@k8s ~]$ 
[ec2-user@k8s ~]$ vi dp-pvc.yaml 
[ec2-user@k8s ~]$ cat dp-pvc.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: busybox
  labels:
    app: busybox
spec:
  replicas: 1
  selector:
    matchLabels:
      app: busybox
  template:
    metadata:
      labels:
        app: busybox
    spec:
      containers:
      - name: busybox
        image: busybox
        command: ['sh', '-c', 'echo "Hello, Kubernetes!" &amp;&amp; sleep 3600']
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        volumeMounts:
        - name: data
          mountPath: /data
          subPathExpr: $(POD_NAME)
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: pvc-nfs

[ec2-user@k8s ~]$ kubectl apply -f dp-pvc.yaml 
deployment.apps/busybox created

[ec2-user@k8s ~]$ kubectl get all 
NAME                           READY   STATUS    RESTARTS   AGE
pod/busybox-5497486bf5-csr6q   1/1     Running   0          7s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   27d

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/busybox   1/1     1            1           7s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/busybox-5497486bf5   1         1         1       7s

[ec2-user@k8s ~]$ kubectl exec -ti pod/busybox-5497486bf5-csr6q -- sh 
/ # ls /data
/ # echo $(hostname) &gt; /data/pvc.txt
/ # exit


# 查看服务端目录下数据
[ec2-user@k8s ~]$ ll /backup/
total 4
-rw-r--r-- 1 root root 33 Apr 14 00:37 1.txt
drwxr-xr-x 2 root root 21 Apr 14 00:51 busybox-5497486bf5-csr6q
[ec2-user@k8s ~]$ cat /backup/busybox-5497486bf5-csr6q/pvc.txt   
busybox-5497486bf5-csr6q
</code></pre>

<p>把replicas改成2，再试试：</p>

<pre><code>
[ec2-user@k8s ~]$ kubectl apply -f dp-pvc.yaml 

[ec2-user@k8s ~]$ kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
busybox-5497486bf5-fkzls   1/1     Running   0          3m8s
busybox-5497486bf5-rv7k7   1/1     Running   0          3m8s

[ec2-user@k8s ~]$ kubectl exec busybox-5497486bf5-fkzls -- sh -c 'echo $(hostname) &gt;/data/$(hostname).txt'
[ec2-user@k8s ~]$ kubectl exec busybox-5497486bf5-rv7k7 -- sh -c 'echo $(hostname) &gt;/data/$(hostname).txt'                        


# 查看服务端目录结构
[ec2-user@k8s ~]$ ll -R /backup/
/backup/:
total 4
-rw-r--r-- 1 root root 33 Apr 14 00:37 1.txt
drwxr-xr-x 2 root root 21 Apr 14 00:51 busybox-5497486bf5-csr6q
drwxr-xr-x 2 root root 42 Apr 14 01:20 busybox-5497486bf5-fkzls
drwxr-xr-x 2 root root 42 Apr 14 01:20 busybox-5497486bf5-rv7k7

/backup/busybox-5497486bf5-csr6q:
total 4
-rw-r--r-- 1 root root 25 Apr 14 00:51 pvc.txt

/backup/busybox-5497486bf5-fkzls:
total 4
-rw-r--r-- 1 root root 25 Apr 14 01:20 busybox-5497486bf5-fkzls.txt

/backup/busybox-5497486bf5-rv7k7:
total 4
-rw-r--r-- 1 root root 25 Apr 14 01:20 busybox-5497486bf5-rv7k7.txt
[ec2-user@k8s ~]$ 
</code></pre>

<h3>NFS Subdir External Provisioner</h3>

<ul>
<li><a href="http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#NFS-Subdir-External-Provisioner">http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#NFS-Subdir-External-Provisioner</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#nfs">https://kubernetes.io/docs/concepts/storage/storage-classes/#nfs</a></li>
<li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner</a></li>
</ul>


<p>NFS subdir external provisioner 使用现有的的NFS 服务器来支持通过 Persistent Volume Claims 动态供应 Kubernetes Persistent Volumes。持久卷默认被配置为${namespace}-${pvcName}-${pvName}，使用这个必须已经拥有 NFS 服务器。</p>

<ul>
<li><a href="http://dockone.io/article/2598">http://dockone.io/article/2598</a> External NFS驱动的工作原理</li>
</ul>


<blockquote><p>K8S的外部NFS驱动，可以按照其工作方式（是作为NFS server还是NFS client）分为两类：</p>

<p>1.nfs-client:</p>

<p>也就是我们接下来演示的这一类，它通过K8S的内置的NFS驱动挂载远端的NFS服务器到本地目录；然后将自身作为storage provider，关联storage class。当用户创建对应的PVC来申请PV时，该provider就将PVC的要求与自身的属性比较，一旦满足就在本地挂载好的NFS目录中创建PV所属的子目录，为Pod提供动态的存储服务。</p>

<p>2.nfs:
与nfs-client不同，该驱动并不使用k8s的NFS驱动来挂载远端的NFS到本地再分配，而是直接将本地文件映射到容器内部，然后在容器内使用ganesha.nfsd来对外提供NFS服务；在每次创建PV的时候，直接在本地的NFS根目录中创建对应文件夹，并export出该子目录。</p>

<p>接下来我们来操作一个nfs-client驱动的例子，先对其有个直观的认识！</p>

<p>External NFS驱动的部署实例</p>

<p>这里，我们将nfs-client驱动做一个deployment部署到K8S集群中，然后对外提供存储服务。</p>

<p>1.部署nfs-client-provisioner</p>

<p>环境变量的PROVISIONER_NAME、NFS服务器地址、NFS对外提供服务的路径信息等需要设置好；部署所使用的yaml文件关键代码如下所示：</p>

<p>2.创建Storage Class</p>

<p>storage class的定义，需要注意的是：provisioner属性要等于驱动所传入的环境变量PROVISIONER_NAME的值。否则，驱动不知道知道如何绑定storage class。</p>

<p>3.创建PVC</p>

<p>这里指定了其对应的storage-class的名字为wise2c-nfs-storage，如下：</p>

<p>4.创建pod</p>

<p>指定该pod使用我们刚刚创建的PVC：henry-claim：</p>

<p>完成之后，如果attach到pod中执行一些文件的读写操作，就可以确定pod的/mnt已经使用了NFS的存储服务了。</p></blockquote>

<ul>
<li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner#without-helm">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner#without-helm</a></li>
</ul>


<p>官方文档中的脚本：</p>

<pre><code># Set the subject of the RBAC objects to the current namespace where the provisioner is being deployed
$ NS=$(kubectl config get-contexts|grep -e "^\*" |awk '{print $5}')
$ NAMESPACE=${NS:-default}
$ sed -i'' "s/namespace:.*/namespace: $NAMESPACE/g" ./deploy/rbac.yaml ./deploy/deployment.yaml
$ kubectl create -f deploy/rbac.yaml
</code></pre>

<p>操作步骤：</p>

<pre><code>[ec2-user@k8s ~]$ git clone https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/
[ec2-user@k8s ~]$ cd nfs-subdir-external-provisioner/

[ec2-user@k8s nfs-subdir-external-provisioner]$ NS=$(kubectl config get-contexts|grep -e "^\*" |awk '{print $5}')
[ec2-user@k8s nfs-subdir-external-provisioner]$ NAMESPACE=${NS:-default}
[ec2-user@k8s nfs-subdir-external-provisioner]$ sed -i'' "s/namespace:.*/namespace: $NAMESPACE/g" ./deploy/rbac.yaml ./deploy/deployment.yaml
[ec2-user@k8s nfs-subdir-external-provisioner]$ kubectl create -f deploy/rbac.yaml
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created


$ vi deploy/deployment.yaml

            - name: NFS_SERVER
              value: 192.168.191.131
            - name: NFS_PATH
              value: /backup
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.191.131
            path: /backup

$ vi deploy/class.yaml

parameters:
  archiveOnDelete: "false"
#Specifies a template for creating a directory path via PVC metadata's such as labels, annotations, name or namespace. To specify metadata use ${.PVC.&lt;metadata&gt;}. Example: If folder should be named like &lt;pvc-namespace&gt;-&lt;pvc-name&gt;, use ${.PVC.namespace}-${.PVC.name} as pathPattern.
#  pathPattern: "${.PVC.namespace}/${.PVC.annotations.nfs.io/storage-path}" # waits for nfs.io/storage-path annotation, if not specified will accept as empty string.
#  onDelete: delete


# 先把镜像拉下来 k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2

[ec2-user@k8s nfs-subdir-external-provisioner]$ kubectl apply -f deploy/deployment.yaml 
deployment.apps/nfs-client-provisioner created

[ec2-user@k8s nfs-subdir-external-provisioner]$ kubectl apply -f deploy/class.yaml 
storageclass.storage.k8s.io/nfs-client created
[ec2-user@k8s nfs-subdir-external-provisioner]$
</code></pre>

<p>测试：</p>

<pre><code># PVC内容
# https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/blob/master/deploy/test-claim.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi

[ec2-user@k8s nfs-subdir-external-provisioner]$ kubectl create -f deploy/test-claim.yaml -f deploy/test-pod.yaml
persistentvolumeclaim/test-claim created
pod/test-pod created

# kubectl delete -f deploy/test-pod.yaml -f deploy/test-claim.yaml
</code></pre>

<p><code>test pod</code> 在共享文件系统下写了一个 <code>touch /mnt/SUCCESS</code> 文件：</p>

<pre><code>[ec2-user@k8s nfs-subdir-external-provisioner]$ ll /backup/default-test-claim-pvc-9857153a-6c2b-42d7-b464-aa5fc2acbf90/
total 0
-rw-r--r-- 1 root root 0 Apr 14 02:14 SUCCESS
</code></pre>

<h3>NFS Ganesha server and external provisioner</h3>

<ul>
<li><a href="https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner">https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner</a></li>
<li><a href="http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#nfs-ganesha-server-and-external-provisioner">http://www.lishuai.fun/2021/08/12/k8s-nfs-pv/#nfs-ganesha-server-and-external-provisioner</a></li>
</ul>


<p>就是直接在k8s集群中装一个NFS server。感觉没有直接在系统安装NFS管理方便，先搁置了。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Minikube Guide]]></title>
    <link href="http://winse.github.io/blog/2022/04/06/minikube-guide/"/>
    <updated>2022-04-06T08:00:07+08:00</updated>
    <id>http://winse.github.io/blog/2022/04/06/minikube-guide</id>
    <content type="html"><![CDATA[<p>正如其名，minikube快速的安装一个k8s的集群，方便新手和应用开发人员调试等。</p>

<p>注：如果资源足够的话，搭建一个kubeadm的集群来的好一些。</p>

<h2>官网文档</h2>

<ul>
<li><a href="https://minikube.sigs.k8s.io/docs/start/">https://minikube.sigs.k8s.io/docs/start/</a></li>
</ul>


<h2>下载</h2>

<pre><code># 墙外下载
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

# 上传
[ec2-user@amazonlinux ~]$ rz
rz waiting to receive.
Starting zmodem transfer.  Press Ctrl+C to cancel.
Transferring minikube-linux-amd64...
  100%   70948 KB    35474 KB/sec    00:00:02       0 Errors  

[ec2-user@amazonlinux ~]$ sudo install minikube-linux-amd64 /usr/local/bin/minikube
</code></pre>

<h2>安装docker</h2>

<pre><code>[ec2-user@amazonlinux ~]$ sudo amazon-linux-extras install docker
Installing docker
Loaded plugins: langpacks, priorities, update-motd
Cleaning repos: amzn2-core amzn2extra-docker
12 metadata files removed
4 sqlite files removed
0 metadata files removed
Loaded plugins: langpacks, priorities, update-motd
amzn2-core                                                                                                                                        | 3.7 kB  00:00:00     
amzn2extra-docker                                                                                                                                 | 3.0 kB  00:00:00     
(1/5): amzn2-core/2/x86_64/group_gz                                                                                                               | 2.5 kB  00:00:01     
(2/5): amzn2-core/2/x86_64/updateinfo                                                                                                             | 452 kB  00:00:01     
(3/5): amzn2extra-docker/2/x86_64/updateinfo                                                                                                      | 5.9 kB  00:00:00     
(4/5): amzn2extra-docker/2/x86_64/primary_db                                                                                                      |  86 kB  00:00:01     
(5/5): amzn2-core/2/x86_64/primary_db                                                                                                             |  60 MB  00:00:04     
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package docker.x86_64 0:20.10.7-5.amzn2 will be installed
--&gt; Processing Dependency: runc &gt;= 1.0.0 for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Processing Dependency: libcgroup &gt;= 0.40.rc1-5.15 for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Processing Dependency: containerd &gt;= 1.3.2 for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Processing Dependency: pigz for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Running transaction check
---&gt; Package containerd.x86_64 0:1.4.6-8.amzn2 will be installed
---&gt; Package libcgroup.x86_64 0:0.41-21.amzn2 will be installed
---&gt; Package pigz.x86_64 0:2.3.4-1.amzn2.0.1 will be installed
---&gt; Package runc.x86_64 0:1.0.0-2.amzn2 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

=========================================================================================================================================================================
 Package                               Arch                              Version                                      Repository                                    Size
=========================================================================================================================================================================
Installing:
 docker                                x86_64                            20.10.7-5.amzn2                              amzn2extra-docker                             42 M
Installing for dependencies:
 containerd                            x86_64                            1.4.6-8.amzn2                                amzn2extra-docker                             24 M
 libcgroup                             x86_64                            0.41-21.amzn2                                amzn2-core                                    66 k
 pigz                                  x86_64                            2.3.4-1.amzn2.0.1                            amzn2-core                                    81 k
 runc                                  x86_64                            1.0.0-2.amzn2                                amzn2extra-docker                            3.3 M

Transaction Summary
=========================================================================================================================================================================
Install  1 Package (+4 Dependent packages)

Total download size: 69 M
Installed size: 285 M
Is this ok [y/d/N]: t
Is this ok [y/d/N]: y
Downloading packages:
(1/5): libcgroup-0.41-21.amzn2.x86_64.rpm                                                                                                         |  66 kB  00:00:01     
(2/5): pigz-2.3.4-1.amzn2.0.1.x86_64.rpm                                                                                                          |  81 kB  00:00:01     
(3/5): docker-20.10.7-5.amzn2.x86_64.rpm                                                                                                          |  42 MB  00:00:07     
(4/5): runc-1.0.0-2.amzn2.x86_64.rpm                                                                                                              | 3.3 MB  00:00:00     
(5/5): containerd-1.4.6-8.amzn2.x86_64.rpm                                                                                                        |  24 MB  00:00:12     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                                    5.5 MB/s |  69 MB  00:00:12     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : runc-1.0.0-2.amzn2.x86_64                                                                                                                             1/5 
  Installing : containerd-1.4.6-8.amzn2.x86_64                                                                                                                       2/5 
  Installing : libcgroup-0.41-21.amzn2.x86_64                                                                                                                        3/5 
  Installing : pigz-2.3.4-1.amzn2.0.1.x86_64                                                                                                                         4/5 
  Installing : docker-20.10.7-5.amzn2.x86_64                                                                                                                         5/5 
  Verifying  : docker-20.10.7-5.amzn2.x86_64                                                                                                                         1/5 
  Verifying  : containerd-1.4.6-8.amzn2.x86_64                                                                                                                       2/5 
  Verifying  : runc-1.0.0-2.amzn2.x86_64                                                                                                                             3/5 
  Verifying  : pigz-2.3.4-1.amzn2.0.1.x86_64                                                                                                                         4/5 
  Verifying  : libcgroup-0.41-21.amzn2.x86_64                                                                                                                        5/5 

Installed:
  docker.x86_64 0:20.10.7-5.amzn2                                                                                                                                        

Dependency Installed:
  containerd.x86_64 0:1.4.6-8.amzn2           libcgroup.x86_64 0:0.41-21.amzn2           pigz.x86_64 0:2.3.4-1.amzn2.0.1           runc.x86_64 0:1.0.0-2.amzn2          

Complete!
  0  ansible2                 available    \
        [ =2.4.2  =2.4.6  =2.8  =stable ]
  2  httpd_modules            available    [ =1.0  =stable ]
  3  memcached1.5             available    \
        [ =1.5.1  =1.5.16  =1.5.17 ]
  5  postgresql9.6            available    \
        [ =9.6.6  =9.6.8  =stable ]
  6  postgresql10             available    [ =10  =stable ]
  9  R3.4                     available    [ =3.4.3  =stable ]
 10  rust1                    available    \
        [ =1.22.1  =1.26.0  =1.26.1  =1.27.2  =1.31.0  =1.38.0
          =stable ]
 11  vim                      available    [ =8.0  =stable ]
 18  libreoffice              available    \
        [ =5.0.6.2_15  =5.3.6.1  =stable ]
 19  gimp                     available    [ =2.8.22 ]
 20  docker=latest            enabled      \
        [ =17.12.1  =18.03.1  =18.06.1  =18.09.9  =stable ]
 21  mate-desktop1.x          available    \
        [ =1.19.0  =1.20.0  =stable ]
 22  GraphicsMagick1.3        available    \
        [ =1.3.29  =1.3.32  =1.3.34  =stable ]
 23  tomcat8.5                available    \
        [ =8.5.31  =8.5.32  =8.5.38  =8.5.40  =8.5.42  =8.5.50
          =stable ]
 24  epel                     available    [ =7.11  =stable ]
 25  testing                  available    [ =1.0  =stable ]
 26  ecs                      available    [ =stable ]
 27  corretto8                available    \
        [ =1.8.0_192  =1.8.0_202  =1.8.0_212  =1.8.0_222  =1.8.0_232
          =1.8.0_242  =stable ]
 28  firecracker              available    [ =0.11  =stable ]
 29  golang1.11               available    \
        [ =1.11.3  =1.11.11  =1.11.13  =stable ]
 30  squid4                   available    [ =4  =stable ]
 32  lustre2.10               available    \
        [ =2.10.5  =2.10.8  =stable ]
 33  java-openjdk11           available    [ =11  =stable ]
 34  lynis                    available    [ =stable ]
 35  kernel-ng                available    [ =stable ]
 36  BCC                      available    [ =0.x  =stable ]
 37  mono                     available    [ =5.x  =stable ]
 38  nginx1                   available    [ =stable ]
 39  ruby2.6                  available    [ =2.6  =stable ]
 40  mock                     available    [ =stable ]
 41  postgresql11             available    [ =11  =stable ]
 42  php7.4                   available    [ =stable ]
 43  livepatch                available    [ =stable ]
 44  python3.8                available    [ =stable ]
 45  haproxy2                 available    [ =stable ]
 46  collectd                 available    [ =stable ]
 47  aws-nitro-enclaves-cli   available    [ =stable ]
 48  R4                       available    [ =stable ]
 49  kernel-5.4               available    [ =stable ]
 50  selinux-ng               available    [ =stable ]
 51  php8.0                   available    [ =stable ]
 52  tomcat9                  available    [ =stable ]
 53  unbound1.13              available    [ =stable ]
 54  mariadb10.5              available    [ =stable ]
 55  kernel-5.10              available    [ =stable ]
 56  redis6                   available    [ =stable ]
 57  ruby3.0                  available    [ =stable ]
 58  postgresql12             available    [ =stable ]
 59  postgresql13             available    [ =stable ]
 60  mock2                    available    [ =stable ]
 61  dnsmasq2.85              available    [ =stable ]

[ec2-user@amazonlinux ~]$ sudo service docker start
Redirecting to /bin/systemctl start docker.service
[ec2-user@amazonlinux ~]$ sudo systemctl enable docker
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.
[ec2-user@amazonlinux ~]$ sudo usermod -a -G docker ec2-user

[ec2-user@amazonlinux ~]$ exit
logout

[ec2-user@amazonlinux ~]$ docker info 
Client:
 Context:    default
 Debug Mode: false

Server:
 Containers: 0
  Running: 0
  Paused: 0
  Stopped: 0
 Images: 0
 Server Version: 20.10.7
 Storage Driver: overlay2
  Backing Filesystem: xfs
  Supports d_type: true
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: cgroupfs
 Cgroup Version: 1
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: d71fcd7d8303cbf684402823e425e9dd2e99285d
 runc version: 84113eef6fc27af1b01b3181f31bbaf708715301
 init version: de40ad0
 Security Options:
  seccomp
   Profile: default
 Kernel Version: 4.14.268-205.500.amzn2.x86_64
 Operating System: Amazon Linux 2
 OSType: linux
 Architecture: x86_64
 CPUs: 2
 Total Memory: 3.828GiB
 Name: amazonlinux.onprem
 ID: MXVZ:LQK7:BVKI:WECH:XNBN:QJUK:IXYU:FADA:4EYI:JOHA:VS3R:LNLX
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Registry: https://index.docker.io/v1/
 Labels:
 Experimental: false
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: false

[ec2-user@amazonlinux ~]$ docker info
Client:
 Context:    default
 Debug Mode: false

Server:
 Containers: 0
  Running: 0
  Paused: 0
  Stopped: 0
 Images: 0
 Server Version: 20.10.7
 Storage Driver: overlay2
  Backing Filesystem: xfs
  Supports d_type: true
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: cgroupfs
 Cgroup Version: 1
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
 Swarm: inactive
 Runtimes: runc io.containerd.runc.v2 io.containerd.runtime.v1.linux
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: d71fcd7d8303cbf684402823e425e9dd2e99285d
 runc version: 84113eef6fc27af1b01b3181f31bbaf708715301
 init version: de40ad0
 Security Options:
  seccomp
   Profile: default
 Kernel Version: 4.14.268-205.500.amzn2.x86_64
 Operating System: Amazon Linux 2
 OSType: linux
 Architecture: x86_64
 CPUs: 2
 Total Memory: 3.828GiB
 Name: amazonlinux.onprem
 ID: MXVZ:LQK7:BVKI:WECH:XNBN:QJUK:IXYU:FADA:4EYI:JOHA:VS3R:LNLX
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Registry: https://index.docker.io/v1/
 Labels:
 Experimental: false
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: false
</code></pre>

<h2>启动minikube</h2>

<ul>
<li><a href="https://minikube.sigs.k8s.io/docs/start/">https://minikube.sigs.k8s.io/docs/start/</a></li>
</ul>


<pre><code>[ec2-user@amazonlinux ~]$ minikube start
* minikube v1.25.2 on Amazon 2
* Automatically selected the docker driver. Other choices: none, ssh
* Starting control plane node minikube in cluster minikube
* Pulling base image ...
* Downloading Kubernetes v1.23.3 preload ...
    &gt; preloaded-images-k8s-v17-v1...: 505.68 MiB / 505.68 MiB  100.00% 14.20 Mi
    &gt; index.docker.io/kicbase/sta...: 379.06 MiB / 379.06 MiB  100.00% 2.11 MiB
! minikube was unable to download gcr.io/k8s-minikube/kicbase:v0.0.30, but successfully downloaded docker.io/kicbase/stable:v0.0.30 as a fallback image
* Creating docker container (CPUs=2, Memory=2200MB) ...
! This container is having trouble accessing https://k8s.gcr.io
* To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
* Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
  - kubelet.housekeeping-interval=5m
  - Generating certificates and keys ...
  - Booting up control plane ...
  - Configuring RBAC rules ...
* Verifying Kubernetes components...
  - Using image gcr.io/k8s-minikube/storage-provisioner:v5
* Enabled addons: default-storageclass, storage-provisioner
* kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
* Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
</code></pre>

<p>Interact with your cluster</p>

<pre><code>[ec2-user@amazonlinux ~]$ minikube kubectl -- get pods -A 
    &gt; kubectl.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s
    &gt; kubectl: 44.43 MiB / 44.43 MiB [-------------] 100.00% 17.41 MiB p/s 2.8s
NAMESPACE     NAME                               READY   STATUS    RESTARTS       AGE
kube-system   coredns-64897985d-cm6h7            1/1     Running   0              2m3s
kube-system   etcd-minikube                      1/1     Running   0              2m15s
kube-system   kube-apiserver-minikube            1/1     Running   0              2m15s
kube-system   kube-controller-manager-minikube   1/1     Running   0              2m15s
kube-system   kube-proxy-s2sf4                   1/1     Running   0              2m3s
kube-system   kube-scheduler-minikube            1/1     Running   0              2m15s
kube-system   storage-provisioner                1/1     Running   1 (101s ago)   2m14s

[ec2-user@amazonlinux ~]$ alias kubectl="minikube kubectl --"
[ec2-user@amazonlinux ~]$ kubectl get pods -A 
NAMESPACE     NAME                               READY   STATUS    RESTARTS        AGE
kube-system   coredns-64897985d-cm6h7            1/1     Running   0               5m5s
kube-system   etcd-minikube                      1/1     Running   0               5m17s
kube-system   kube-apiserver-minikube            1/1     Running   0               5m17s
kube-system   kube-controller-manager-minikube   1/1     Running   0               5m17s
kube-system   kube-proxy-s2sf4                   1/1     Running   0               5m5s
kube-system   kube-scheduler-minikube            1/1     Running   0               5m17s
kube-system   storage-provisioner                1/1     Running   1 (4m43s ago)   5m16s

# 查看配置
[ec2-user@amazonlinux ~]$ cat .kube/config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/ec2-user/.minikube/ca.crt
    extensions:
    - extension:
        last-update: Sun, 03 Apr 2022 16:43:46 UTC
        provider: minikube.sigs.k8s.io
        version: v1.25.2
      name: cluster_info
    server: https://192.168.49.2:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    extensions:
    - extension:
        last-update: Sun, 03 Apr 2022 16:43:46 UTC
        provider: minikube.sigs.k8s.io
        version: v1.25.2
      name: context_info
    namespace: default
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/ec2-user/.minikube/profiles/minikube/client.crt
    client-key: /home/ec2-user/.minikube/profiles/minikube/client.key
[ec2-user@amazonlinux ~]$ 
</code></pre>

<p>或者做一个软连接的kubectl：</p>

<pre><code># https://minikube.sigs.k8s.io/docs/handbook/kubectl/
# https://kubernetes.io/docs/tasks/tools/included/optional-kubectl-configs-bash-linux/
# https://minikube.sigs.k8s.io/docs/handbook/kubectl/

ln -s $(which minikube) /usr/local/bin/kubectl
</code></pre>

<h3>对系统做了些什么？</h3>

<pre><code>[ec2-user@amazonlinux ~]$ docker images 
REPOSITORY       TAG       IMAGE ID       CREATED       SIZE
kicbase/stable   v0.0.30   1312ccd2422d   7 weeks ago   1.14GB
[ec2-user@amazonlinux ~]$ docker ps 
CONTAINER ID   IMAGE                    COMMAND                  CREATED         STATUS         PORTS                                                                                                                                  NAMES
19887e6799fb   kicbase/stable:v0.0.30   "/usr/local/bin/entr…"   7 minutes ago   Up 7 minutes   127.0.0.1:49157-&gt;22/tcp, 127.0.0.1:49156-&gt;2376/tcp, 127.0.0.1:49155-&gt;5000/tcp, 127.0.0.1:49154-&gt;8443/tcp, 127.0.0.1:49153-&gt;32443/tcp   minikube

# 进到容器内查看
[ec2-user@amazonlinux ~]$ docker exec -ti 19887e6799fb bash 

[ec2-user@amazonlinux ~]$ minikube ssh
docker@minikube:~$ sudo su -
root@minikube:/# 
root@minikube:/# docker ps -a 
CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS                     PORTS     NAMES
317a4dc12504   6e38f40d628d           "/storage-provisioner"   7 minutes ago   Up 7 minutes                         k8s_storage-provisioner_storage-provisioner_kube-system_c4ea2d31-2c3e-4672-9479-719b0082ac5c_1
e0876840ef56   a4ca41631cc7           "/coredns -conf /etc…"   7 minutes ago   Up 7 minutes                         k8s_coredns_coredns-64897985d-cm6h7_kube-system_0bbdb9ae-d64a-4d74-8b38-ba5bd66af499_0
485899a5fe0c   9b7cc9982109           "/usr/local/bin/kube…"   7 minutes ago   Up 7 minutes                         k8s_kube-proxy_kube-proxy-s2sf4_kube-system_2e24cec7-d9b3-430e-8869-b9af785588de_0
0faab30374f5   k8s.gcr.io/pause:3.6   "/pause"                 7 minutes ago   Up 7 minutes                         k8s_POD_coredns-64897985d-cm6h7_kube-system_0bbdb9ae-d64a-4d74-8b38-ba5bd66af499_0
ae99f0b5b873   k8s.gcr.io/pause:3.6   "/pause"                 7 minutes ago   Up 7 minutes                         k8s_POD_kube-proxy-s2sf4_kube-system_2e24cec7-d9b3-430e-8869-b9af785588de_0
fdeeb06fda78   6e38f40d628d           "/storage-provisioner"   7 minutes ago   Exited (1) 7 minutes ago             k8s_storage-provisioner_storage-provisioner_kube-system_c4ea2d31-2c3e-4672-9479-719b0082ac5c_0
f83e36d2d77e   k8s.gcr.io/pause:3.6   "/pause"                 7 minutes ago   Up 7 minutes                         k8s_POD_storage-provisioner_kube-system_c4ea2d31-2c3e-4672-9479-719b0082ac5c_0
df73834cbaf8   b07520cd7ab7           "kube-controller-man…"   8 minutes ago   Up 8 minutes                         k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_b965983ec05322d0973594a01d5e8245_0
49ad661cee86   25f8c7f3da61           "etcd --advertise-cl…"   8 minutes ago   Up 8 minutes                         k8s_etcd_etcd-minikube_kube-system_9d3d310935e5fabe942511eec3e2cd0c_0
2c28a97b3875   99a3486be4f2           "kube-scheduler --au…"   8 minutes ago   Up 8 minutes                         k8s_kube-scheduler_kube-scheduler-minikube_kube-system_be132fe5c6572cb34d93f5e05ce2a540_0
72aca4e710b6   f40be0088a83           "kube-apiserver --ad…"   8 minutes ago   Up 8 minutes                         k8s_kube-apiserver_kube-apiserver-minikube_kube-system_cd6e47233d36a9715b0ab9632f871843_0
97c6ecde381c   k8s.gcr.io/pause:3.6   "/pause"                 8 minutes ago   Up 8 minutes                         k8s_POD_kube-scheduler-minikube_kube-system_be132fe5c6572cb34d93f5e05ce2a540_0
9140211bc570   k8s.gcr.io/pause:3.6   "/pause"                 8 minutes ago   Up 8 minutes                         k8s_POD_kube-controller-manager-minikube_kube-system_b965983ec05322d0973594a01d5e8245_0
b27ec09ec789   k8s.gcr.io/pause:3.6   "/pause"                 8 minutes ago   Up 8 minutes                         k8s_POD_kube-apiserver-minikube_kube-system_cd6e47233d36a9715b0ab9632f871843_0
0aef74ead92e   k8s.gcr.io/pause:3.6   "/pause"                 8 minutes ago   Up 8 minutes                         k8s_POD_etcd-minikube_kube-system_9d3d310935e5fabe942511eec3e2cd0c_0

root@minikube:/# docker images 
REPOSITORY                                TAG       IMAGE ID       CREATED         SIZE
k8s.gcr.io/kube-apiserver                 v1.23.3   f40be0088a83   2 months ago    135MB
k8s.gcr.io/kube-proxy                     v1.23.3   9b7cc9982109   2 months ago    112MB
k8s.gcr.io/kube-scheduler                 v1.23.3   99a3486be4f2   2 months ago    53.5MB
k8s.gcr.io/kube-controller-manager        v1.23.3   b07520cd7ab7   2 months ago    125MB
k8s.gcr.io/etcd                           3.5.1-0   25f8c7f3da61   5 months ago    293MB
k8s.gcr.io/coredns/coredns                v1.8.6    a4ca41631cc7   5 months ago    46.8MB
k8s.gcr.io/pause                          3.6       6270bb605e12   7 months ago    683kB
kubernetesui/dashboard                    v2.3.1    e1482a24335a   9 months ago    220MB
kubernetesui/metrics-scraper              v1.0.7    7801cfc6d5c0   9 months ago    34.4MB
gcr.io/k8s-minikube/storage-provisioner   v5        6e38f40d628d   12 months ago   31.5MB
</code></pre>

<p>更便捷的管理minikube docker：</p>

<ul>
<li><a href="https://minikube.sigs.k8s.io/docs/handbook/pushing/">https://minikube.sigs.k8s.io/docs/handbook/pushing/</a></li>
</ul>


<pre><code># https://minikube.sigs.k8s.io/docs/handbook/pushing/#1-pushing-directly-to-the-in-cluster-docker-daemon-docker-env
[ec2-user@amazonlinux ~]$ eval $(minikube docker-env)
# 原理就是设置了环境变量，docker连上了远程服务：export DOCKER_HOST="tcp://192.168.49.2:2376"

[ec2-user@amazonlinux ~]$ docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS     NAMES
344dc0e8c1e6   7801cfc6d5c0           "/metrics-sidecar"       13 minutes ago   Up 13 minutes             k8s_dashboard-metrics-scraper_dashboard-metrics-scraper-58549894f-mzhg2_kubernetes-dashboard_4d6591df-2ee1-46b1-b44c-46ef748a3cd8_0
276a2b6b591f   e1482a24335a           "/dashboard --insecu…"   13 minutes ago   Up 13 minutes             k8s_kubernetes-dashboard_kubernetes-dashboard-ccd587f44-w22lx_kubernetes-dashboard_eb06974d-a4e6-459a-b135-b2426696a75f_0
739f9fdc02bf   k8s.gcr.io/pause:3.6   "/pause"                 13 minutes ago   Up 13 minutes             k8s_POD_dashboard-metrics-scraper-58549894f-mzhg2_kubernetes-dashboard_4d6591df-2ee1-46b1-b44c-46ef748a3cd8_0
9b9d38bdb6d7   k8s.gcr.io/pause:3.6   "/pause"                 13 minutes ago   Up 13 minutes             k8s_POD_kubernetes-dashboard-ccd587f44-w22lx_kubernetes-dashboard_eb06974d-a4e6-459a-b135-b2426696a75f_0
8fd387311710   k8s.gcr.io/pause:3.6   "/pause"                 8 hours ago      Up 8 hours                k8s_POD_hello-minikube-74c6b47596-gfxrl_default_689891ce-a761-46a6-aa16-fb33dd037c45_0
317a4dc12504   6e38f40d628d           "/storage-provisioner"   8 hours ago      Up 8 hours                k8s_storage-provisioner_storage-provisioner_kube-system_c4ea2d31-2c3e-4672-9479-719b0082ac5c_1
e0876840ef56   a4ca41631cc7           "/coredns -conf /etc…"   8 hours ago      Up 8 hours                k8s_coredns_coredns-64897985d-cm6h7_kube-system_0bbdb9ae-d64a-4d74-8b38-ba5bd66af499_0
485899a5fe0c   9b7cc9982109           "/usr/local/bin/kube…"   8 hours ago      Up 8 hours                k8s_kube-proxy_kube-proxy-s2sf4_kube-system_2e24cec7-d9b3-430e-8869-b9af785588de_0
0faab30374f5   k8s.gcr.io/pause:3.6   "/pause"                 8 hours ago      Up 8 hours                k8s_POD_coredns-64897985d-cm6h7_kube-system_0bbdb9ae-d64a-4d74-8b38-ba5bd66af499_0
ae99f0b5b873   k8s.gcr.io/pause:3.6   "/pause"                 8 hours ago      Up 8 hours                k8s_POD_kube-proxy-s2sf4_kube-system_2e24cec7-d9b3-430e-8869-b9af785588de_0
f83e36d2d77e   k8s.gcr.io/pause:3.6   "/pause"                 8 hours ago      Up 8 hours                k8s_POD_storage-provisioner_kube-system_c4ea2d31-2c3e-4672-9479-719b0082ac5c_0
df73834cbaf8   b07520cd7ab7           "kube-controller-man…"   8 hours ago      Up 8 hours                k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_b965983ec05322d0973594a01d5e8245_0
49ad661cee86   25f8c7f3da61           "etcd --advertise-cl…"   8 hours ago      Up 8 hours                k8s_etcd_etcd-minikube_kube-system_9d3d310935e5fabe942511eec3e2cd0c_0
2c28a97b3875   99a3486be4f2           "kube-scheduler --au…"   8 hours ago      Up 8 hours                k8s_kube-scheduler_kube-scheduler-minikube_kube-system_be132fe5c6572cb34d93f5e05ce2a540_0
72aca4e710b6   f40be0088a83           "kube-apiserver --ad…"   8 hours ago      Up 8 hours                k8s_kube-apiserver_kube-apiserver-minikube_kube-system_cd6e47233d36a9715b0ab9632f871843_0
97c6ecde381c   k8s.gcr.io/pause:3.6   "/pause"                 8 hours ago      Up 8 hours                k8s_POD_kube-scheduler-minikube_kube-system_be132fe5c6572cb34d93f5e05ce2a540_0
9140211bc570   k8s.gcr.io/pause:3.6   "/pause"                 8 hours ago      Up 8 hours                k8s_POD_kube-controller-manager-minikube_kube-system_b965983ec05322d0973594a01d5e8245_0
b27ec09ec789   k8s.gcr.io/pause:3.6   "/pause"                 8 hours ago      Up 8 hours                k8s_POD_kube-apiserver-minikube_kube-system_cd6e47233d36a9715b0ab9632f871843_0
0aef74ead92e   k8s.gcr.io/pause:3.6   "/pause"                 8 hours ago      Up 8 hours                k8s_POD_etcd-minikube_kube-system_9d3d310935e5fabe942511eec3e2cd0c_0
</code></pre>

<p>管理：</p>

<ul>
<li><a href="https://minikube.sigs.k8s.io/docs/handbook/pushing/#2-push-images-using-cache-command">https://minikube.sigs.k8s.io/docs/handbook/pushing/#2-push-images-using-cache-command</a></li>
</ul>


<pre><code>minikube cache add alpine:latest
minikube cache reload
minikube cache list
minikube cache delete &lt;image name&gt;
</code></pre>

<h2>dashboard</h2>

<pre><code>[ec2-user@amazonlinux ~]$ minikube dashboard
* Enabling dashboard ...
  - Using image kubernetesui/metrics-scraper:v1.0.7
  - Using image kubernetesui/dashboard:v2.3.1
* Verifying dashboard health ...
* Launching proxy ...
* Verifying proxy health ...
* Opening http://127.0.0.1:37163/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser...
  http://127.0.0.1:37163/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/

[ec2-user@amazonlinux ~]$ sudo netstat -anp | grep 37163 | grep LISTEN
tcp        0      0 127.0.0.1:37163         0.0.0.0:*               LISTEN      71016/kubectl       

[ec2-user@amazonlinux ~]$ ps aux|grep 71016
ec2-user   71016  0.0  0.9 750808 38932 pts/2    Sl+  00:59   0:00 /home/ec2-user/.minikube/cache/linux/amd64/v1.23.3/kubectl --cluster minikube --context minikube proxy --port 0


# 再获取访问地址
[ec2-user@amazonlinux ~]$ minikube dashboard --url
* Verifying dashboard health ...
* Launching proxy ...
* Verifying proxy health ...
http://127.0.0.1:43247/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/
</code></pre>

<h2>hello world</h2>

<pre><code>[ec2-user@amazonlinux ~]$ eval $(minikube docker-env)
[ec2-user@amazonlinux ~]$ docker load -i echoserver.tar.gz 
6cc9890d69b6: Loading layer [==================================================&gt;]  61.68MB/61.68MB
5f70bf18a086: Loading layer [==================================================&gt;]  1.024kB/1.024kB
e105cd217163: Loading layer [==================================================&gt;]  8.704kB/8.704kB
9f9b8efa9a34: Loading layer [==================================================&gt;]  83.34MB/83.34MB
4cc84b7b3aba: Loading layer [==================================================&gt;]  3.072kB/3.072kB
e2615e4925e2: Loading layer [==================================================&gt;]  3.072kB/3.072kB
1787713d6d5d: Loading layer [==================================================&gt;]   5.12kB/5.12kB
67639a8a7916: Loading layer [==================================================&gt;]  2.048kB/2.048kB
Loaded image: k8s.gcr.io/echoserver:1.4

[ec2-user@amazonlinux ~]$ docker images 
REPOSITORY                                TAG       IMAGE ID       CREATED         SIZE
k8s.gcr.io/kube-apiserver                 v1.23.3   f40be0088a83   2 months ago    135MB
k8s.gcr.io/kube-scheduler                 v1.23.3   99a3486be4f2   2 months ago    53.5MB
k8s.gcr.io/kube-proxy                     v1.23.3   9b7cc9982109   2 months ago    112MB
k8s.gcr.io/kube-controller-manager        v1.23.3   b07520cd7ab7   2 months ago    125MB
k8s.gcr.io/etcd                           3.5.1-0   25f8c7f3da61   5 months ago    293MB
k8s.gcr.io/coredns/coredns                v1.8.6    a4ca41631cc7   6 months ago    46.8MB
k8s.gcr.io/pause                          3.6       6270bb605e12   7 months ago    683kB
kubernetesui/dashboard                    v2.3.1    e1482a24335a   10 months ago   220MB
kubernetesui/metrics-scraper              v1.0.7    7801cfc6d5c0   10 months ago   34.4MB
gcr.io/k8s-minikube/storage-provisioner   v5        6e38f40d628d   12 months ago   31.5MB
k8s.gcr.io/echoserver                     1.4       a90209bb39e3   5 years ago     140MB


# kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4
# kubectl expose deployment hello-minikube --type=NodePort --port=8080

# kubectl --help

[ec2-user@amazonlinux ~]$ kubectl get svc 
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
hello-minikube   NodePort    10.98.195.3   &lt;none&gt;        8080:32754/TCP   7h
kubernetes       ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP          12d

[ec2-user@amazonlinux ~]$ kubectl get services hello-minikube
NAME             TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
hello-minikube   NodePort   10.98.195.3   &lt;none&gt;        8080:32754/TCP   7h59m


# kubectl port-forward service/hello-minikube 7080:8080

[ec2-user@amazonlinux ~]$ kubectl port-forward --address 0.0.0.0 service/hello-minikube 7080:8080
Forwarding from 0.0.0.0:7080 -&gt; 8080
Handling connection for 7080
Handling connection for 7080

浏览器访问 http://192.168.191.133:7080/
</code></pre>

<h2>load balancer</h2>

<ul>
<li><a href="https://minikube.sigs.k8s.io/docs/start/#loadbalancer-deployments">https://minikube.sigs.k8s.io/docs/start/#loadbalancer-deployments</a></li>
</ul>


<pre><code>[ec2-user@amazonlinux ~]$ kubectl create deployment balanced --image=k8s.gcr.io/echoserver:1.4  
deployment.apps/balanced created
[ec2-user@amazonlinux ~]$ kubectl expose deployment balanced --type=LoadBalancer --port=8080
service/balanced exposed
[ec2-user@amazonlinux ~]$ minikube tunnel


[ec2-user@amazonlinux ~]$ kubectl get services balanced
NAME       TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
balanced   LoadBalancer   10.100.193.49   10.100.193.49   8080:30406/TCP   13s
[ec2-user@amazonlinux ~]$ curl 10.100.193.49:8080
CLIENT VALUES:
client_address=172.17.0.1
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://10.100.193.49:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=10.100.193.49:8080
user-agent=curl/7.79.1
BODY:
[ec2-user@amazonlinux ~]$ 
</code></pre>

<h2>清理</h2>

<pre><code>[ec2-user@amazonlinux ~]$ minikube pause
* Pausing node minikube ... 
* Paused 18 containers in: kube-system, kubernetes-dashboard, storage-gluster, istio-operator
[ec2-user@amazonlinux ~]$ minikube unpause
* Unpausing node minikube ... 
* Unpaused 18 containers in: kube-system, kubernetes-dashboard, storage-gluster, istio-operator

###!!!!
[ec2-user@amazonlinux ~]$ minikube config set memory 16384
! These changes will take effect upon a minikube delete and then a minikube start

[ec2-user@amazonlinux ~]$ minikube config unset memory


[ec2-user@amazonlinux ~]$ docker ps 
CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS          PORTS     NAMES
01cf3a66eb7c   a90209bb39e3           "nginx -g 'daemon of…"   6 minutes ago    Up 6 minutes              k8s_echoserver_balanced-5b98d98bf8-t464f_default_c6b70ff1-d799-4633-bf13-e481052a5da1_0
6a23b4b82131   k8s.gcr.io/pause:3.6   "/pause"                 6 minutes ago    Up 6 minutes              k8s_POD_balanced-5b98d98bf8-t464f_default_c6b70ff1-d799-4633-bf13-e481052a5da1_0
b5817aab4b25   a90209bb39e3           "nginx -g 'daemon of…"   17 minutes ago   Up 17 minutes             k8s_echoserver_hello-minikube-7bc9d7884c-m4r4s_default_cef90979-1c63-4497-acb0-d13847c5a60b_0
9d5677d088c6   k8s.gcr.io/pause:3.6   "/pause"                 17 minutes ago   Up 17 minutes             k8s_POD_hello-minikube-7bc9d7884c-m4r4s_default_cef90979-1c63-4497-acb0-d13847c5a60b_0
c13da70206de   e1482a24335a           "/dashboard --insecu…"   50 minutes ago   Up 50 minutes             k8s_kubernetes-dashboard_kubernetes-dashboard-ccd587f44-w22lx_kubernetes-dashboard_eb06974d-a4e6-459a-b135-b2426696a75f_4
0cead3ead613   6e38f40d628d           "/storage-provisioner"   50 minutes ago   Up 50 minutes             k8s_storage-provisioner_storage-provisioner_kube-system_c4ea2d31-2c3e-4672-9479-719b0082ac5c_5
3a1c19a7a51f   7801cfc6d5c0           "/metrics-sidecar"       51 minutes ago   Up 51 minutes             k8s_dashboard-metrics-scraper_dashboard-metrics-scraper-58549894f-mzhg2_kubernetes-dashboard_4d6591df-2ee1-46b1-b44c-46ef748a3cd8_2
0c48d4f778de   a4ca41631cc7           "/coredns -conf /etc…"   51 minutes ago   Up 51 minutes             k8s_coredns_coredns-64897985d-cm6h7_kube-system_0bbdb9ae-d64a-4d74-8b38-ba5bd66af499_2
ac2ee5d973c6   9b7cc9982109           "/usr/local/bin/kube…"   51 minutes ago   Up 51 minutes             k8s_kube-proxy_kube-proxy-s2sf4_kube-system_2e24cec7-d9b3-430e-8869-b9af785588de_2
dc17216220bb   k8s.gcr.io/pause:3.6   "/pause"                 51 minutes ago   Up 51 minutes             k8s_POD_kubernetes-dashboard-ccd587f44-w22lx_kubernetes-dashboard_eb06974d-a4e6-459a-b135-b2426696a75f_2
9d7f883afb88   k8s.gcr.io/pause:3.6   "/pause"                 51 minutes ago   Up 51 minutes             k8s_POD_storage-provisioner_kube-system_c4ea2d31-2c3e-4672-9479-719b0082ac5c_2
d28d9a0a91b1   k8s.gcr.io/pause:3.6   "/pause"                 51 minutes ago   Up 51 minutes             k8s_POD_coredns-64897985d-cm6h7_kube-system_0bbdb9ae-d64a-4d74-8b38-ba5bd66af499_2
7e7a16e16d03   k8s.gcr.io/pause:3.6   "/pause"                 51 minutes ago   Up 51 minutes             k8s_POD_kube-proxy-s2sf4_kube-system_2e24cec7-d9b3-430e-8869-b9af785588de_2
ace45942773c   k8s.gcr.io/pause:3.6   "/pause"                 51 minutes ago   Up 51 minutes             k8s_POD_dashboard-metrics-scraper-58549894f-mzhg2_kubernetes-dashboard_4d6591df-2ee1-46b1-b44c-46ef748a3cd8_2
8f00f611d713   f40be0088a83           "kube-apiserver --ad…"   51 minutes ago   Up 51 minutes             k8s_kube-apiserver_kube-apiserver-minikube_kube-system_cd6e47233d36a9715b0ab9632f871843_2
79f528bbd4d1   99a3486be4f2           "kube-scheduler --au…"   51 minutes ago   Up 51 minutes             k8s_kube-scheduler_kube-scheduler-minikube_kube-system_be132fe5c6572cb34d93f5e05ce2a540_2
75ce75e37ceb   b07520cd7ab7           "kube-controller-man…"   51 minutes ago   Up 51 minutes             k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_b965983ec05322d0973594a01d5e8245_2
50c302912f4a   25f8c7f3da61           "etcd --advertise-cl…"   51 minutes ago   Up 51 minutes             k8s_etcd_etcd-minikube_kube-system_9d3d310935e5fabe942511eec3e2cd0c_2
0b1c094b5824   k8s.gcr.io/pause:3.6   "/pause"                 51 minutes ago   Up 51 minutes             k8s_POD_kube-controller-manager-minikube_kube-system_b965983ec05322d0973594a01d5e8245_2
7b6d52353935   k8s.gcr.io/pause:3.6   "/pause"                 51 minutes ago   Up 51 minutes             k8s_POD_kube-scheduler-minikube_kube-system_be132fe5c6572cb34d93f5e05ce2a540_2
64ba4b4c1a84   k8s.gcr.io/pause:3.6   "/pause"                 51 minutes ago   Up 51 minutes             k8s_POD_etcd-minikube_kube-system_9d3d310935e5fabe942511eec3e2cd0c_2
92f05324a447   k8s.gcr.io/pause:3.6   "/pause"                 51 minutes ago   Up 51 minutes             k8s_POD_kube-apiserver-minikube_kube-system_cd6e47233d36a9715b0ab9632f871843_2


[ec2-user@amazonlinux ~]$ minikube addons list
|-----------------------------|----------|--------------|--------------------------------|
|         ADDON NAME          | PROFILE  |    STATUS    |           MAINTAINER           |
|-----------------------------|----------|--------------|--------------------------------|
| ambassador                  | minikube | disabled     | third-party (ambassador)       |
| auto-pause                  | minikube | disabled     | google                         |
| csi-hostpath-driver         | minikube | disabled     | kubernetes                     |
| dashboard                   | minikube | enabled ✅   | kubernetes                     |
| default-storageclass        | minikube | enabled ✅   | kubernetes                     |
| efk                         | minikube | disabled     | third-party (elastic)          |
| freshpod                    | minikube | disabled     | google                         |
| gcp-auth                    | minikube | disabled     | google                         |
| gvisor                      | minikube | disabled     | google                         |
| helm-tiller                 | minikube | disabled     | third-party (helm)             |
| ingress                     | minikube | disabled     | unknown (third-party)          |
| ingress-dns                 | minikube | disabled     | google                         |
| istio                       | minikube | disabled     | third-party (istio)            |
| istio-provisioner           | minikube | disabled     | third-party (istio)            |
| kong                        | minikube | disabled     | third-party (Kong HQ)          |
| kubevirt                    | minikube | disabled     | third-party (kubevirt)         |
| logviewer                   | minikube | disabled     | unknown (third-party)          |
| metallb                     | minikube | disabled     | third-party (metallb)          |
| metrics-server              | minikube | disabled     | kubernetes                     |
| nvidia-driver-installer     | minikube | disabled     | google                         |
| nvidia-gpu-device-plugin    | minikube | disabled     | third-party (nvidia)           |
| olm                         | minikube | disabled     | third-party (operator          |
|                             |          |              | framework)                     |
| pod-security-policy         | minikube | disabled     | unknown (third-party)          |
| portainer                   | minikube | disabled     | portainer.io                   |
| registry                    | minikube | disabled     | google                         |
| registry-aliases            | minikube | disabled     | unknown (third-party)          |
| registry-creds              | minikube | disabled     | third-party (upmc enterprises) |
| storage-provisioner         | minikube | enabled ✅   | google                         |
| storage-provisioner-gluster | minikube | disabled     | unknown (third-party)          |
| volumesnapshots             | minikube | disabled     | kubernetes                     |
|-----------------------------|----------|--------------|--------------------------------|

[ec2-user@amazonlinux ~]$ minikube delete --all
* Deleting "minikube" in docker ...
* Removing /home/ec2-user/.minikube/machines/minikube ...
* Removed all traces of the "minikube" cluster.
* Successfully deleted all profiles


#Create a second cluster running an older Kubernetes release:
# minikube start -p aged --kubernetes-version=v1.16.1
</code></pre>

<h2>k3d</h2>

<ul>
<li><a href="https://k3d.io/v5.4.1/">https://k3d.io/v5.4.1/</a>
k3d is a lightweight wrapper to run k3s (Rancher Lab’s minimal Kubernetes distribution) in docker.</li>
</ul>


<pre><code>[ec2-user@amazonlinux ~]$ wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash
Preparing to install k3d into /usr/local/bin
k3d installed into /usr/local/bin/k3d
Run 'k3d --help' to see what you can do with it.

[ec2-user@amazonlinux ~]$ k3d cluster create mycluster
INFO[0000] Prep: Network                                
INFO[0000] Created network 'k3d-mycluster'              
INFO[0000] Created image volume k3d-mycluster-images    
INFO[0000] Starting new tools node...                   
INFO[0001] Creating node 'k3d-mycluster-server-0'       
INFO[0003] Pulling image 'ghcr.io/k3d-io/k3d-tools:5.4.1' 
INFO[0007] Pulling image 'docker.io/rancher/k3s:v1.22.7-k3s1' 
INFO[0009] Starting Node 'k3d-mycluster-tools'          
INFO[0084] Creating LoadBalancer 'k3d-mycluster-serverlb' 
INFO[0087] Pulling image 'ghcr.io/k3d-io/k3d-proxy:5.4.1' 
INFO[0098] Using the k3d-tools node to gather environment information 
INFO[0098] HostIP: using network gateway 172.18.0.1 address 
INFO[0098] Starting cluster 'mycluster'                 
INFO[0098] Starting servers...                          
INFO[0098] Starting Node 'k3d-mycluster-server-0'       
INFO[0104] All agents already running.                  
INFO[0104] Starting helpers...                          
INFO[0104] Starting Node 'k3d-mycluster-serverlb'       
INFO[0111] Injecting records for hostAliases (incl. host.k3d.internal) and for 2 network members into CoreDNS configmap... 
INFO[0114] Cluster 'mycluster' created successfully!    
INFO[0114] You can now use it like this:                
kubectl cluster-info

[ec2-user@amazonlinux ~]$ docker ps 
CONTAINER ID   IMAGE                            COMMAND                  CREATED         STATUS         PORTS                             NAMES
5d57c1328e66   ghcr.io/k3d-io/k3d-proxy:5.4.1   "/bin/sh -c nginx-pr…"   6 minutes ago   Up 6 minutes   80/tcp, 0.0.0.0:41489-&gt;6443/tcp   k3d-mycluster-serverlb
add7ac133348   rancher/k3s:v1.22.7-k3s1         "/bin/k3s server --t…"   6 minutes ago   Up 6 minutes                                     k3d-mycluster-server-0

# 注意：上面的kubectl是minikue的，需要下载
[ec2-user@amazonlinux ~]$ curl -LO https://dl.k8s.io/release/v1.23.0/bin/linux/amd64/kubectl
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   154  100   154    0     0    230      0 --:--:-- --:--:-- --:--:--   230
100 44.4M  100 44.4M    0     0  1989k      0  0:00:22  0:00:22 --:--:-- 2256k
[ec2-user@amazonlinux ~]$ ll
[ec2-user@amazonlinux ~]$ sudo cp kubectl /usr/local/bin/
[ec2-user@amazonlinux ~]$ which kubectl
/usr/local/bin/kubectl

[ec2-user@amazonlinux ~]$ kubectl get nodes
NAME                     STATUS   ROLES                  AGE   VERSION
k3d-mycluster-server-0   Ready    control-plane,master   47m   v1.22.7+k3s1

[ec2-user@amazonlinux ~]$ kubectl get pods -A 
NAMESPACE     NAME                                      READY   STATUS      RESTARTS      AGE
kube-system   local-path-provisioner-84bb864455-s8t7f   1/1     Running     0             45m
kube-system   coredns-96cc4f57d-vvmrx                   1/1     Running     0             45m
kube-system   helm-install-traefik-crd--1-ghv9f         0/1     Completed   0             45m
kube-system   helm-install-traefik--1-s4ktb             0/1     Completed   1             45m
kube-system   svclb-traefik-9jt5d                       2/2     Running     1 (44m ago)   44m
kube-system   metrics-server-ff9dbcb6c-72mwc            1/1     Running     0             45m
kube-system   traefik-56c4b88c4b-f454f                  1/1     Running     0             44m
[ec2-user@amazonlinux ~]$ 

[ec2-user@amazonlinux ~]$ k3d cluster list 
NAME        SERVERS   AGENTS   LOADBALANCER
mycluster   1/1       0/0      true
[ec2-user@amazonlinux ~]$ k3d node create worker1 --cluster mycluster
INFO[0000] Adding 1 node(s) to the runtime local cluster 'mycluster'... 
INFO[0000] Using the k3d-tools node to gather environment information 
INFO[0000] Starting new tools node...                   
INFO[0000] Starting Node 'k3d-mycluster-tools'          
INFO[0001] HostIP: using network gateway 172.18.0.1 address 
INFO[0001] Starting Node 'k3d-worker1-0'                
INFO[0009] Successfully created 1 node(s)!              
[ec2-user@amazonlinux ~]$ kubectl get nodes
NAME                     STATUS     ROLES                  AGE   VERSION
k3d-mycluster-server-0   Ready      control-plane,master   50m   v1.22.7+k3s1
k3d-worker1-0            NotReady   &lt;none&gt;                 7s    v1.22.7+k3s1
[ec2-user@amazonlinux ~]$ 

[ec2-user@amazonlinux ~]$ kubectl get nodes                          
NAME                     STATUS   ROLES                  AGE   VERSION
k3d-mycluster-server-0   Ready    control-plane,master   50m   v1.22.7+k3s1
k3d-worker1-0            Ready    &lt;none&gt;                 24s   v1.22.7+k3s1
[ec2-user@amazonlinux ~]$ 
</code></pre>

<ul>
<li><a href="https://k3d.io/v5.4.1/usage/exposing_services/">https://k3d.io/v5.4.1/usage/exposing_services/</a></li>
</ul>


<pre><code>[ec2-user@amazonlinux ~]$ cat .vimrc 
set paste

[ec2-user@amazonlinux ~]$ cat thatfile.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx
  annotations:
    ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx
            port:
              number: 80

[ec2-user@amazonlinux ~]$ export KUBECONFIG="$(k3d kubeconfig write mycluster)"  
[ec2-user@amazonlinux ~]$ kubectl create deployment nginx --image=nginx
deployment.apps/nginx created
[ec2-user@amazonlinux ~]$ kubectl create service clusterip nginx --tcp=80:80
service/nginx created

[ec2-user@amazonlinux ~]$ kubectl apply -f thatfile.yaml
ingress.networking.k8s.io/nginx created
</code></pre>

<p>查看结果：</p>

<p>```
[ec2-user@amazonlinux ~]$ kubectl get all -A -o wide
NAMESPACE     NAME                                          READY   STATUS      RESTARTS      AGE     IP          NODE                     NOMINATED NODE   READINESS GATES
kube-system   pod/local-path-provisioner-84bb864455-s8t7f   1/1     Running     0             64m     10.42.0.5   k3d-mycluster-server-0   <none>           <none>
kube-system   pod/coredns-96cc4f57d-vvmrx                   1/1     Running     0             64m     10.42.0.6   k3d-mycluster-server-0   <none>           <none>
kube-system   pod/helm-install-traefik-crd&ndash;1-ghv9f         0/1     Completed   0             64m     10.42.0.4   k3d-mycluster-server-0   <none>           <none>
kube-system   pod/helm-install-traefik&ndash;1-s4ktb             0/1     Completed   1             64m     10.42.0.3   k3d-mycluster-server-0   <none>           <none>
kube-system   pod/svclb-traefik-9jt5d                       2/2     Running     1 (63m ago)   63m     10.42.0.7   k3d-mycluster-server-0   <none>           <none>
kube-system   pod/metrics-server-ff9dbcb6c-72mwc            1/1     Running     0             64m     10.42.0.2   k3d-mycluster-server-0   <none>           <none>
kube-system   pod/traefik-56c4b88c4b-f454f                  1/1     Running     0             63m     10.42.0.8   k3d-mycluster-server-0   <none>           <none>
kube-system   pod/svclb-traefik-dvvrp                       2/2     Running     1 (13m ago)   14m     10.42.1.2   k3d-worker1-0            <none>           <none>
default       pod/nginx-6799fc88d8-vgc9x                    1/1     Running     0             4m34s   10.42.1.3   k3d-worker1-0            <none>           <none></p>

<p>NAMESPACE     NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP             PORT(S)                      AGE     SELECTOR
default       service/kubernetes       ClusterIP      10.43.0.1       <none>                  443/TCP                      64m     <none>
kube-system   service/kube-dns         ClusterIP      10.43.0.10      <none>                  53/UDP,53/TCP,9153/TCP       64m     k8s-app=kube-dns
kube-system   service/metrics-server   ClusterIP      10.43.88.129    <none>                  443/TCP                      64m     k8s-app=metrics-server
kube-system   service/traefik          LoadBalancer   10.43.4.2       172.18.0.2,172.18.0.4   80:32037/TCP,443:30484/TCP   63m     app.kubernetes.io/instance=traefik,app.kubernetes.io/name=traefik
default       service/nginx            ClusterIP      10.43.206.223   <none>                  80/TCP                       4m30s   app=nginx</p>

<p>NAMESPACE     NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE   CONTAINERS               IMAGES                                                SELECTOR
kube-system   daemonset.apps/svclb-traefik   2         2         2       2            2           <none>          63m   lb-port-80,lb-port-443   rancher/klipper-lb:v0.3.4,rancher/klipper-lb:v0.3.4   app=svclb-traefik</p>

<p>NAMESPACE     NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS               IMAGES                                   SELECTOR
kube-system   deployment.apps/local-path-provisioner   1/1     1            1           64m     local-path-provisioner   rancher/local-path-provisioner:v0.0.21   app=local-path-provisioner
kube-system   deployment.apps/coredns                  1/1     1            1           64m     coredns                  rancher/mirrored-coredns-coredns:1.8.6   k8s-app=kube-dns
kube-system   deployment.apps/metrics-server           1/1     1            1           64m     metrics-server           rancher/mirrored-metrics-server:v0.5.2   k8s-app=metrics-server
kube-system   deployment.apps/traefik                  1/1     1            1           63m     traefik                  rancher/mirrored-library-traefik:2.6.1   app.kubernetes.io/instance=traefik,app.kubernetes.io/name=traefik
default       deployment.apps/nginx                    1/1     1            1           4m34s   nginx                    nginx                                    app=nginx</p>

<p>NAMESPACE     NAME                                                DESIRED   CURRENT   READY   AGE     CONTAINERS               IMAGES                                   SELECTOR
kube-system   replicaset.apps/local-path-provisioner-84bb864455   1         1         1       64m     local-path-provisioner   rancher/local-path-provisioner:v0.0.21   app=local-path-provisioner,pod-template-hash=84bb864455
kube-system   replicaset.apps/coredns-96cc4f57d                   1         1         1       64m     coredns                  rancher/mirrored-coredns-coredns:1.8.6   k8s-app=kube-dns,pod-template-hash=96cc4f57d
kube-system   replicaset.apps/metrics-server-ff9dbcb6c            1         1         1       64m     metrics-server           rancher/mirrored-metrics-server:v0.5.2   k8s-app=metrics-server,pod-template-hash=ff9dbcb6c
kube-system   replicaset.apps/traefik-56c4b88c4b                  1         1         1       63m     traefik                  rancher/mirrored-library-traefik:2.6.1   app.kubernetes.io/instance=traefik,app.kubernetes.io/name=traefik,pod-template-hash=56c4b88c4b
default       replicaset.apps/nginx-6799fc88d8                    1         1         1       4m34s   nginx                    nginx                                    app=nginx,pod-template-hash=6799fc88d8</p>

<p>NAMESPACE     NAME                                 COMPLETIONS   DURATION   AGE   CONTAINERS   IMAGES                                      SELECTOR
kube-system   job.batch/helm-install-traefik-crd   1/1           52s        64m   helm         rancher/klipper-helm:v0.6.6-build20211022   controller-uid=1d4ac20d-0436-4d54-9d8b-e37fe7c46e6e
kube-system   job.batch/helm-install-traefik       1/1           53s        64m   helm         rancher/klipper-helm:v0.6.6-build20211022   controller-uid=b51475a5-3fac-4b96-b218-7dd7a094512b</p>

<p>[ec2-user@amazonlinux ~]$ kubectl get ingress
NAME    CLASS    HOSTS   ADDRESS                 PORTS   AGE
nginx   <none>   *       172.18.0.2,172.18.0.4   80      3m18s</p>

<p>[ec2-user@amazonlinux ~]$ curl 172.18.0.2
&lt;!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title></p>



<p></head>
<body></p>

<h1>Welcome to nginx!</h1>


<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>




<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>




<p><em>Thank you for using nginx.</em></p>


<p></body>
</html>
[ec2-user@amazonlinux ~]$</p>

<p>```</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K8s Ingress]]></title>
    <link href="http://winse.github.io/blog/2022/03/26/k8s-ingress/"/>
    <updated>2022-03-26T13:59:16+08:00</updated>
    <id>http://winse.github.io/blog/2022/03/26/k8s-ingress</id>
    <content type="html"><![CDATA[<p>Ingress是一个集中的集群应用网关，自动化的k8s反向代理组件（功能类比nginx）。</p>

<p>Ingress涉及到LoadBalancer，Ingress Controller, Ingress config等相关概念。controller从LoadBalancer/NodePort把当前的服务发布出去，同时监听Ingress config实时的修改当前Ingress配置(实时更新nginx.conf配置文件，并重载)</p>

<p>这里仅从helloworld入门实践操作进行。</p>

<h2>入门指南</h2>

<h3>参考</h3>

<ul>
<li><a href="https://kubernetes.github.io/ingress-nginx/deploy/#quick-start">https://kubernetes.github.io/ingress-nginx/deploy/#quick-start</a></li>
<li><p><a href="https://kubernetes.io/zh/docs/concepts/services-networking/ingress/">https://kubernetes.io/zh/docs/concepts/services-networking/ingress/</a></p></li>
<li><p><a href="https://docs.jdcloud.com/cn/jcs-for-kubernetes/deploy-k8s-ingress-nginx">https://docs.jdcloud.com/cn/jcs-for-kubernetes/deploy-k8s-ingress-nginx</a></p></li>
<li><a href="https://jimmysong.io/kubernetes-handbook/concepts/ingress.html">https://jimmysong.io/kubernetes-handbook/concepts/ingress.html</a></li>
</ul>


<p>版本兼容性：
* <a href="https://github.com/kubernetes/ingress-nginx/#support-versions-table">https://github.com/kubernetes/ingress-nginx/#support-versions-table</a></p>

<h3>下载镜像</h3>

<p>镜像在gcr上面，先远程下载回来：</p>

<pre><code># kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.2/deploy/static/provider/cloud/deploy.yaml 
[ec2-user@k8s ~]$ vi ingress-nginx-controller-v1.1.2.yaml

[ec2-user@k8s ~]$ grep image: ingress-nginx-controller-v1.1.2.yaml | sed 's/image: //' | sort -u | xargs echo 
k8s.gcr.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660

[root@izt4nhcmmx33bjwcsdmf8oz ~]# docker pull k8s.gcr.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c 
k8s.gcr.io/ingress-nginx/controller@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c: Pulling from ingress-nginx/controller
a0d0a0d46f8b: Pull complete 
3aae86482564: Pull complete 
c0d03781abb3: Pull complete 
0297e2ef8f7f: Pull complete 
866a68ce3c13: Pull complete 
1c2a7ca65b54: Pull complete 
41fd2de30e46: Pull complete 
637f10464e4d: Pull complete 
998064a16da4: Pull complete 
e63d23220e8c: Pull complete 
8128610547fb: Pull complete 
ae07a1a7f038: Pull complete 
ceb23c4cb607: Pull complete 
Digest: sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
Status: Downloaded newer image for k8s.gcr.io/ingress-nginx/controller@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
k8s.gcr.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c

[root@izt4nhcmmx33bjwcsdmf8oz ~]# docker pull k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 
k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660: Pulling from ingress-nginx/kube-webhook-certgen
ec52731e9273: Pull complete 
b90aa28117d4: Pull complete 
Digest: sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660
Status: Downloaded newer image for k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660
k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660

[root@izt4nhcmmx33bjwcsdmf8oz ~]# docker save k8s.gcr.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 -o ingress-nginx-v1.1.2.tar
[root@izt4nhcmmx33bjwcsdmf8oz ~]# gzip ingress-nginx-v1.1.2.tar 

[root@izt4nhcmmx33bjwcsdmf8oz ~]# ll -h ingress-nginx-v1.1.2.tar.gz 
-rw------- 1 root root 116M Mar 24 23:49 ingress-nginx-v1.1.2.tar.gz
</code></pre>

<p>本地Linux服务器加载镜像</p>

<pre><code>
[ec2-user@k8s ~]$ docker load -i k8s.gcr.io-ingress-nginx-v1.1.2.tar.gz 
c0d270ab7e0d: Loading layer [==================================================&gt;]  3.697MB/3.697MB
ce7a3c1169b6: Loading layer [==================================================&gt;]  45.38MB/45.38MB
e2eb06d8af82: Loading layer [==================================================&gt;]  5.865MB/5.865MB
ab1476f3fdd9: Loading layer [==================================================&gt;]  120.9MB/120.9MB
ad20729656ef: Loading layer [==================================================&gt;]  4.096kB/4.096kB
0d5022138006: Loading layer [==================================================&gt;]  38.09MB/38.09MB
8f757e3fe5e4: Loading layer [==================================================&gt;]  21.42MB/21.42MB
d2bc6b915bc9: Loading layer [==================================================&gt;]  4.019MB/4.019MB
bbeb6784ed45: Loading layer [==================================================&gt;]  313.9kB/313.9kB
0c411e83ee78: Loading layer [==================================================&gt;]  6.141MB/6.141MB
9c2d86dc137f: Loading layer [==================================================&gt;]  38.45MB/38.45MB
7797e5b3a760: Loading layer [==================================================&gt;]  2.754MB/2.754MB
98ef19df5514: Loading layer [==================================================&gt;]  4.096kB/4.096kB
4cde87c7ecaf: Loading layer [==================================================&gt;]  51.75MB/51.75MB
11536690d74a: Loading layer [==================================================&gt;]  3.584kB/3.584kB
Loaded image ID: sha256:c41e9fcadf5a291120de706b7dfa1af598b9f2ed5138b6dcb9f79a68aad0ef4c
Loaded image ID: sha256:7e5c1cecb086f36c6ef4b319a60853020820997f3600c3687e8ba6139e83674d

[ec2-user@k8s ~]$ cat k8s.gcr.io-ingress-nginx-v1.1.2.tar.gz | ssh worker1 docker load 

[ec2-user@k8s ~]$ 
docker tag 7e5c1cecb086 k8s.gcr.io/ingress-nginx/controller:v1.1.2
docker tag c41e9fcadf5a k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1


注：由于配置中用了sha码，下载后tag不同，把image最后的 @sha256:xxx 删掉
vi ingress-nginx-controller-v1.1.2.yaml
</code></pre>

<h3>创建服务</h3>

<pre><code>[ec2-user@k8s ~]$ kubectl apply -f ingress-nginx-controller-v1.1.2.yaml
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
serviceaccount/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
configmap/ingress-nginx-controller created
service/ingress-nginx-controller created
service/ingress-nginx-controller-admission created
deployment.apps/ingress-nginx-controller created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
ingressclass.networking.k8s.io/nginx created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created

[ec2-user@k8s ~]$ kubectl wait --namespace ingress-nginx \
   --for=condition=ready pod \
   --selector=app.kubernetes.io/component=controller \
   --timeout=120s
pod/ingress-nginx-controller-755447bb4d-rnxvl condition met


# 状态参考：
# https://kubernetes.io/zh/docs/tasks/access-application-cluster/ingress-minikube/
[ec2-user@k8s ~]$ kubectl get pods --namespace=ingress-nginx

[ec2-user@k8s ~]$ kubectl get all -n ingress-nginx
NAME                                            READY   STATUS      RESTARTS   AGE
pod/ingress-nginx-admission-create-hbt9d        0/1     Completed   0          2m51s
pod/ingress-nginx-admission-patch-j8qfh         0/1     Completed   1          2m51s
pod/ingress-nginx-controller-755447bb4d-rnxvl   1/1     Running     0          2m51s

NAME                                         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
service/ingress-nginx-controller             LoadBalancer   10.104.8.155    &lt;pending&gt;     80:31031/TCP,443:31845/TCP   2m51s
service/ingress-nginx-controller-admission   ClusterIP      10.108.67.255   &lt;none&gt;        443/TCP                      2m51s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ingress-nginx-controller   1/1     1            1           2m51s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/ingress-nginx-controller-755447bb4d   1         1         1       2m51s

NAME                                       COMPLETIONS   DURATION   AGE
job.batch/ingress-nginx-admission-create   1/1           3s         2m51s
job.batch/ingress-nginx-admission-patch    1/1           3s         2m51s
</code></pre>

<p>看到 ingress-nginx-controller 服务的 <code>EXTERNAL-IP</code> 为 <code>&lt;pending&gt;</code> ，由于本地搭建并没有配备负载均衡器，所以没有手段，获取不到对外的IP。</p>

<h3>本地测试</h3>

<pre><code>[ec2-user@k8s ~]$ kubectl create deployment demo --image=httpd --port=80
deployment.apps/demo created
[ec2-user@k8s ~]$ kubectl expose deployment demo
service/demo exposed

[ec2-user@k8s ~]$ kubectl create ingress demo-localhost --class=nginx \
   --rule=demo.localdev.me/*=demo:80
ingress.networking.k8s.io/demo-localhost created

[ec2-user@k8s ~]$ kubectl port-forward --namespace=ingress-nginx service/ingress-nginx-controller 8080:80
Forwarding from 127.0.0.1:8080 -&gt; 80
Forwarding from [::1]:8080 -&gt; 80
Handling connection for 8080
Handling connection for 8080

[ec2-user@k8s ~]$ curl http://demo.localdev.me:8080/
&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
</code></pre>

<p>clean</p>

<pre><code>kubectl delete deployment demo 
kubectl delete service demo 
kubectl delete ingress demo
</code></pre>

<h2>集成（发布）</h2>

<h3>cloud</h3>

<ul>
<li><a href="https://kubernetes.github.io/ingress-nginx/deploy/#aws">https://kubernetes.github.io/ingress-nginx/deploy/#aws</a></li>
</ul>


<p>配置云厂商的负载均衡器。</p>

<h3>baremetal: nodeport</h3>

<p>适用于部署在裸机服务器上的 Kubernetes 集群，以及使用通用 Linux 发行版手动安装 Kubernetes 的 [原始] 虚拟机</p>

<p>为了快速测试，您可以使用 NodePort。这应该适用于几乎每个集群，但它通常会使用 30000-32767 范围内的端口。</p>

<ul>
<li><a href="https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters">https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters</a></li>
<li><a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#over-a-nodeport-service">https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#over-a-nodeport-service</a></li>
</ul>


<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.2/deploy/static/provider/baremetal/deploy.yaml
</code></pre>

<p>注：也可以通过修改配置使用80，443等端口，但不推荐。</p>

<h3>baremetal: hostNetwork</h3>

<p>ingress nginx controller的pod网络直接使用主机网络，这个比Service Nodeport稍微灵活一点，可以自己选择/管理端口。</p>

<h3>A pure software solution: MetalLB</h3>

<ul>
<li><a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#a-pure-software-solution-metallb">https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#a-pure-software-solution-metallb</a></li>
<li><a href="https://metallb.universe.tf/concepts/">https://metallb.universe.tf/concepts/</a></li>
</ul>


<p>It has two features that work together to provide this service: address allocation, and external announcement.</p>

<p>After MetalLB has assigned an external IP address to a service, it needs to make the network beyond the cluster aware that the IP “lives” in the cluster. MetalLB uses standard routing protocols to achieve this: ARP, NDP, or BGP.</p>

<h4>安装</h4>

<ul>
<li><a href="https://metallb.universe.tf/installation/">https://metallb.universe.tf/installation/</a></li>
</ul>


<p>需要kube-proxy配置arp为true。得与局域网进行广播通信，所以需要开启arp功能（标准路由协议）。</p>

<pre><code>kubectl edit configmap -n kube-system kube-proxy

apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"
ipvs:
  strictARP: true
</code></pre>

<p>或者批处理一步到位</p>

<pre><code># see what changes would be made, returns nonzero returncode if different
kubectl get configmap kube-proxy -n kube-system -o yaml | \
sed -e "s/strictARP: false/strictARP: true/" | \
kubectl diff -f - -n kube-system

# actually apply the changes, returns nonzero returncode on errors only
kubectl get configmap kube-proxy -n kube-system -o yaml | \
sed -e "s/strictARP: false/strictARP: true/" | \
kubectl apply -f - -n kube-system
</code></pre>

<p>安装</p>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml
</code></pre>

<p>下载镜像慢一点，需要稍微多等等。再查看状态：</p>

<pre><code>[ec2-user@k8s ~]$ kubectl get all -n metallb-system
NAME                             READY   STATUS    RESTARTS   AGE
pod/controller-57fd9c5bb-kc5zt   1/1     Running   0          5m55s
pod/speaker-8pg4v                1/1     Running   0          5m55s
pod/speaker-95bs8                1/1     Running   0          5m55s

NAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/speaker   2         2         2       2            2           kubernetes.io/os=linux   5m55s

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/controller   1/1     1            1           5m55s

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/controller-57fd9c5bb   1         1         1       5m55s
</code></pre>

<h4>配置地址</h4>

<ul>
<li><p><a href="https://metallb.universe.tf/configuration/#layer-2-configuration">https://metallb.universe.tf/configuration/#layer-2-configuration</a></p></li>
<li><p><a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#a-pure-software-solution-metallb">https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#a-pure-software-solution-metallb</a></p></li>
</ul>


<pre><code># 查看主机ip，避开这些节点IP的区间
[ec2-user@k8s ~]$ kubectl get node -o wide
NAME      STATUS   ROLES                  AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
k8s       Ready    control-plane,master   10d   v1.23.5   192.168.191.131   &lt;none&gt;        Amazon Linux 2   4.14.268-205.500.amzn2.x86_64   docker://20.10.7
worker1   Ready    &lt;none&gt;                 10d   v1.23.5   192.168.191.132   &lt;none&gt;        Amazon Linux 2   4.14.268-205.500.amzn2.x86_64   docker://20.10.7

[ec2-user@k8s ~]$ cat metallb-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.191.200-192.168.191.220
[ec2-user@k8s ~]$ kubectl apply -f metallb-config.yml 
configmap/config created
</code></pre>

<p>然后，再回过头重新安装一遍nginx-ingress：</p>

<pre><code>[ec2-user@k8s ~]$ kubectl get pods --namespace=ingress-nginx
NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-b9hkn        0/1     Completed   0          17s
ingress-nginx-admission-patch-xmnbr         0/1     Completed   1          17s
ingress-nginx-controller-755447bb4d-lfrwk   0/1     Running     0          17s
[ec2-user@k8s ~]$ kubectl get pods --namespace=ingress-nginx
NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-b9hkn        0/1     Completed   0          25s
ingress-nginx-admission-patch-xmnbr         0/1     Completed   1          25s
ingress-nginx-controller-755447bb4d-lfrwk   1/1     Running     0          25s

[ec2-user@k8s ~]$ kubectl -n ingress-nginx get svc
NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.107.221.243   192.168.191.200   80:31443/TCP,443:30099/TCP   57s
ingress-nginx-controller-admission   ClusterIP      10.105.12.185    &lt;none&gt;            443/TCP                      57s
</code></pre>

<p>这次 EXTERNAL-IP 的ip就有值了，上面配置的ip段里面一个ip。</p>

<h3>在线/集成测试</h3>

<ul>
<li><a href="https://kubernetes.github.io/ingress-nginx/deploy/#online-testing">https://kubernetes.github.io/ingress-nginx/deploy/#online-testing</a></li>
</ul>


<pre><code>[ec2-user@k8s ~]$ kubectl get service ingress-nginx-controller --namespace=ingress-nginx
NAME                       TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)                      AGE
ingress-nginx-controller   LoadBalancer   10.107.221.243   192.168.191.200   80:31443/TCP,443:30099/TCP   4m


[ec2-user@k8s ~]$ kubectl create deployment demo --image=httpd --port=80
deployment.apps/demo created
[ec2-user@k8s ~]$ kubectl expose deployment demo
service/demo exposed

[ec2-user@k8s ~]$ kubectl create ingress demo --class=nginx \
   --rule="www.demo.io/*=demo:80"
ingress.networking.k8s.io/demo created


[ec2-user@k8s ~]$ kubectl get ingress 
NAME   CLASS   HOSTS         ADDRESS   PORTS   AGE
demo   nginx   www.demo.io             80      42s

[ec2-user@k8s ~]$ kubectl get ingress 
NAME   CLASS   HOSTS         ADDRESS           PORTS   AGE
demo   nginx   www.demo.io   192.168.191.200   80      27m
</code></pre>

<p>在本地windows机器的 <code>C:/Windows/System32/drivers/etc/hosts</code> 增加 <code>192.168.191.200 www.demo.io</code> ，然后浏览器访问 <code>http://www.demo.io/</code> ，顺利的话就能在浏览器看到：</p>

<pre><code>It works!
</code></pre>

<h3>后记</h3>

<h4>理一下网络调用，其实就是nginx的方式：</h4>

<pre><code>[ec2-user@k8s ~]$ kubectl logs --tail=2 pod/demo-764c97f6fd-q5xts
10.244.2.79 - - [28/Mar/2022:09:52:36 +0000] "GET / HTTP/1.1" 200 45
10.244.2.79 - - [28/Mar/2022:09:52:36 +0000] "GET /favicon.ico HTTP/1.1" 404 196

[ec2-user@k8s ~]$ kubectl get pods -n ingress-nginx -o wide 
NAME                                        READY   STATUS      RESTARTS   AGE    IP            NODE      NOMINATED NODE   READINESS GATES
ingress-nginx-admission-create-b9hkn        0/1     Completed   0          157m   10.244.2.78   worker1   &lt;none&gt;           &lt;none&gt;
ingress-nginx-admission-patch-xmnbr         0/1     Completed   1          157m   10.244.2.77   worker1   &lt;none&gt;           &lt;none&gt;
ingress-nginx-controller-755447bb4d-lfrwk   1/1     Running     0          157m   10.244.2.79   worker1   &lt;none&gt;           &lt;none&gt;

# 192.168.191.1是vmware虚拟网卡的地址
[ec2-user@k8s ~]$ kubectl logs --tail=2 pod/ingress-nginx-controller-755447bb4d-lfrwk -n ingress-nginx 
192.168.191.1 - - [28/Mar/2022:09:52:36 +0000] "GET / HTTP/1.1" 200 45 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36" 566 0.000 [default-demo-80] [] 10.244.2.80:80 45 0.000 200 6d52ef8349eb3101c31c3cc6377b982b
192.168.191.1 - - [28/Mar/2022:09:52:36 +0000] "GET /favicon.ico HTTP/1.1" 404 196 "http://www.demo.io/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36" 506 0.001 [default-demo-80] [] 10.244.2.80:80 196 0.000 404 fefe172a57273977cdcd1455bcf322ac


## web
Request URL: http://www.demo.io/
Request Method: GET
Status Code: 200 OK
Remote Address: 192.168.191.200:80
Referrer Policy: strict-origin-when-cross-origin
</code></pre>

<h4>看看metallb的日志，ip是怎么分配的</h4>

<pre><code># https://metallb.universe.tf/concepts/layer2/

[ec2-user@k8s ~]$ kubectl get pods -n metallb-system -o wide
NAME                         READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
controller-57fd9c5bb-kc5zt   1/1     Running   0          3h34m   10.244.2.76       worker1   &lt;none&gt;           &lt;none&gt;
speaker-8pg4v                1/1     Running   0          3h34m   192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
speaker-95bs8                1/1     Running   0          3h34m   192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;

[ec2-user@k8s ~]$ kubectl logs pod/controller-57fd9c5bb-kc5zt -n metallb-system
{"caller":"level.go:63","event":"ipAllocated","ip":["192.168.191.200"],"level":"info","msg":"IP address assigned by controller","service":"ingress-nginx/ingress-nginx-controller","ts":"2022-03-28T07:16:22.675599527Z"}
{"caller":"level.go:63","event":"serviceUpdated","level":"info","msg":"updated service object","service":"ingress-nginx/ingress-nginx-controller","ts":"2022-03-28T07:1

[ec2-user@k8s ~]$ kubectl logs speaker-8pg4v -n metallb-system 
{"caller":"level.go:63","event":"serviceAnnounced","ips":["192.168.191.200"],"level":"info","msg":"service has IP, announcing","pool":"default","protocol":"layer2","service":"ingress-nginx/ingress-nginx-controller","ts":"2022-03-28T07:16:42.775467559Z"}

[ec2-user@k8s ~]$ ping 192.168.191.200
PING 192.168.191.200 (192.168.191.200) 56(84) bytes of data.
^C
--- 192.168.191.200 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2055ms

[ec2-user@k8s ~]$ arp 
Address                  HWtype  HWaddress           Flags Mask            Iface
192.168.191.200          ether   00:0c:29:d5:4f:0f   C                     eth0

# worker1节点的MAC
[ec2-user@worker1 ~]$ ip a | grep -i -C 10 '00:0c:29:d5:4f:0f' 
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:0c:29:d5:4f:0f brd ff:ff:ff:ff:ff:ff
    inet 192.168.191.132/24 brd 192.168.191.255 scope global dynamic eth0
       valid_lft 1714sec preferred_lft 1714sec

# In layer 2 mode, all traffic for a service IP goes to one node. From there, kube-proxy spreads the traffic to all the service’s pods.

[ec2-user@k8s ~]$ kubectl get pods -A -o wide | grep 192.168.191.132
kube-system            kube-flannel-ds-q4qkt                        1/1     Running     8 (2d10h ago)   11d     192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
kube-system            kube-proxy-pd77m                             1/1     Running     6 (3d2h ago)    11d     192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
metallb-system         speaker-8pg4v                                1/1     Running     0               5h31m   192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
</code></pre>

<h2>例子：</h2>

<ul>
<li><a href="https://kubernetes.github.io/ingress-nginx/deploy/#local-testing">https://kubernetes.github.io/ingress-nginx/deploy/#local-testing</a></li>
<li><a href="https://kubernetes.io/zh/docs/tasks/access-application-cluster/ingress-minikube/">https://kubernetes.io/zh/docs/tasks/access-application-cluster/ingress-minikube/</a></li>
</ul>


<h2>验证</h2>

<ul>
<li><a href="https://www.qikqiak.com/post/visually-explained-k8s-service/">图解 Kubernetes Service</a></li>
<li><a href="https://www.qikqiak.com/post/visually-explained-k8s-ingress/">图解 Kubernetes Ingress</a></li>
</ul>


<p>文中说loadbalancer是通过了nodeport（会创建nodeport），还是有点诧异的。验证一番，果真如此！</p>

<pre><code>[ec2-user@k8s ~]$ kubectl get service ingress-nginx-controller --namespace=ingress-nginx
NAME                       TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)                      AGE
ingress-nginx-controller   LoadBalancer   10.107.221.243   192.168.191.200   80:31443/TCP,443:30099/TCP   34m

[ec2-user@k8s ~]$ kubectl describe service ingress-nginx-controller --namespace=ingress-nginx
Name:                     ingress-nginx-controller
Namespace:                ingress-nginx
Labels:                   app.kubernetes.io/component=controller
                          app.kubernetes.io/instance=ingress-nginx
                          app.kubernetes.io/managed-by=Helm
                          app.kubernetes.io/name=ingress-nginx
                          app.kubernetes.io/part-of=ingress-nginx
                          app.kubernetes.io/version=1.1.2
                          helm.sh/chart=ingress-nginx-4.0.18
Annotations:              &lt;none&gt;
Selector:                 app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.107.221.243
IPs:                      10.107.221.243
LoadBalancer Ingress:     192.168.191.200
Port:                     http  80/TCP
TargetPort:               http/TCP
NodePort:                 http  31443/TCP
Endpoints:                10.244.2.79:80
Port:                     https  443/TCP
TargetPort:               https/TCP
NodePort:                 https  30099/TCP
Endpoints:                10.244.2.79:443
Session Affinity:         None
External Traffic Policy:  Local
HealthCheck NodePort:     31942
Events:
  Type    Reason        Age   From                Message
  ----    ------        ----  ----                -------
  Normal  IPAllocated   38m   metallb-controller  Assigned IP ["192.168.191.200"]
  Normal  nodeAssigned  38m   metallb-speaker     announcing from node "worker1"


[ec2-user@k8s ~]$ netstat -anp | grep 31443 
(No info could be read for "-p": geteuid()=1002 but you should be root.)
tcp        0      0 0.0.0.0:31443           0.0.0.0:*               LISTEN      -       

[ec2-user@worker1 ~]$ netstat -anp | grep 31443
(No info could be read for "-p": geteuid()=1002 but you should be root.)
tcp        0      0 0.0.0.0:31443           0.0.0.0:*               LISTEN      -                   
</code></pre>

<h2>其他参考</h2>

<ul>
<li><a href="https://blog.51cto.com/tansong/4850092">多种方式访问AWS EKS的 Kubernetes Dashboard 下篇</a></li>
</ul>


<pre><code># 创建证书
openssl genrsa 2048 &gt; k8s-dashboard-private.key
openssl req -new -x509 -nodes -sha1 -days 3650 -extensions v3_ca -key k8s-dashboard-private.key &gt; k8s-dashboard.crt
</code></pre>

<ul>
<li><a href="https://segmentfault.com/a/1190000019908991">k8s ingress原理及ingress-nginx部署测试</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[k8s-v1.23.5安装指南 - 使用kubeadm]]></title>
    <link href="http://winse.github.io/blog/2022/03/18/k8s-guide-use-kubeadm/"/>
    <updated>2022-03-18T11:58:48+08:00</updated>
    <id>http://winse.github.io/blog/2022/03/18/k8s-guide-use-kubeadm</id>
    <content type="html"><![CDATA[<p>注[2022-03-25]：如果后面需要用AWS，用Amazon的操作系统会便利一点。aws的命令这些都自带了。</p>

<p>本文是在 Amazon Linux 2 系统上安装部署的，和centos7.3基本相似。</p>

<p>和k8s软件依赖需要访问google的，已经在前面一篇文章中下载好，本文中会直接使用。依赖的软件可以在百度网盘下载：</p>

<pre><code>链接：https://pan.baidu.com/s/1P3ABqKGt1JhNkg-9yB22yQ 
提取码：k7af
</code></pre>

<h2>安装 amazon-linux-2 操作系统</h2>

<ul>
<li><a href="https://docs.amazonaws.cn/AWSEC2/latest/UserGuide/amazon-linux-2-virtual-machine.html#amazon-linux-2-virtual-machine-download">步骤 2：下载 Amazon Linux 2 VM 映像</a></li>
<li>下载 <a href="https://cdn.amazonlinux.com/os-images/2.0.20220218.3/">https://cdn.amazonlinux.com/os-images/2.0.20220218.3/</a></li>
</ul>


<p>这里下载vmware使用的镜像 <code>amzn2-vmware_esx-2.0.20220218.3-x86_64.xfs.gpt.ova</code> 和初始化配置 <code>Seed.iso</code> 。</p>

<p>这里简单说下，其实ova已经是可以直接用的，文档中讲的很多内容是辅助系统定制初始化的。user-data用于创建用户和修改文件内容，meta-data配置主机名和网络ip设置。为了本地开发测试，我们直接用默认提供 <code>Seed.iso</code> 即可，登录使用 <code>ec2-user:amazon</code> 。</p>

<p>然后双击 ova 文件，就可以导入创建一个虚拟机出来了。</p>

<ul>
<li>修改网络适配器为NAT模式；</li>
<li>添加CD/DVD设备，选择 <code>Seed.iso</code> ISO映射文件；</li>
<li>开机登录系统后，打开sshd的密码登录。</li>
</ul>


<pre><code>$ sudo ifup eth0
$ ip a

$ sudo vi /etc/ssh/sshd_config
#PasswordAuthentication no

$ sudo service sshd reload 
</code></pre>

<h2>安装docker</h2>

<p>k8s需要容器运行时软件，我们先安装好docker。</p>

<ul>
<li><a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker">https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker</a></li>
</ul>


<p>aws linux 2有它自己的docker源，使用docker官网文档的方式依赖有些找不到。直接按照aws官方文档中提供的方式安装。</p>

<h3>坑</h3>

<p>一开始是按照docker官网在centos的方式安装的，但yum repo的变量不对上，改了releasever后，然后依赖的版本找不到。</p>

<pre><code>## https://docs.docker.com/engine/install/centos/
[ec2-user@amazonlinux ~]$ cat /etc/issue
\S
Kernel \r on an \m

[ec2-user@amazonlinux ~]$ yum-debug-dump
Loaded plugins: langpacks, priorities, update-motd
Output written to: /home/ec2-user/yum_debug_dump-amazonlinux.onprem-2022-03-17_02:16:37.txt.gz
[ec2-user@amazonlinux ~]$ less /home/ec2-user/yum_debug_dump-amazonlinux.onprem-2022-03-17_02:16:37.txt.gz
[ec2-user@amazonlinux ~]$ 

$releasever的值,这个表示当前系统的发行版本，可以通过rpm -qi centos-release命令查看，结果如下：
$basearch是我们的系统硬件架构(CPU指令集),使用命令arch得到

[ec2-user@amazonlinux ~]$ sudo sed -i 's/$releasever/7/g' /etc/yum.repos.d/docker-ce.repo 

## 缺少依赖
</code></pre>

<h3>正式安装docker</h3>

<pre><code>## https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html
[ec2-user@amazonlinux ~]$ sudo yum update -y

[ec2-user@amazonlinux ~]$ sudo amazon-linux-extras install docker
Installing docker
Loaded plugins: langpacks, priorities, update-motd
Cleaning repos: amzn2-core amzn2extra-docker
12 metadata files removed
4 sqlite files removed
0 metadata files removed
Loaded plugins: langpacks, priorities, update-motd
amzn2-core                                                                                                                                        | 3.7 kB  00:00:00     
amzn2extra-docker                                                                                                                                 | 3.0 kB  00:00:00     
(1/5): amzn2-core/2/x86_64/group_gz                                                                                                               | 2.5 kB  00:00:00     
(2/5): amzn2extra-docker/2/x86_64/updateinfo                                                                                                      | 5.9 kB  00:00:00     
(3/5): amzn2-core/2/x86_64/updateinfo                                                                                                             | 452 kB  00:00:01     
(4/5): amzn2extra-docker/2/x86_64/primary_db                                                                                                      |  86 kB  00:00:00     
(5/5): amzn2-core/2/x86_64/primary_db                                                                                                             |  60 MB  00:01:42     
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package docker.x86_64 0:20.10.7-5.amzn2 will be installed
--&gt; Processing Dependency: runc &gt;= 1.0.0 for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Processing Dependency: libcgroup &gt;= 0.40.rc1-5.15 for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Processing Dependency: containerd &gt;= 1.3.2 for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Processing Dependency: pigz for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Running transaction check
---&gt; Package containerd.x86_64 0:1.4.6-8.amzn2 will be installed
---&gt; Package libcgroup.x86_64 0:0.41-21.amzn2 will be installed
---&gt; Package pigz.x86_64 0:2.3.4-1.amzn2.0.1 will be installed
---&gt; Package runc.x86_64 0:1.0.0-2.amzn2 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

=========================================================================================================================================================================
 Package                               Arch                              Version                                      Repository                                    Size
=========================================================================================================================================================================
Installing:
 docker                                x86_64                            20.10.7-5.amzn2                              amzn2extra-docker                             42 M
Installing for dependencies:
 containerd                            x86_64                            1.4.6-8.amzn2                                amzn2extra-docker                             24 M
 libcgroup                             x86_64                            0.41-21.amzn2                                amzn2-core                                    66 k
 pigz                                  x86_64                            2.3.4-1.amzn2.0.1                            amzn2-core                                    81 k
 runc                                  x86_64                            1.0.0-2.amzn2                                amzn2extra-docker                            3.3 M

Transaction Summary
=========================================================================================================================================================================
Install  1 Package (+4 Dependent packages)

Total download size: 69 M
Installed size: 285 M
Is this ok [y/d/N]: y
Downloading packages:
(1/5): pigz-2.3.4-1.amzn2.0.1.x86_64.rpm                                                                                                          |  81 kB  00:00:00     
(2/5): libcgroup-0.41-21.amzn2.x86_64.rpm                                                                                                         |  66 kB  00:00:00     
(3/5): containerd-1.4.6-8.amzn2.x86_64.rpm                                                                                                        |  24 MB  00:01:14     
(4/5): runc-1.0.0-2.amzn2.x86_64.rpm                                                                                                              | 3.3 MB  00:00:10     
(5/5): docker-20.10.7-5.amzn2.x86_64.rpm                                                                                                          |  42 MB  00:01:50     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                                    641 kB/s |  69 MB  00:01:50     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : runc-1.0.0-2.amzn2.x86_64                                                                                                                             1/5 
  Installing : containerd-1.4.6-8.amzn2.x86_64                                                                                                                       2/5 
  Installing : libcgroup-0.41-21.amzn2.x86_64                                                                                                                        3/5 
  Installing : pigz-2.3.4-1.amzn2.0.1.x86_64                                                                                                                         4/5 
  Installing : docker-20.10.7-5.amzn2.x86_64                                                                                                                         5/5 
  Verifying  : docker-20.10.7-5.amzn2.x86_64                                                                                                                         1/5 
  Verifying  : containerd-1.4.6-8.amzn2.x86_64                                                                                                                       2/5 
  Verifying  : runc-1.0.0-2.amzn2.x86_64                                                                                                                             3/5 
  Verifying  : pigz-2.3.4-1.amzn2.0.1.x86_64                                                                                                                         4/5 
  Verifying  : libcgroup-0.41-21.amzn2.x86_64                                                                                                                        5/5 

Installed:
  docker.x86_64 0:20.10.7-5.amzn2                                                                                                                                        

Dependency Installed:
  containerd.x86_64 0:1.4.6-8.amzn2           libcgroup.x86_64 0:0.41-21.amzn2           pigz.x86_64 0:2.3.4-1.amzn2.0.1           runc.x86_64 0:1.0.0-2.amzn2          

Complete!
  0  ansible2                 available    \
        [ =2.4.2  =2.4.6  =2.8  =stable ]
  2  httpd_modules            available    [ =1.0  =stable ]
  3  memcached1.5             available    \
        [ =1.5.1  =1.5.16  =1.5.17 ]
  5  postgresql9.6            available    \
        [ =9.6.6  =9.6.8  =stable ]
  6  postgresql10             available    [ =10  =stable ]
  9  R3.4                     available    [ =3.4.3  =stable ]
 10  rust1                    available    \
        [ =1.22.1  =1.26.0  =1.26.1  =1.27.2  =1.31.0  =1.38.0
          =stable ]
 11  vim                      available    [ =8.0  =stable ]
 18  libreoffice              available    \
        [ =5.0.6.2_15  =5.3.6.1  =stable ]
 19  gimp                     available    [ =2.8.22 ]
 20  docker=latest            enabled      \
        [ =17.12.1  =18.03.1  =18.06.1  =18.09.9  =stable ]
 21  mate-desktop1.x          available    \
        [ =1.19.0  =1.20.0  =stable ]
 22  GraphicsMagick1.3        available    \
        [ =1.3.29  =1.3.32  =1.3.34  =stable ]
 23  tomcat8.5                available    \
        [ =8.5.31  =8.5.32  =8.5.38  =8.5.40  =8.5.42  =8.5.50
          =stable ]
 24  epel                     available    [ =7.11  =stable ]
 25  testing                  available    [ =1.0  =stable ]
 26  ecs                      available    [ =stable ]
 27  corretto8                available    \
        [ =1.8.0_192  =1.8.0_202  =1.8.0_212  =1.8.0_222  =1.8.0_232
          =1.8.0_242  =stable ]
 28  firecracker              available    [ =0.11  =stable ]
 29  golang1.11               available    \
        [ =1.11.3  =1.11.11  =1.11.13  =stable ]
 30  squid4                   available    [ =4  =stable ]
 32  lustre2.10               available    \
        [ =2.10.5  =2.10.8  =stable ]
 33  java-openjdk11           available    [ =11  =stable ]
 34  lynis                    available    [ =stable ]
 35  kernel-ng                available    [ =stable ]
 36  BCC                      available    [ =0.x  =stable ]
 37  mono                     available    [ =5.x  =stable ]
 38  nginx1                   available    [ =stable ]
 39  ruby2.6                  available    [ =2.6  =stable ]
 40  mock                     available    [ =stable ]
 41  postgresql11             available    [ =11  =stable ]
 42  php7.4                   available    [ =stable ]
 43  livepatch                available    [ =stable ]
 44  python3.8                available    [ =stable ]
 45  haproxy2                 available    [ =stable ]
 46  collectd                 available    [ =stable ]
 47  aws-nitro-enclaves-cli   available    [ =stable ]
 48  R4                       available    [ =stable ]
 49  kernel-5.4               available    [ =stable ]
 50  selinux-ng               available    [ =stable ]
 51  php8.0                   available    [ =stable ]
 52  tomcat9                  available    [ =stable ]
 53  unbound1.13              available    [ =stable ]
 54  mariadb10.5              available    [ =stable ]
 55  kernel-5.10              available    [ =stable ]
 56  redis6                   available    [ =stable ]
 57  ruby3.0                  available    [ =stable ]
 58  postgresql12             available    [ =stable ]
 59  postgresql13             available    [ =stable ]
 60  mock2                    available    [ =stable ]
 61  dnsmasq2.85              available    [ =stable ]
[ec2-user@amazonlinux ~]$ 

[ec2-user@amazonlinux ~]$ sudo service docker start
Redirecting to /bin/systemctl start docker.service
[ec2-user@amazonlinux ~]$ sudo systemctl enable docker
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.


[ec2-user@amazonlinux ~]$ sudo usermod -a -G docker ec2-user

[ec2-user@amazonlinux ~]$ docker info
Client:
 Context:    default
 Debug Mode: false

Server:
ERROR: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/info": dial unix /var/run/docker.sock: connect: permission denied
errors pretty printing info
[ec2-user@amazonlinux ~]$ exit
退出后再次连接：
[ec2-user@amazonlinux ~]$ docker info 
Client:
 Context:    default
 Debug Mode: false

Server:
 Containers: 0
  Running: 0
  Paused: 0
  Stopped: 0
 Images: 0
 Server Version: 20.10.7
 Storage Driver: overlay2
  Backing Filesystem: xfs
  Supports d_type: true
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: cgroupfs
 Cgroup Version: 1
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: d71fcd7d8303cbf684402823e425e9dd2e99285d
 runc version: 84113eef6fc27af1b01b3181f31bbaf708715301
 init version: de40ad0
 Security Options:
  seccomp
   Profile: default
 Kernel Version: 4.14.268-205.500.amzn2.x86_64
 Operating System: Amazon Linux 2
 OSType: linux
 Architecture: x86_64
 CPUs: 2
 Total Memory: 3.828GiB
 Name: amazonlinux.onprem
 ID: GENW:47BV:UJR2:247P:CPFE:PHSO:RA6Z:H4RK:HYEE:LXN3:XDIZ:SI6Q
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Registry: https://index.docker.io/v1/
 Labels:
 Experimental: false
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: false

[ec2-user@amazonlinux ~]$ sudo yum install telnet -y
</code></pre>

<h2>准备工作</h2>

<p>可以先了解一些基本概念
* <a href="https://www.jianshu.com/p/7bc34ff88d9d">Kubernetes in Action 笔记 —— 部署第一个应用</a></p>

<ul>
<li><a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B">准备开始</a></li>
</ul>


<pre><code>## 确保每个节点上 MAC 地址和 product_uuid 的唯一性
[ec2-user@amazonlinux ~]$ ip link
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
    link/ether 00:0c:29:a4:a6:fc brd ff:ff:ff:ff:ff:ff
[ec2-user@amazonlinux ~]$ ifconfig -a
eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 192.168.191.131  netmask 255.255.255.0  broadcast 192.168.191.255
        inet6 fe80::20c:29ff:fea4:a6fc  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether 00:0c:29:a4:a6:fc  txqueuelen 1000  (Ethernet)
        RX packets 1737  bytes 288390 (281.6 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 1704  bytes 139101 (135.8 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 625  bytes 57768 (56.4 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 625  bytes 57768 (56.4 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

[ec2-user@amazonlinux ~]$ sudo cat /sys/class/dmi/id/product_uuid
564DD81E-DEBE-B06D-CF35-D7E3DDA4A6FC

## 检查网络适配器
# 只有一个网卡，跳过

## 允许 iptables 检查桥接流量
[ec2-user@amazonlinux ~]$ lsmod | grep br_netfilter
[ec2-user@amazonlinux ~]$ 
[ec2-user@amazonlinux ~]$ sudo modprobe br_netfilter
[ec2-user@amazonlinux ~]$ lsmod | grep br_netfilter 
br_netfilter           24576  0
bridge                172032  1 br_netfilter
[ec2-user@amazonlinux ~]$ 

[ec2-user@amazonlinux ~]$ cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

[ec2-user@amazonlinux ~]$ cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

[ec2-user@amazonlinux ~]$ sudo sysctl --system
* Applying /etc/sysctl.d/00-defaults.conf ...
kernel.printk = 8 4 1 7
kernel.panic = 30
net.ipv4.neigh.default.gc_thresh1 = 0
net.ipv6.neigh.default.gc_thresh1 = 0
net.ipv4.neigh.default.gc_thresh2 = 15360
net.ipv6.neigh.default.gc_thresh2 = 15360
net.ipv4.neigh.default.gc_thresh3 = 16384
net.ipv6.neigh.default.gc_thresh3 = 16384
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
kernel.kptr_restrict = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-amazon.conf ...
kernel.sched_autogroup_enabled = 0
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
* Applying /etc/sysctl.conf ...

## SELINUX
sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config 
setenforce 0

## Docker
cat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

sudo systemctl enable docker
sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre>

<p>修改时区</p>

<pre><code>sudo rm -rf /etc/localtime 
sudo ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 
</code></pre>

<h2>主节点安装kubeadm并加载docker镜像</h2>

<pre><code>## 修改主机名
[ec2-user@amazonlinux ~]$ sudo hostnamectl --static set-hostname k8s 
[ec2-user@amazonlinux ~]$ sudo hostname k8s 

[ec2-user@k8s ~]$ rz
rz waiting to receive.
Starting zmodem transfer.  Press Ctrl+C to cancel.
Transferring ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm...
  100%    9253 KB    9253 KB/sec    00:00:01       0 Errors  
Transferring d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm...
  100%   21041 KB    21041 KB/sec    00:00:01       0 Errors  
Transferring 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm...
  100%    7228 KB    7228 KB/sec    00:00:01       0 Errors  
Transferring db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm...
  100%   19030 KB    19030 KB/sec    00:00:01       0 Errors  
Transferring 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm...
  100%    9689 KB    9689 KB/sec    00:00:01       0 Errors  

[ec2-user@k8s ~]$ sudo yum install -y *.rpm
Loaded plugins: langpacks, priorities, update-motd
Examining 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm: cri-tools-1.23.0-0.x86_64
Marking 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm to be installed
Examining 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm: kubectl-1.23.5-0.x86_64
Marking 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm to be installed
Examining ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm: kubeadm-1.23.5-0.x86_64
Marking ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm to be installed
Examining d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm: kubelet-1.23.5-0.x86_64
Marking d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm to be installed
Examining db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm: kubernetes-cni-0.8.7-0.x86_64
Marking db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm to be installed
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package cri-tools.x86_64 0:1.23.0-0 will be installed
---&gt; Package kubeadm.x86_64 0:1.23.5-0 will be installed
---&gt; Package kubectl.x86_64 0:1.23.5-0 will be installed
---&gt; Package kubelet.x86_64 0:1.23.5-0 will be installed
--&gt; Processing Dependency: conntrack for package: kubelet-1.23.5-0.x86_64
--&gt; Processing Dependency: ebtables for package: kubelet-1.23.5-0.x86_64
--&gt; Processing Dependency: socat for package: kubelet-1.23.5-0.x86_64
---&gt; Package kubernetes-cni.x86_64 0:0.8.7-0 will be installed
--&gt; Running transaction check
---&gt; Package conntrack-tools.x86_64 0:1.4.4-5.amzn2.2 will be installed
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.1)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.0)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cthelper.so.0(LIBNETFILTER_CTHELPER_1.0)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_queue.so.1()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cthelper.so.0()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
---&gt; Package ebtables.x86_64 0:2.0.10-16.amzn2.0.1 will be installed
---&gt; Package socat.x86_64 0:1.7.3.2-2.amzn2.0.1 will be installed
--&gt; Running transaction check
---&gt; Package libnetfilter_cthelper.x86_64 0:1.0.0-10.amzn2.1 will be installed
---&gt; Package libnetfilter_cttimeout.x86_64 0:1.0.0-6.amzn2.1 will be installed
---&gt; Package libnetfilter_queue.x86_64 0:1.0.2-2.amzn2.0.2 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

======================================================================================================================================================
 Package                Arch   Version             Repository                                                                                    Size
======================================================================================================================================================
Installing:
 cri-tools              x86_64 1.23.0-0            /4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64   34 M
 kubeadm                x86_64 1.23.5-0            /ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64     43 M
 kubectl                x86_64 1.23.5-0            /96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64     44 M
 kubelet                x86_64 1.23.5-0            /d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64    119 M
 kubernetes-cni         x86_64 0.8.7-0             /db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64
                                                                                                                                                 55 M
Installing for dependencies:
 conntrack-tools        x86_64 1.4.4-5.amzn2.2     amzn2-core                                                                                   186 k
 ebtables               x86_64 2.0.10-16.amzn2.0.1 amzn2-core                                                                                   122 k
 libnetfilter_cthelper  x86_64 1.0.0-10.amzn2.1    amzn2-core                                                                                    18 k
 libnetfilter_cttimeout x86_64 1.0.0-6.amzn2.1     amzn2-core                                                                                    18 k
 libnetfilter_queue     x86_64 1.0.2-2.amzn2.0.2   amzn2-core                                                                                    24 k
 socat                  x86_64 1.7.3.2-2.amzn2.0.1 amzn2-core                                                                                   291 k

Transaction Summary
======================================================================================================================================================
Install  5 Packages (+6 Dependent packages)

Total size: 296 M
Total download size: 658 k
Installed size: 298 M
Downloading packages:
(1/6): ebtables-2.0.10-16.amzn2.0.1.x86_64.rpm                                                                                 | 122 kB  00:00:10     
(2/6): libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64.rpm                                                                       |  18 kB  00:00:00     
(3/6): conntrack-tools-1.4.4-5.amzn2.2.x86_64.rpm                                                                              | 186 kB  00:00:10     
(4/6): libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64.rpm                                                                       |  18 kB  00:00:00     
(5/6): libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64.rpm                                                                         |  24 kB  00:00:00     
(6/6): socat-1.7.3.2-2.amzn2.0.1.x86_64.rpm                                                                                    | 291 kB  00:00:00     
------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                  45 kB/s | 658 kB  00:00:14     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64                                                                                     1/11 
  Installing : libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64                                                                                     2/11 
  Installing : libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64                                                                                       3/11 
  Installing : conntrack-tools-1.4.4-5.amzn2.2.x86_64                                                                                            4/11 
  Installing : ebtables-2.0.10-16.amzn2.0.1.x86_64                                                                                               5/11 
  Installing : cri-tools-1.23.0-0.x86_64                                                                                                         6/11 
  Installing : socat-1.7.3.2-2.amzn2.0.1.x86_64                                                                                                  7/11 
  Installing : kubelet-1.23.5-0.x86_64                                                                                                           8/11 
  Installing : kubernetes-cni-0.8.7-0.x86_64                                                                                                     9/11 
  Installing : kubectl-1.23.5-0.x86_64                                                                                                          10/11 
  Installing : kubeadm-1.23.5-0.x86_64                                                                                                          11/11 
  Verifying  : kubernetes-cni-0.8.7-0.x86_64                                                                                                     1/11 
  Verifying  : kubectl-1.23.5-0.x86_64                                                                                                           2/11 
  Verifying  : socat-1.7.3.2-2.amzn2.0.1.x86_64                                                                                                  3/11 
  Verifying  : cri-tools-1.23.0-0.x86_64                                                                                                         4/11 
  Verifying  : ebtables-2.0.10-16.amzn2.0.1.x86_64                                                                                               5/11 
  Verifying  : libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64                                                                                       6/11 
  Verifying  : conntrack-tools-1.4.4-5.amzn2.2.x86_64                                                                                            7/11 
  Verifying  : libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64                                                                                     8/11 
  Verifying  : kubeadm-1.23.5-0.x86_64                                                                                                           9/11 
  Verifying  : kubelet-1.23.5-0.x86_64                                                                                                          10/11 
  Verifying  : libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64                                                                                    11/11 

Installed:
  cri-tools.x86_64 0:1.23.0-0   kubeadm.x86_64 0:1.23.5-0   kubectl.x86_64 0:1.23.5-0   kubelet.x86_64 0:1.23.5-0   kubernetes-cni.x86_64 0:0.8.7-0  

Dependency Installed:
  conntrack-tools.x86_64 0:1.4.4-5.amzn2.2          ebtables.x86_64 0:2.0.10-16.amzn2.0.1           libnetfilter_cthelper.x86_64 0:1.0.0-10.amzn2.1  
  libnetfilter_cttimeout.x86_64 0:1.0.0-6.amzn2.1   libnetfilter_queue.x86_64 0:1.0.2-2.amzn2.0.2   socat.x86_64 0:1.7.3.2-2.amzn2.0.1               

Complete!
[ec2-user@k8s ~]$ 

[ec2-user@k8s ~]$ sudo yum install ebtables ethtool             
</code></pre>

<p>加载docker镜像：</p>

<pre><code>[ec2-user@k8s ~]$ docker load -i k8s.tar.gz 
194a408e97d8: Loading layer [==================================================&gt;]  68.57MB/68.57MB
2b8347a02bc5: Loading layer [==================================================&gt;]  1.509MB/1.509MB
618b3e11ccba: Loading layer [==================================================&gt;]  44.17MB/44.17MB
Loaded image: k8s.gcr.io/kube-proxy:v1.23.5
5b1fa8e3e100: Loading layer [==================================================&gt;]  3.697MB/3.697MB
83e216f0eb98: Loading layer [==================================================&gt;]  1.509MB/1.509MB
a70573edad24: Loading layer [==================================================&gt;]  121.1MB/121.1MB
Loaded image: k8s.gcr.io/kube-controller-manager:v1.23.5
46576c5a6a97: Loading layer [==================================================&gt;]  49.63MB/49.63MB
Loaded image: k8s.gcr.io/kube-scheduler:v1.23.5
6d75f23be3dd: Loading layer [==================================================&gt;]  3.697MB/3.697MB
b6e8c573c18d: Loading layer [==================================================&gt;]  2.257MB/2.257MB
d80003ff5706: Loading layer [==================================================&gt;]    267MB/267MB
664dd6f2834b: Loading layer [==================================================&gt;]  2.137MB/2.137MB
62ae031121b1: Loading layer [==================================================&gt;]  18.86MB/18.86MB
Loaded image: k8s.gcr.io/etcd:3.5.1-0
256bc5c338a6: Loading layer [==================================================&gt;]  336.4kB/336.4kB
80e4a2390030: Loading layer [==================================================&gt;]  46.62MB/46.62MB
Loaded image: k8s.gcr.io/coredns/coredns:v1.8.6
1021ef88c797: Loading layer [==================================================&gt;]  684.5kB/684.5kB
Loaded image: k8s.gcr.io/pause:3.6
50098fdfecae: Loading layer [==================================================&gt;]  131.3MB/131.3MB
Loaded image: k8s.gcr.io/kube-apiserver:v1.23.5

[ec2-user@k8s ~]$ docker images 
REPOSITORY                           TAG       IMAGE ID       CREATED        SIZE
k8s.gcr.io/kube-apiserver            v1.23.5   3fc1d62d6587   15 hours ago   135MB
k8s.gcr.io/kube-proxy                v1.23.5   3c53fa8541f9   15 hours ago   112MB
k8s.gcr.io/kube-controller-manager   v1.23.5   b0c9e5e4dbb1   15 hours ago   125MB
k8s.gcr.io/kube-scheduler            v1.23.5   884d49d6d8c9   15 hours ago   53.5MB
k8s.gcr.io/etcd                      3.5.1-0   25f8c7f3da61   4 months ago   293MB
k8s.gcr.io/coredns/coredns           v1.8.6    a4ca41631cc7   5 months ago   46.8MB
k8s.gcr.io/pause                     3.6       6270bb605e12   6 months ago   683kB
[ec2-user@k8s ~]$ 
</code></pre>

<h2>主节点(控制平面control-plane node)启动服务</h2>

<pre><code>[ec2-user@k8s ~]$ sudo su - 

[root@k8s ~]# kubeadm init 
[init] Using Kubernetes version: v1.23.5
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
        [WARNING Hostname]: hostname "k8s" could not be reached
        [WARNING Hostname]: hostname "k8s": lookup k8s on 192.168.191.2:53: no such host
        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.191.131]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s localhost] and IPs [192.168.191.131 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s localhost] and IPs [192.168.191.131 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[apiclient] All control plane components are healthy after 87.001525 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: sj6fff.bpak7gkd3hnyzcm5
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.191.131:6443 --token sj6fff.bpak7gkd3hnyzcm5 \
        --discovery-token-ca-cert-hash sha256:8e15649afc0771e80cce7f1dfdbb0933f4fdbd45ea1f9e03be1f3b78449a6d3c 
[root@k8s ~]# 
</code></pre>

<p>普通用户配置kubectl：</p>

<pre><code>
[ec2-user@k8s ~]$ mkdir -p $HOME/.kube
[ec2-user@k8s ~]$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[ec2-user@k8s ~]$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

[ec2-user@k8s ~]$ which kubectl
/usr/bin/kubectl
[ec2-user@k8s ~]$ 
[ec2-user@k8s ~]$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.191.131:6443
CoreDNS is running at https://192.168.191.131:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

[ec2-user@k8s ~]$ kubectl get nodes -o wide
NAME   STATUS     ROLES                  AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
k8s    NotReady   control-plane,master   14m   v1.23.5   192.168.191.131   &lt;none&gt;        Amazon Linux 2   4.14.268-205.500.amzn2.x86_64   docker://20.10.7

[ec2-user@k8s ~]$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE   IP                NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-64897985d-pcxpd       0/1     Pending   0          14m   &lt;none&gt;            &lt;none&gt;   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-64897985d-pfsj6       0/1     Pending   0          14m   &lt;none&gt;            &lt;none&gt;   &lt;none&gt;           &lt;none&gt;
kube-system   etcd-k8s                      1/1     Running   0          14m   192.168.191.131   k8s      &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-k8s            1/1     Running   0          14m   192.168.191.131   k8s      &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-k8s   1/1     Running   0          14m   192.168.191.131   k8s      &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-qj6lw              1/1     Running   0          14m   192.168.191.131   k8s      &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-k8s            1/1     Running   0          14m   192.168.191.131   k8s      &lt;none&gt;           &lt;none&gt;
[ec2-user@k8s ~]$ 
</code></pre>

<p>如果希望主节点（控制平面节点control-plane node)上也调度 Pod， 例如用于开发的单机 Kubernetes 集群，请运行：</p>

<pre><code>[root@k8s ~]# export KUBECONFIG=/etc/kubernetes/admin.conf
[root@k8s ~]# kubectl taint nodes --all node-role.kubernetes.io/master-
node/k8s untainted
[root@k8s ~]# 
</code></pre>

<h2>加入工作节点(nodes)</h2>

<p>先把docker安装好，以及系统基础配置，参考上面的步骤。然后安装kubeadm，以及加载gcr的docker镜像。</p>

<pre><code>[ec2-user@amazonlinux ~]$ ll
total 285480
-rw-r--r-- 1 ec2-user ec2-user   7401938 Mar 17 15:22 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm
-rw-r--r-- 1 ec2-user ec2-user   9921646 Mar 17 15:22 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm
-rw-r--r-- 1 ec2-user ec2-user   9475514 Mar 17 15:22 ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm
-rw-r--r-- 1 ec2-user ec2-user  21546750 Mar 17 15:22 d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm
-rw-r--r-- 1 ec2-user ec2-user  19487362 Mar 17 15:22 db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm
-rw-r--r-- 1 ec2-user ec2-user 224482960 Mar 17 15:22 k8s.tar.gz

[ec2-user@amazonlinux ~]$ sudo yum install *.rpm 
Loaded plugins: langpacks, priorities, update-motd
Examining 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm: cri-tools-1.23.0-0.x86_64
Marking 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm to be installed
Examining 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm: kubectl-1.23.5-0.x86_64
Marking 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm to be installed
Examining ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm: kubeadm-1.23.5-0.x86_64
Marking ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm to be installed
Examining d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm: kubelet-1.23.5-0.x86_64
Marking d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm to be installed
Examining db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm: kubernetes-cni-0.8.7-0.x86_64
Marking db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm to be installed
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package cri-tools.x86_64 0:1.23.0-0 will be installed
---&gt; Package kubeadm.x86_64 0:1.23.5-0 will be installed
---&gt; Package kubectl.x86_64 0:1.23.5-0 will be installed
---&gt; Package kubelet.x86_64 0:1.23.5-0 will be installed
--&gt; Processing Dependency: conntrack for package: kubelet-1.23.5-0.x86_64
amzn2-core                                                                                                                                  | 3.7 kB  00:00:00     
--&gt; Processing Dependency: ebtables for package: kubelet-1.23.5-0.x86_64
--&gt; Processing Dependency: socat for package: kubelet-1.23.5-0.x86_64
---&gt; Package kubernetes-cni.x86_64 0:0.8.7-0 will be installed
--&gt; Running transaction check
---&gt; Package conntrack-tools.x86_64 0:1.4.4-5.amzn2.2 will be installed
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.1)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.0)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cthelper.so.0(LIBNETFILTER_CTHELPER_1.0)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_queue.so.1()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cthelper.so.0()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
---&gt; Package ebtables.x86_64 0:2.0.10-16.amzn2.0.1 will be installed
---&gt; Package socat.x86_64 0:1.7.3.2-2.amzn2.0.1 will be installed
--&gt; Running transaction check
---&gt; Package libnetfilter_cthelper.x86_64 0:1.0.0-10.amzn2.1 will be installed
---&gt; Package libnetfilter_cttimeout.x86_64 0:1.0.0-6.amzn2.1 will be installed
---&gt; Package libnetfilter_queue.x86_64 0:1.0.2-2.amzn2.0.2 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

===================================================================================================================================================================
 Package                  Arch     Version                 Repository                                                                                         Size
===================================================================================================================================================================
Installing:
 cri-tools                x86_64   1.23.0-0                /4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64        34 M
 kubeadm                  x86_64   1.23.5-0                /ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64          43 M
 kubectl                  x86_64   1.23.5-0                /96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64          44 M
 kubelet                  x86_64   1.23.5-0                /d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64         119 M
 kubernetes-cni           x86_64   0.8.7-0                 /db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64    55 M
Installing for dependencies:
 conntrack-tools          x86_64   1.4.4-5.amzn2.2         amzn2-core                                                                                        186 k
 ebtables                 x86_64   2.0.10-16.amzn2.0.1     amzn2-core                                                                                        122 k
 libnetfilter_cthelper    x86_64   1.0.0-10.amzn2.1        amzn2-core                                                                                         18 k
 libnetfilter_cttimeout   x86_64   1.0.0-6.amzn2.1         amzn2-core                                                                                         18 k
 libnetfilter_queue       x86_64   1.0.2-2.amzn2.0.2       amzn2-core                                                                                         24 k
 socat                    x86_64   1.7.3.2-2.amzn2.0.1     amzn2-core                                                                                        291 k

Transaction Summary
===================================================================================================================================================================
Install  5 Packages (+6 Dependent packages)

Total size: 296 M
Total download size: 658 k
Installed size: 298 M
Is this ok [y/d/N]: y
Downloading packages:
(1/6): ebtables-2.0.10-16.amzn2.0.1.x86_64.rpm                                                                                              | 122 kB  00:00:00     
(2/6): libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64.rpm                                                                                    |  18 kB  00:00:00     
(3/6): libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64.rpm                                                                                    |  18 kB  00:00:00     
(4/6): conntrack-tools-1.4.4-5.amzn2.2.x86_64.rpm                                                                                           | 186 kB  00:00:00     
(5/6): libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64.rpm                                                                                      |  24 kB  00:00:00     
(6/6): socat-1.7.3.2-2.amzn2.0.1.x86_64.rpm                                                                                                 | 291 kB  00:00:00     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                              1.0 MB/s | 658 kB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64                                                                                                  1/11 
  Installing : libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64                                                                                                  2/11 
  Installing : libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64                                                                                                    3/11 
  Installing : conntrack-tools-1.4.4-5.amzn2.2.x86_64                                                                                                         4/11 
  Installing : ebtables-2.0.10-16.amzn2.0.1.x86_64                                                                                                            5/11 
  Installing : cri-tools-1.23.0-0.x86_64                                                                                                                      6/11 
  Installing : socat-1.7.3.2-2.amzn2.0.1.x86_64                                                                                                               7/11 
  Installing : kubelet-1.23.5-0.x86_64                                                                                                                        8/11 
  Installing : kubernetes-cni-0.8.7-0.x86_64                                                                                                                  9/11 
  Installing : kubectl-1.23.5-0.x86_64                                                                                                                       10/11 
  Installing : kubeadm-1.23.5-0.x86_64                                                                                                                       11/11 
  Verifying  : kubernetes-cni-0.8.7-0.x86_64                                                                                                                  1/11 
  Verifying  : kubectl-1.23.5-0.x86_64                                                                                                                        2/11 
  Verifying  : socat-1.7.3.2-2.amzn2.0.1.x86_64                                                                                                               3/11 
  Verifying  : cri-tools-1.23.0-0.x86_64                                                                                                                      4/11 
  Verifying  : ebtables-2.0.10-16.amzn2.0.1.x86_64                                                                                                            5/11 
  Verifying  : libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64                                                                                                    6/11 
  Verifying  : conntrack-tools-1.4.4-5.amzn2.2.x86_64                                                                                                         7/11 
  Verifying  : libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64                                                                                                  8/11 
  Verifying  : kubeadm-1.23.5-0.x86_64                                                                                                                        9/11 
  Verifying  : kubelet-1.23.5-0.x86_64                                                                                                                       10/11 
  Verifying  : libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64                                                                                                 11/11 

Installed:
  cri-tools.x86_64 0:1.23.0-0     kubeadm.x86_64 0:1.23.5-0     kubectl.x86_64 0:1.23.5-0     kubelet.x86_64 0:1.23.5-0     kubernetes-cni.x86_64 0:0.8.7-0    

Dependency Installed:
  conntrack-tools.x86_64 0:1.4.4-5.amzn2.2              ebtables.x86_64 0:2.0.10-16.amzn2.0.1               libnetfilter_cthelper.x86_64 0:1.0.0-10.amzn2.1      
  libnetfilter_cttimeout.x86_64 0:1.0.0-6.amzn2.1       libnetfilter_queue.x86_64 0:1.0.2-2.amzn2.0.2       socat.x86_64 0:1.7.3.2-2.amzn2.0.1                   

Complete!

[ec2-user@amazonlinux ~]$ sudo yum install ebtables ethtool  
Loaded plugins: langpacks, priorities, update-motd
Package ebtables-2.0.10-16.amzn2.0.1.x86_64 already installed and latest version
Package 2:ethtool-4.8-10.amzn2.x86_64 already installed and latest version
Nothing to do
</code></pre>

<p>加载docker镜像</p>

<pre><code>
[ec2-user@amazonlinux ~]$ docker load -i k8s.tar.gz 
194a408e97d8: Loading layer [==================================================&gt;]  68.57MB/68.57MB
2b8347a02bc5: Loading layer [==================================================&gt;]  1.509MB/1.509MB
618b3e11ccba: Loading layer [==================================================&gt;]  44.17MB/44.17MB
Loaded image: k8s.gcr.io/kube-proxy:v1.23.5
5b1fa8e3e100: Loading layer [==================================================&gt;]  3.697MB/3.697MB
83e216f0eb98: Loading layer [==================================================&gt;]  1.509MB/1.509MB
a70573edad24: Loading layer [==================================================&gt;]  121.1MB/121.1MB
Loaded image: k8s.gcr.io/kube-controller-manager:v1.23.5
46576c5a6a97: Loading layer [==================================================&gt;]  49.63MB/49.63MB
Loaded image: k8s.gcr.io/kube-scheduler:v1.23.5
6d75f23be3dd: Loading layer [==================================================&gt;]  3.697MB/3.697MB
b6e8c573c18d: Loading layer [==================================================&gt;]  2.257MB/2.257MB
d80003ff5706: Loading layer [==================================================&gt;]    267MB/267MB
664dd6f2834b: Loading layer [==================================================&gt;]  2.137MB/2.137MB
62ae031121b1: Loading layer [==================================================&gt;]  18.86MB/18.86MB
Loaded image: k8s.gcr.io/etcd:3.5.1-0
256bc5c338a6: Loading layer [==================================================&gt;]  336.4kB/336.4kB
80e4a2390030: Loading layer [==================================================&gt;]  46.62MB/46.62MB
Loaded image: k8s.gcr.io/coredns/coredns:v1.8.6
1021ef88c797: Loading layer [==================================================&gt;]  684.5kB/684.5kB
Loaded image: k8s.gcr.io/pause:3.6
50098fdfecae: Loading layer [==================================================&gt;]  131.3MB/131.3MB
Loaded image: k8s.gcr.io/kube-apiserver:v1.23.5

[ec2-user@amazonlinux ~]$ docker images 
REPOSITORY                           TAG       IMAGE ID       CREATED        SIZE
k8s.gcr.io/kube-apiserver            v1.23.5   3fc1d62d6587   15 hours ago   135MB
k8s.gcr.io/kube-proxy                v1.23.5   3c53fa8541f9   15 hours ago   112MB
k8s.gcr.io/kube-controller-manager   v1.23.5   b0c9e5e4dbb1   15 hours ago   125MB
k8s.gcr.io/kube-scheduler            v1.23.5   884d49d6d8c9   15 hours ago   53.5MB
k8s.gcr.io/etcd                      3.5.1-0   25f8c7f3da61   4 months ago   293MB
k8s.gcr.io/coredns/coredns           v1.8.6    a4ca41631cc7   5 months ago   46.8MB
k8s.gcr.io/pause                     3.6       6270bb605e12   6 months ago   683kB
[ec2-user@amazonlinux ~]$ 
</code></pre>

<p>中间出了个小插曲，一开始没有改主机名，导致加入节点的时刻用的是默认的，这样看起来不清晰，后面改了名称后就不认了。得重新弄一遍。</p>

<pre><code>## 加入节点
[ec2-user@amazonlinux ~]$ sudo su -
[root@amazonlinux ~]# kubeadm join 192.168.191.131:6443 --token sj6fff.bpak7gkd3hnyzcm5 \
         --discovery-token-ca-cert-hash sha256:8e15649afc0771e80cce7f1dfdbb0933f4fdbd45ea1f9e03be1f3b78449a6d3c 
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
        [WARNING Hostname]: hostname "amazonlinux.onprem" could not be reached
        [WARNING Hostname]: hostname "amazonlinux.onprem": lookup amazonlinux.onprem on 192.168.191.2:53: no such host
        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

[root@amazonlinux ~]# 

## 改下主机名称：
[root@amazonlinux ~]# hostnamectl --static set-hostname worker1
[root@amazonlinux ~]# hostname worker1
[root@amazonlinux ~]# exit

## 改了一下名，重启后不行了，重新加入
[ec2-user@worker1 ~]$ sudo su - 
Last login: Thu Mar 17 15:24:32 CST 2022 on pts/0

[root@worker1 ~]# kubeadm join 192.168.191.131:6443 --token sj6fff.bpak7gkd3hnyzcm5 \
          --discovery-token-ca-cert-hash sha256:8e15649afc0771e80cce7f1dfdbb0933f4fdbd45ea1f9e03be1f3b78449a6d3c 
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
        [WARNING Hostname]: hostname "worker1" could not be reached
        [WARNING Hostname]: hostname "worker1": lookup worker1 on 192.168.191.2:53: no such host
        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
        [ERROR Port-10250]: Port 10250 is in use
        [ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

## 直接重新加入不行，需要先重置再加入
[root@worker1 ~]# kubeadm reset 
[reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0317 17:42:03.050519    6887 removeetcdmember.go:80] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] No etcd config found. Assuming external etcd
[reset] Please, manually reset etcd to prevent further issues
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.

[root@worker1 ~]# kubeadm join 192.168.191.131:6443 --token sj6fff.bpak7gkd3hnyzcm5 \
         --discovery-token-ca-cert-hash sha256:8e15649afc0771e80cce7f1dfdbb0933f4fdbd45ea1f9e03be1f3b78449a6d3c 
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
        [WARNING Hostname]: hostname "worker1" could not be reached
        [WARNING Hostname]: hostname "worker1": lookup worker1 on 192.168.191.2:53: no such host
        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
</code></pre>

<p>加入节点后，查看状态：</p>

<pre><code>[ec2-user@k8s ~]$ kubectl get nodes 
NAME      STATUS     ROLES                  AGE    VERSION
k8s       Ready      control-plane,master   166m   v1.23.5
worker1   NotReady   &lt;none&gt;                 30s    v1.23.5
</code></pre>

<h2>安装网络</h2>

<ul>
<li><a href="https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model">https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model</a></li>
<li><a href="https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/">https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/</a></li>
<li><a href="https://github.com/flannel-io/flannel#deploying-flannel-manually">https://github.com/flannel-io/flannel#deploying-flannel-manually</a></li>
</ul>


<p>github上的资源好像也不能下载了，打开后复制内容到新建的文件中。</p>

<pre><code># For Kubernetes v1.17+ kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
[ec2-user@k8s ~]$ vi kube-flannel.yml 

[ec2-user@k8s ~]$ kubectl apply -f kube-flannel.yml
</code></pre>

<p>由于初始化 <code>kubeadm init</code> 时没有添加网络参数，导致这里flannel网络插件一直处于 CrashLoopBackOff 状态，查看日志提示没有分配 cidr 报错查日志</p>

<pre><code>## https://cloud-atlas.readthedocs.io/zh_CN/latest/kubernetes/debug/k8s_crashloopbackoff.html
# 查看日志
[ec2-user@k8s ~]$ kubectl describe -n kube-system pod kube-flannel-ds-sbx86 
  Warning  BackOff    2m5s (x69 over 16m)  kubelet            Back-off restarting failed container

[ec2-user@k8s ~]$ kubectl logs -n kube-system kube-flannel-ds-sbx86 
E0317 09:19:27.915383       1 main.go:317] Error registering network: failed to acquire lease: node "k8s" pod cidr not assigned
W0317 09:19:27.915664       1 reflector.go:436] github.com/flannel-io/flannel/subnet/kube/kube.go:379: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding

可以再通过docker查看flannel日志
[root@test4 profile]# docker ps -l
[root@test4 profile]# docker logs f7be3ebe77fd 

## https://www.talkwithtrend.com/Article/251751
# kube-controller-manager 没有给新加入的节点分配 IP 段，init 的时候没有指定 IP 段
# 加最后两行，和 kube-flannel.yml 中的 net-conf.json/Network 对应：
[ec2-user@k8s ~]$ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml 
  - command:
    - kube-controller-manager
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --use-service-account-credentials=true
    - --allocate-node-cidrs=true
    - --cluster-cidr=10.244.0.0/16

# 重启服务
## https://stackoverflow.com/questions/51375940/kubernetes-master-node-is-down-after-restarting-host-machine
[ec2-user@k8s ~]$ sudo systemctl restart kubelet  

# 重新部署
# 然后删除flannel容器，重新部署
[ec2-user@k8s ~]$ kubectl delete -f kube-flannel.yml 
[ec2-user@k8s ~]$ kubectl apply -f kube-flannel.yml
</code></pre>

<p>注：还有可以临时编辑节点的配置 手动分配podCIDR。这里不做具体描述，参考： <a href="http://www.hyhblog.cn/2021/02/21/k8s-flannel-pod-cidr-not-assigned/">http://www.hyhblog.cn/2021/02/21/k8s-flannel-pod-cidr-not-assigned/</a> 。</p>

<p>再次查看pod状态，网络组件安装好后，dns组件也跑起来了。</p>

<pre><code>[ec2-user@k8s ~]$ kubectl get pods --all-namespaces -o wide 
NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
kube-system   coredns-64897985d-4d5rx       1/1     Running   0          2m30s   10.244.0.3        k8s       &lt;none&gt;           &lt;none&gt;
kube-system   coredns-64897985d-m9p9q       1/1     Running   0          2m30s   10.244.0.2        k8s       &lt;none&gt;           &lt;none&gt;
kube-system   etcd-k8s                      1/1     Running   0          166m    192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-k8s            1/1     Running   0          166m    192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-k8s   1/1     Running   0          12m     192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kube-flannel-ds-q4qkt         1/1     Running   0          60s     192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
kube-system   kube-flannel-ds-ttcwt         1/1     Running   0          6m1s    192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-pd77m              1/1     Running   0          60s     192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-qj6lw              1/1     Running   0          166m    192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-k8s            1/1     Running   0          166m    192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>

<h2>安装dashboard</h2>

<pre><code>## https://github.com/kubernetes/dashboard#kubernetes-dashboard
# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml

[ec2-user@k8s ~]$ kubectl apply -f dashboard-v2.5.1.yml 

[ec2-user@k8s ~]$ kubectl get pods -A -o wide
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
kube-system            coredns-64897985d-4d5rx                      1/1     Running   0          36m     10.244.0.3        k8s       &lt;none&gt;           &lt;none&gt;
kube-system            coredns-64897985d-m9p9q                      1/1     Running   0          36m     10.244.0.2        k8s       &lt;none&gt;           &lt;none&gt;
kube-system            etcd-k8s                                     1/1     Running   0          3h20m   192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system            kube-apiserver-k8s                           1/1     Running   0          3h19m   192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system            kube-controller-manager-k8s                  1/1     Running   0          46m     192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system            kube-flannel-ds-q4qkt                        1/1     Running   0          34m     192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
kube-system            kube-flannel-ds-ttcwt                        1/1     Running   0          39m     192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system            kube-proxy-pd77m                             1/1     Running   0          34m     192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
kube-system            kube-proxy-qj6lw                             1/1     Running   0          3h20m   192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system            kube-scheduler-k8s                           1/1     Running   0          3h20m   192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kubernetes-dashboard   dashboard-metrics-scraper-799d786dbf-q87wv   1/1     Running   0          59s     10.244.2.3        worker1   &lt;none&gt;           &lt;none&gt;
kubernetes-dashboard   kubernetes-dashboard-fb8648fd9-vprpt         1/1     Running   0          59s     10.244.2.2        worker1   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>安装还是很便捷和容易的，访问搞起来比较麻烦，由于是在虚拟机里面部署，kubectl命令也都在虚拟机操作，用 <code>kubectl proxy</code> 访问dashboard，如果不是localhost或者https的话不给访问的。</p>

<p>尝试了绑定网卡ip，但是由于不是https，还是不能访问dashboard：</p>

<pre><code>[ec2-user@k8s ~]$ kubectl proxy --address='0.0.0.0' --accept-hosts='.*'
Starting to serve on [::]:8001
</code></pre>

<p>访问dashboard方法一：kubectl proxy 加上 ssh的locally port forward，把本地的8001的请求转发到 远程服务器的localhost:8001。</p>

<pre><code>## https://github.com/kubernetes/dashboard#access
[ec2-user@k8s ~]$ kubectl proxy 
Starting to serve on 127.0.0.1:8001
</code></pre>

<p>在SecureCRT的ssh会话的配置 Session Options 的 Connection - Port Forwarding 增加 Local Port Forwarding 的端口转发。在Local和Remote的Port输入框中都填入8001即可。</p>

<p>重新连接，这样我们访问 <code>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login</code> 就能访问到dashboard页面了。</p>

<p>方法二：后面直接通过查看dashboard服务的ip，通过 ssh的socks5代理 来访问 使用内部地址的dashboard。</p>

<pre><code>[ec2-user@k8s ~]$ kubectl -n kubernetes-dashboard get service kubernetes-dashboard
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes-dashboard   ClusterIP   10.101.193.109   &lt;none&gt;        443/TCP   6h38m

## 通过服务ip访问（Locally Port Forwarding - socks5方式代理）：
https://10.101.193.109/#/login
https://10.101.193.109/#/pod?namespace=kube-system
</code></pre>

<p>方法三：或者网上说的使用端口转发：</p>

<pre><code>## https://kubernetes.io/zh/docs/tasks/access-application-cluster/port-forward-access-application-cluster/
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 8080:443 --address='0.0.0.0'
</code></pre>

<p>能访问了，接下来就是获取token进行登录。同样有两种方式，第一种暴力设置跳过登录，第二种方式从系统中获取/创建一个token来登录。</p>

<p>登录方法一：</p>

<pre><code>## https://www.cnblogs.com/tylerzhou/p/11117956.html
# 在1.10.1里面默认不再显然skip按钮,其实dashboard安装有很多坑,如果有读者按照以上设置仍然不能正常成功登陆,但是仍然想要体验dashboard,可以开启默认关闭的skip按钮,这样就可以进入到dashboard管理界面了.
      containers:
      - args:
        - --auto-generate-certificates
        - --enable-skip-login            # &lt;-- add this line
        image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
</code></pre>

<p>改了配置后记得重新加载。</p>

<p>方法二：另一种方式是从系统获取token，然后填到界面上然后登录。访问dashboard：</p>

<ul>
<li><a href="https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard">https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard</a></li>
<li><a href="https://jimmysong.io/kubernetes-handbook/guide/auth-with-kubeconfig-or-token.html">https://jimmysong.io/kubernetes-handbook/guide/auth-with-kubeconfig-or-token.html</a></li>
</ul>


<pre><code>[ec2-user@k8s ~]$ kubectl -n kube-system get secret
# 这些secrets中的大部分都可以用来访问dashboard的,只有不同的账户权限不同,很多账户被限制不能进行操作.

[ec2-user@k8s ~]$ kubectl -n kube-system describe secret deployment-controller-token

# 使用一条命令来显示token
[ec2-user@k8s ~]$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | awk '/^deployment-controller-token-/{print $1}') | awk '$1=="token:"{print $2}'
eyJhbGciOiJSUzI1NiIsImtpZCI6IjQzNllWOFFBYU5qaXdtUmdLelJQSDU5T2FVbGVpREJFZTlMQU12MXFhN1UifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4tejVwbWQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNTcwNWJiMzYtMTMyNi00MGY5LWI3ZWUtNzE3ZTAyMTM1NzA2Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.Av-RwOQGdEyZn56xmH_siz-7yU07OrhLhfiPqfJRaNJ5DL8wEDIZkxgNMzHrrthTsOJl7Tky3ABo5z3c_4xjgADGSqKqP0rvWtaLSHZFZR16c5S2c08aHdSH7KIAdoCy0muMiKHRw67QRf7zo5bPUyqfCyPY2vcB-pxqYnrTTAw71f34rgIPA-LACc5LIQwv8DT5O-KE1TopYF7lX5hXZIHOGP3sYpmbR7yIzO3MDNRUIfiZutYiQnHwXRQGBwHu1iUVk8Lu69gnqggkjp2cXa4d2ZUpCxrpeLGGdjPv6JPZEFLDhLbiBLF04b7IOdFQO4bH6BbXBNs9e0AGPbvp4Q
[ec2-user@k8s ~]$ 
</code></pre>

<p>方法三：当然，也可以创建一个dashboard的拥有完整权限的token：</p>

<pre><code>$ kubectl create serviceaccount cluster-admin-dashboard-sa -n kube-system

$ kubectl create clusterrolebinding cluster-admin-dashboard-sa \
  --clusterrole=cluster-admin \
  --serviceaccount=kube-system:cluster-admin-dashboard-sa -n kube-system

And then, you can use the token of just created cluster admin service account.
$ kubectl get secret | grep cluster-admin-dashboard-sa
cluster-admin-dashboard-sa-token-6xm8l   kubernetes.io/service-account-token   3         18m
$ kubectl describe secret cluster-admin-dashboard-sa-token-6xm8l

# Parse the token
$ TOKEN=$(kubectl describe secret -n kube-system $(kubectl get secret -n kube-system | awk '/^cluster-admin-dashboard-sa-token-/{print $1}') | awk '$1=="token:"{print $2}')
$ echo $TOKEN

## -OR-
[ec2-user@k8s ~]$ kubectl describe secret cluster-admin-dashboard-sa
## -OR-
[ec2-user@k8s ~]$ kubectl describe secret -n kube-system | grep deployment -A 12
</code></pre>

<p>如果使用token登录，一段事件没有操作就会有超时的困扰，可以修改token-ttl配置。</p>

<pre><code>##--&gt; Unauthorized (401): You have been logged out because your token has expired.

## https://blog.csdn.net/otoyix/article/details/118758736
# 增加一行参数 token-ttl=68400
  containers:
    - name: kubernetes-dashboard
      image: 'kubernetesui/dashboard:v2.0.0-rc5'
      args:
        - '--auto-generate-certificates'
        - '--namespace=kubernetes-dashboard'
        - '--token-ttl=68400'    -- 增加了此行
</code></pre>

<h2>安装metrics-server</h2>

<p>如果没有安装metrics-server，在dashboard中不能看到cpu/内存使用情况图形，kubectl top的命令也获取不到数据。</p>

<pre><code>[ec2-user@k8s ~]$ kubectl top nodes
error: Metrics API not available
[ec2-user@k8s ~]$ kubectl top pods -A
error: Metrics API not available
</code></pre>

<p>安装metrics-server会有镜像下载和证书的问题：</p>

<pre><code># 每台主机都导入一下该镜像
[ec2-user@k8s ~]$ docker load -i metrics-server-v0.6.1.tar.gz 
3dc34f14eb83: Loading layer [==================================================&gt;]  66.43MB/66.43MB
Loaded image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1

# kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
[ec2-user@k8s ~]$ vi metrics-server.yml
[ec2-user@k8s ~]$ kubectl apply -f metrics-server.yml 
</code></pre>

<p>还是启动不起来，由于metrics-server需要连服务端，证书不对，为了先跑起来，先忽略安全证书。在containers参数最后加上 <code>--kubelet-insecure-tls</code> ，然后删除后重新创建一次。</p>

<pre><code>## [k8s metrics-server 轻量化监控](https://www.jianshu.com/p/5fe108d70310)
## https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#cannot-use-the-metrics-server-securely-in-a-kubeadm-cluster

## 证书 https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs

## https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md#how-to-run-metrics-server-securely
## https://github.com/kubernetes-sigs/metrics-server/issues/196
## https://cloud.tencent.com/developer/article/1819955
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
        image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1

[ec2-user@k8s ~]$ kubectl delete -f metrics-server.yml 
[ec2-user@k8s ~]$ kubectl apply -f metrics-server.yml 
</code></pre>

<p>等一小会，再次查看top命令。</p>

<pre><code>[ec2-user@k8s ~]$ kubectl top nodes 
NAME      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s       91m          4%     913Mi           23%       
worker1   38m          1%     431Mi           11%       
[ec2-user@k8s ~]$ 
[ec2-user@k8s ~]$ kubectl top pods -n kube-system
NAME                              CPU(cores)   MEMORY(bytes)   
coredns-64897985d-4d5rx           1m           12Mi            
coredns-64897985d-m9p9q           1m           12Mi            
etcd-k8s                          11m          60Mi            
kube-apiserver-k8s                32m          312Mi           
kube-controller-manager-k8s       13m          46Mi            
kube-flannel-ds-q4qkt             2m           11Mi            
kube-flannel-ds-ttcwt             2m           11Mi            
kube-proxy-pd77m                  7m           16Mi            
kube-proxy-qj6lw                  2m           16Mi            
kube-scheduler-k8s                3m           17Mi            
metrics-server-7cf8b65d65-trtcj   33m          11Mi            
[ec2-user@k8s ~]$ 
</code></pre>

<p>同时dashboard web界面就能看到cpu/内存的性能图形了。</p>

<h2>Hello world</h2>

<p>编写一个配置，然后运行一个实例，看看两台机器上的pod网络是否互通。</p>

<pre><code>## docker run -it public.ecr.aws/amazonlinux/amazonlinux /bin/bash

[ec2-user@k8s ~]$ cat replicaset.yml 
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: hello-world
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hello-world
  template:
    metadata:
      labels:
        app: hello-world
    spec:
      containers:
      - name: hello-world
        image: amazonlinux:2
        command: ["/bin/sh"]
        args: ["-c", "while true; do echo hello; sleep 10;done"]
[ec2-user@k8s ~]$ 

[ec2-user@k8s ~]$ kubectl apply -f replicaset.yml  
replicaset.apps/hello-world created

[ec2-user@k8s ~]$ kubectl get pods -o wide
NAME                READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
hello-world-d2tss   1/1     Running   0          8s    10.244.0.7    k8s       &lt;none&gt;           &lt;none&gt;
hello-world-h9jxq   1/1     Running   0          8s    10.244.2.12   worker1   &lt;none&gt;           &lt;none&gt;
[ec2-user@k8s ~]$ 

[ec2-user@k8s ~]$ kubectl exec -ti hello-world-d2tss bash 
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
bash-4.2# cat /etc/hosts 

bash-4.2# yum install -y iputils net-tools 

bash-4.2# ping hello-world-h9jxq           
ping: hello-world-h9jxq: Name or service not known
服务service才有域名。后面试一下服务的，来ping域名，测试下dns。


bash-4.2# ping 10.244.0.7 
PING 10.244.0.7 (10.244.0.7) 56(84) bytes of data.
64 bytes from 10.244.0.7: icmp_seq=1 ttl=255 time=0.012 ms
64 bytes from 10.244.0.7: icmp_seq=2 ttl=255 time=0.021 ms
^C
--- 10.244.0.7 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1007ms
rtt min/avg/max/mdev = 0.012/0.016/0.021/0.006 ms

bash-4.2# ping 10.244.2.12
PING 10.244.2.12 (10.244.2.12) 56(84) bytes of data.
64 bytes from 10.244.2.12: icmp_seq=1 ttl=253 time=0.508 ms
64 bytes from 10.244.2.12: icmp_seq=2 ttl=253 time=0.425 ms
^C
--- 10.244.2.12 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1027ms
rtt min/avg/max/mdev = 0.425/0.466/0.508/0.046 ms
</code></pre>

<h2>Service domain</h2>

<p>配置启动容器和服务：</p>

<pre><code>[ec2-user@k8s ~]$ cat pg-db.yml 
---
apiVersion: v1
kind: Pod
metadata:
  name: db-op-1 
  labels:
    name: postgres
spec:
  hostname: db-op-1
  containers:
  - name: postgres
    image: postgis/postgis:9.6-2.5
    imagePullPolicy: IfNotPresent
---
apiVersion: v1
kind: Service
metadata:
  name: db-op-1
spec:
  ports:
  - protocol: TCP
    port: 5432
  selector:
    name: postgres

[ec2-user@k8s ~]$ kubectl apply -f pg-db.yml 
</code></pre>

<p>在默认的namespace中再启动一个postgis的容器，用来测试访问域名：</p>

<pre><code>[ec2-user@k8s ~]$ kubectl run busybox --image=postgis/postgis:9.6-2.5 -ti --restart=Never --rm  --command -- sh                
If you don't see a command prompt, try pressing enter.

# apt update ; apt-get install net-tools iproute2 iputils-ping -y

# cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local localdomain
options ndots:5

# ping db-op-1
PING db-op-1.default.svc.cluster.local (10.107.190.149) 56(84) bytes of data.

# psql -h db-op-1 -U postgres
Password for user postgres: 
psql (9.6.24)
Type "help" for help.

postgres=# \q
</code></pre>

<h2>参考</h2>

<ul>
<li><a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
