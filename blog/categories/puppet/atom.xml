<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Puppet | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/puppet/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2016-11-10T13:29:11+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Puppet批量修改用户密码]]></title>
    <link href="http://winseliu.com/blog/2016/09/06/puppet-modify-password/"/>
    <updated>2016-09-06T11:00:57+08:00</updated>
    <id>http://winseliu.com/blog/2016/09/06/puppet-modify-password</id>
    <content type="html"><![CDATA[<ol>
<li>先在一台机器修改成想要修改的密码，然后获取该用户的信息。</li>
</ol>


<pre><code>[root@hadoop-master1 ~]# puppet resource user root
user { 'root':
  ensure           =&gt; 'present',
  comment          =&gt; 'root',
  gid              =&gt; '0',
  home             =&gt; '/root',
  password         =&gt; '$6$C6EXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',
  password_max_age =&gt; '99999',
  password_min_age =&gt; '0',
  shell            =&gt; '/bin/bash',
  uid              =&gt; '0',
}
</code></pre>

<ol>
<li><p>保存到文件 user.pp ，使用 <code>puppet apply user.pp</code> 测试看看文件是否有问题。毕竟是生产，出了问题就要进机房的啊，谨慎点好。</p></li>
<li><p>把用户的资源信息写入的site.pp(不知道是啥的话，去看看puppet的文档先)。先搞几台机器测试下 <code>puppet agent -t</code></p></li>
<li><p>然后使用 <code>mco puppet runall 10</code> 全部同步进行密码修改。或者 <code>mco shell run -- /opt/puppetlabs/bin/puppet agent -t</code></p></li>
</ol>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Puppet安装配置Ganglia]]></title>
    <link href="http://winseliu.com/blog/2016/06/17/ganglia-install-on-centos-with-puppet/"/>
    <updated>2016-06-17T09:30:50+08:00</updated>
    <id>http://winseliu.com/blog/2016/06/17/ganglia-install-on-centos-with-puppet</id>
    <content type="html"><![CDATA[<p>前面写过完全纯手工和用yum安装依赖来安装ganglia的文章，最近生产安装了puppet，既然已经手上已有牛刀，杀鸡就不用再取菜刀了。今天记录下前几天使用puppet安装ganglia的经历。</p>

<h2>前提（自己操作过熟悉怎么用）</h2>

<ul>
<li>配置过私有仓库 (createrepo)</li>
<li>安装好puppet</li>
<li>编译过自己的rpm (rpmbuild)</li>
</ul>


<h2>编译gmetad，gmond，gweb</h2>

<p>点击链接下载SPEC：</p>

<ul>
<li><a href="/files/ganglia-puppet/gmetad.spec">gmetad.spec</a></li>
<li><a href="/files/ganglia-puppet/gmond.spec">gmond.spec</a></li>
<li><a href="/files/ganglia-puppet/gweb.spec">gweb.spec</a></li>
</ul>


<p></p>

<p>然后编译打包：</p>

<p>先手动编译安装 ganglia ，把依赖的问题处理好。编译安装没问题，然后再使用 rpmbuild 编译生成 rpm 包！！</p>

<pre><code># 1&gt; 建立目录结构
mkdir ganglia-build
cd ganglia-build
mkdir BUILD RPMS SOURCES SPECS SRPMS

# 2&gt; 修改配置
# ganglia-web-3.7.1.tar.gz的makefile、conf_default.php.in修改下，根据等下要配置gmetad的参数进行修改

less ganglia-web-3.7.1/Makefile 
    # Location where gweb should be installed to (excluding conf, dwoo dirs).
    GDESTDIR = /var/www/html/ganglia

    # Location where default apache configuration should be installed to.
    GCONFDIR = /usr/local/ganglia/etc/

    # Gweb statedir (where conf dir and Dwoo templates dir are stored)
    GWEB_STATEDIR = /var/www/html/ganglia

    # Gmetad rootdir (parent location of rrd folder)
    GMETAD_ROOTDIR = /data/ganglia

    APACHE_USER = apache

# 连外网太慢，下载放到本地
less ganglia-web-3.7.1/conf_default.php.in 
    #$conf['cubism_js_path'] = "js/cubism.v1.min.js";
    $conf['jquery_js_path'] = "js/jquery.min.js";
    $conf['jquerymobile_js_path'] = "js/jquery.mobile.min.js";
    $conf['jqueryui_js_path'] = "js/jquery-ui.min.js";
    $conf['rickshaw_js_path'] = "js/rickshaw.min.js";
    $conf['cubism_js_path'] = "js/cubism.v1.min.js";
    $conf['d3_js_path'] = "js/d3.min.js";
    $conf['protovis_js_path'] = "js/protovis.min.js";

# 3&gt; 源文件
# 把文件放到SOURCES目录下，
ls SOURCES/
    ganglia-3.7.2.tar.gz  ganglia-web-3.7.1.tar.gz

# 4&gt; 编译生成RPM
rpmbuild -v -ba SPECS/gmetad.spec 
rpmbuild -v -ba SPECS/gmond.spec 
rpmbuild -v -ba SPECS/gweb.spec 

# 5&gt; 查看内容
rpm -qpl RPMS/x86_64/ganglia-3.7.2-1.el6.x86_64.rpm 
</code></pre>

<h2>本地仓库</h2>

<p>这里假设已经把系统光盘做成了本地仓库。</p>

<p><strong>先安装httpd、php、createrepo</strong>，然后按照下面的步骤创建本地仓库：</p>

<pre><code># 系统带的可以从光盘拷贝，直接映射到httpd的目录下即可
[hadoop@hadoop-master1 rhel6.3]$ ls 
Packages  repodata
[hadoop@hadoop-master1 html]$ pwd
/var/www/html
[hadoop@hadoop-master1 html]$ ll
lrwxrwxrwx.  1 root root   20 2月  15 2014 rhel6.3 -&gt; /opt/rhel6.3

[hadoop@hadoop-master1 ~]$ sudo mkdir -p /opt/dta/repo
[hadoop@hadoop-master1 ~]$ cd /opt/dta/repo
[hadoop@hadoop-master1 repo]$ ls *.rpm
gmetad-3.7.2-1.el6.x86_64.rpm  gmond-3.7.2-1.el6.x86_64.rpm  gweb-3.7.1-1.el6.x86_64.rpm  libconfuse-2.7-4.el6.x86_64.rpm

[hadoop@hadoop-master1 repo]$ sudo createrepo .
3/3 - libconfuse-2.7-4.el6.x86_64.rpm                                           
Saving Primary metadata
Saving file lists metadata
Saving other metadata

# 映射到httpd目录下
[hadoop@hadoop-master1 yum.repos.d]$ cd /var/www/html/
[hadoop@hadoop-master1 html]$ sudo ln -s /opt/dta/repo dta

# 加入本地仓库源
[hadoop@hadoop-master1 yum.repos.d]$ sudo cp puppet.repo dta.repo
[hadoop@hadoop-master1 yum.repos.d]$ sudo vi dta.repo 
[dta]
name=DTA Local
baseurl=http://hadoop-master1:801/dta
enabled=1
gpgcheck=0
</code></pre>

<p>注意： 在安装的时刻找不到gmond，可以先清理yum的缓冲： <code>yum clean all</code></p>

<h2>puppet模块</h2>

<p>添加了三个模块，用于主机添加repo配置和sudo配置，以及安装配置gmond。</p>

<pre><code>[root@hadoop-master1 modules]# tree $PWD
/etc/puppetlabs/code/environments/production/modules
├── dtarepo
│   ├── manifests
│   │   └── init.pp
│   └── templates
│       └── dta.repo
├── gmond
│   ├── manifests
│   │   └── init.pp
│   └── templates
│       └── gmond.conf
└── sudo
    ├── manifests
    │   └── init.pp
    └── templates
        └── sudo.erb
</code></pre>

<p>都比较简单，通过init.pp来进行配置，然后加载模板，写入到同步主机本地文件中。</p>

<ul>
<li>dtarepo</li>
</ul>


<pre><code>./dtarepo/manifests/init.pp
class dtarepo {

file{'/etc/yum.repos.d/dta.repo':
  ensure =&gt; file,
  content =&gt; template('dtarepo/dta.repo'),
}

}

./dtarepo/templates/dta.repo
[dta]
name=DTA Local
baseurl=http://hadoop-master1:801/dta
enabled=1
gpgcheck=0
</code></pre>

<ul>
<li>sudo</li>
</ul>


<pre><code>./sudo/manifests/init.pp
class sudo {

if ( $::hostname =~ /(^cu-omc)/ ) {
  $user = 'omc'
} elsif ( $::hostname =~ /(^cu-uc)/ ) {
  $user = 'uc'
} elsif ( $::hostname =~ /(^cu-ud)/ ) {
  $user = 'ud'
} elsif ( $::hostname =~ /(^cu-db)/ ) {
  $user = 'mysql'
} else {
  $user = 'hadoop'
}


file { "/etc/sudoers.d/10_$user":
  ensure =&gt; file,
  mode =&gt; '0440', 
  content =&gt; template('sudo/sudo.erb'),
}


}

./sudo/templates/sudo.erb
&lt;%= scope.lookupvar('sudo::user') %&gt; ALL=(ALL) NOPASSWD: ALL
</code></pre>

<ul>
<li>gmond</li>
</ul>


<p>在默认的gmond.conf基础上修改一下两个配置: globals.deaf, cluster.name</p>

<pre><code>./gmond/manifests/init.pp
class gmond {

$deaf = $::hostname ? {
  'hadoop-master1' =&gt; 'no',
  'cu-omc1' =&gt; 'no',
  default =&gt; 'yes',
}

if ( $::hostname =~ /(^cu-)/ ) {
  $cluster_name = 'CU'
} else {
  $cluster_name = 'HADOOP'
}

package { 'gmond':
  ensure =&gt; present,
  before =&gt; File['/usr/local/ganglia/etc/gmond.conf'],
}

file { '/usr/local/ganglia/etc/gmond.conf':
  ensure =&gt; file,
  content =&gt; template('gmond/gmond.conf'),
  notify =&gt; Service['gmond'],
}

service { 'gmond':
  ensure    =&gt; running,
  enable    =&gt; true,
}

}

./gmond/templates/gmond.conf
/* This configuration is as close to 2.5.x default behavior as possible
   The values closely match ./gmond/metric.h definitions in 2.5.x */
globals {
...
  mute = no
  deaf = &lt;%= scope.lookupvar('gmond::deaf') %&gt;
  allow_extra_data = yes
...
cluster {
  name = "&lt;%= scope.lookupvar('gmond::cluster_name') %&gt;"
</code></pre>

<p>参考下逻辑即可（也可以通过hiera配置）。</p>

<p>最后在 site.pp 引用加载编写的Module：</p>

<pre><code>[root@hadoop-master1 modules]# cd ../manifests/
[root@hadoop-master1 manifests]# cat site.pp 
file{'/etc/puppetlabs/mcollective/facts.yaml':
  owner    =&gt; root,
  group    =&gt; root,
  mode     =&gt; '400',
  loglevel =&gt; debug, # reduce noise in Puppet reports
  content  =&gt; inline_template("&lt;%= scope.to_hash.reject { |k,v| k.to_s =~ /(uptime_seconds|timestamp|free)/ }.to_yaml %&gt;"), # exclude rapidly changing facts
}


include dtarepo
include gmond

# include sudo
</code></pre>

<h2>一键安装</h2>

<p>安装gmetad：</p>

<p>首先在主机上安装gmetad，由于只需要在一台机器安装，配置没有整成模板，这里直接手动弄。</p>

<pre><code>[root@hadoop-master1 dtarepo]# mco rpc package install package=gmetad -I cu-omc1 （或者直接yum install -y gmetad）

# 注意：主机多网卡时可能需要添加route
[root@cu-omc1 ~]# route add -host 239.2.11.71 dev bond0

[root@cu-omc1 ~]# /etc/ganglia/gmetad.conf 注意!! 这里的rrd_rootdir配置与上面gweb/makefile是对应的！！
data_source "HADOOP" hadoop-master1
data_source "CU" cu-omc1
gridname "CQCU"
rrd_rootdir "/data/ganglia/rrds"
</code></pre>

<p>注意： data_source 需要配合 gmond/manifests/init.pp 中的 deaf 属性值。</p>

<p>安装gmond：</p>

<p>在cu-omc2上安装gmond（正则表达式，想怎么匹配就怎么写）：</p>

<pre><code>[root@hadoop-master1 production]# mco shell -I /^cu-omc2/ run -- "/opt/puppetlabs/bin/puppet agent -t"
</code></pre>

<p>puppet同步好后，就安装好puppet，以及启动gmond服务。</p>

<p>同时看看web是否已经有图像。没有话可以试着重启gmond:</p>

<pre><code>[root@hadoop-master1 production]# mco shell -I /cu-/ run -- service gmond restart
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puppetboard Install]]></title>
    <link href="http://winseliu.com/blog/2016/05/05/puppetboard-install/"/>
    <updated>2016-05-05T10:54:26+08:00</updated>
    <id>http://winseliu.com/blog/2016/05/05/puppetboard-install</id>
    <content type="html"><![CDATA[<p>对于我这样的python小白来说，有网络来安装 puppetboard 还是比较容易的（离线安装依赖处理可能比较麻烦）。</p>

<pre><code># https://fedoraproject.org/wiki/EPEL/zh-cn
[root@cu2 ~]# yum search epel
[root@cu2 ~]# yum install epel-release


[root@cu2 ~]# yum repolist
Loaded plugins: fastestmirror, priorities
Loading mirror speeds from cached hostfile
 * base: mirrors.skyshe.cn
 * centosplus: mirrors.pubyun.com
 * epel: mirror01.idc.hinet.net
 * extras: mirrors.skyshe.cn
 * updates: mirrors.skyshe.cn
193 packages excluded due to repository priority protections
repo id                                   repo name                                                                   status
base                                      CentOS-6 - Base                                                                  6,575
centosplus                                CentOS-6 - Centosplus                                                             0+76
epel                                      Extra Packages for Enterprise Linux 6 - x86_64                              12,127+117
extras                                    CentOS-6 - Extras                                                                   62
puppet-local                              Puppet Local                                                                         5
updates                                   CentOS-6 - Updates                                                               1,607
repolist: 20,376


[root@cu2 ~]# yum install python-pip -y


[root@cu2 ~]# pip install puppetboard
/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
You are using pip version 7.1.0, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Collecting puppetboard
/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
  Downloading puppetboard-0.1.3.tar.gz (598kB)
    100% |████████████████████████████████| 602kB 726kB/s 
Collecting Flask&gt;=0.10.1 (from puppetboard)
  Downloading Flask-0.10.1.tar.gz (544kB)
    100% |████████████████████████████████| 544kB 734kB/s 
Collecting Flask-WTF&lt;=0.9.5,&gt;=0.9.4 (from puppetboard)
  Downloading Flask-WTF-0.9.5.tar.gz (245kB)
    100% |████████████████████████████████| 249kB 320kB/s 
Collecting WTForms&lt;2.0 (from puppetboard)
  Downloading WTForms-1.0.5.zip (355kB)
    100% |████████████████████████████████| 356kB 1.3MB/s 
Collecting pypuppetdb&lt;0.3.0,&gt;=0.2.1 (from puppetboard)
  Downloading pypuppetdb-0.2.1.tar.gz
Collecting Werkzeug&gt;=0.7 (from Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading Werkzeug-0.11.9-py2.py3-none-any.whl (306kB)
    100% |████████████████████████████████| 307kB 1.5MB/s 
Collecting Jinja2&gt;=2.4 (from Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB)
    100% |████████████████████████████████| 266kB 2.3MB/s 
Collecting itsdangerous&gt;=0.21 (from Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading itsdangerous-0.24.tar.gz (46kB)
    100% |████████████████████████████████| 49kB 7.2MB/s 
Collecting requests&gt;=1.2.3 (from pypuppetdb&lt;0.3.0,&gt;=0.2.1-&gt;puppetboard)
  Downloading requests-2.10.0-py2.py3-none-any.whl (506kB)
    100% |████████████████████████████████| 507kB 920kB/s 
Collecting MarkupSafe (from Jinja2&gt;=2.4-&gt;Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading MarkupSafe-0.23.tar.gz
Installing collected packages: Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask, WTForms, Flask-WTF, requests, pypuppetdb, puppetboard
  Running setup.py install for MarkupSafe
  Running setup.py install for itsdangerous
  Running setup.py install for Flask
  Running setup.py install for WTForms
  Running setup.py install for Flask-WTF
  Running setup.py install for pypuppetdb
  Running setup.py install for puppetboard
Successfully installed Flask-0.10.1 Flask-WTF-0.9.5 Jinja2-2.8 MarkupSafe-0.23 WTForms-1.0.5 Werkzeug-0.11.9 itsdangerous-0.24 puppetboard-0.1.3 pypuppetdb-0.2.1 requests-2.10.0


[root@cu2 ~]# pip show puppetboard
You are using pip version 7.1.0, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
---
Metadata-Version: 1.0
Name: puppetboard
Version: 0.1.3
Summary: Web frontend for PuppetDB
Home-page: https://github.com/puppet-community/puppetboard
Author: Daniele Sluijters
Author-email: daniele.sluijters+pypi@gmail.com
License: Apache License 2.0
Location: /usr/lib/python2.6/site-packages
Requires: Flask, Flask-WTF, WTForms, pypuppetdb
[root@cu2 ~]# ll /usr/lib/python2.6/site-packages/puppetboard
total 100
-rw-r--r-- 1 root root 31629 May  5 09:12 app.py
-rw-r--r-- 1 root root 30481 May  5 09:12 app.pyc
-rw-r--r-- 1 root root  1206 May  5 09:12 default_settings.py
-rw-r--r-- 1 root root  1477 May  5 09:12 default_settings.pyc
-rw-r--r-- 1 root root  1025 May  5 09:12 forms.py
-rw-r--r-- 1 root root  1982 May  5 09:12 forms.pyc
-rw-r--r-- 1 root root     0 May  5 09:12 __init__.py
-rw-r--r-- 1 root root   143 May  5 09:12 __init__.pyc
drwxr-xr-x 9 root root  4096 May  5 09:12 static
drwxr-xr-x 2 root root  4096 May  5 09:12 templates
-rw-r--r-- 1 root root  2155 May  5 09:12 utils.py
-rw-r--r-- 1 root root  3433 May  5 09:12 utils.pyc


[root@cu2 ~]# pip install uwsgi
You are using pip version 7.1.0, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Collecting uwsgi
/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
  Downloading uwsgi-2.0.12.tar.gz (784kB)
    100% |████████████████████████████████| 786kB 143kB/s 
Installing collected packages: uwsgi
  Running setup.py install for uwsgi
Successfully installed uwsgi-2.0.12


[root@cu2 ~]# mkdir -p /var/www/puppetboard
[root@cu2 ~]# cd /var/www/puppetboard/
[root@cu2 puppetboard]# cp /usr/lib/python2.6/site-packages/puppetboard/default_settings.py ./settings.py
# 修改配置 
# https://github.com/voxpupuli/puppetboard#settings
PUPPETDB_HOST = 'cu3'
PUPPETDB_PORT = 8080
REPORTS_COUNT = 21
ENABLE_CATALOG = True

[root@cu2 puppetboard]# vi wsgi.py 
from __future__ import absolute_import
import os

os.environ['PUPPETDOARD_SETTINGS'] = '/var/www/puppetboard/settings.py'
from puppetboard.app import app as application


# A 直接用uwsgi-http
# http://yongqing.is-programmer.com/posts/43688.html
[root@cu2 puppetboard]# uwsgi --http :9091 --wsgi-file /var/www/puppetboard/wsgi.py 

# 使用 supervisord 管理
[root@cu2 supervisord.d]# cat uwsgi.ini 
[program:puppetboard]
command=uwsgi --http :9091 --wsgi-file /var/www/puppetboard/wsgi.py 
[root@cu2 supervisord.d]# supervisorctl update


# B nginx + uwsgi-socket
# 需要对应到 / ，新增一个9091的server
[root@cu2 puppetboard]# vi /home/hadoop/nginx/conf/nginx.conf
server {
  listen 9091;

  location /static {
    alias /usr/lib/python2.6/site-packages/puppetboard/static;
  }
  location / {
    include uwsgi_params;
    uwsgi_pass 127.0.0.1:9090;
  }
}

[root@cu2 puppetboard]# uwsgi --socket :9090 --wsgi-file /var/www/puppetboard/wsgi.py 

[root@cu2 puppetboard]# /home/hadoop/nginx/sbin/nginx -s reload
</code></pre>

<p><img src="/images/blogs/puppetboard-install.png" alt="" /></p>

<p>配置SSL访问需要把ssl_verify设置为false。</p>

<pre><code># 2.7.9+网上说好像就没问题
# http://stackoverflow.com/questions/29099404/ssl-insecureplatform-error-when-using-requests-package
# https://github.com/pypa/pip/issues/2681
[root@cu2 ~]# yum install -y  libffi-devel libffi 
[root@cu2 ~]# pip install 'requests[security]'

# [重要] 两个链接内容一样的：
# * https://groups.google.com/forum/#!msg/puppet-users/m7Sakf4bQ7Q/y6uAa0AUsZIJ
# * http://grokbase.com/t/gg/puppet-users/1428vjkncr/puppetboard-and-ssl
# You have two choices now, set SSL_VERIFY to False and trust that you're
# always talking to your actual PuppetDB or copy from the Puppet CA
# $vardir/ssl/ca_crt.pem to /etc/puppetboard and set SSL_VERIFY to the path
# of ca_crt.pem. In that case the file SSL_VERIFY points to will be used to
# verify PuppetDB's server certificate instead of the OS truststore.
[root@cu2 puppetboard]# vi settings.py 
PUPPETDB_HOST = 'cu3.eshore.cn'
PUPPETDB_PORT = 8081
PUPPETDB_SSL_VERIFY = False  # 这里设置为false
PUPPETDB_KEY = '/etc/puppetlabs/puppet/ssl/private_keys/cu2.eshore.cn.pem'
PUPPETDB_CERT = '/etc/puppetlabs/puppet/ssl/ca/signed/cu2.eshore.cn.pem'

# 重启uwsgi-http服务
[root@cu2 ~]# supervisorctl restart puppetboard
</code></pre>

<p>如果 puppetboard 和 puppetdb 安装在同一机器，可以使用 puppetdb/ssl 路径下的ssl文件（puppetdb/ssl也是从puppet/ssl拷贝过来的）：</p>

<pre><code>[root@cu3 ~]# puppetdb ssl-setup -f
PEM files in /etc/puppetlabs/puppetdb/ssl are missing, we will move them into place for you
Copying files: /etc/puppetlabs/puppet/ssl/certs/ca.pem, /etc/puppetlabs/puppet/ssl/private_keys/cu3.eshore.cn.pem and /etc/puppetlabs/puppet/ssl/certs/cu3.eshore.cn.pem to /etc/puppetlabs/puppetdb/ssl
...

[root@cu3 ~]# tree /etc/puppetlabs/puppetdb/ssl/
/etc/puppetlabs/puppetdb/ssl/
├── ca.pem
├── private.pem
└── public.pem
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MCollective Plugins]]></title>
    <link href="http://winseliu.com/blog/2016/04/28/mcollective-plugins/"/>
    <updated>2016-04-28T21:37:51+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/28/mcollective-plugins</id>
    <content type="html"><![CDATA[<p>上一篇介绍了mcollective的安装。乘着这股热情把 mco 命令行和插件的安装弄通，记录下来。</p>

<h2>基本命令使用</h2>

<ul>
<li><a href="https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html">https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html</a></li>
</ul>


<pre><code>[root@hadoop-master2 ~]# mco help
The Marionette Collective version 2.8.8

  completion      Helper for shell completion systems
  describe_filter Display human readable interpretation of filters
  facts           Reports on usage for a specific fact
  find            Find hosts using the discovery system matching filter criteria
  help            Application list and help
  inventory       General reporting tool for nodes, collectives and subcollectives
  ping            Ping all nodes
  plugin          MCollective Plugin Application
  rpc             Generic RPC agent client application
</code></pre>

<p>自带的插件只能用来查看环境情况(下面列出来的命令<a href="/blog/2016/04/28/mcollective-quick-start/#cli-simple-usage">上一篇:MCollective安装配置</a>都已记录过)。</p>

<pre><code>mco ping
mco inventory [server_host]
mco facts [fact]
</code></pre>

<p>mcollective 的 filter（适配节点）功能很强大，具体查看文档：<a href="https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html#selecting-request-targets-using-filters">Selecting Request Targets Using Filters</a></p>

<p><strong> 使用filter功能需要结合facts，需要先把节点的信息写入到mcollective/facts.yaml文件 </strong></p>

<pre><code>[hadoop@hadoop-master1 ~]$ sudo mco ping -I /^hadoop/
[hadoop@hadoop-master1 ~]$ sudo mco puppet runall 8 -I /^hadoop/
[hadoop@hadoop-master1 ~]$ sudo mco service iptables status -I "/cu-ud.*/"

[root@hadoop-master1 manifests]# mco ping -S "hostname=hadoop-master2"
[hadoop@hadoop-master1 ~]$ sudo mco ping -S 'hostname=/hadoop.*/'

[hadoop@hadoop-master1 ~]$ sudo mco facts hostname
</code></pre>

<h2>插件安装</h2>

<ul>
<li><a href="https://docs.puppet.com/mcollective/deploy/standard.html#install-agent-plugins">https://docs.puppet.com/mcollective/deploy/standard.html#install-agent-plugins</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/plugins.html">Installing Plugins</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/plugins.html#example">https://docs.puppet.com/mcollective/deploy/plugins.html#example</a></li>
</ul>


<p>文档中描述了 <code>Use packages</code> 和 <code>Put files directly into the libdir</code> 两种安装插件的方式。但是 Packages 都是放在 <a href="http://yum.puppetlabs.com/el/6/products/x86_64/">旧的repo</a> 里面，我们这里使用第二种方式把github下载源码放到libdir来安装。</p>

<h4>安装mcollective-puppet-agent</h4>

<pre><code># 使用文档 https://github.com/puppetlabs/mcollective-puppet-agent#readme
# 直接下载release版本 
[root@hadoop-master2 ~]# cd /usr/libexec/mcollective/
[root@hadoop-master2 mcollective]# ll
total 44
-rw-r--r-- 1 root root 44759 Apr 29 11:53 mcollective-puppet-agent-1.10.0.tar.gz
[root@hadoop-master2 mcollective]# tar zxf mcollective-puppet-agent-1.10.0.tar.gz  
[root@hadoop-master2 mcollective]# ll mcollective-puppet-agent-1.10.0
total 60
drwxrwxr-x 2 root root  4096 Apr 13  2015 agent
drwxrwxr-x 2 root root  4096 Apr 13  2015 aggregate
drwxrwxr-x 2 root root  4096 Apr 13  2015 application
-rw-rw-r-- 1 root root  3456 Apr 13  2015 CHANGELOG.md
drwxrwxr-x 2 root root  4096 Apr 13  2015 data
drwxrwxr-x 4 root root  4096 Apr 13  2015 ext
-rw-rw-r-- 1 root root   349 Apr 13  2015 Gemfile
-rw-rw-r-- 1 root root  3036 Apr 13  2015 Rakefile
-rw-rw-r-- 1 root root 14739 Apr 13  2015 README.md
drwxrwxr-x 9 root root  4096 Apr 13  2015 spec
drwxrwxr-x 3 root root  4096 Apr 13  2015 util
drwxrwxr-x 2 root root  4096 Apr 13  2015 validator
# 官网提供example有区分服务端和客户端文件。反正多了没问题，直接全部放就行咯。。。
[root@hadoop-master2 mcollective]# mv mcollective-puppet-agent-1.10.0 mcollective

# 验证
# 多了puppet的命令！
[root@hadoop-master2 mcollective]# mco help
The Marionette Collective version 2.8.8

  completion      Helper for shell completion systems
  describe_filter Display human readable interpretation of filters
  facts           Reports on usage for a specific fact
  find            Find hosts using the discovery system matching filter criteria
  help            Application list and help
  inventory       General reporting tool for nodes, collectives and subcollectives
  ping            Ping all nodes
  plugin          MCollective Plugin Application
  puppet          Schedule runs, enable, disable and interrogate the Puppet Agent
  rpc             Generic RPC agent client application


# 同步到mcollective-servers （172.17.0.2对应hadoop-slaver1）
[root@hadoop-master2 mcollective]# rsync -az /usr/libexec/mcollective 172.17.0.2:/usr/libexec/

# mcollective-server添加插件后，重启mcollective服务
# 也可以使用 reload-agents 来重新加载agents： service mcollective reload-agents
[root@hadoop-slaver1 libexec]# service mcollective restart
Shutting down mcollective:                                 [  OK  ]
Starting mcollective:                                      [  OK  ]


# 验证server，已经可以看到新添加的puppet命令了
[root@hadoop-master2 mcollective]# mco inventory hadoop-slaver1
Inventory for hadoop-slaver1:

   Server Statistics:
                      Version: 2.8.8
                   Start Time: 2016-04-29 12:01:40 +0800
                  Config File: /etc/puppetlabs/mcollective/server.cfg
                  Collectives: mcollective
              Main Collective: mcollective
                   Process ID: 123
               Total Messages: 1
      Messages Passed Filters: 1
            Messages Filtered: 0
             Expired Messages: 0
                 Replies Sent: 0
         Total Processor Time: 0.67 seconds
                  System Time: 0.8 seconds

   Agents:
      discovery       puppet          rpcutil        

   Data Plugins:
      agent           collective      fact           
      fstat           puppet          resource       

[root@hadoop-master2 mcollective]# mco help puppet

[root@hadoop-master2 mcollective]# mco puppet status    

 * [ ============================================================&gt; ] 3 / 3

   hadoop-slaver1: Currently stopped; last completed run 10 hours 57 minutes 20 seconds ago
   hadoop-master1: Currently stopped; last completed run 11 hours 1 minutes 05 seconds ago
   hadoop-slaver2: Currently stopped; last completed run 10 hours 57 minutes 16 seconds ago
...


# 配置server.conf
# 注意：真正要执行puppet命令，为了适配puppet4需要添加/修改配置
-bash-4.1# cat /etc/puppetlabs/mcollective/server.cfg 
...
plugin.puppet.command = /opt/puppetlabs/bin/puppet agent
plugin.puppet.config = /etc/puppetlabs/puppet/puppet.conf

# 重启所有mcollective（重新加载agent也可以不重启，使用 mco shell run service mcollective reload-agents 来重新加载）

[root@hadoop-master2 mcollective]# mco puppet runall 1
2016-04-29 16:52:46: Running all nodes with a concurrency of 1
2016-04-29 16:52:46: Discovering enabled Puppet nodes to manage
2016-04-29 16:52:49: Found 3 enabled nodes
2016-04-29 16:52:50: hadoop-slaver1 schedule status: Started a Puppet run using the '/opt/puppetlabs/bin/puppet agent --onetime --no-daemonize --color=false --show_diff --verbose --no-splay' command
2016-04-29 16:52:55: hadoop-slaver2 schedule status: Started a Puppet run using the '/opt/puppetlabs/bin/puppet agent --onetime --no-daemonize --color=false --show_diff --verbose --no-splay' command
2016-04-29 16:52:59: hadoop-master1 schedule status: Started a Puppet run using the '/opt/puppetlabs/bin/puppet agent --onetime --no-daemonize --color=false --show_diff --verbose --no-splay' command
2016-04-29 16:52:59: Iteration complete. Initiated a Puppet run on 3 nodes.

[root@hadoop-master2 puppetlabs]# mco puppet status

 * [ ============================================================&gt; ] 3 / 3

   hadoop-master1: Currently stopped; last completed run 10 seconds ago
   hadoop-slaver1: Currently stopped; last completed run 15 seconds ago
   hadoop-slaver2: Currently stopped; last completed run 04 seconds ago
...
# 或者通过 puppetexplorer 查看节点最后的更新时间
</code></pre>

<h4>安装 package / service 插件</h4>

<p>为了更好的管理，再添加 package 和 service 两个插件</p>

<ul>
<li><a href="https://github.com/puppetlabs/mcollective-package-agent#readme">https://github.com/puppetlabs/mcollective-package-agent#readme</a></li>
<li><a href="https://github.com/puppetlabs/mcollective-service-agent#readme">https://github.com/puppetlabs/mcollective-service-agent#readme</a></li>
</ul>


<pre><code># http://stackoverflow.com/questions/8488253/how-to-force-cp-to-overwrite-without-confirmation
[root@hadoop-master2 mcollective]# unalias cp
[root@hadoop-master2 mcollective]# cp -rf mcollective-service-agent-3.1.3/* mcollective/   
[root@hadoop-master2 mcollective]# cp -rf mcollective-package-agent-4.4.0/* mcollective/

[root@hadoop-master2 mcollective]# rsync -az /usr/libexec/mcollective 172.17.0.2:/usr/libexec/

# 重启mcollective服务（或者 mco shell run service mcollective reload-agents 重新加载）

# updated 2016-5-11 17:15:08
# 还是重启比较好，reload-agents Data Plugins 没有重新加载
[root@hadoop-master1 puppet]# mco inventory hadoop-master2
Inventory for hadoop-master2:

   Server Statistics:
                      Version: 2.8.8
                   Start Time: 2016-05-11 17:12:45 +0800
                  Config File: /etc/puppetlabs/mcollective/server.cfg
                  Collectives: mcollective
              Main Collective: mcollective
                   Process ID: 39878
               Total Messages: 1
      Messages Passed Filters: 1
            Messages Filtered: 0
             Expired Messages: 0
                 Replies Sent: 0
         Total Processor Time: 1.17 seconds
                  System Time: 0.1 seconds

   Agents:
      discovery       package         puppet         
      rpcutil         service         shell          

   Data Plugins:
      agent           collective      fact           
      fstat           puppet          resource       
      service                                        

   Configuration Management Classes:
      No classes applied

   Facts:
      mcollective =&gt; 1
[root@hadoop-master1 puppet]# mco inventory hadoop-slaver2
Inventory for hadoop-slaver2:

   Server Statistics:
                      Version: 2.8.8
                   Start Time: 2016-05-11 16:56:09 +0800
                  Config File: /etc/puppetlabs/mcollective/server.cfg
                  Collectives: mcollective
              Main Collective: mcollective
                   Process ID: 14062
               Total Messages: 9
      Messages Passed Filters: 7
            Messages Filtered: 2
             Expired Messages: 0
                 Replies Sent: 6
         Total Processor Time: 1.31 seconds
                  System Time: 0.23 seconds

   Agents:
      discovery       package         puppet         
      rpcutil         service         shell          

   Data Plugins:
      agent           collective      fact           
      fstat                                          

   Configuration Management Classes:
      No classes applied

   Facts:
      mcollective =&gt; 1
</code></pre>

<p>验证下package的实力：</p>

<pre><code>[root@hadoop-master2 mcollective]# mco package lrzsz status

 * [ ============================================================&gt; ] 3 / 3

   hadoop-slaver1: lrzsz-0.12.20-27.1.el6.x86_64
   hadoop-master1: -purged.
   hadoop-slaver2: -purged.

Summary of Arch:

   x86_64 = 1

Summary of Ensure:

             purged = 2
   0.12.20-27.1.el6 = 1


Finished processing 3 / 3 hosts in 1488.41 ms

[root@hadoop-master2 mcollective]# mco rpc package install package=lrzsz
Discovering hosts using the mc method for 2 second(s) .... 3

 * [ ============================================================&gt; ] 3 / 3


hadoop-slaver1                           Unknown Request Status
   Package is already installed


Summary of Ensure:

   0.12.20-27.1.el6 = 3


Finished processing 3 / 3 hosts in 14525.03 ms
[root@hadoop-master2 mcollective]# mco package lrzsz status

 * [ ============================================================&gt; ] 3 / 3

   hadoop-master1: lrzsz-0.12.20-27.1.el6.x86_64
   hadoop-slaver2: lrzsz-0.12.20-27.1.el6.x86_64
   hadoop-slaver1: lrzsz-0.12.20-27.1.el6.x86_64

Summary of Arch:

   x86_64 = 3

Summary of Ensure:

   0.12.20-27.1.el6 = 3


Finished processing 3 / 3 hosts in 572.13 ms
</code></pre>

<p>还有很多的插件：</p>

<ul>
<li><a href="https://docs.puppet.com/mcollective/plugin_directory/index.html">https://docs.puppet.com/mcollective/plugin_directory/index.html</a></li>
<li>shell插件也不错，安装的时刻注意一下目录结构！<a href="https://github.com/puppetlabs/mcollective-shell-agent">https://github.com/puppetlabs/mcollective-shell-agent</a></li>
</ul>


<p>添加了 service，package，shell，puppet 插件后，用 mco 来执行管理集群太爽了！！</p>

<h4>后期统一安装</h4>

<pre><code>[root@hadoop-master1 mcollective]# ll
total 100
-rw-r--r-- 1 root root 17101 Apr 29 12:15 mcollective-package-agent-4.4.0.tar.gz
-rw-r--r-- 1 root root 44759 Apr 29 11:53 mcollective-puppet-agent-1.10.0.tar.gz
-rw-r--r-- 1 root root 12483 Apr 29 12:15 mcollective-service-agent-3.1.3.tar.gz
-rw-r--r-- 1 root root 17984 Apr 29 19:24 mcollective-shell-agent-0.0.2.tar.gz
[root@hadoop-master1 mcollective]# ls | xargs -I{} tar zxf {}

# TODO 可以考虑打包成rpm
[root@hadoop-master1 mcollective]# mkdir mcollective
[root@hadoop-master1 mcollective]# unalias cp
[root@hadoop-master1 mcollective]# cp -rf mcollective-package-agent-4.4.0/* mcollective/
[root@hadoop-master1 mcollective]# cp -rf mcollective-puppet-agent-1.10.0/* mcollective/
[root@hadoop-master1 mcollective]# cp -rf mcollective-service-agent-3.1.3/* mcollective/
[root@hadoop-master1 mcollective]# cp -rf mcollective-shell-agent-0.0.2/lib/mcollective/* mcollective/
[root@hadoop-master1 mcollective]# rm -rf mcollective-*

# 验证
[root@hadoop-master1 mcollective]# mco help
The Marionette Collective version 2.8.8

  completion      Helper for shell completion systems
  describe_filter Display human readable interpretation of filters
  facts           Reports on usage for a specific fact
  find            Find hosts using the discovery system matching filter criteria
  help            Application list and help
  inventory       General reporting tool for nodes, collectives and subcollectives
  package         Install, uninstall, update, purge and perform other actions to packages
  ping            Ping all nodes
  plugin          MCollective Plugin Application
  puppet          Schedule runs, enable, disable and interrogate the Puppet Agent
  rpc             Generic RPC agent client application
  service         Manages system services
  shell           Run shell commands

# 同步
[root@hadoop-master1 mcollective]# cd ..
[root@hadoop-master1 libexec]# rsync -az mcollective hadoop-master2:/usr/libexec/

# filter
[root@hadoop-master1 manifests]# mco shell run hostname -S "hostname=hadoop-master2"
[root@hadoop-master1 manifests]# mco ping -S "not hostname=hadoop-master1"
[root@hadoop-master1 manifests]# mco ping -S "! hostname=hadoop-master1"

[hadoop@hadoop-master1 ~]$ sudo mco shell --sort -I /cu-ud[1234]{1}$/ run -- ' ls /home/ud/ftpxdr | wc -l  '

# 少配置了
[root@hadoop-master1 manifests]# mco shell run "echo -e '\n\nplugin.puppet.command = /opt/puppetlabs/bin/puppet agent\nplugin.puppet.config = /etc/puppetlabs/puppet/puppet.conf' &gt;&gt; /etc/puppetlabs/mcollective/server.cfg" 


# 重启 mcollective 服务
[root@hadoop-master1 manifests]# mco shell run "echo service mcollective restart &gt;/tmp/mcollective_restart.sh ; nohup sh /tmp/mcollective_restart.sh "


# ---
[root@hadoop-master1 dtarepo]# mco rpc package install package=lrzsz -I cu-omc1
[root@hadoop-master1 dtarepo]# mco rpc package install package=gmetad -I cu-omc1

[root@hadoop-master1 gmond]# mco shell -I cu-ud2 run -- "/opt/puppetlabs/bin/puppet agent -t"
[root@hadoop-master1 production]# mco shell -I /^cu-omc2/ run -- "/opt/puppetlabs/bin/puppet agent -t"

# gmond 多网卡情况确认
[root@hadoop-master1 production]# route add -host 239.2.11.71 dev bond0

# puppet 基本语法
# https://docs.puppet.com/puppet/latest/reference/lang_conditional.html#if-statements
# https://docs.puppet.com/puppet/latest/reference/lang_relationships.html

# puppet使用tag可以更灵活的使用
# https://docs.puppet.com/puppet/latest/reference/lang_tags.html
apache::vhost {'docs.puppetlabs.com':
  port =&gt; 80,
  tag  =&gt; ['us_mirror1', 'us_mirror2'],
}

$ sudo puppet agent --test --tags apache,us_mirror1
</code></pre>

<p>再次强调Filter ： <a href="https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html#selecting-request-targets-using-filters">Selecting Request Targets Using Filters</a></p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MCollective安装配置]]></title>
    <link href="http://winseliu.com/blog/2016/04/28/mcollective-quick-start/"/>
    <updated>2016-04-28T08:39:23+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/28/mcollective-quick-start</id>
    <content type="html"><![CDATA[<p>puppet agent 通过定时拉取的方式来更新本地系统，但无法满足实时更新的需求。 mcollective 通过 <strong>消息中间件</strong> 的方式，mclient/mservers通过消息的推送/订阅，实现mservers实时执行mclient提交的请求。（添加 m 说明是mcollective的组件！）</p>

<p>最新版的安装除了官网，没有其他可以直接学习的资料（只能参考）。先看官网的资料：</p>

<ul>
<li>组件功能(必须看看) <a href="https://docs.puppet.com/mcollective/overview_components.html">https://docs.puppet.com/mcollective/overview_components.html</a></li>
<li>部署 <a href="https://docs.puppet.com/mcollective/deploy/install.html">https://docs.puppet.com/mcollective/deploy/install.html</a></li>
<li>部署规范/准则 <a href="https://docs.puppet.com/mcollective/deploy/standard.html">https://docs.puppet.com/mcollective/deploy/standard.html</a></li>
</ul>


<p>摘录官网安装描述：[Installing MCollective requires the following steps]</p>

<ul>
<li>Make sure your middleware is up and running and your firewalls are in order.</li>
<li>Install the mcollective package on servers, then make sure the mcollective service is running.</li>
<li>Install the mcollective-client package on admin workstations.</li>
<li>Most Debian-like and Red Hat-like systems can use the official Puppet Labs packages. Enable the Puppet Labs repos, or import the packages into your own repos.

<ul>
<li>If you’re on Debian/Ubuntu, mind the missing package dependency.</li>
</ul>
</li>
<li>If your systems can’t use the official packages, check the system requirements and either build your own or run from source.</li>
</ul>


<p>mcollective对于puppet来说是一个锦上添花的组件，没有puppet一样正常运转。部署主要由两个部分组成：</p>

<ul>
<li>部署消息中间件</li>
<li>配置mcollective(puppet4.4 agent已经安装该功能，redhat也自带装了Stomp包：<code>/opt/puppetlabs/puppet/lib/ruby/gems/2.1.0/gems/</code> 目录下面)

<ul>
<li>配置mclient/mserver</li>
<li>配置Stomp with TLS</li>
<li>配置security</li>
</ul>
</li>
</ul>


<p>本文先简单实现连接远程主机，然后配置安全功能，最后用puppet来重新实现 mcollective 的安装和配置。</p>

<h1>环境说明</h1>

<ul>
<li>hadoop-master2:

<ul>
<li>172.17.42.1</li>
<li>puppetserver, activemq-server, mcollective-client</li>
</ul>
</li>
<li>hadoop-master1/hadoop-slaver1/hadoop-slaver2:

<ul>
<li>172.17.0.2/&frac34;</li>
<li>puppet-agent, mcollective-server</li>
</ul>
</li>
</ul>


<h1>ActiveMQ部署</h1>

<p>activemq的服务端是一个spring-jetty项目，直接解压运行启动脚本即可。</p>

<pre><code># http://activemq.apache.org/download-archives.html
# 直接下载最新的 tar.gz

# 解压，启动
On Unix:
From a command shell, change to the installation directory and run ActiveMQ as a foregroud process:
cd [activemq_install_dir]/bin
./activemq console
From a command shell, change to the installation directory and run ActiveMQ as a daemon process:
cd [activemq_install_dir]/bin
./activemq start

# 确认
URL: http://127.0.0.1:8161/admin/
Login: admin
Passwort: admin
# 起了好多端口，随便试一个
netstat -nl|grep 61616
netstat -anp|grep PID

# 数据/日志目录
[root@hadoop-master2 apache-activemq-5.13.2]# ll data/
total 16
-rw-r--r-- 1 root users 4276 Apr 27 21:36 activemq.log
-rw-r--r-- 1 root root     5 Apr 27 21:36 activemq.pid
-rw-r--r-- 1 root root     0 Apr 27 21:36 audit.log
drwxr-xr-x 2 root root  4096 Apr 27 21:36 kahadb
</code></pre>

<p><img src="/images/blogs/mcollective-activemq.png" alt="" /></p>

<p>查看连接密码：</p>

<pre><code>[root@hadoop-master2 conf]# cat credentials.properties
...
activemq.username=system
activemq.password=manager
guest.password=password[root@hadoop-master2 conf]# 
</code></pre>

<h1>简单配置(unencrypted Stomp) <a name="cli-simple-usage"></a></h1>

<p>安装puppet4.4后，mcollective已经安装好了！直接修改配置连接到activemq即可。</p>

<pre><code>[root@hadoop-master2 puppetlabs]# chkconfig --list | grep mco
mcollective     0:off   1:off   2:off   3:off   4:off   5:off   6:off

# puppetserver作为mcollective-client
[root@hadoop-master2 mcollective]# cat client.cfg                    
...
connector = activemq
plugin.activemq.pool.size = 1
plugin.activemq.pool.1.host = hadoop-master2.example.com
plugin.activemq.pool.1.port = 61613
plugin.activemq.pool.1.user = system
plugin.activemq.pool.1.password = manager
...

[root@hadoop-master2 mcollective]# mco ping


---- ping statistics ----
No responses received

# puppet agent作为mcollective-server
-bash-4.1# cat server.cfg 
...
connector = activemq
plugin.activemq.pool.size = 1
plugin.activemq.pool.1.host = hadoop-master2.example.com
plugin.activemq.pool.1.port = 61613
plugin.activemq.pool.1.user = system
plugin.activemq.pool.1.password = manager
...

-bash-4.1# service mcollective start
Starting mcollective:                                      [  OK  ]
-bash-4.1# service mcollective status
mcollectived (pid  202) is running...

# 其他两台agent机器一样的配置操作

# 1. mcollective-client(puppetserver) 测试
[root@hadoop-master2 ~]# mco find
hadoop-master1
hadoop-slaver2
hadoop-slaver1
[root@hadoop-master2 mcollective]# mco ping
hadoop-master1                           time=148.29 ms
hadoop-slaver2                           time=187.99 ms
hadoop-slaver1                           time=190.21 ms


---- ping statistics ----
3 replies max: 190.21 min: 148.29 avg: 175.50 

# 2. 先查看/扫描节点状态。（如果配置了facts后，会输出一长串的Facts！）
[root@hadoop-master2 ssl]# mco inventory hadoop-master1
Inventory for hadoop-master1:

   Server Statistics:
                      Version: 2.8.8
                   Start Time: 2016-04-29 00:21:31 +0800
                  Config File: /etc/puppetlabs/mcollective/server.cfg
                  Collectives: mcollective
              Main Collective: mcollective
                   Process ID: 155
               Total Messages: 13
      Messages Passed Filters: 3
            Messages Filtered: 0
             Expired Messages: 0
                 Replies Sent: 2
         Total Processor Time: 2.32 seconds
                  System Time: 0.3 seconds

   Agents:
      discovery       rpcutil                        

   Data Plugins:
      agent           collective      fact           
      fstat                                          

   Configuration Management Classes:
      No classes applied

   Facts:
      mcollective =&gt; 1

# 3. 获取节点facts，需要配合puppet一起来使用
# puppetserver节点 配置更新agent facts.yaml信息
[root@hadoop-master2 manifests]# cat site.pp 
file{'/etc/puppetlabs/mcollective/facts.yaml':
  owner    =&gt; root,
  group    =&gt; root,
  mode     =&gt; '400',
  loglevel =&gt; debug, # reduce noise in Puppet reports
  content  =&gt; inline_template("&lt;%= scope.to_hash.reject { |k,v| k.to_s =~ /(uptime_seconds|timestamp|free)/ }.to_yaml %&gt;"), # exclude rapidly changing facts
}
# 读取facts
[root@hadoop-master2 manifests]# mco facts hostname
Report for fact: hostname

        hadoop-master1                           found 1 times
        hadoop-slaver1                           found 1 times
        hadoop-slaver2                           found 1 times

Finished processing 3 / 3 hosts in 579.93 ms
</code></pre>

<p>自带的插件功能比较少，要真正把 mcollective 用起来需要安装插件：puppet, service, package等等。这篇主要记录安装过程，<a href="/blog/2016/04/28/mcollective-plugins/">插件安装以及使用</a>后面具体实践了再写。</p>

<p>我觉得内网生产环境安装，到这一步已经差不多了！下面的安全配置就当深入学习吧。</p>

<h1>Stomp with TLS 配置</h1>

<ul>
<li><a href="https://docs.puppet.com/mcollective/reference/integration/activemq_ssl.html">https://docs.puppet.com/mcollective/reference/integration/activemq_ssl.html</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/middleware/activemq_keystores.html">https://docs.puppet.com/mcollective/deploy/middleware/activemq_keystores.html</a></li>
</ul>


<p><strong>Anonymous TLS</strong> 步骤简单一点，这里就不列出来了，自己去看官网的文档: <a href="https://docs.puppet.com/mcollective/reference/integration/activemq_ssl.html#anonymous-tls">Anonymous TLS</a></p>

<pre><code># CA-Verified TLS

# 1 手动配置activemq

# 1.1 可以直接用puppet的cert/private-keys，我这里新生成一个activemq的证书
[root@hadoop-master2 puppetlabs]# puppet master --configprint ssldir
/etc/puppetlabs/puppet/ssl
# 一个不冲突的名称即可，不需要是hostname/FQDN
[root@hadoop-master2 puppetlabs]# puppet cert generate activemq
Notice: activemq has a waiting certificate request
Notice: Signed certificate request for activemq
Notice: Removing file Puppet::SSL::CertificateRequest activemq at '/etc/puppetlabs/puppet/ssl/ca/requests/activemq.pem'
Notice: Removing file Puppet::SSL::CertificateRequest activemq at '/etc/puppetlabs/puppet/ssl/certificate_requests/activemq.pem'
[root@hadoop-master2 puppetlabs]# tree /etc/puppetlabs/puppet/ssl/
/etc/puppetlabs/puppet/ssl/
...
├── certificate_requests
├── certs
│   ├── activemq.pem
│   ├── ca.pem
│   └── hadoop-master2.example.com.pem
├── crl.pem
├── private
├── private_keys
│   ├── activemq.pem
│   └── hadoop-master2.example.com.pem
└── public_keys
    ├── activemq.pem
    └── hadoop-master2.example.com.pem

9 directories, 22 files

# certs/activemq.pem, certs/ca.pem, private_keys/activemq.pem 就是我们需要的。


# 1.2 创建Truststore
[root@hadoop-master2 puppetlabs]# which keytool
/opt/jdk1.7.0_60/bin/keytool
[root@hadoop-master2 puppetlabs]# cd /etc/puppetlabs/puppet/ssl            
[root@hadoop-master2 ssl]# keytool -import -alias "CU CA" -file certs/ca.pem -keystore truststore.jks
Enter keystore password:  
Re-enter new password: 
Owner: CN=Puppet CA: hadoop-master2.example.com
Issuer: CN=Puppet CA: hadoop-master2.example.com
...
Trust this certificate? [no]:  y
Certificate was added to keystore
[root@hadoop-master2 ssl]# ll
total 32
drwxr-xr-x 5 puppet puppet 4096 Apr 23 00:01 ca
drwxr-xr-x 2 puppet puppet 4096 Apr 28 19:53 certificate_requests
drwxr-xr-x 2 puppet puppet 4096 Apr 28 19:53 certs
-rw-r--r-- 1 puppet puppet  979 Apr 28 10:33 crl.pem
drwxr-x--- 2 puppet puppet 4096 Apr 22 23:51 private
drwxr-x--- 2 puppet puppet 4096 Apr 28 19:53 private_keys
drwxr-xr-x 2 puppet puppet 4096 Apr 28 19:53 public_keys
-rw-r--r-- 1 root   root   1496 Apr 28 20:01 truststore.jks
# 验证下指纹fingerprints
[root@hadoop-master2 ssl]# keytool -list -keystore truststore.jks 
Enter keystore password:  

Keystore type: JKS
Keystore provider: SUN

Your keystore contains 1 entry

cu ca, Apr 28, 2016, trustedCertEntry, 
Certificate fingerprint (SHA1): 40:2C:45:37:6B:C7:9C:92:E7:4D:1E:4F:2B:C4:17:F4:A3:5F:EB:56
[root@hadoop-master2 ssl]# openssl x509 -in certs/ca.pem -fingerprint -sha1
SHA1 Fingerprint=40:2C:45:37:6B:C7:9C:92:E7:4D:1E:4F:2B:C4:17:F4:A3:5F:EB:56


# 1.3 创建Keystore
[root@hadoop-master2 ssl]# cat private_keys/activemq.pem certs/activemq.pem &gt;activemq.pem
# 所有密码都需一致！！ All of these passwords must be the same.
[root@hadoop-master2 ssl]# openssl pkcs12 -export -in activemq.pem -out activemq.p12 -name activemq      
Enter Export Password:
Verifying - Enter Export Password:
[root@hadoop-master2 ssl]# keytool -importkeystore -destkeystore keystore.jks -srckeystore activemq.p12 \
&gt; -srcstoretype PKCS12 -alias activemq
Enter destination keystore password:  XXX
Re-enter new password: XXX
Enter source keystore password:  XXX
[root@hadoop-master2 ssl]# ll -t
total 52
-rw-r--r-- 1 root   root   3918 Apr 28 20:12 keystore.jks
-rw-r--r-- 1 root   root   4230 Apr 28 20:08 activemq.p12
-rw-r--r-- 1 root   root   5203 Apr 28 20:07 activemq.pem
-rw-r--r-- 1 root   root   1496 Apr 28 20:01 truststore.jks
...
# 验证指纹
[root@hadoop-master2 ssl]# keytool -list -keystore keystore.jks 
Enter keystore password:  

Keystore type: JKS
Keystore provider: SUN

Your keystore contains 1 entry

activemq, Apr 28, 2016, PrivateKeyEntry, 
Certificate fingerprint (SHA1): 4F:DF:DE:64:13:36:0E:74:8B:7F:D3:61:78:29:C4:AA:4F:A4:ED:D8
[root@hadoop-master2 ssl]# openssl x509 -in certs/activemq.pem -fingerprint -sha1
SHA1 Fingerprint=4F:DF:DE:64:13:36:0E:74:8B:7F:D3:61:78:29:C4:AA:4F:A4:ED:D8


# 1.4 配置activemq
# http://activemq.apache.org/how-do-i-use-ssl.html
# https://docs.puppet.com/mcollective/deploy/middleware/activemq.html#tls-credentials
# https://docs.puppet.com/mcollective/deploy/middleware/activemq.html#stomp
[root@hadoop-master2 ssl]# mv keystore.jks truststore.jks /opt/puppetlabs/apache-activemq-5.13.2/conf
[root@hadoop-master2 ssl]# cd /opt/puppetlabs/apache-activemq-5.13.2/conf/
# 填上面步骤设置的密码
[root@hadoop-master2 conf]# vi activemq.xml 
...
&lt;sslContext&gt;
  &lt;sslContext keyStore="keystore.jks" keyStorePassword="XXXX"
              trustStrore="truststore.jks" trustStorePassword="XXXX" /&gt;
&lt;/sslContext&gt;

&lt;transportConnectors&gt;
    &lt;!-- DOS protection, limit concurrent connections to 1000 and frame size to 100MB --&gt;
    &lt;transportConnector name="stomp+nio+ssl" uri="stomp+nio+ssl://0.0.0.0:61614?maximumConnections=1000&amp;amp;wireFormat.maxFrameSize=104857600&amp;amp;needClientAuth=true&amp;amp;transport.enabledProtocols=TLSv1,TLSv1.1,TLSv1.2"/&gt;
&lt;/transportConnectors&gt;

[root@hadoop-master2 apache-activemq-5.13.2]# chmod 600 conf/activemq.xml 
[root@hadoop-master2 apache-activemq-5.13.2]# bin/activemq stop
[root@hadoop-master2 apache-activemq-5.13.2]# bin/activemq start
# 日志查看
[root@hadoop-master2 apache-activemq-5.13.2]# less data/activemq.log 


# 2 puppetserver(mcollective client)
# https://docs.puppet.com/mcollective/configure/client.html
[root@hadoop-master2 ~]# cd /etc/puppetlabs/mcollective/
[root@hadoop-master2 mcollective]# cat client.cfg
...
connector = activemq
plugin.activemq.pool.size = 1
plugin.activemq.pool.1.host = hadoop-master2.example.com
plugin.activemq.pool.1.port = 61614
plugin.activemq.pool.1.user = system
plugin.activemq.pool.1.password = manager
plugin.activemq.pool.1.ssl = true
plugin.activemq.pool.1.ssl.ca = /etc/puppetlabs/puppet/ssl/certs/ca.pem
plugin.activemq.pool.1.ssl.key = /etc/puppetlabs/puppet/ssl/private_keys/hadoop-master2.example.com.pem
plugin.activemq.pool.1.ssl.cert = /etc/puppetlabs/puppet/ssl/certs/hadoop-master2.example.com.pem
...
[root@hadoop-master2 mcollective]# mco ping -v


---- ping statistics ----
No responses received

# 3 puppet agents(mcollective servers)
# https://docs.puppet.com/mcollective/configure/server.html
-bash-4.1# puppet agent --configprint confdir
/etc/puppetlabs/puppet
-bash-4.1# puppet agent --configprint ssldir
/etc/puppetlabs/puppet/ssl
-bash-4.1# puppet agent --configprint hostprivkey
/etc/puppetlabs/puppet/ssl/private_keys/hadoop-master1.example.com.pem
-bash-4.1# puppet agent --configprint hostcert
/etc/puppetlabs/puppet/ssl/certs/hadoop-master1.example.com.pem
-bash-4.1# puppet agent --configprint localcacert
/etc/puppetlabs/puppet/ssl/certs/ca.pem

-bash-4.1# cd /etc/puppetlabs/mcollective/
-bash-4.1# cat server.cfg 
...
connector = activemq
plugin.activemq.pool.size = 1
plugin.activemq.pool.1.host = hadoop-master2.example.com
plugin.activemq.pool.1.port = 61614
plugin.activemq.pool.1.user = system
plugin.activemq.pool.1.password = manager
plugin.activemq.pool.1.ssl = true
plugin.activemq.pool.1.ssl.ca = /etc/puppetlabs/puppet/ssl/certs/ca.pem
plugin.activemq.pool.1.ssl.key = /etc/puppetlabs/puppet/ssl/private_keys/hadoop-master1.example.com.pem
plugin.activemq.pool.1.ssl.cert = /etc/puppetlabs/puppet/ssl/certs/hadoop-master1.example.com.pem
...
-bash-4.1# service mcollective restart
Shutting down mcollective: 
Starting mcollective:                                      [  OK  ]

# 其他两台机器一样的操作

# 测试
[root@hadoop-master2 mcollective]# mco ping -v
hadoop-master1                           time=41.99 ms
hadoop-slaver2                           time=84.87 ms
hadoop-slaver1                           time=85.46 ms


---- ping statistics ----
3 replies max: 85.46 min: 41.99 avg: 70.77 
</code></pre>

<p>更多activemq的设置查看官方文档： <a href="https://docs.puppet.com/mcollective/deploy/middleware/activemq.html">ActiveMQ Config Reference for MCollective Users</a> <a href="https://raw.github.com/puppetlabs/marionette-collective/master/ext/activemq/examples/single-broker/activemq.xml">example activemq.xml</a></p>

<h1>SSL Security plugin</h1>

<p>Stomp with TLS (安全传输层协议)用于加密数据。而 security plugin 主要功能有：</p>

<ul>
<li>mcollective server要授权才会执行 client 发送的请求。</li>
<li>create a token that uniquely identify the client - based on the filename of the public key。</li>
<li>在请求中添加创建时间和TTL保证数据的完整性(不被拦截、篡改以及重复)。</li>
</ul>


<p>参考：</p>

<ul>
<li><a href="https://docs.puppet.com/mcollective/configure/client.html#security-plugin-settings">https://docs.puppet.com/mcollective/configure/client.html#security-plugin-settings</a></li>
<li><a href="https://docs.puppet.com/mcollective/security.html">https://docs.puppet.com/mcollective/security.html</a></li>
<li><a href="https://docs.puppet.com/mcollective/reference/plugins/security_ssl.html">https://docs.puppet.com/mcollective/reference/plugins/security_ssl.html</a></li>
</ul>


<pre><code># 1 生成server秘钥(公钥、私钥)
[root@hadoop-master2 mcollective-security]# openssl genrsa -out server-private.pem 1024
...
[root@hadoop-master2 mcollective-security]# openssl rsa -in server-private.pem -out server-public.pem -outform PEM -pubout  
writing RSA key
[root@hadoop-master2 mcollective-security]# ll
total 12
-rw-r--r-- 1 root root 7915 Apr 29 00:06 server-private.pem
-rw-r--r-- 1 root root 1836 Apr 29 00:07 server-public.pem

# 把 private/public 复制到所有的mcollective-servers节点
# 把 public 复制到mcollective-clients节点
[root@hadoop-master2 mcollective-security]# ssh 172.17.0.2 mkdir -p /etc/puppetlabs/mcollective/ssl/clients
[root@hadoop-master2 mcollective-security]# scp * 172.17.0.2:/etc/puppetlabs/mcollective/ssl/
server-private.pem   100% 7915     7.7KB/s   00:00    
server-public.pem    100% 1836     1.8KB/s   00:00    

[root@hadoop-master2 mcollective-security]# mkdir -p /etc/puppetlabs/mcollective/ssl
[root@hadoop-master2 mcollective-security]# cp server-public.pem /etc/puppetlabs/mcollective/ssl/

# 2 配置mcollective-servers。节点间配置不能同步，TLS配置的证书名称是不一样的！！
-bash-4.1# vi /etc/puppetlabs/mcollective/server.cfg 
...
# Plugins
#securityprovider = psk
#plugin.psk = unset

securityprovider = ssl
plugin.ssl_server_private = /etc/puppetlabs/mcollective/ssl/server-private.pem
plugin.ssl_server_public = /etc/puppetlabs/mcollective/ssl/server-public.pem
plugin.ssl_client_cert_dir = /etc/puppetlabs/mcollective/ssl/clients/
plugin.ssl.enfore_ttl = 0
...

-bash-4.1# service mcollective restart
Shutting down mcollective:                                 [  OK  ]
Starting mcollective:                                      [  OK  ]
# 可以通过 /var/log/puppetlabs/mcollective.log 查看详细日志

# 配置一个节点后，mco ping已经不再显示hadoop-master1了！！

# 3 生成client秘钥
[root@hadoop-master2 mcollective-security]# cd /etc/puppetlabs/mcollective/ssl
[root@hadoop-master2 ssl]# ll
total 8
drwxr-xr-x 2 root root 4096 Apr 29 00:15 clients
-rw-r--r-- 1 root root 1836 Apr 29 00:15 server-public.pem
[root@hadoop-master2 ssl]# openssl genrsa -out winse-private.pem 1024    
...
[root@hadoop-master2 ssl]# openssl rsa -in winse-private.pem -out winse-public.pem -outform PEM -pubout
writing RSA key
[root@hadoop-master2 ssl]# ll
total 16
drwxr-xr-x 2 root root 4096 Apr 29 00:15 clients
-rw-r--r-- 1 root root 1836 Apr 29 00:15 server-public.pem
-rw-r--r-- 1 root root  887 Apr 29 00:26 winse-private.pem
-rw-r--r-- 1 root root  272 Apr 29 00:26 winse-public.pem

# 把client用户的公钥拷贝到所有mcollective-servers的ssl/clients目录下
[root@hadoop-master2 ssl]# scp winse-public.pem 172.17.0.2:/etc/puppetlabs/mcollective/ssl/clients
winse-public.pem 100%  272     0.3KB/s   00:00    

# 4 配置clients
[root@hadoop-master2 ~]# vi /etc/puppetlabs/mcollective/client.cfg 
...
# Plugins
#securityprovider = psk
#plugin.psk = unset
securityprovider = ssl
plugin.ssl_server_public = /etc/puppetlabs/mcollective/ssl/server-public.pem
plugin.ssl_client_private = /etc/puppetlabs/mcollective/ssl/winse-private.pem
plugin.ssl_client_public = /etc/puppetlabs/mcollective/ssl/winse-public.pem
...

# mcollective-server不需要重启！客户端连接测试
[root@hadoop-master2 ssl]# mco ping -v
hadoop-master1                           time=561.29 ms
hadoop-slaver2                           time=601.91 ms
hadoop-slaver1                           time=608.31 ms


---- ping statistics ----
3 replies max: 608.31 min: 561.29 avg: 590.50 
</code></pre>

<p>理解了功能后，再按条理配置其实感觉就不是那么难了。遇到问题先查看日志！！</p>

<h1>最佳实践</h1>

<p>官网推荐使用 站点管理工具 统一来安装管理，如puppet。下面使用puppet来配置mcollective：</p>

<ul>
<li><a href="https://docs.puppet.com/mcollective/deploy/install.html#example">https://docs.puppet.com/mcollective/deploy/install.html#example</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/middleware/activemq_keystores.html#creating-keystores-with-puppet">https://docs.puppet.com/mcollective/deploy/middleware/activemq_keystores.html#creating-keystores-with-puppet</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/standard.html#write-the-server-config-file">https://docs.puppet.com/mcollective/deploy/standard.html#write-the-server-config-file</a></li>
</ul>


<p>TODO</p>

<p></p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
