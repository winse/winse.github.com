<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Puppet | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/puppet/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2016-05-06T11:26:47+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Puppetboard Install]]></title>
    <link href="http://winseliu.com/blog/2016/05/05/puppetboard-install/"/>
    <updated>2016-05-05T10:54:26+08:00</updated>
    <id>http://winseliu.com/blog/2016/05/05/puppetboard-install</id>
    <content type="html"><![CDATA[<p>对于我这样的python小白来说，有网络来安装 puppetboard 还是比较容易的（离线安装依赖处理可能比较麻烦）。</p>

<pre><code># https://fedoraproject.org/wiki/EPEL/zh-cn
[root@cu2 ~]# yum search epel
[root@cu2 ~]# yum install epel-release


[root@cu2 ~]# yum repolist
Loaded plugins: fastestmirror, priorities
Loading mirror speeds from cached hostfile
 * base: mirrors.skyshe.cn
 * centosplus: mirrors.pubyun.com
 * epel: mirror01.idc.hinet.net
 * extras: mirrors.skyshe.cn
 * updates: mirrors.skyshe.cn
193 packages excluded due to repository priority protections
repo id                                   repo name                                                                   status
base                                      CentOS-6 - Base                                                                  6,575
centosplus                                CentOS-6 - Centosplus                                                             0+76
epel                                      Extra Packages for Enterprise Linux 6 - x86_64                              12,127+117
extras                                    CentOS-6 - Extras                                                                   62
puppet-local                              Puppet Local                                                                         5
updates                                   CentOS-6 - Updates                                                               1,607
repolist: 20,376


[root@cu2 ~]# yum install python-pip -y


[root@cu2 ~]# pip install puppetboard
/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
You are using pip version 7.1.0, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Collecting puppetboard
/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
  Downloading puppetboard-0.1.3.tar.gz (598kB)
    100% |████████████████████████████████| 602kB 726kB/s 
Collecting Flask&gt;=0.10.1 (from puppetboard)
  Downloading Flask-0.10.1.tar.gz (544kB)
    100% |████████████████████████████████| 544kB 734kB/s 
Collecting Flask-WTF&lt;=0.9.5,&gt;=0.9.4 (from puppetboard)
  Downloading Flask-WTF-0.9.5.tar.gz (245kB)
    100% |████████████████████████████████| 249kB 320kB/s 
Collecting WTForms&lt;2.0 (from puppetboard)
  Downloading WTForms-1.0.5.zip (355kB)
    100% |████████████████████████████████| 356kB 1.3MB/s 
Collecting pypuppetdb&lt;0.3.0,&gt;=0.2.1 (from puppetboard)
  Downloading pypuppetdb-0.2.1.tar.gz
Collecting Werkzeug&gt;=0.7 (from Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading Werkzeug-0.11.9-py2.py3-none-any.whl (306kB)
    100% |████████████████████████████████| 307kB 1.5MB/s 
Collecting Jinja2&gt;=2.4 (from Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB)
    100% |████████████████████████████████| 266kB 2.3MB/s 
Collecting itsdangerous&gt;=0.21 (from Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading itsdangerous-0.24.tar.gz (46kB)
    100% |████████████████████████████████| 49kB 7.2MB/s 
Collecting requests&gt;=1.2.3 (from pypuppetdb&lt;0.3.0,&gt;=0.2.1-&gt;puppetboard)
  Downloading requests-2.10.0-py2.py3-none-any.whl (506kB)
    100% |████████████████████████████████| 507kB 920kB/s 
Collecting MarkupSafe (from Jinja2&gt;=2.4-&gt;Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading MarkupSafe-0.23.tar.gz
Installing collected packages: Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask, WTForms, Flask-WTF, requests, pypuppetdb, puppetboard
  Running setup.py install for MarkupSafe
  Running setup.py install for itsdangerous
  Running setup.py install for Flask
  Running setup.py install for WTForms
  Running setup.py install for Flask-WTF
  Running setup.py install for pypuppetdb
  Running setup.py install for puppetboard
Successfully installed Flask-0.10.1 Flask-WTF-0.9.5 Jinja2-2.8 MarkupSafe-0.23 WTForms-1.0.5 Werkzeug-0.11.9 itsdangerous-0.24 puppetboard-0.1.3 pypuppetdb-0.2.1 requests-2.10.0


[root@cu2 ~]# pip show puppetboard
You are using pip version 7.1.0, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
---
Metadata-Version: 1.0
Name: puppetboard
Version: 0.1.3
Summary: Web frontend for PuppetDB
Home-page: https://github.com/puppet-community/puppetboard
Author: Daniele Sluijters
Author-email: daniele.sluijters+pypi@gmail.com
License: Apache License 2.0
Location: /usr/lib/python2.6/site-packages
Requires: Flask, Flask-WTF, WTForms, pypuppetdb
[root@cu2 ~]# ll /usr/lib/python2.6/site-packages/puppetboard
total 100
-rw-r--r-- 1 root root 31629 May  5 09:12 app.py
-rw-r--r-- 1 root root 30481 May  5 09:12 app.pyc
-rw-r--r-- 1 root root  1206 May  5 09:12 default_settings.py
-rw-r--r-- 1 root root  1477 May  5 09:12 default_settings.pyc
-rw-r--r-- 1 root root  1025 May  5 09:12 forms.py
-rw-r--r-- 1 root root  1982 May  5 09:12 forms.pyc
-rw-r--r-- 1 root root     0 May  5 09:12 __init__.py
-rw-r--r-- 1 root root   143 May  5 09:12 __init__.pyc
drwxr-xr-x 9 root root  4096 May  5 09:12 static
drwxr-xr-x 2 root root  4096 May  5 09:12 templates
-rw-r--r-- 1 root root  2155 May  5 09:12 utils.py
-rw-r--r-- 1 root root  3433 May  5 09:12 utils.pyc


[root@cu2 ~]# pip install uwsgi
You are using pip version 7.1.0, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Collecting uwsgi
/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
  Downloading uwsgi-2.0.12.tar.gz (784kB)
    100% |████████████████████████████████| 786kB 143kB/s 
Installing collected packages: uwsgi
  Running setup.py install for uwsgi
Successfully installed uwsgi-2.0.12


[root@cu2 ~]# mkdir -p /var/www/puppetboard
[root@cu2 ~]# cd /var/www/puppetboard/
[root@cu2 puppetboard]# cp /usr/lib/python2.6/site-packages/puppetboard/default_settings.py ./settings.py
# 修改配置 
# https://github.com/voxpupuli/puppetboard#settings
PUPPETDB_HOST = 'cu3'
PUPPETDB_PORT = 8080
REPORTS_COUNT = 21
ENABLE_CATALOG = True

[root@cu2 puppetboard]# vi wsgi.py 
from __future__ import absolute_import
import os

os.environ['PUPPETDOARD_SETTINGS'] = '/var/www/puppetboard/settings.py'
from puppetboard.app import app as application


# A 直接用uwsgi-http
# http://yongqing.is-programmer.com/posts/43688.html
[root@cu2 puppetboard]# uwsgi --http :9091 --wsgi-file /var/www/puppetboard/wsgi.py 

# 使用 supervisord 管理
[root@cu2 supervisord.d]# cat uwsgi.ini 
[program:puppetboard]
command=uwsgi --http :9091 --wsgi-file /var/www/puppetboard/wsgi.py 
[root@cu2 supervisord.d]# supervisorctl update


# B nginx + uwsgi-socket
# 需要对应到 / ，新增一个9091的server
[root@cu2 puppetboard]# vi /home/hadoop/nginx/conf/nginx.conf
server {
  listen 9091;

  location /static {
    alias /usr/lib/python2.6/site-packages/puppetboard/static;
  }
  location / {
    include uwsgi_params;
    uwsgi_pass 127.0.0.1:9090;
  }
}

[root@cu2 puppetboard]# uwsgi --socket :9090 --wsgi-file /var/www/puppetboard/wsgi.py 

[root@cu2 puppetboard]# /home/hadoop/nginx/sbin/nginx -s reload
</code></pre>

<p><img src="/images/blogs/puppetboard-install.png" alt="" /></p>

<p>配置SSL访问需要把ssl_verify设置为false。</p>

<pre><code># 2.7.9+网上说好像就没问题
# http://stackoverflow.com/questions/29099404/ssl-insecureplatform-error-when-using-requests-package
# https://github.com/pypa/pip/issues/2681
[root@cu2 ~]# yum install -y  libffi-devel libffi 
[root@cu2 ~]# pip install 'requests[security]'

# [重要] 两个链接内容一样的：
# * https://groups.google.com/forum/#!msg/puppet-users/m7Sakf4bQ7Q/y6uAa0AUsZIJ
# * http://grokbase.com/t/gg/puppet-users/1428vjkncr/puppetboard-and-ssl
# You have two choices now, set SSL_VERIFY to False and trust that you're
# always talking to your actual PuppetDB or copy from the Puppet CA
# $vardir/ssl/ca_crt.pem to /etc/puppetboard and set SSL_VERIFY to the path
# of ca_crt.pem. In that case the file SSL_VERIFY points to will be used to
# verify PuppetDB's server certificate instead of the OS truststore.
[root@cu2 puppetboard]# vi settings.py 
PUPPETDB_HOST = 'cu3.eshore.cn'
PUPPETDB_PORT = 8081
PUPPETDB_SSL_VERIFY = False  # 这里设置为false
PUPPETDB_KEY = '/etc/puppetlabs/puppet/ssl/private_keys/cu2.eshore.cn.pem'
PUPPETDB_CERT = '/etc/puppetlabs/puppet/ssl/ca/signed/cu2.eshore.cn.pem'

# 重启uwsgi-http服务
[root@cu2 ~]# supervisorctl restart puppetboard
</code></pre>

<p>如果 puppetboard 和 puppetdb 安装在同一机器，可以使用 puppetdb/ssl 路径下的ssl文件（puppetdb/ssl也是从puppet/ssl拷贝过来的）：</p>

<pre><code>[root@cu3 ~]# puppetdb ssl-setup -f
PEM files in /etc/puppetlabs/puppetdb/ssl are missing, we will move them into place for you
Copying files: /etc/puppetlabs/puppet/ssl/certs/ca.pem, /etc/puppetlabs/puppet/ssl/private_keys/cu3.eshore.cn.pem and /etc/puppetlabs/puppet/ssl/certs/cu3.eshore.cn.pem to /etc/puppetlabs/puppetdb/ssl
...

[root@cu3 ~]# tree /etc/puppetlabs/puppetdb/ssl/
/etc/puppetlabs/puppetdb/ssl/
├── ca.pem
├── private.pem
└── public.pem
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MCollective Plugins]]></title>
    <link href="http://winseliu.com/blog/2016/04/28/mcollective-plugins/"/>
    <updated>2016-04-28T21:37:51+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/28/mcollective-plugins</id>
    <content type="html"><![CDATA[<p>上一篇介绍了mcollective的安装。乘着这股热情把 mco 命令行和插件的安装弄通，记录下来。</p>

<h2>基本命令使用</h2>

<ul>
<li><a href="https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html">https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html</a></li>
</ul>


<pre><code>[root@hadoop-master2 ~]# mco help
The Marionette Collective version 2.8.8

  completion      Helper for shell completion systems
  describe_filter Display human readable interpretation of filters
  facts           Reports on usage for a specific fact
  find            Find hosts using the discovery system matching filter criteria
  help            Application list and help
  inventory       General reporting tool for nodes, collectives and subcollectives
  ping            Ping all nodes
  plugin          MCollective Plugin Application
  rpc             Generic RPC agent client application
</code></pre>

<p>自带的插件只能用来查看环境情况(下面列出来的命令<a href="/blog/2016/04/28/mcollective-quick-start/#cli-simple-usage">上一篇:MCollective安装配置</a>都已记录过)。</p>

<pre><code>mco ping
mco inventory [server_host]
mco facts [fact]
</code></pre>

<p>mcollective 的 filter（适配节点）功能很强大，具体查看文档：<a href="https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html#selecting-request-targets-using-filters">Selecting Request Targets Using Filters</a></p>

<h2>插件安装</h2>

<ul>
<li><a href="https://docs.puppet.com/mcollective/deploy/standard.html#install-agent-plugins">https://docs.puppet.com/mcollective/deploy/standard.html#install-agent-plugins</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/plugins.html">Installing Plugins</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/plugins.html#example">https://docs.puppet.com/mcollective/deploy/plugins.html#example</a></li>
</ul>


<p>文档中描述了 Use packages 和 Put files directly into the libdir 两种安装插件的方式。但是 Packages 都是放在<a href="http://yum.puppetlabs.com/el/6/products/x86_64/">旧的repo</a>里面，我们这里使用第二种方式把github下载源码放到libdir来安装。</p>

<h4>安装mcollective-puppet-agent</h4>

<pre><code># 使用文档 https://github.com/puppetlabs/mcollective-puppet-agent#readme
# 直接下载release版本 
[root@hadoop-master2 ~]# cd /usr/libexec/mcollective/
[root@hadoop-master2 mcollective]# ll
total 44
-rw-r--r-- 1 root root 44759 Apr 29 11:53 mcollective-puppet-agent-1.10.0.tar.gz
[root@hadoop-master2 mcollective]# tar zxf mcollective-puppet-agent-1.10.0.tar.gz  
[root@hadoop-master2 mcollective]# ll mcollective-puppet-agent-1.10.0
total 60
drwxrwxr-x 2 root root  4096 Apr 13  2015 agent
drwxrwxr-x 2 root root  4096 Apr 13  2015 aggregate
drwxrwxr-x 2 root root  4096 Apr 13  2015 application
-rw-rw-r-- 1 root root  3456 Apr 13  2015 CHANGELOG.md
drwxrwxr-x 2 root root  4096 Apr 13  2015 data
drwxrwxr-x 4 root root  4096 Apr 13  2015 ext
-rw-rw-r-- 1 root root   349 Apr 13  2015 Gemfile
-rw-rw-r-- 1 root root  3036 Apr 13  2015 Rakefile
-rw-rw-r-- 1 root root 14739 Apr 13  2015 README.md
drwxrwxr-x 9 root root  4096 Apr 13  2015 spec
drwxrwxr-x 3 root root  4096 Apr 13  2015 util
drwxrwxr-x 2 root root  4096 Apr 13  2015 validator
# 官网提供example有区分服务端和客户端文件。反正多了没问题，直接全部放就行咯。。。
[root@hadoop-master2 mcollective]# mv mcollective-puppet-agent-1.10.0 mcollective

# 验证
# 多了puppet的命令！
[root@hadoop-master2 mcollective]# mco help
The Marionette Collective version 2.8.8

  completion      Helper for shell completion systems
  describe_filter Display human readable interpretation of filters
  facts           Reports on usage for a specific fact
  find            Find hosts using the discovery system matching filter criteria
  help            Application list and help
  inventory       General reporting tool for nodes, collectives and subcollectives
  ping            Ping all nodes
  plugin          MCollective Plugin Application
  puppet          Schedule runs, enable, disable and interrogate the Puppet Agent
  rpc             Generic RPC agent client application


# 同步到mcollective-servers （172.17.0.2对应hadoop-slaver1）
[root@hadoop-master2 mcollective]# rsync -az /usr/libexec/mcollective 172.17.0.2:/usr/libexec/

# mcollective-server添加插件后，重启mcollective服务
# 也可以使用 reload-agents 来重新加载agents： service mcollective reload-agents
[root@hadoop-slaver1 libexec]# service mcollective restart
Shutting down mcollective:                                 [  OK  ]
Starting mcollective:                                      [  OK  ]


# 验证server，已经可以看到新添加的puppet命令了
[root@hadoop-master2 mcollective]# mco inventory hadoop-slaver1
Inventory for hadoop-slaver1:

   Server Statistics:
                      Version: 2.8.8
                   Start Time: 2016-04-29 12:01:40 +0800
                  Config File: /etc/puppetlabs/mcollective/server.cfg
                  Collectives: mcollective
              Main Collective: mcollective
                   Process ID: 123
               Total Messages: 1
      Messages Passed Filters: 1
            Messages Filtered: 0
             Expired Messages: 0
                 Replies Sent: 0
         Total Processor Time: 0.67 seconds
                  System Time: 0.8 seconds

   Agents:
      discovery       puppet          rpcutil        

   Data Plugins:
      agent           collective      fact           
      fstat           puppet          resource       

[root@hadoop-master2 mcollective]# mco help puppet

[root@hadoop-master2 mcollective]# mco puppet status    

 * [ ============================================================&gt; ] 3 / 3

   hadoop-slaver1: Currently stopped; last completed run 10 hours 57 minutes 20 seconds ago
   hadoop-master1: Currently stopped; last completed run 11 hours 1 minutes 05 seconds ago
   hadoop-slaver2: Currently stopped; last completed run 10 hours 57 minutes 16 seconds ago
...


# 配置server.conf
# 注意：真正要执行puppet命令，为了适配puppet4需要添加/修改配置
-bash-4.1# cat /etc/puppetlabs/mcollective/server.cfg 
...
plugin.puppet.command = /opt/puppetlabs/bin/puppet agent
plugin.puppet.config = /etc/puppetlabs/puppet/puppet.conf

# 重启所有mcollective（也可以不重启，重新加载agent即可： 使用 mco shell run service mcollective reload-agents 来重新加载）

[root@hadoop-master2 mcollective]# mco puppet runall 1
2016-04-29 16:52:46: Running all nodes with a concurrency of 1
2016-04-29 16:52:46: Discovering enabled Puppet nodes to manage
2016-04-29 16:52:49: Found 3 enabled nodes
2016-04-29 16:52:50: hadoop-slaver1 schedule status: Started a Puppet run using the '/opt/puppetlabs/bin/puppet agent --onetime --no-daemonize --color=false --show_diff --verbose --no-splay' command
2016-04-29 16:52:55: hadoop-slaver2 schedule status: Started a Puppet run using the '/opt/puppetlabs/bin/puppet agent --onetime --no-daemonize --color=false --show_diff --verbose --no-splay' command
2016-04-29 16:52:59: hadoop-master1 schedule status: Started a Puppet run using the '/opt/puppetlabs/bin/puppet agent --onetime --no-daemonize --color=false --show_diff --verbose --no-splay' command
2016-04-29 16:52:59: Iteration complete. Initiated a Puppet run on 3 nodes.

[root@hadoop-master2 puppetlabs]# mco puppet status

 * [ ============================================================&gt; ] 3 / 3

   hadoop-master1: Currently stopped; last completed run 10 seconds ago
   hadoop-slaver1: Currently stopped; last completed run 15 seconds ago
   hadoop-slaver2: Currently stopped; last completed run 04 seconds ago
...
# 或者通过 puppetexplorer 查看节点最后的更新时间
</code></pre>

<h4>安装 package / service 插件</h4>

<p>为了更好的管理，再添加 package 和 service 两个插件</p>

<ul>
<li><a href="https://github.com/puppetlabs/mcollective-package-agent#readme">https://github.com/puppetlabs/mcollective-package-agent#readme</a></li>
<li><a href="https://github.com/puppetlabs/mcollective-service-agent#readme">https://github.com/puppetlabs/mcollective-service-agent#readme</a></li>
</ul>


<pre><code># http://stackoverflow.com/questions/8488253/how-to-force-cp-to-overwrite-without-confirmation
[root@hadoop-master2 mcollective]# unalias cp
[root@hadoop-master2 mcollective]# cp -rf mcollective-service-agent-3.1.3/* mcollective/   
[root@hadoop-master2 mcollective]# cp -rf mcollective-package-agent-4.4.0/* mcollective/

[root@hadoop-master2 mcollective]# rsync -az /usr/libexec/mcollective 172.17.0.2:/usr/libexec/

# 重启mcollective服务（或者 mco shell run service mcollective reload-agents 重新加载）
</code></pre>

<p>验证下package的实力：</p>

<pre><code>[root@hadoop-master2 mcollective]# mco package lrzsz status

 * [ ============================================================&gt; ] 3 / 3

   hadoop-slaver1: lrzsz-0.12.20-27.1.el6.x86_64
   hadoop-master1: -purged.
   hadoop-slaver2: -purged.

Summary of Arch:

   x86_64 = 1

Summary of Ensure:

             purged = 2
   0.12.20-27.1.el6 = 1


Finished processing 3 / 3 hosts in 1488.41 ms

[root@hadoop-master2 mcollective]# mco rpc package install package=lrzsz
Discovering hosts using the mc method for 2 second(s) .... 3

 * [ ============================================================&gt; ] 3 / 3


hadoop-slaver1                           Unknown Request Status
   Package is already installed


Summary of Ensure:

   0.12.20-27.1.el6 = 3


Finished processing 3 / 3 hosts in 14525.03 ms
[root@hadoop-master2 mcollective]# mco package lrzsz status

 * [ ============================================================&gt; ] 3 / 3

   hadoop-master1: lrzsz-0.12.20-27.1.el6.x86_64
   hadoop-slaver2: lrzsz-0.12.20-27.1.el6.x86_64
   hadoop-slaver1: lrzsz-0.12.20-27.1.el6.x86_64

Summary of Arch:

   x86_64 = 3

Summary of Ensure:

   0.12.20-27.1.el6 = 3


Finished processing 3 / 3 hosts in 572.13 ms
</code></pre>

<p>还有很多的插件：</p>

<ul>
<li><a href="https://docs.puppet.com/mcollective/plugin_directory/index.html">https://docs.puppet.com/mcollective/plugin_directory/index.html</a></li>
<li>shell插件也不错，安装的时刻注意一下目录结构！<a href="https://github.com/puppetlabs/mcollective-shell-agent">https://github.com/puppetlabs/mcollective-shell-agent</a></li>
</ul>


<p>添加了 service，package，shell，puppet 插件后，用 mco 来执行管理集群太爽了！！</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MCollective安装配置]]></title>
    <link href="http://winseliu.com/blog/2016/04/28/mcollective-quick-start/"/>
    <updated>2016-04-28T08:39:23+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/28/mcollective-quick-start</id>
    <content type="html"><![CDATA[<p>puppet agent 通过定时拉取的方式来更新本地系统，但无法满足实时更新的需求。 mcollective 通过 <strong>消息中间件</strong> 的方式，mclient/mservers通过消息的推送/订阅，实现mservers实时执行mclient提交的请求。（添加 m 说明是mcollective的组件！）</p>

<p>最新版的安装除了官网，没有其他可以直接学习的资料（只能参考）。先看官网的资料：</p>

<ul>
<li>组件功能(必须看看) <a href="https://docs.puppet.com/mcollective/overview_components.html">https://docs.puppet.com/mcollective/overview_components.html</a></li>
<li>部署 <a href="https://docs.puppet.com/mcollective/deploy/install.html">https://docs.puppet.com/mcollective/deploy/install.html</a></li>
<li>部署规范/准则 <a href="https://docs.puppet.com/mcollective/deploy/standard.html">https://docs.puppet.com/mcollective/deploy/standard.html</a></li>
</ul>


<p>摘录官网安装描述：[Installing MCollective requires the following steps]</p>

<ul>
<li>Make sure your middleware is up and running and your firewalls are in order.</li>
<li>Install the mcollective package on servers, then make sure the mcollective service is running.</li>
<li>Install the mcollective-client package on admin workstations.</li>
<li>Most Debian-like and Red Hat-like systems can use the official Puppet Labs packages. Enable the Puppet Labs repos, or import the packages into your own repos.

<ul>
<li>If you’re on Debian/Ubuntu, mind the missing package dependency.</li>
</ul>
</li>
<li>If your systems can’t use the official packages, check the system requirements and either build your own or run from source.</li>
</ul>


<p>mcollective对于puppet来说是一个锦上添花的组件，没有puppet一样正常运转。部署主要由两个部分组成：</p>

<ul>
<li>部署消息中间件</li>
<li>配置mcollective(puppet4.4 agent已经安装该功能，redhat也自带装了Stomp包：<code>/opt/puppetlabs/puppet/lib/ruby/gems/2.1.0/gems/</code> 目录下面)

<ul>
<li>配置mclient/mserver</li>
<li>配置Stomp with TLS</li>
<li>配置security</li>
</ul>
</li>
</ul>


<p>本文先简单实现连接远程主机，然后配置安全功能，最后用puppet来重新实现 mcollective 的安装和配置。</p>

<h1>环境说明</h1>

<ul>
<li>hadoop-master2:

<ul>
<li>172.17.42.1</li>
<li>puppetserver, activemq-server, mcollective-client</li>
</ul>
</li>
<li>hadoop-master1/hadoop-slaver1/hadoop-slaver2:

<ul>
<li>172.17.0.2/&frac34;</li>
<li>puppet-agent, mcollective-server</li>
</ul>
</li>
</ul>


<h1>ActiveMQ部署</h1>

<p>activemq的服务端是一个spring-jetty项目，直接解压运行启动脚本即可。</p>

<pre><code># http://activemq.apache.org/download-archives.html
# 直接下载最新的 tar.gz

# 解压，启动
On Unix:
From a command shell, change to the installation directory and run ActiveMQ as a foregroud process:
cd [activemq_install_dir]/bin
./activemq console
From a command shell, change to the installation directory and run ActiveMQ as a daemon process:
cd [activemq_install_dir]/bin
./activemq start

# 确认
URL: http://127.0.0.1:8161/admin/
Login: admin
Passwort: admin
# 起了好多端口，随便试一个
netstat -nl|grep 61616
netstat -anp|grep PID

# 数据/日志目录
[root@hadoop-master2 apache-activemq-5.13.2]# ll data/
total 16
-rw-r--r-- 1 root users 4276 Apr 27 21:36 activemq.log
-rw-r--r-- 1 root root     5 Apr 27 21:36 activemq.pid
-rw-r--r-- 1 root root     0 Apr 27 21:36 audit.log
drwxr-xr-x 2 root root  4096 Apr 27 21:36 kahadb
</code></pre>

<p><img src="/images/blogs/mcollective-activemq.png" alt="" /></p>

<p>查看连接密码：</p>

<pre><code>[root@hadoop-master2 conf]# cat credentials.properties
...
activemq.username=system
activemq.password=manager
guest.password=password[root@hadoop-master2 conf]# 
</code></pre>

<h1>简单配置(unencrypted Stomp) <a name="cli-simple-usage"></a></h1>

<p>安装puppet4.4后，mcollective已经安装好了！直接修改配置连接到activemq即可。</p>

<pre><code>[root@hadoop-master2 puppetlabs]# chkconfig --list | grep mco
mcollective     0:off   1:off   2:off   3:off   4:off   5:off   6:off

# puppetserver作为mcollective-client
[root@hadoop-master2 mcollective]# cat client.cfg                    
...
connector = activemq
plugin.activemq.pool.size = 1
plugin.activemq.pool.1.host = hadoop-master2.example.com
plugin.activemq.pool.1.port = 61613
plugin.activemq.pool.1.user = system
plugin.activemq.pool.1.password = manager
...

[root@hadoop-master2 mcollective]# mco ping


---- ping statistics ----
No responses received

# puppet agent作为mcollective-server
-bash-4.1# cat server.cfg 
...
connector = activemq
plugin.activemq.pool.size = 1
plugin.activemq.pool.1.host = hadoop-master2.example.com
plugin.activemq.pool.1.port = 61613
plugin.activemq.pool.1.user = system
plugin.activemq.pool.1.password = manager
...

-bash-4.1# service mcollective start
Starting mcollective:                                      [  OK  ]
-bash-4.1# service mcollective status
mcollectived (pid  202) is running...

# 其他两台agent机器一样的配置操作

# 1. mcollective-client(puppetserver) 测试
[root@hadoop-master2 ~]# mco find
hadoop-master1
hadoop-slaver2
hadoop-slaver1
[root@hadoop-master2 mcollective]# mco ping
hadoop-master1                           time=148.29 ms
hadoop-slaver2                           time=187.99 ms
hadoop-slaver1                           time=190.21 ms


---- ping statistics ----
3 replies max: 190.21 min: 148.29 avg: 175.50 

# 2. 先查看/扫描节点状态。（如果配置了facts后，会输出一长串的Facts！）
[root@hadoop-master2 ssl]# mco inventory hadoop-master1
Inventory for hadoop-master1:

   Server Statistics:
                      Version: 2.8.8
                   Start Time: 2016-04-29 00:21:31 +0800
                  Config File: /etc/puppetlabs/mcollective/server.cfg
                  Collectives: mcollective
              Main Collective: mcollective
                   Process ID: 155
               Total Messages: 13
      Messages Passed Filters: 3
            Messages Filtered: 0
             Expired Messages: 0
                 Replies Sent: 2
         Total Processor Time: 2.32 seconds
                  System Time: 0.3 seconds

   Agents:
      discovery       rpcutil                        

   Data Plugins:
      agent           collective      fact           
      fstat                                          

   Configuration Management Classes:
      No classes applied

   Facts:
      mcollective =&gt; 1

# 3. 获取节点facts，需要配合puppet一起来使用
# puppetserver 配置更新agent facts.yaml信息
[root@hadoop-master2 manifests]# cat site.pp 
file{'/etc/puppetlabs/mcollective/facts.yaml':
  owner    =&gt; root,
  group    =&gt; root,
  mode     =&gt; '400',
  loglevel =&gt; debug, # reduce noise in Puppet reports
  content  =&gt; inline_template("&lt;%= scope.to_hash.reject { |k,v| k.to_s =~ /(uptime_seconds|timestamp|free)/ }.to_yaml %&gt;"), # exclude rapidly changing facts
}
# 读取facts
[root@hadoop-master2 manifests]# mco facts hostname
Report for fact: hostname

        hadoop-master1                           found 1 times
        hadoop-slaver1                           found 1 times
        hadoop-slaver2                           found 1 times

Finished processing 3 / 3 hosts in 579.93 ms
</code></pre>

<p>自带的插件功能比较少，要真正把 mcollective 用起来需要安装插件：puppet, service, package等等。这篇主要记录安装过程，<a href="/blog/2016/04/28/mcollective-plugins/">插件安装以及使用</a>后面具体实践了再写。</p>

<p>我觉得内网生产环境安装，到这一步已经差不多了！下面的安全配置就当深入学习吧。</p>

<h1>Stomp with TLS 配置</h1>

<ul>
<li><a href="https://docs.puppet.com/mcollective/reference/integration/activemq_ssl.html">https://docs.puppet.com/mcollective/reference/integration/activemq_ssl.html</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/middleware/activemq_keystores.html">https://docs.puppet.com/mcollective/deploy/middleware/activemq_keystores.html</a></li>
</ul>


<p><strong>Anonymous TLS</strong> 步骤简单一点，这里就不列出来了，自己去看官网的文档: <a href="https://docs.puppet.com/mcollective/reference/integration/activemq_ssl.html#anonymous-tls">Anonymous TLS</a></p>

<pre><code># CA-Verified TLS

# 1 手动配置activemq

# 1.1 可以直接用puppet的cert/private-keys，我这里新生成一个activemq的证书
[root@hadoop-master2 puppetlabs]# puppet master --configprint ssldir
/etc/puppetlabs/puppet/ssl
# 一个不冲突的名称即可，不需要是hostname/FQDN
[root@hadoop-master2 puppetlabs]# puppet cert generate activemq
Notice: activemq has a waiting certificate request
Notice: Signed certificate request for activemq
Notice: Removing file Puppet::SSL::CertificateRequest activemq at '/etc/puppetlabs/puppet/ssl/ca/requests/activemq.pem'
Notice: Removing file Puppet::SSL::CertificateRequest activemq at '/etc/puppetlabs/puppet/ssl/certificate_requests/activemq.pem'
[root@hadoop-master2 puppetlabs]# tree /etc/puppetlabs/puppet/ssl/
/etc/puppetlabs/puppet/ssl/
...
├── certificate_requests
├── certs
│   ├── activemq.pem
│   ├── ca.pem
│   └── hadoop-master2.example.com.pem
├── crl.pem
├── private
├── private_keys
│   ├── activemq.pem
│   └── hadoop-master2.example.com.pem
└── public_keys
    ├── activemq.pem
    └── hadoop-master2.example.com.pem

9 directories, 22 files

# certs/activemq.pem, certs/ca.pem, private_keys/activemq.pem 就是我们需要的。


# 1.2 创建Truststore
[root@hadoop-master2 puppetlabs]# which keytool
/opt/jdk1.7.0_60/bin/keytool
[root@hadoop-master2 puppetlabs]# cd /etc/puppetlabs/puppet/ssl            
[root@hadoop-master2 ssl]# keytool -import -alias "CU CA" -file certs/ca.pem -keystore truststore.jks
Enter keystore password:  
Re-enter new password: 
Owner: CN=Puppet CA: hadoop-master2.example.com
Issuer: CN=Puppet CA: hadoop-master2.example.com
...
Trust this certificate? [no]:  y
Certificate was added to keystore
[root@hadoop-master2 ssl]# ll
total 32
drwxr-xr-x 5 puppet puppet 4096 Apr 23 00:01 ca
drwxr-xr-x 2 puppet puppet 4096 Apr 28 19:53 certificate_requests
drwxr-xr-x 2 puppet puppet 4096 Apr 28 19:53 certs
-rw-r--r-- 1 puppet puppet  979 Apr 28 10:33 crl.pem
drwxr-x--- 2 puppet puppet 4096 Apr 22 23:51 private
drwxr-x--- 2 puppet puppet 4096 Apr 28 19:53 private_keys
drwxr-xr-x 2 puppet puppet 4096 Apr 28 19:53 public_keys
-rw-r--r-- 1 root   root   1496 Apr 28 20:01 truststore.jks
# 验证下指纹fingerprints
[root@hadoop-master2 ssl]# keytool -list -keystore truststore.jks 
Enter keystore password:  

Keystore type: JKS
Keystore provider: SUN

Your keystore contains 1 entry

cu ca, Apr 28, 2016, trustedCertEntry, 
Certificate fingerprint (SHA1): 40:2C:45:37:6B:C7:9C:92:E7:4D:1E:4F:2B:C4:17:F4:A3:5F:EB:56
[root@hadoop-master2 ssl]# openssl x509 -in certs/ca.pem -fingerprint -sha1
SHA1 Fingerprint=40:2C:45:37:6B:C7:9C:92:E7:4D:1E:4F:2B:C4:17:F4:A3:5F:EB:56


# 1.3 创建Keystore
[root@hadoop-master2 ssl]# cat private_keys/activemq.pem certs/activemq.pem &gt;activemq.pem
# 所有密码都需一致！！ All of these passwords must be the same.
[root@hadoop-master2 ssl]# openssl pkcs12 -export -in activemq.pem -out activemq.p12 -name activemq      
Enter Export Password:
Verifying - Enter Export Password:
[root@hadoop-master2 ssl]# keytool -importkeystore -destkeystore keystore.jks -srckeystore activemq.p12 \
&gt; -srcstoretype PKCS12 -alias activemq
Enter destination keystore password:  XXX
Re-enter new password: XXX
Enter source keystore password:  XXX
[root@hadoop-master2 ssl]# ll -t
total 52
-rw-r--r-- 1 root   root   3918 Apr 28 20:12 keystore.jks
-rw-r--r-- 1 root   root   4230 Apr 28 20:08 activemq.p12
-rw-r--r-- 1 root   root   5203 Apr 28 20:07 activemq.pem
-rw-r--r-- 1 root   root   1496 Apr 28 20:01 truststore.jks
...
# 验证指纹
[root@hadoop-master2 ssl]# keytool -list -keystore keystore.jks 
Enter keystore password:  

Keystore type: JKS
Keystore provider: SUN

Your keystore contains 1 entry

activemq, Apr 28, 2016, PrivateKeyEntry, 
Certificate fingerprint (SHA1): 4F:DF:DE:64:13:36:0E:74:8B:7F:D3:61:78:29:C4:AA:4F:A4:ED:D8
[root@hadoop-master2 ssl]# openssl x509 -in certs/activemq.pem -fingerprint -sha1
SHA1 Fingerprint=4F:DF:DE:64:13:36:0E:74:8B:7F:D3:61:78:29:C4:AA:4F:A4:ED:D8


# 1.4 配置activemq
# http://activemq.apache.org/how-do-i-use-ssl.html
# https://docs.puppet.com/mcollective/deploy/middleware/activemq.html#tls-credentials
# https://docs.puppet.com/mcollective/deploy/middleware/activemq.html#stomp
[root@hadoop-master2 ssl]# mv keystore.jks truststore.jks /opt/puppetlabs/apache-activemq-5.13.2/conf
[root@hadoop-master2 ssl]# cd /opt/puppetlabs/apache-activemq-5.13.2/conf/
# 填上面步骤设置的密码
[root@hadoop-master2 conf]# vi activemq.xml 
...
&lt;sslContext&gt;
  &lt;sslContext keyStore="keystore.jks" keyStorePassword="XXXX"
              trustStrore="truststore.jks" trustStorePassword="XXXX" /&gt;
&lt;/sslContext&gt;

&lt;transportConnectors&gt;
    &lt;!-- DOS protection, limit concurrent connections to 1000 and frame size to 100MB --&gt;
    &lt;transportConnector name="stomp+nio+ssl" uri="stomp+nio+ssl://0.0.0.0:61614?maximumConnections=1000&amp;amp;wireFormat.maxFrameSize=104857600&amp;amp;needClientAuth=true&amp;amp;transport.enabledProtocols=TLSv1,TLSv1.1,TLSv1.2"/&gt;
&lt;/transportConnectors&gt;

[root@hadoop-master2 apache-activemq-5.13.2]# chmod 600 conf/activemq.xml 
[root@hadoop-master2 apache-activemq-5.13.2]# bin/activemq stop
[root@hadoop-master2 apache-activemq-5.13.2]# bin/activemq start
# 日志查看
[root@hadoop-master2 apache-activemq-5.13.2]# less data/activemq.log 


# 2 puppetserver(mcollective client)
# https://docs.puppet.com/mcollective/configure/client.html
[root@hadoop-master2 ~]# cd /etc/puppetlabs/mcollective/
[root@hadoop-master2 mcollective]# cat client.cfg
...
connector = activemq
plugin.activemq.pool.size = 1
plugin.activemq.pool.1.host = hadoop-master2.example.com
plugin.activemq.pool.1.port = 61614
plugin.activemq.pool.1.user = system
plugin.activemq.pool.1.password = manager
plugin.activemq.pool.1.ssl = true
plugin.activemq.pool.1.ssl.ca = /etc/puppetlabs/puppet/ssl/certs/ca.pem
plugin.activemq.pool.1.ssl.key = /etc/puppetlabs/puppet/ssl/private_keys/hadoop-master2.example.com.pem
plugin.activemq.pool.1.ssl.cert = /etc/puppetlabs/puppet/ssl/certs/hadoop-master2.example.com.pem
...
[root@hadoop-master2 mcollective]# mco ping -v


---- ping statistics ----
No responses received

# 3 puppet agents(mcollective servers)
# https://docs.puppet.com/mcollective/configure/server.html
-bash-4.1# puppet agent --configprint confdir
/etc/puppetlabs/puppet
-bash-4.1# puppet agent --configprint ssldir
/etc/puppetlabs/puppet/ssl
-bash-4.1# puppet agent --configprint hostprivkey
/etc/puppetlabs/puppet/ssl/private_keys/hadoop-master1.example.com.pem
-bash-4.1# puppet agent --configprint hostcert
/etc/puppetlabs/puppet/ssl/certs/hadoop-master1.example.com.pem
-bash-4.1# puppet agent --configprint localcacert
/etc/puppetlabs/puppet/ssl/certs/ca.pem

-bash-4.1# cd /etc/puppetlabs/mcollective/
-bash-4.1# cat server.cfg 
...
connector = activemq
plugin.activemq.pool.size = 1
plugin.activemq.pool.1.host = hadoop-master2.example.com
plugin.activemq.pool.1.port = 61614
plugin.activemq.pool.1.user = system
plugin.activemq.pool.1.password = manager
plugin.activemq.pool.1.ssl = true
plugin.activemq.pool.1.ssl.ca = /etc/puppetlabs/puppet/ssl/certs/ca.pem
plugin.activemq.pool.1.ssl.key = /etc/puppetlabs/puppet/ssl/private_keys/hadoop-master1.example.com.pem
plugin.activemq.pool.1.ssl.cert = /etc/puppetlabs/puppet/ssl/certs/hadoop-master1.example.com.pem
...
-bash-4.1# service mcollective restart
Shutting down mcollective: 
Starting mcollective:                                      [  OK  ]

# 其他两台机器一样的操作

# 测试
[root@hadoop-master2 mcollective]# mco ping -v
hadoop-master1                           time=41.99 ms
hadoop-slaver2                           time=84.87 ms
hadoop-slaver1                           time=85.46 ms


---- ping statistics ----
3 replies max: 85.46 min: 41.99 avg: 70.77 
</code></pre>

<p>更多activemq的设置查看官方文档： <a href="https://docs.puppet.com/mcollective/deploy/middleware/activemq.html">ActiveMQ Config Reference for MCollective Users</a> <a href="https://raw.github.com/puppetlabs/marionette-collective/master/ext/activemq/examples/single-broker/activemq.xml">example activemq.xml</a></p>

<h1>SSL Security plugin</h1>

<p>Stomp with TLS (安全传输层协议)用于加密数据。而 security plugin 主要功能有：</p>

<ul>
<li>mcollective server要授权才会执行 client 发送的请求。</li>
<li>create a token that uniquely identify the client - based on the filename of the public key。</li>
<li>在请求中添加创建时间和TTL保证数据的完整性(不被拦截、篡改以及重复)。</li>
</ul>


<p>参考：</p>

<ul>
<li><a href="https://docs.puppet.com/mcollective/configure/client.html#security-plugin-settings">https://docs.puppet.com/mcollective/configure/client.html#security-plugin-settings</a></li>
<li><a href="https://docs.puppet.com/mcollective/security.html">https://docs.puppet.com/mcollective/security.html</a></li>
<li><a href="https://docs.puppet.com/mcollective/reference/plugins/security_ssl.html">https://docs.puppet.com/mcollective/reference/plugins/security_ssl.html</a></li>
</ul>


<pre><code># 1 生成server秘钥(公钥、私钥)
[root@hadoop-master2 mcollective-security]# openssl genrsa -out server-private.pem 1024
...
[root@hadoop-master2 mcollective-security]# openssl rsa -in server-private.pem -out server-public.pem -outform PEM -pubout  
writing RSA key
[root@hadoop-master2 mcollective-security]# ll
total 12
-rw-r--r-- 1 root root 7915 Apr 29 00:06 server-private.pem
-rw-r--r-- 1 root root 1836 Apr 29 00:07 server-public.pem

# 把 private/public 复制到所有的mcollective-servers节点
# 把 public 复制到mcollective-clients节点
[root@hadoop-master2 mcollective-security]# ssh 172.17.0.2 mkdir -p /etc/puppetlabs/mcollective/ssl/clients
[root@hadoop-master2 mcollective-security]# scp * 172.17.0.2:/etc/puppetlabs/mcollective/ssl/
server-private.pem   100% 7915     7.7KB/s   00:00    
server-public.pem    100% 1836     1.8KB/s   00:00    

[root@hadoop-master2 mcollective-security]# mkdir -p /etc/puppetlabs/mcollective/ssl
[root@hadoop-master2 mcollective-security]# cp server-public.pem /etc/puppetlabs/mcollective/ssl/

# 2 配置mcollective-servers。节点间配置不能同步，TLS配置的证书名称是不一样的！！
-bash-4.1# vi /etc/puppetlabs/mcollective/server.cfg 
...
# Plugins
#securityprovider = psk
#plugin.psk = unset

securityprovider = ssl
plugin.ssl_server_private = /etc/puppetlabs/mcollective/ssl/server-private.pem
plugin.ssl_server_public = /etc/puppetlabs/mcollective/ssl/server-public.pem
plugin.ssl_client_cert_dir = /etc/puppetlabs/mcollective/ssl/clients/
plugin.ssl.enfore_ttl = 0
...

-bash-4.1# service mcollective restart
Shutting down mcollective:                                 [  OK  ]
Starting mcollective:                                      [  OK  ]
# 可以通过 /var/log/puppetlabs/mcollective.log 查看详细日志

# 配置一个节点后，mco ping已经不再显示hadoop-master1了！！

# 3 生成client秘钥
[root@hadoop-master2 mcollective-security]# cd /etc/puppetlabs/mcollective/ssl
[root@hadoop-master2 ssl]# ll
total 8
drwxr-xr-x 2 root root 4096 Apr 29 00:15 clients
-rw-r--r-- 1 root root 1836 Apr 29 00:15 server-public.pem
[root@hadoop-master2 ssl]# openssl genrsa -out winse-private.pem 1024    
...
[root@hadoop-master2 ssl]# openssl rsa -in winse-private.pem -out winse-public.pem -outform PEM -pubout
writing RSA key
[root@hadoop-master2 ssl]# ll
total 16
drwxr-xr-x 2 root root 4096 Apr 29 00:15 clients
-rw-r--r-- 1 root root 1836 Apr 29 00:15 server-public.pem
-rw-r--r-- 1 root root  887 Apr 29 00:26 winse-private.pem
-rw-r--r-- 1 root root  272 Apr 29 00:26 winse-public.pem

# 把client用户的公钥拷贝到所有mcollective-servers的ssl/clients目录下
[root@hadoop-master2 ssl]# scp winse-public.pem 172.17.0.2:/etc/puppetlabs/mcollective/ssl/clients
winse-public.pem 100%  272     0.3KB/s   00:00    

# 4 配置clients
[root@hadoop-master2 ~]# vi /etc/puppetlabs/mcollective/client.cfg 
...
# Plugins
#securityprovider = psk
#plugin.psk = unset
securityprovider = ssl
plugin.ssl_server_public = /etc/puppetlabs/mcollective/ssl/server-public.pem
plugin.ssl_client_private = /etc/puppetlabs/mcollective/ssl/winse-private.pem
plugin.ssl_client_public = /etc/puppetlabs/mcollective/ssl/winse-public.pem
...

# mcollective-server不需要重启！客户端连接测试
[root@hadoop-master2 ssl]# mco ping -v
hadoop-master1                           time=561.29 ms
hadoop-slaver2                           time=601.91 ms
hadoop-slaver1                           time=608.31 ms


---- ping statistics ----
3 replies max: 608.31 min: 561.29 avg: 590.50 
</code></pre>

<p>理解了功能后，再按条理配置其实感觉就不是那么难了。遇到问题先查看日志！！</p>

<h1>最佳实践</h1>

<p>官网推荐使用 站点管理工具 统一来安装管理，如puppet。下面使用puppet来配置mcollective：</p>

<ul>
<li><a href="https://docs.puppet.com/mcollective/deploy/install.html#example">https://docs.puppet.com/mcollective/deploy/install.html#example</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/middleware/activemq_keystores.html#creating-keystores-with-puppet">https://docs.puppet.com/mcollective/deploy/middleware/activemq_keystores.html#creating-keystores-with-puppet</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/standard.html#write-the-server-config-file">https://docs.puppet.com/mcollective/deploy/standard.html#write-the-server-config-file</a></li>
</ul>


<p>TODO</p>

<p></p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puppetexplorer设置]]></title>
    <link href="http://winseliu.com/blog/2016/04/21/puppetexplorer-setting/"/>
    <updated>2016-04-21T14:28:11+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/21/puppetexplorer-setting</id>
    <content type="html"><![CDATA[<p>注意： 使用 <a href="https://github.com/spotify/puppetexplorer/releases">PuppetExplorer</a> 的前提是已经安装 PuppetDB （安装参考：<a href="/blog/2016/04/21/puppetdb-install-and-config/">Puppetdb安装配置</a>）。</p>

<p>PuppetDB 提供的8080界面太过于简单，其实8080主要提供非常多的接口。PuppetExplorer 就是使用这些 restful 查询接口来进行展示。比默认的 PuppetDB-UI 更具体和详细。</p>

<p>配置 PuppetExplorer 有两种方式：</p>

<ul>
<li>两个服务在 <strong>同一个域</strong> 下面，配置 /api 跳转到 PuppetDB:8080</li>
<li>两个服务，<strong>配置各自的地址</strong> 。修改config.js，同时处理跨域的问题。</li>
</ul>


<blockquote><p><a href="https://github.com/spotify/puppetexplorer">https://github.com/spotify/puppetexplorer</a></p>

<ul>
<li>The recommended way to install it is on the same host as your PuppetDB instance. Then proxy /api to port 8080 of your PuppetDB instance (except the /commands endpoint). This avoids the need for any CORS headers.</li>
<li>It is possible to have it on a separate domain from your PuppetDB though. If you do, make sure you have the correct Access-Control-Allow-Origin header and a Access-Control-Expose-Headers: X-Records header.</li>
</ul>
</blockquote>

<h2>适配 PuppetDB4</h2>

<p>官网的版本已经几个月没有更新，新的 API 接口略有不同：</p>

<pre><code># puppetdb-4.0
/metrics/v1/mbeans/puppetlabs.puppetdb.population:name=num-active-nodes

# puppetexplorer-2.0.0
/metrics/v1/mbeans/puppetlabs.puppetdb.query.population:type=default,name=num-nodes
</code></pre>

<p>修改 app.js 拼接链接的字符串即可，删除 <strong>.query.</strong> 和 <strong>type=default</strong> :</p>

<p><img src="/images/blogs/puppetdb4-puppetexplorer.png" alt="" /></p>

<p>配置好后的效果：</p>

<p><img src="/images/blogs/puppetexplorer.png" alt="" /></p>

<h1>同一服务器下访问配置</h1>

<p>使用 nginx 作为html的服务器，同时 proxy_pass 代理跳转到 cu3:8080(PuppetDB服务) :</p>

<pre><code>[hadoop@cu2 puppetexplorer-2.0.0]$ mv config.js.example config.js

[hadoop@cu2 nginx]$ vi conf/nginx.conf
...
# 路径最后带上 / ！！
location /api/ {
    proxy_pass http://cu3:8080/;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header Host $http_host;
}
location /puppetexplorer {
    alias /opt/puppetlabs/puppetexplorer-2.0.0;
}

[hadoop@cu2 nginx]$ sbin/nginx -s reload
</code></pre>

<p>然后打开网页访问 <a href="http://cu2:8888/puppetexplorer">http://cu2:8888/puppetexplorer</a> 即可。</p>

<p>nginx的配置参考： <a href="http://wangwei007.blog.51cto.com/68019/1103734">Nginx配置proxy_pass转发的/路径问题</a>, <a href="http://stackoverflow.com/questions/20730858/how-do-i-configure-nginx-as-proxy-to-jetty">nginx as proxy to jetty</a></p>

<h1>不同服务器，跨域访问</h1>

<p>老实说，完全不推荐这种做法。但是跨域的设置震惊到我了，原来自认为的方式完全不对。例如A javascript访问B，<strong>跨域头设置在B服务</strong>，是要B容许A访问！！</p>

<pre><code>[root@hadoop-master2 puppetexplorer]# vi config.js 
// List of PuppetDB servers, pairs of name, URL and $http config object
// The first one will be used as the default server
PUPPETDB_SERVERS = [
  ['production', 'http://cu2:8888'],
  ['testing', 'http://cu2:8888']
];

# Nginx配置，加上跨域访问源范围控制
location ~ /(metrics|pdb) {
add_header "Access-Control-Allow-Origin" "*";
proxy_pass http://cu3:8080;
}
</code></pre>

<p></p>

<h2>参考</h2>

<ul>
<li><a href="http://www.html5rocks.com/en/tutorials/cors/?redirect_from_locale=zh">http://www.html5rocks.com/en/tutorials/cors/?redirect_from_locale=zh</a></li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS#Requests_with_credentials">https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS#Requests_with_credentials</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cross-origin_resource_sharing">https://en.wikipedia.org/wiki/Cross-origin_resource_sharing</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puppetdb安装配置]]></title>
    <link href="http://winseliu.com/blog/2016/04/21/puppetdb-install-and-config/"/>
    <updated>2016-04-21T00:39:11+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/21/puppetdb-install-and-config</id>
    <content type="html"><![CDATA[<p>安装 PuppetDB 后，还得修改 PuppetServer 的配置。由于测试环境机器硬件一般般，把 PuppetDB 安装在 cu3。</p>

<ul>
<li>cu2: master server, ca server, postgresql</li>
<li>cu3: puppetdb, agent</li>
</ul>


<pre><code>[root@cu3 puppet]# puppetdb -v
puppetdb version: 4.0.0

[root@cu2 ~]# puppetserver -v
puppetserver version: 2.3.1
[root@cu2 ~]# puppet -V
4.4.1
</code></pre>

<p>原来老的版本有资源(清单)导出的功能，到了Puppet4后被PuppetDB取代了。见官网文档: <a href="https://docs.puppet.com/guides/inventory_service.html">Inventory Service</a></p>

<p>同时老版本用ruby写的 puppet-dashboard 也没有必要安装了，前后端分离大势所趋：后端提供接口，前端用ajax来展现。</p>

<h1>安装PuppetDB</h1>

<p><a href="https://docs.puppetlabs.com/puppetdb/latest/install_from_packages.html">https://docs.puppetlabs.com/puppetdb/latest/install_from_packages.html</a></p>

<p>由于天朝特殊环境，本地repo的创建参考第一篇文章: <a href="/blog/2016/04/08/puppet-install">puppet4.4.1入门安装</a></p>

<pre><code>[root@cu3 ~]# yum install puppetdb
Loaded plugins: fastestmirror
Setting up Install Process
Loading mirror speeds from cached hostfile
 * epel: ftp.cuhk.edu.hk
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package puppetdb.noarch 0:4.0.0-1.el6 will be installed
--&gt; Processing Dependency: java-1.8.0-openjdk-headless for package: puppetdb-4.0.0-1.el6.noarch
--&gt; Running transaction check
---&gt; Package java-1.8.0-openjdk-headless.x86_64 1:1.8.0.77-0.b03.el6_7 will be installed
--&gt; Processing Dependency: tzdata-java &gt;= 2014f-1 for package: 1:java-1.8.0-openjdk-headless-1.8.0.77-0.b03.el6_7.x86_64
--&gt; Processing Dependency: jpackage-utils for package: 1:java-1.8.0-openjdk-headless-1.8.0.77-0.b03.el6_7.x86_64
--&gt; Running transaction check
---&gt; Package jpackage-utils.noarch 0:1.7.5-3.14.el6 will be installed
---&gt; Package tzdata-java.noarch 0:2016c-1.el6 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

===========================================================================================================================================================================================
 Package                                                Arch                              Version                                            Repository                               Size
===========================================================================================================================================================================================
Installing:
 puppetdb                                               noarch                            4.0.0-1.el6                                        puppet-local                             21 M
Installing for dependencies:
 java-1.8.0-openjdk-headless                            x86_64                            1:1.8.0.77-0.b03.el6_7                             updates                                  32 M
 jpackage-utils                                         noarch                            1.7.5-3.14.el6                                     base                                     60 k
 tzdata-java                                            noarch                            2016c-1.el6                                        updates                                 179 k

Transaction Summary
===========================================================================================================================================================================================
Install       4 Package(s)

Total size: 53 M
Total download size: 53 M
Installed size: 126 M
Is this ok [y/N]: y
Downloading Packages:
(1/3): java-1.8.0-openjdk-headless-1.8.0.77-0.b03.el6_7.x86_64.rpm                                                                                                  |  32 MB     00:00     
(2/3): puppetdb-4.0.0-1.el6.noarch.rpm                                                                                                                              |  21 MB     00:00     
(3/3): tzdata-java-2016c-1.el6.noarch.rpm                                                                                                                           | 179 kB     00:00     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                                                       32 MB/s |  53 MB     00:01     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : tzdata-java-2016c-1.el6.noarch                                                                                                                                          1/4 
  Installing : jpackage-utils-1.7.5-3.14.el6.noarch                                                                                                                                    2/4 
  Installing : 1:java-1.8.0-openjdk-headless-1.8.0.77-0.b03.el6_7.x86_64                                                                                                               3/4 
  Installing : puppetdb-4.0.0-1.el6.noarch                                                                                                                                             4/4 
Config archive not found. Not proceeding with migration
PEM files in /etc/puppetlabs/puppetdb/ssl are missing, we will move them into place for you
Warning: Unable to find all puppet certificates to copy

  This tool requires the following certificates to exist:

  * /etc/puppetlabs/puppet/ssl/certs/ca.pem
  * /etc/puppetlabs/puppet/ssl/private_keys/cu3.eshore.cn.pem
  * /etc/puppetlabs/puppet/ssl/certs/cu3.eshore.cn.pem

  These files may be missing due to the fact that your host's Puppet
  certificates may not have been signed yet, probably due to the
  lack of a complete Puppet agent run. Try running puppet first, for
  example:

      puppet agent --test

  Afterwards re-run this tool then restart PuppetDB to complete the SSL
  setup:

      puppetdb ssl-setup -f
  Verifying  : jpackage-utils-1.7.5-3.14.el6.noarch                                                                                                                                    1/4 
  Verifying  : tzdata-java-2016c-1.el6.noarch                                                                                                                                          2/4 
  Verifying  : puppetdb-4.0.0-1.el6.noarch                                                                                                                                             3/4 
  Verifying  : 1:java-1.8.0-openjdk-headless-1.8.0.77-0.b03.el6_7.x86_64                                                                                                               4/4 

Installed:
  puppetdb.noarch 0:4.0.0-1.el6                                                                                                                                                            

Dependency Installed:
  java-1.8.0-openjdk-headless.x86_64 1:1.8.0.77-0.b03.el6_7                   jpackage-utils.noarch 0:1.7.5-3.14.el6                   tzdata-java.noarch 0:2016c-1.el6                  

Complete!
</code></pre>

<p>PuppetDB 需要与 puppetserver 通信，需要签名证书。如果安装之前本机 Puppet-agent 证书已签名，安装会自动把证书拷贝到 puppetdb/ssl 目录下。我们这里先签名agent再配置 puppetdb-ssl 。</p>

<pre><code>[root@cu3 ~]# puppet agent --server cu2.eshore.cn --test
Info: Creating a new SSL key for cu3.eshore.cn
Info: Caching certificate for ca
Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
Info: Creating a new SSL certificate request for cu3.eshore.cn
Info: Certificate Request fingerprint (SHA256): 16:CB:A3:6D:21:69:78:D0:0D:37:1F:A7:C1:86:2E:55:7F:B1:60:77:05:EC:F5:37:81:12:28:73:61:1A:4F:20
Info: Caching certificate for ca
Exiting; no certificate found and waitforcert is disabled

# 服务端签名: puppet cert sign cu3.eshore.cn

[root@cu3 ~]# puppet agent --server cu2.eshore.cn --test
Info: Caching certificate for cu3.eshore.cn
Info: Caching certificate_revocation_list for ca
Info: Caching certificate for cu3.eshore.cn
Info: Using configured environment 'production'
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for cu3.eshore.cn
Info: Applying configuration version '1461159906'
Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml
Notice: Applied catalog in 0.02 seconds
[root@cu3 ~]# puppetdb ssl-setup -f
PEM files in /etc/puppetlabs/puppetdb/ssl are missing, we will move them into place for you
Copying files: /etc/puppetlabs/puppet/ssl/certs/ca.pem, /etc/puppetlabs/puppet/ssl/private_keys/cu3.eshore.cn.pem and /etc/puppetlabs/puppet/ssl/certs/cu3.eshore.cn.pem to /etc/puppetlabs/puppetdb/ssl
Backing up /etc/puppetlabs/puppetdb/conf.d/jetty.ini to /etc/puppetlabs/puppetdb/conf.d/jetty.ini.bak.1461159930 before making changes
Updated default settings from package installation for ssl-host in /etc/puppetlabs/puppetdb/conf.d/jetty.ini.
Updated default settings from package installation for ssl-port in /etc/puppetlabs/puppetdb/conf.d/jetty.ini.
Updated default settings from package installation for ssl-key in /etc/puppetlabs/puppetdb/conf.d/jetty.ini.
Updated default settings from package installation for ssl-cert in /etc/puppetlabs/puppetdb/conf.d/jetty.ini.
Updated default settings from package installation for ssl-ca-cert in /etc/puppetlabs/puppetdb/conf.d/jetty.ini.
[root@cu3 ~]# 
</code></pre>

<h1>安装Postgres</h1>

<p>配置好 ssl 后，下一步就是连接数据库。puppet4.4 默认配置里面只有 postgres 数据库。直接用 yum 安装，这里简单列出配置过程。</p>

<p><a href="https://docs.puppetlabs.com/puppetdb/latest/configure.html#using-postgresql">https://docs.puppetlabs.com/puppetdb/latest/configure.html#using-postgresql</a></p>

<pre><code>[root@cu2 ~]# yum localinstall http://yum.postgresql.org/9.4/redhat/rhel-6-x86_64/pgdg-centos94-9.4-1.noarch.rpm
[root@cu2 ~]# yum install postgresql94-server
[root@cu2 ~]# yum install postgresql94-contrib

[root@cu2 ~]# service postgresql-9.4 initdb
Initializing database:                                     [  OK  ]
[root@cu2 ~]# service postgresql-9.4 status
postgresql-9.4 is stopped
[root@cu2 ~]# service postgresql-9.4 start
Starting postgresql-9.4 service:                           [  OK  ]


# 先查看 PGDATA 的目录！！
[root@cu2 data]# grep "PGDATA=" /etc/init.d/postgresql-9.4 
PGDATA=/usr/local/pgsql/data
OLDPGDATA=` sed -n 's/^PGDATA=//p' /etc/init.d/postgresql-$PGPREVMAJORVERSION`
NEWPGDATA=` sed -n 's/^PGDATA=//p' /etc/init.d/postgresql-$PGMAJORVERSION`


# 切换到 postgres 用户，先验证环境变量 PGDATA 是否正确！！否则自己修改 .bash_profile 文件！！
[root@cu2 puppet]# su - postgres
-bash-4.1$ echo $PGDATA
/usr/local/pgsql/data

# 创建用户
-bash-4.1$ createuser -DRSP puppetdb
Enter password for new role: 
Enter it again: 
-bash-4.1$ 
-bash-4.1$ createdb -E utf8 -O puppetdb puppetdb

-bash-4.1$ psql puppetdb -c 'create extension pg_trgm'
CREATE EXTENSION

# 配置连接选项（相当于mysql的privilege）
-bash-4.1$ vi $PGDATA/pg_hba.conf 
host    all             all              0.0.0.0/0               md5

# 重启
[root@cu2 puppet]# service postgresql-9.4 restart
Stopping postgresql-9.4 service:                           [  OK  ]
Starting postgresql-9.4 service:                           [  OK  ]

# 测试 
[root@cu2 puppet]# psql -h localhost puppetdb puppetdb
psql (9.4.5)
Type "help" for help.

puppetdb=&gt; 
puppetdb=&gt; \q
</code></pre>

<p>查看 postgres 的端口:</p>

<pre><code>[root@cu2 puppet]# netstat -anp | grep post
tcp        0      0 0.0.0.0:5432                0.0.0.0:*                   LISTEN      8126/postmaster     
tcp        0      0 :::5432                     :::*                        LISTEN      8126/postmaster     
udp        0      0 ::1:39400                   ::1:39400                   ESTABLISHED 8126/postmaster     
unix  2      [ ACC ]     STREAM     LISTENING     954965338 8126/postmaster     /tmp/.s.PGSQL.5432

# 有客户端连上来后：
[root@cu2 ~]# netstat -anp | grep post
tcp        0      0 0.0.0.0:5432                0.0.0.0:*                   LISTEN      8126/postmaster     
tcp        0      0 192.168.0.214:5432          192.168.0.148:60626         ESTABLISHED 20589/postgres 
...
</code></pre>

<h1>启动PuppetDB</h1>

<pre><code>[root@cu3 ~]# vi /etc/puppetlabs/puppetdb/conf.d/database.ini 
[database]
classname = org.postgresql.Driver
subprotocol = postgresql

# The database address, i.e. //HOST:PORT/DATABASE_NAME
subname = //cu2:5432/puppetdb

# Connect as a specific user
username = puppetdb

# Use a specific password
password = puppetdb

# How often (in minutes) to compact the database
# gc-interval = 60
# 通过api/name=num-active-nodes查询不到了，但是pgsql数据库中还没有删除。也可以通过 puppet node deactivate 手动执行
# node-ttl = 30d
# 默认没有设置，disabled。格式与node-ttl一样
# node-purge-ttl = 
# report-ttl = 14d

# Number of seconds before any SQL query is considered 'slow'; offending
# queries will not be interrupted, but will be logged at the WARN log level.
log-slow-statements = 10


# 注意修改，不然web-ui就只能localhost访问了！！
[root@cu3 ~]# vi /etc/puppetlabs/puppetdb/conf.d/jetty.ini
...
host = 0.0.0.0

# JVM 参数修改
[root@cu3 ~]# less /etc/sysconfig/puppetdb 

[root@cu3 ~]# service puppetdb start
Starting puppetdb:                                         [  OK  ]
[root@cu3 ~]# 
[root@cu3 ~]# service puppetdb status
puppetdb (pid  8452) is running...

# 8081 为 puppetserver 写数据的https接口。8080 为http web-ui端口
[root@cu3 ~]# netstat -anp | grep 8081
tcp        0      0 :::8081                     :::*                        LISTEN      8794/java           
</code></pre>

<p>查看 8080 端口通过网页查看集群的状态，现在还什么数据都获取不到，需要配置服务端把数据发送给puppetdb。</p>

<h1>服务端配置</h1>

<p><a href="https://docs.puppet.com/puppetdb/latest/connect_puppet_master.html">https://docs.puppet.com/puppetdb/latest/connect_puppet_master.html</a></p>

<pre><code># 安装Plug-in
# 服务端还要安装 puppetdb-termini ，不然会报错。
[root@cu2 puppet]# yum install puppetdb-termini
Loaded plugins: fastestmirror, priorities
Setting up Install Process
Loading mirror speeds from cached hostfile
 * epel: mirrors.opencas.cn
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package puppetdb-termini.noarch 0:3.2.4-1.el6 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

==========================================================================================================================================================================
 Package                                      Arch                               Version                                   Repository                                Size
==========================================================================================================================================================================
Installing:
 puppetdb-termini                             noarch                             3.2.4-1.el6                               puppet-local                              25 k

Transaction Summary
==========================================================================================================================================================================
Install       1 Package(s)

Total download size: 25 k
Installed size: 69 k
Is this ok [y/N]: y
Downloading Packages:
puppetdb-termini-3.2.4-1.el6.noarch.rpm                                                                                                            |  25 kB     00:00     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : puppetdb-termini-3.2.4-1.el6.noarch                                                                                                                    1/1 
  Verifying  : puppetdb-termini-3.2.4-1.el6.noarch                                                                                                                    1/1 

Installed:
  puppetdb-termini.noarch 0:3.2.4-1.el6                                                                                                                                   

Complete!


# 注意这里URL的域名，要与CA中的名称对应！！ 设置成 cu3 是不正确的！！
[root@cu2 puppet]# vi puppetdb.conf 
[main]
server_urls = https://cu3.eshore.cn:8081

[root@cu2 puppet]# vi puppet.conf 
# This file can be used to override the default puppet settings.
# See the following links for more details on what settings are available:
# - https://docs.puppetlabs.com/puppet/latest/reference/config_important_settings.html
# - https://docs.puppetlabs.com/puppet/latest/reference/config_about_settings.html
# - https://docs.puppetlabs.com/puppet/latest/reference/config_file_main.html
# - https://docs.puppetlabs.com/puppet/latest/reference/configuration.html
[master]
vardir = /opt/puppetlabs/server/data/puppetserver
logdir = /var/log/puppetlabs/puppetserver
rundir = /var/run/puppetlabs/puppetserver
pidfile = /var/run/puppetlabs/puppetserver/puppetserver.pid
codedir = /etc/puppetlabs/code

storeconfigs = true
storeconfigs_backend = puppetdb
reports = store,puppetdb

[root@cu2 puppet]# puppet master --configprint route_file
/etc/puppetlabs/puppet/routes.yaml

[root@hadoop-master2 puppet]# cat routes.yaml 
---
master:
  facts:
    terminus: puppetdb
    cache: yaml

[root@cu2 puppet]# service puppetserver restart
Stopping puppetserver:                                     [  OK  ]
Starting puppetserver:                                     [  OK  ]

[root@cu2 puppet]# puppet agent --server cu2.eshore.cn --test 
Info: Using configured environment 'production'
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for cu2.eshore.cn
Info: Applying configuration version '1461162748'
Notice: Applied catalog in 0.01 seconds
</code></pre>

<p>如果 puppet-agent 服务没有启动，分别在各台机器上面执行 &ndash;test 连一下 PuppetServer，就可以在8080 puppetdb页面看到主机的数量了。</p>

<p><img src="/images/blogs/puppetdb-ui.png" alt="" /></p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
