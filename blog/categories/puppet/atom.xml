<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Puppet | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/puppet/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2018-01-20T19:13:16+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Puppet批量自动化部署实战]]></title>
    <link href="http://winseliu.com/blog/2017/05/05/puppet-automate-deploy-hosts/"/>
    <updated>2017-05-05T08:33:37+08:00</updated>
    <id>http://winseliu.com/blog/2017/05/05/puppet-automate-deploy-hosts</id>
    <content type="html"><![CDATA[<p>断断续续使用Puppet近一年，多次体验到Puppet的强大：SSH更新、需ROOT权限批量处理等等。这次集群新上架了又爽了一把。把整个过程记录下来，方便今后参考。</p>

<p>运维的同事也想了解puppet，在docker容器上安装了一遍，把具体的内容附上：<a href="/files/expect+puppet.txt">expect+puppet.txt</a></p>

<p>这次操作是对以前零零碎碎积累的一次检验和温习。需要用到的工具比较多：</p>

<ul>
<li>RPM打包、本地YUM仓库 - RPMBUILD、CREATEREPO</li>
<li>SSH无密钥登录 - EXPECT&amp;FOR</li>
<li>时间同步、host配置 - SCP、SSH&amp;FOR</li>
<li>创建用户、新用户无密钥等 - PUPPET</li>
<li>ssh_known_hosts - PUPPETDB</li>
<li>rhel.repo、gmond、时区设置 - PUPPET</li>
</ul>


<p>远程配置机器首先当然是进行无密钥登录的设置，这样才能进行批量操作，不然几百台机器每次都需要干预太烦人、工作量太大。无密钥登录使用原来写好的EXPECT脚本，使用FOR循环执行，等待结果即可。</p>

<pre><code>[root@hadoop-master1 ~]# cat ssh-copy-id.expect 
#!/usr/bin/expect  

## Usage $0 [user@]host password

set host [lrange $argv 0 0];
set password [lrange $argv 1 1] ;

set timeout 30;

spawn ssh-copy-id $host ;

expect {
  "(yes/no)?" { send yes\n; exp_continue; }
  "password:" { send $password\n; exp_continue; }
}

exec sleep 1;

# 用for，不要用while
for h in `cat /etc/hosts | grep -v '^#' | grep slaver | grep -E '\.36\.|\.37\.' | awk '{print $2}' ` ; do 
  ./ssh-copy-id.expect $h 'PASSWD';
done
</code></pre>

<p>做好无密钥登录，拷贝 /etc/hosts, /etc/cron.daily/ntp.cron, /etc/yum.repos.d/puppet.repo 到全部的新机器。这里puppet.repo是自己编译搭建的私有仓库（具体编译配置步骤查看puppet分类下的文章），通过 <code>yum install mcollective-plugins-simple</code> 就可以把mcolletive和puppet-agent安装好。把所有步骤封装到一个prepare.sh脚本，内容如下：</p>

<pre><code>#!/bin/sh

# must be hostname!!
HOSTS="$@"
PASSWD=${PASSWD:-'root'}
PUPPETSERVER="hadoop-master1"

for h in $HOSTS ; do ./ssh-copy-id.expect $h "$PASSWD" ; done

for h in $HOSTS ; do
scp /etc/hosts $h:/etc ;
scp /etc/yum.repos.d/puppet.repo $h:/etc/yum.repos.d/ ;
scp /etc/cron.daily/ntp.cron $h:/etc/cron.daily/ ;

ssh $h '
#ntpdate cu-omc1 #着重注意
rm -rf /etc/yum.repos.d/CentOS-*
yum install mcollective-plugins-simple -y
' ;

scp /etc/puppetlabs/mcollective/server.cfg $h:/etc/puppetlabs/mcollective/
ssh $h "
sed -i '/HOSTNAME/ {
i \
HOSTNAME=$h
d
} ' /etc/sysconfig/network
hostname $h

echo -e '\n\n[agent]\nserver = $PUPPETSERVER\ncertname=$h' &gt; /etc/puppetlabs/puppet/puppet.conf
chkconfig mcollective on
service mcollective start
"

done
</code></pre>

<p>然后执行 <code>./prepare.sh hadoop-slaver{200..500}</code> 就可以了。</p>

<p>接下来重点讲讲PUPPET配置的编写。</p>

<p>首先根据当前需要创建的用户、组把创建用户的配置写好：</p>

<ul>
<li><a href="https://docs.puppet.com/puppet/4.10/quick_start_user_group.html">https://docs.puppet.com/puppet/4.10/quick_start_user_group.html</a></li>
</ul>


<pre><code>[root@hadoop-master1 ~]# puppet resource -e group hadoop
group { 'hadoop':
  ensure =&gt; 'present',
  gid    =&gt; '501',
}
[root@hadoop-master1 ~]# puppet resource -e user hadoop
user { 'hadoop':
  ensure           =&gt; 'present',
  gid              =&gt; '501',
  groups           =&gt; ['wheel'],
  home             =&gt; '/home/hadoop',
  password         =&gt; '$6$AfnA...uIhHC9I.',
  password_max_age =&gt; '99999',
  password_min_age =&gt; '0',
  shell            =&gt; '/bin/bash',
  uid              =&gt; '501',
}
</code></pre>

<p>添加require、groups，然后删除uid、gid。最后需要添加 managehome => true, 否则用户目录就不会自动创建：</p>

<ul>
<li><a href="http://www.dbalex.com/category/devops/puppet">http://www.dbalex.com/category/devops/puppet</a></li>
</ul>


<pre><code># 默认不创建用户目录
[root@hadoop-slaver200 ~]# su - hadoop
su: warning: cannot change directory to /home/hadoop: No such file or directory
-bash-4.1$ 

# 创建用户配置成品
group { 'hadoop':
  ensure =&gt; 'present',
}

user { 'hadoop':
  ensure           =&gt; 'present',
  groups           =&gt; ['hadoop', 'wheel'],
  home             =&gt; '/home/hadoop',
  password         =&gt; '$6$Af...IhHC9I.',
  password_max_age =&gt; '99999',
  password_min_age =&gt; '0',
  shell            =&gt; '/bin/bash',
  managehome       =&gt; true,
  require          =&gt; Group['hadoop'],
}
</code></pre>

<ul>
<li><a href="https://ask.puppet.com/question/15753/how-can-i-chown-directories-recursivley/">https://ask.puppet.com/question/15753/how-can-i-chown-directories-recursivley/</a></li>
<li><a href="https://serverfault.com/questions/542947/issue-with-changing-permission-and-owner-recursively-on-files-with-puppet-and-va">https://serverfault.com/questions/542947/issue-with-changing-permission-and-owner-recursively-on-files-with-puppet-and-va</a></li>
<li><a href="https://serverfault.com/questions/416254/adding-an-existing-user-to-a-group-with-puppet">https://serverfault.com/questions/416254/adding-an-existing-user-to-a-group-with-puppet</a></li>
</ul>


<p>添加好用户后，就是把无密钥登录也让PUPPET来弄。其实就是把 id_rsa.pub 的内容写入都行机器的 authorized_keys ，PUPPET已经自带了这个类：ssh_authorized_key。把id_ras.pub的内容（中间的内容）赋值给 key 属性即可。</p>

<pre><code>ssh_authorized_key {'root@hadoop-master1':
  user =&gt; 'root',
  type =&gt; 'ssh-rsa',
  key =&gt; 'AAAAB3NzaC1y...O1Q==',
}

ssh_authorized_key {'hadoop@hadoop-master1':
  user =&gt; 'hadoop',
  type =&gt; 'ssh-rsa',
  key =&gt; 'AAAAB3Nza...IZYPw==',
  require  =&gt; User['hadoop'],
}
</code></pre>

<p>无密钥登录比较容易，没有涉及到收集节点信息。仅仅把公钥写入新机器还不够，还得把 known_hosts 也处理好，不然第一次连接新机器都需要输入一下yes。内容如下：</p>

<pre><code>[hadoop@hadoop-slaver200 ~]$ ssh hadoop-slaver202
The authenticity of host 'hadoop-slaver202 (192.168.36.59)' can't be established.
RSA key fingerprint is fe:7e:26:c4:56:ea:f4:21:61:82:6d:9b:4a:72:93:a4.
Are you sure you want to continue connecting (yes/no)? 
</code></pre>

<ul>
<li><a href="https://docs.puppet.com/puppet/4.4/lang_virtual.html">https://docs.puppet.com/puppet/4.4/lang_virtual.html</a></li>
<li><a href="https://docs.puppet.com/puppet/4.4/lang_collectors.html">https://docs.puppet.com/puppet/4.4/lang_collectors.html</a></li>
<li><a href="https://docs.puppet.com/puppet/4.4/lang_exported.html">https://docs.puppet.com/puppet/4.4/lang_exported.html</a></li>
<li><a href="https://docs.puppet.com/puppet/4.4/lang_resources_advanced.html#amending-attributes-with-a-collector">https://docs.puppet.com/puppet/4.4/lang_resources_advanced.html#amending-attributes-with-a-collector</a></li>
<li><a href="https://docs.puppet.com/puppet/latest/types/ssh_authorized_key.html">https://docs.puppet.com/puppet/latest/types/ssh_authorized_key.html</a></li>
<li><a href="https://www.puppetcookbook.com/posts/install-package.html">https://www.puppetcookbook.com/posts/install-package.html</a></li>
<li><a href="https://docs.puppet.com/puppet/4.10/lang_conditional.html">https://docs.puppet.com/puppet/4.10/lang_conditional.html</a></li>
</ul>


<p>正如上面官网介绍的，需要用到虚拟资源，自动把新机器指纹（fingerprint）写入到机器需要PUPPETDB的支持，安装配置又需要PGSQL的配合。需要耗费一番功夫，但是还是划得来的（具体安装步骤查看puppet分类下的文章）。</p>

<pre><code>if $hostname =~ /^hadoop-/ {

  $host_aliases = [ $ipaddress, $hostname ]

  # Export hostkeys from all hosts.
  @@sshkey { $::fqdn:
    ensure =&gt; present,
    host_aliases =&gt; $host_aliases,
    type =&gt; 'ssh-rsa',
    key =&gt; $sshrsakey,
  }

  if $hostname =~ /^hadoop-master/ {
    # realize all exported
    Sshkey &lt;&lt;| |&gt;&gt;
  }

}
</code></pre>

<p>先在所有slaver机器运行一遍 puppet agent -t ，然后再在master节点把收集的指纹写入到 /etc/ssh/ssh_known_hosts 。</p>

<p>这里说个插曲：机器的hosts和hostname是通过 FOR&amp;SSH 命令来统一修改的，有些可能没有配置好导致机器的主机名有重复。通过执行配置known_hosts竟然帮我找出了hostname重复的机器，意外的收获。该问题的处理我是直接登录到PGSQL改了对应表的数据处理的。</p>

<p>到这里机器基本能用了。主机名、hosts、时间同步、hadoop用户以及master到该用户的无密钥登录都已经配置好了。</p>

<p></p>

<p>接下来把实战过程中安装gmond的步骤帖出来：</p>

<pre><code>$$ cd /etc/puppetlabs/code/environments/production/manifests/

$$ vi change_site.sh
#!/bin/sh

## Usage:
##  ./change_site.sh nrpe.site
##

[[ $# != 1 ]] &amp;&amp; exit 1

cd $(cd $(dirname $0); pwd)

rm -rf site.pp
ln -s $1 site.pp

$$ vi pexec.sh
#!/bin/sh

## Usage:
##   ./pexec.sh /cu-ud/ sudo_revert.site 
##

case $# in
1)
  FUNC="$1"
  HOST_PARAM=
  ;;
2)
  FUNC="$2"
  HOST_PARAM="-I $1"
  ;;
*)
  while [ $# -gt 1 ] ; do 
    HOST_PARAM="$HOST_PARAM -I $1"
    shift
  done
  FUNC=$1
  ;;
esac

cd $(cd $(dirname $0); pwd)

./change_site.sh "$FUNC"

if [[ "$HOST_PARAM" != "" &amp;&amp; ! "$HOST_PARAM" =~ */* ]] ; then
  mco shell $HOST_PARAM run -- `which puppet` agent -t
else
  mco puppet $HOST_PARAM runall 20
fi
</code></pre>

<p>由于机器增加比较多，且网络环境变的复杂化。把原来的2个分组修改成4个。不同的网络段和功能分别设置不同的广播端口。</p>

<pre><code>$ ./pexec.sh /hadoop-slaver.$/ gmond.site 

# 采集数据的节点重启后，其他发送数据的节点貌似都需要重启。
$ screen
$ for ((i=1;i&lt;=53;i++)); do  mco shell -I /hadoop-slaver${i}.$/ run -- ' service gmond restart ' ; done 
# 这个确认搞的很麻烦，
# 想通过ganglia-web获取数据然后判断是否有数据进行重启。
</code></pre>

<p>Ganglia删除某节点后，如果要从rrds上去掉改节点的信息，需要：重启对应收集的gmond，对应集群的rrds目录，然后重启gmetad。或者等够一段时间，gmetad会自动去掉。</p>

<h2>总结</h2>

<p>现在添加机器，直接连上puppetserver机器然后执行几个命令就可以搞定；</p>

<pre><code>HOST=new-host-name 
# 无密钥登录和puppet/mco
PASSWD=new-host-root-password ./prepare.sh $HOST

./pexec.sh $HOST new-hadoop.site
./pexec.sh $HOST gmond.site # 当前需要到web界面确认新节点的数据是否被采集
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puppet批量修改用户密码]]></title>
    <link href="http://winseliu.com/blog/2016/09/06/puppet-modify-password/"/>
    <updated>2016-09-06T11:00:57+08:00</updated>
    <id>http://winseliu.com/blog/2016/09/06/puppet-modify-password</id>
    <content type="html"><![CDATA[<ol>
<li>先在一台机器修改成想要修改的密码，然后获取该用户的信息。</li>
</ol>


<pre><code>[root@hadoop-master1 ~]# puppet resource user root
user { 'root':
  ensure           =&gt; 'present',
  comment          =&gt; 'root',
  gid              =&gt; '0',
  home             =&gt; '/root',
  password         =&gt; '$6$C6EXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',
  password_max_age =&gt; '99999',
  password_min_age =&gt; '0',
  shell            =&gt; '/bin/bash',
  uid              =&gt; '0',
}
</code></pre>

<ol>
<li><p>保存到文件 user.pp ，使用 <code>puppet apply user.pp</code> 测试看看文件是否有问题。毕竟是生产，出了问题就要进机房的啊，谨慎点好。</p></li>
<li><p>把用户的资源信息写入的site.pp(不知道是啥的话，去看看puppet的文档先)。先搞几台机器测试下 <code>puppet agent -t</code></p></li>
<li><p>然后使用 <code>mco puppet runall 10</code> 全部同步进行密码修改。或者 <code>mco shell run -- /opt/puppetlabs/bin/puppet agent -t</code></p></li>
</ol>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Puppet安装配置Ganglia]]></title>
    <link href="http://winseliu.com/blog/2016/06/17/ganglia-install-on-centos-with-puppet/"/>
    <updated>2016-06-17T09:30:50+08:00</updated>
    <id>http://winseliu.com/blog/2016/06/17/ganglia-install-on-centos-with-puppet</id>
    <content type="html"><![CDATA[<p>前面写过完全纯手工和用yum安装依赖来安装ganglia的文章，最近生产安装了puppet，既然已经手上已有牛刀，杀鸡就不用再取菜刀了。今天记录下前几天使用puppet安装ganglia的经历。</p>

<h2>前提（自己操作过熟悉怎么用）</h2>

<ul>
<li>配置过私有仓库 (createrepo)</li>
<li>安装好puppet</li>
<li>编译过自己的rpm (rpmbuild)</li>
</ul>


<h2>编译gmetad，gmond，gweb</h2>

<p>点击链接下载SPEC：</p>

<ul>
<li><a href="/files/ganglia-puppet/gmetad.spec">gmetad.spec</a></li>
<li><a href="/files/ganglia-puppet/gmond.spec">gmond.spec</a></li>
<li><a href="/files/ganglia-puppet/gweb.spec">gweb.spec</a></li>
</ul>


<p></p>

<p>然后编译打包：</p>

<p>先手动编译安装 ganglia ，把依赖的问题处理好。编译安装没问题，然后再使用 rpmbuild 编译生成 rpm 包！！</p>

<pre><code># 1&gt; 建立目录结构
mkdir ganglia-build
cd ganglia-build
mkdir BUILD RPMS SOURCES SPECS SRPMS

# 2&gt; 修改配置
# ganglia-web-3.7.1.tar.gz的makefile、conf_default.php.in修改下，根据等下要配置gmetad的参数进行修改

less ganglia-web-3.7.1/Makefile 
    # Location where gweb should be installed to (excluding conf, dwoo dirs).
    GDESTDIR = /var/www/html/ganglia

    # Location where default apache configuration should be installed to.
    GCONFDIR = /usr/local/ganglia/etc/

    # Gweb statedir (where conf dir and Dwoo templates dir are stored)
    GWEB_STATEDIR = /var/www/html/ganglia

    # Gmetad rootdir (parent location of rrd folder)
    GMETAD_ROOTDIR = /data/ganglia

    APACHE_USER = apache

# 连外网太慢，下载放到本地
less ganglia-web-3.7.1/conf_default.php.in 
    #$conf['cubism_js_path'] = "js/cubism.v1.min.js";
    $conf['jquery_js_path'] = "js/jquery.min.js";
    $conf['jquerymobile_js_path'] = "js/jquery.mobile.min.js";
    $conf['jqueryui_js_path'] = "js/jquery-ui.min.js";
    $conf['rickshaw_js_path'] = "js/rickshaw.min.js";
    $conf['cubism_js_path'] = "js/cubism.v1.min.js";
    $conf['d3_js_path'] = "js/d3.min.js";
    $conf['protovis_js_path'] = "js/protovis.min.js";

# 3&gt; 源文件
# 把文件放到SOURCES目录下，
ls SOURCES/
    ganglia-3.7.2.tar.gz  ganglia-web-3.7.1.tar.gz

# 4&gt; 编译生成RPM
rpmbuild -v -ba SPECS/gmetad.spec 
rpmbuild -v -ba SPECS/gmond.spec 
rpmbuild -v -ba SPECS/gweb.spec 

# 5&gt; 查看内容
rpm -qpl RPMS/x86_64/ganglia-3.7.2-1.el6.x86_64.rpm 
</code></pre>

<h2>本地仓库</h2>

<p>这里假设已经把系统光盘做成了本地仓库。</p>

<p><strong>先安装httpd、php、createrepo</strong>，然后按照下面的步骤创建本地仓库：</p>

<pre><code># 系统带的可以从光盘拷贝，直接映射到httpd的目录下即可
[hadoop@hadoop-master1 rhel6.3]$ ls 
Packages  repodata
[hadoop@hadoop-master1 html]$ pwd
/var/www/html
[hadoop@hadoop-master1 html]$ ll
lrwxrwxrwx.  1 root root   20 2月  15 2014 rhel6.3 -&gt; /opt/rhel6.3

[hadoop@hadoop-master1 ~]$ sudo mkdir -p /opt/dta/repo
[hadoop@hadoop-master1 ~]$ cd /opt/dta/repo
[hadoop@hadoop-master1 repo]$ ls *.rpm
gmetad-3.7.2-1.el6.x86_64.rpm  gmond-3.7.2-1.el6.x86_64.rpm  gweb-3.7.1-1.el6.x86_64.rpm  libconfuse-2.7-4.el6.x86_64.rpm

[hadoop@hadoop-master1 repo]$ sudo createrepo .
3/3 - libconfuse-2.7-4.el6.x86_64.rpm                                           
Saving Primary metadata
Saving file lists metadata
Saving other metadata

# 映射到httpd目录下
[hadoop@hadoop-master1 yum.repos.d]$ cd /var/www/html/
[hadoop@hadoop-master1 html]$ sudo ln -s /opt/dta/repo dta

# 加入本地仓库源
[hadoop@hadoop-master1 yum.repos.d]$ sudo cp puppet.repo dta.repo
[hadoop@hadoop-master1 yum.repos.d]$ sudo vi dta.repo 
[dta]
name=DTA Local
baseurl=http://hadoop-master1:801/dta
enabled=1
gpgcheck=0
</code></pre>

<p>注意： 在安装的时刻找不到gmond，可以先清理yum的缓冲： <code>yum clean all</code></p>

<h2>puppet模块</h2>

<p>添加了三个模块，用于主机添加repo配置和sudo配置，以及安装配置gmond。</p>

<pre><code>[root@hadoop-master1 modules]# tree $PWD
/etc/puppetlabs/code/environments/production/modules
├── dtarepo
│   ├── manifests
│   │   └── init.pp
│   └── templates
│       └── dta.repo
├── gmond
│   ├── manifests
│   │   └── init.pp
│   └── templates
│       └── gmond.conf
└── sudo
    ├── manifests
    │   └── init.pp
    └── templates
        └── sudo.erb
</code></pre>

<p>都比较简单，通过init.pp来进行配置，然后加载模板，写入到同步主机本地文件中。</p>

<ul>
<li>dtarepo</li>
</ul>


<pre><code>./dtarepo/manifests/init.pp
class dtarepo {

file{'/etc/yum.repos.d/dta.repo':
  ensure =&gt; file,
  content =&gt; template('dtarepo/dta.repo'),
}

}

./dtarepo/templates/dta.repo
[dta]
name=DTA Local
baseurl=http://hadoop-master1:801/dta
enabled=1
gpgcheck=0
</code></pre>

<ul>
<li>sudo：用于sudo切root，方便调试</li>
</ul>


<pre><code>./sudo/manifests/init.pp
class sudo {

if ( $::hostname =~ /(^cu-omc)/ ) {
  $user = 'omc'
} elsif ( $::hostname =~ /(^cu-uc)/ ) {
  $user = 'uc'
} elsif ( $::hostname =~ /(^cu-ud)/ ) {
  $user = 'ud'
} elsif ( $::hostname =~ /(^cu-db)/ ) {
  $user = 'mysql'
} else {
  $user = 'hadoop'
}


file { "/etc/sudoers.d/10_$user":
  ensure =&gt; file,
  mode =&gt; '0440', 
  content =&gt; template('sudo/sudo.erb'),
}


}

./sudo/templates/sudo.erb
&lt;%= scope.lookupvar('sudo::user') %&gt; ALL=(ALL) NOPASSWD: ALL
</code></pre>

<ul>
<li>gmond</li>
</ul>


<p>在默认的gmond.conf基础上修改一下两个配置: globals.deaf, cluster.name</p>

<pre><code>./gmond/manifests/init.pp
class gmond {

$deaf = $::hostname ? {
  'hadoop-master1' =&gt; 'no',
  'cu-omc1' =&gt; 'no',
  default =&gt; 'yes',
}

if ( $::hostname =~ /(^cu-)/ ) {
  $cluster_name = 'CU'
} else {
  $cluster_name = 'HADOOP'
}

package { 'gmond':
  ensure =&gt; present,
  before =&gt; File['/usr/local/ganglia/etc/gmond.conf'],
}

file { '/usr/local/ganglia/etc/gmond.conf':
  ensure =&gt; file,
  content =&gt; template('gmond/gmond.conf'),
  notify =&gt; Service['gmond'],
}

service { 'gmond':
  ensure    =&gt; running,
  enable    =&gt; true,
}

}

./gmond/templates/gmond.conf
/* This configuration is as close to 2.5.x default behavior as possible
   The values closely match ./gmond/metric.h definitions in 2.5.x */
globals {
...
  mute = no
  deaf = &lt;%= scope.lookupvar('gmond::deaf') %&gt;
  allow_extra_data = yes
...
cluster {
  name = "&lt;%= scope.lookupvar('gmond::cluster_name') %&gt;"
</code></pre>

<p>参考下逻辑即可（也可以通过hiera配置）。</p>

<p>最后在 site.pp 引用加载编写的Module：</p>

<pre><code>[root@hadoop-master1 modules]# cd ../manifests/
[root@hadoop-master1 manifests]# cat site.pp 
file{'/etc/puppetlabs/mcollective/facts.yaml':
  owner    =&gt; root,
  group    =&gt; root,
  mode     =&gt; '400',
  loglevel =&gt; debug, # reduce noise in Puppet reports
  content  =&gt; inline_template("&lt;%= scope.to_hash.reject { |k,v| k.to_s =~ /(uptime_seconds|timestamp|free)/ }.to_yaml %&gt;"), # exclude rapidly changing facts
}


include dtarepo
include gmond

# include sudo
</code></pre>

<h2>一键安装</h2>

<p>安装gmetad：</p>

<p>首先在主机上安装gmetad，由于只需要在一台机器安装，配置没有整成模板，这里直接手动弄。</p>

<pre><code>[root@hadoop-master1 dtarepo]# mco rpc package install package=gmetad -I cu-omc1 （或者直接yum install -y gmetad）

# 注意：主机多网卡时可能需要添加route
[root@cu-omc1 ~]# route add -host 239.2.11.71 dev bond0

[root@cu-omc1 ~]# /etc/ganglia/gmetad.conf 注意!! 这里的rrd_rootdir配置与上面gweb/makefile是对应的！！
data_source "HADOOP" hadoop-master1
data_source "CU" cu-omc1
gridname "CQCU"
rrd_rootdir "/data/ganglia/rrds"
</code></pre>

<p>注意： data_source 需要配合 gmond/manifests/init.pp 中的 deaf 属性值。</p>

<p><strong>php的时区调整</strong>：vi /etc/php.ini date.timezone = &ldquo;Asia/Shanghai&rdquo;</p>

<p>安装gmond：</p>

<p>在cu-omc2上安装gmond（正则表达式，想怎么匹配就怎么写）：</p>

<pre><code>[root@hadoop-master1 production]# mco shell -I /^cu-omc2/ run -- "/opt/puppetlabs/bin/puppet agent -t"
</code></pre>

<p>puppet同步好后，就安装好puppet，以及启动gmond服务。</p>

<p>同时看看web是否已经有图像。<strong>不要看一分钟负载，搞一个明显一点的，如磁盘容量内存容量可以明确判断数据有没有采集到的。</strong> 没有话可以试着重启gmond:</p>

<pre><code>[root@hadoop-master1 production]# mco shell -I /cu-/ run -- service gmond restart
</code></pre>

<p>不要一次重启太多机器，时间比较长的话可以结合screen命令使用：</p>

<pre><code>[root@hadoop-master1 ~]# screen
[root@hadoop-master1 ~]# mco shell -I hadoop-slaver2 -I hadoop-slaver3 -I hadoop-slaver4  -I hadoop-slaver5 -I hadoop-slaver6 -I hadoop-slaver7 -I hadoop-slaver8 -I hadoop-slaver9  run -- service gmond restart ; 
[root@hadoop-master1 ~]# for ((i=1;i&lt;17;i++)) ; do mco shell -I /hadoop-slaver${i}.$/ run -- service gmond restart ; sleep 60 ; done  

[root@hadoop-master1 ~]# screen -ls 
[root@hadoop-master1 ~]# screen -r 22929
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puppetboard Install]]></title>
    <link href="http://winseliu.com/blog/2016/05/05/puppetboard-install/"/>
    <updated>2016-05-05T10:54:26+08:00</updated>
    <id>http://winseliu.com/blog/2016/05/05/puppetboard-install</id>
    <content type="html"><![CDATA[<p>对于我这样的python小白来说，有网络来安装 puppetboard 还是比较容易的（离线安装依赖处理可能比较麻烦）。</p>

<pre><code># https://fedoraproject.org/wiki/EPEL/zh-cn
[root@cu2 ~]# yum search epel
[root@cu2 ~]# yum install epel-release


[root@cu2 ~]# yum repolist
Loaded plugins: fastestmirror, priorities
Loading mirror speeds from cached hostfile
 * base: mirrors.skyshe.cn
 * centosplus: mirrors.pubyun.com
 * epel: mirror01.idc.hinet.net
 * extras: mirrors.skyshe.cn
 * updates: mirrors.skyshe.cn
193 packages excluded due to repository priority protections
repo id                                   repo name                                                                   status
base                                      CentOS-6 - Base                                                                  6,575
centosplus                                CentOS-6 - Centosplus                                                             0+76
epel                                      Extra Packages for Enterprise Linux 6 - x86_64                              12,127+117
extras                                    CentOS-6 - Extras                                                                   62
puppet-local                              Puppet Local                                                                         5
updates                                   CentOS-6 - Updates                                                               1,607
repolist: 20,376


[root@cu2 ~]# yum install python-pip -y


[root@cu2 ~]# pip install puppetboard
/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
You are using pip version 7.1.0, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Collecting puppetboard
/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
  Downloading puppetboard-0.1.3.tar.gz (598kB)
    100% |████████████████████████████████| 602kB 726kB/s 
Collecting Flask&gt;=0.10.1 (from puppetboard)
  Downloading Flask-0.10.1.tar.gz (544kB)
    100% |████████████████████████████████| 544kB 734kB/s 
Collecting Flask-WTF&lt;=0.9.5,&gt;=0.9.4 (from puppetboard)
  Downloading Flask-WTF-0.9.5.tar.gz (245kB)
    100% |████████████████████████████████| 249kB 320kB/s 
Collecting WTForms&lt;2.0 (from puppetboard)
  Downloading WTForms-1.0.5.zip (355kB)
    100% |████████████████████████████████| 356kB 1.3MB/s 
Collecting pypuppetdb&lt;0.3.0,&gt;=0.2.1 (from puppetboard)
  Downloading pypuppetdb-0.2.1.tar.gz
Collecting Werkzeug&gt;=0.7 (from Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading Werkzeug-0.11.9-py2.py3-none-any.whl (306kB)
    100% |████████████████████████████████| 307kB 1.5MB/s 
Collecting Jinja2&gt;=2.4 (from Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB)
    100% |████████████████████████████████| 266kB 2.3MB/s 
Collecting itsdangerous&gt;=0.21 (from Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading itsdangerous-0.24.tar.gz (46kB)
    100% |████████████████████████████████| 49kB 7.2MB/s 
Collecting requests&gt;=1.2.3 (from pypuppetdb&lt;0.3.0,&gt;=0.2.1-&gt;puppetboard)
  Downloading requests-2.10.0-py2.py3-none-any.whl (506kB)
    100% |████████████████████████████████| 507kB 920kB/s 
Collecting MarkupSafe (from Jinja2&gt;=2.4-&gt;Flask&gt;=0.10.1-&gt;puppetboard)
  Downloading MarkupSafe-0.23.tar.gz
Installing collected packages: Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask, WTForms, Flask-WTF, requests, pypuppetdb, puppetboard
  Running setup.py install for MarkupSafe
  Running setup.py install for itsdangerous
  Running setup.py install for Flask
  Running setup.py install for WTForms
  Running setup.py install for Flask-WTF
  Running setup.py install for pypuppetdb
  Running setup.py install for puppetboard
Successfully installed Flask-0.10.1 Flask-WTF-0.9.5 Jinja2-2.8 MarkupSafe-0.23 WTForms-1.0.5 Werkzeug-0.11.9 itsdangerous-0.24 puppetboard-0.1.3 pypuppetdb-0.2.1 requests-2.10.0


[root@cu2 ~]# pip show puppetboard
You are using pip version 7.1.0, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
---
Metadata-Version: 1.0
Name: puppetboard
Version: 0.1.3
Summary: Web frontend for PuppetDB
Home-page: https://github.com/puppet-community/puppetboard
Author: Daniele Sluijters
Author-email: daniele.sluijters+pypi@gmail.com
License: Apache License 2.0
Location: /usr/lib/python2.6/site-packages
Requires: Flask, Flask-WTF, WTForms, pypuppetdb
[root@cu2 ~]# ll /usr/lib/python2.6/site-packages/puppetboard
total 100
-rw-r--r-- 1 root root 31629 May  5 09:12 app.py
-rw-r--r-- 1 root root 30481 May  5 09:12 app.pyc
-rw-r--r-- 1 root root  1206 May  5 09:12 default_settings.py
-rw-r--r-- 1 root root  1477 May  5 09:12 default_settings.pyc
-rw-r--r-- 1 root root  1025 May  5 09:12 forms.py
-rw-r--r-- 1 root root  1982 May  5 09:12 forms.pyc
-rw-r--r-- 1 root root     0 May  5 09:12 __init__.py
-rw-r--r-- 1 root root   143 May  5 09:12 __init__.pyc
drwxr-xr-x 9 root root  4096 May  5 09:12 static
drwxr-xr-x 2 root root  4096 May  5 09:12 templates
-rw-r--r-- 1 root root  2155 May  5 09:12 utils.py
-rw-r--r-- 1 root root  3433 May  5 09:12 utils.pyc


[root@cu2 ~]# pip install uwsgi
You are using pip version 7.1.0, however version 8.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Collecting uwsgi
/usr/lib/python2.6/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
  Downloading uwsgi-2.0.12.tar.gz (784kB)
    100% |████████████████████████████████| 786kB 143kB/s 
Installing collected packages: uwsgi
  Running setup.py install for uwsgi
Successfully installed uwsgi-2.0.12


[root@cu2 ~]# mkdir -p /var/www/puppetboard
[root@cu2 ~]# cd /var/www/puppetboard/
[root@cu2 puppetboard]# cp /usr/lib/python2.6/site-packages/puppetboard/default_settings.py ./settings.py
# 修改配置 
# https://github.com/voxpupuli/puppetboard#settings
PUPPETDB_HOST = 'cu3'
PUPPETDB_PORT = 8080
REPORTS_COUNT = 21
ENABLE_CATALOG = True

[root@cu2 puppetboard]# vi wsgi.py 
from __future__ import absolute_import
import os

os.environ['PUPPETDOARD_SETTINGS'] = '/var/www/puppetboard/settings.py'
from puppetboard.app import app as application


# A 直接用uwsgi-http
# http://yongqing.is-programmer.com/posts/43688.html
[root@cu2 puppetboard]# uwsgi --http :9091 --wsgi-file /var/www/puppetboard/wsgi.py 

# 使用 supervisord 管理
[root@cu2 supervisord.d]# cat uwsgi.ini 
[program:puppetboard]
command=uwsgi --http :9091 --wsgi-file /var/www/puppetboard/wsgi.py 
[root@cu2 supervisord.d]# supervisorctl update


# B nginx + uwsgi-socket
# 需要对应到 / ，新增一个9091的server
[root@cu2 puppetboard]# vi /home/hadoop/nginx/conf/nginx.conf
server {
  listen 9091;

  location /static {
    alias /usr/lib/python2.6/site-packages/puppetboard/static;
  }
  location / {
    include uwsgi_params;
    uwsgi_pass 127.0.0.1:9090;
  }
}

[root@cu2 puppetboard]# uwsgi --socket :9090 --wsgi-file /var/www/puppetboard/wsgi.py 

[root@cu2 puppetboard]# /home/hadoop/nginx/sbin/nginx -s reload
</code></pre>

<p><img src="/images/blogs/puppetboard-install.png" alt="" /></p>

<p>配置SSL访问需要把ssl_verify设置为false。</p>

<pre><code># 2.7.9+网上说好像就没问题
# http://stackoverflow.com/questions/29099404/ssl-insecureplatform-error-when-using-requests-package
# https://github.com/pypa/pip/issues/2681
[root@cu2 ~]# yum install -y  libffi-devel libffi 
[root@cu2 ~]# pip install 'requests[security]'

# [重要] 两个链接内容一样的：
# * https://groups.google.com/forum/#!msg/puppet-users/m7Sakf4bQ7Q/y6uAa0AUsZIJ
# * http://grokbase.com/t/gg/puppet-users/1428vjkncr/puppetboard-and-ssl
# You have two choices now, set SSL_VERIFY to False and trust that you're
# always talking to your actual PuppetDB or copy from the Puppet CA
# $vardir/ssl/ca_crt.pem to /etc/puppetboard and set SSL_VERIFY to the path
# of ca_crt.pem. In that case the file SSL_VERIFY points to will be used to
# verify PuppetDB's server certificate instead of the OS truststore.
[root@cu2 puppetboard]# vi settings.py 
PUPPETDB_HOST = 'cu3.eshore.cn'
PUPPETDB_PORT = 8081
PUPPETDB_SSL_VERIFY = False  # 这里设置为false
PUPPETDB_KEY = '/etc/puppetlabs/puppet/ssl/private_keys/cu2.eshore.cn.pem'
PUPPETDB_CERT = '/etc/puppetlabs/puppet/ssl/ca/signed/cu2.eshore.cn.pem'

# 重启uwsgi-http服务
[root@cu2 ~]# supervisorctl restart puppetboard
</code></pre>

<p>如果 puppetboard 和 puppetdb 安装在同一机器，可以使用 puppetdb/ssl 路径下的ssl文件（puppetdb/ssl也是从puppet/ssl拷贝过来的）：</p>

<pre><code>[root@cu3 ~]# puppetdb ssl-setup -f
PEM files in /etc/puppetlabs/puppetdb/ssl are missing, we will move them into place for you
Copying files: /etc/puppetlabs/puppet/ssl/certs/ca.pem, /etc/puppetlabs/puppet/ssl/private_keys/cu3.eshore.cn.pem and /etc/puppetlabs/puppet/ssl/certs/cu3.eshore.cn.pem to /etc/puppetlabs/puppetdb/ssl
...

[root@cu3 ~]# tree /etc/puppetlabs/puppetdb/ssl/
/etc/puppetlabs/puppetdb/ssl/
├── ca.pem
├── private.pem
└── public.pem
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MCollective Plugins]]></title>
    <link href="http://winseliu.com/blog/2016/04/28/mcollective-plugins/"/>
    <updated>2016-04-28T21:37:51+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/28/mcollective-plugins</id>
    <content type="html"><![CDATA[<p>上一篇介绍了mcollective的安装。乘着这股热情把 mco 命令行和插件的安装弄通，记录下来。</p>

<h2>基本命令使用</h2>

<ul>
<li><a href="https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html">https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html</a></li>
</ul>


<pre><code>[root@hadoop-master2 ~]# mco help
The Marionette Collective version 2.8.8

  completion      Helper for shell completion systems
  describe_filter Display human readable interpretation of filters
  facts           Reports on usage for a specific fact
  find            Find hosts using the discovery system matching filter criteria
  help            Application list and help
  inventory       General reporting tool for nodes, collectives and subcollectives
  ping            Ping all nodes
  plugin          MCollective Plugin Application
  rpc             Generic RPC agent client application
</code></pre>

<p>自带的插件只能用来查看环境情况(下面列出来的命令<a href="/blog/2016/04/28/mcollective-quick-start/#cli-simple-usage">上一篇:MCollective安装配置</a>都已记录过)。</p>

<pre><code>mco ping
mco inventory [server_host]
mco facts [fact]
</code></pre>

<p>mcollective 的 filter（适配节点）功能很强大，具体查看文档：<a href="https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html#selecting-request-targets-using-filters">Selecting Request Targets Using Filters</a></p>

<p><strong> 使用filter功能需要结合facts，需要先把节点的信息写入到mcollective/facts.yaml文件 </strong></p>

<pre><code>[hadoop@hadoop-master1 ~]$ sudo mco ping -I /^hadoop/
[hadoop@hadoop-master1 ~]$ sudo mco puppet runall 8 -I /^hadoop/
[hadoop@hadoop-master1 ~]$ sudo mco service iptables status -I "/cu-ud.*/"

[root@hadoop-master1 manifests]# mco ping -S "hostname=hadoop-master2"
[hadoop@hadoop-master1 ~]$ sudo mco ping -S 'hostname=/hadoop.*/'

[hadoop@hadoop-master1 ~]$ sudo mco facts hostname
</code></pre>

<h2>插件安装</h2>

<ul>
<li><a href="https://docs.puppet.com/mcollective/deploy/standard.html#install-agent-plugins">https://docs.puppet.com/mcollective/deploy/standard.html#install-agent-plugins</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/plugins.html">Installing Plugins</a></li>
<li><a href="https://docs.puppet.com/mcollective/deploy/plugins.html#example">https://docs.puppet.com/mcollective/deploy/plugins.html#example</a></li>
</ul>


<p>文档中描述了 <code>Use packages</code> 和 <code>Put files directly into the libdir</code> 两种安装插件的方式。但是 Packages 都是放在 <a href="http://yum.puppetlabs.com/el/6/products/x86_64/">旧的repo</a> 里面，我们这里使用第二种方式把github下载源码放到libdir来安装。</p>

<h4>安装mcollective-puppet-agent</h4>

<pre><code># 使用文档 https://github.com/puppetlabs/mcollective-puppet-agent#readme
# 直接下载release版本 
[root@hadoop-master2 ~]# cd /usr/libexec/mcollective/
[root@hadoop-master2 mcollective]# ll
total 44
-rw-r--r-- 1 root root 44759 Apr 29 11:53 mcollective-puppet-agent-1.10.0.tar.gz
[root@hadoop-master2 mcollective]# tar zxf mcollective-puppet-agent-1.10.0.tar.gz  
[root@hadoop-master2 mcollective]# ll mcollective-puppet-agent-1.10.0
total 60
drwxrwxr-x 2 root root  4096 Apr 13  2015 agent
drwxrwxr-x 2 root root  4096 Apr 13  2015 aggregate
drwxrwxr-x 2 root root  4096 Apr 13  2015 application
-rw-rw-r-- 1 root root  3456 Apr 13  2015 CHANGELOG.md
drwxrwxr-x 2 root root  4096 Apr 13  2015 data
drwxrwxr-x 4 root root  4096 Apr 13  2015 ext
-rw-rw-r-- 1 root root   349 Apr 13  2015 Gemfile
-rw-rw-r-- 1 root root  3036 Apr 13  2015 Rakefile
-rw-rw-r-- 1 root root 14739 Apr 13  2015 README.md
drwxrwxr-x 9 root root  4096 Apr 13  2015 spec
drwxrwxr-x 3 root root  4096 Apr 13  2015 util
drwxrwxr-x 2 root root  4096 Apr 13  2015 validator
# 官网提供example有区分服务端和客户端文件。反正多了没问题，直接全部放就行咯。。。
[root@hadoop-master2 mcollective]# mv mcollective-puppet-agent-1.10.0 mcollective

# 验证
# 多了puppet的命令！
[root@hadoop-master2 mcollective]# mco help
The Marionette Collective version 2.8.8

  completion      Helper for shell completion systems
  describe_filter Display human readable interpretation of filters
  facts           Reports on usage for a specific fact
  find            Find hosts using the discovery system matching filter criteria
  help            Application list and help
  inventory       General reporting tool for nodes, collectives and subcollectives
  ping            Ping all nodes
  plugin          MCollective Plugin Application
  puppet          Schedule runs, enable, disable and interrogate the Puppet Agent
  rpc             Generic RPC agent client application


# 同步到mcollective-servers （172.17.0.2对应hadoop-slaver1）
[root@hadoop-master2 mcollective]# rsync -az /usr/libexec/mcollective 172.17.0.2:/usr/libexec/

# mcollective-server添加插件后，重启mcollective服务
# 也可以使用 reload-agents 来重新加载agents： service mcollective reload-agents
[root@hadoop-slaver1 libexec]# service mcollective restart
Shutting down mcollective:                                 [  OK  ]
Starting mcollective:                                      [  OK  ]


# 验证server，已经可以看到新添加的puppet命令了
[root@hadoop-master2 mcollective]# mco inventory hadoop-slaver1
Inventory for hadoop-slaver1:

   Server Statistics:
                      Version: 2.8.8
                   Start Time: 2016-04-29 12:01:40 +0800
                  Config File: /etc/puppetlabs/mcollective/server.cfg
                  Collectives: mcollective
              Main Collective: mcollective
                   Process ID: 123
               Total Messages: 1
      Messages Passed Filters: 1
            Messages Filtered: 0
             Expired Messages: 0
                 Replies Sent: 0
         Total Processor Time: 0.67 seconds
                  System Time: 0.8 seconds

   Agents:
      discovery       puppet          rpcutil        

   Data Plugins:
      agent           collective      fact           
      fstat           puppet          resource       

[root@hadoop-master2 mcollective]# mco help puppet

[root@hadoop-master2 mcollective]# mco puppet status    

 * [ ============================================================&gt; ] 3 / 3

   hadoop-slaver1: Currently stopped; last completed run 10 hours 57 minutes 20 seconds ago
   hadoop-master1: Currently stopped; last completed run 11 hours 1 minutes 05 seconds ago
   hadoop-slaver2: Currently stopped; last completed run 10 hours 57 minutes 16 seconds ago
...


# 配置server.conf
# 注意：真正要执行puppet命令，为了适配puppet4需要添加/修改配置
-bash-4.1# cat /etc/puppetlabs/mcollective/server.cfg 
...
plugin.puppet.command = /opt/puppetlabs/bin/puppet agent
plugin.puppet.config = /etc/puppetlabs/puppet/puppet.conf

# 重启所有mcollective（重新加载agent也可以不重启，使用 mco shell run service mcollective reload-agents 来重新加载）

[root@hadoop-master2 mcollective]# mco puppet runall 1
2016-04-29 16:52:46: Running all nodes with a concurrency of 1
2016-04-29 16:52:46: Discovering enabled Puppet nodes to manage
2016-04-29 16:52:49: Found 3 enabled nodes
2016-04-29 16:52:50: hadoop-slaver1 schedule status: Started a Puppet run using the '/opt/puppetlabs/bin/puppet agent --onetime --no-daemonize --color=false --show_diff --verbose --no-splay' command
2016-04-29 16:52:55: hadoop-slaver2 schedule status: Started a Puppet run using the '/opt/puppetlabs/bin/puppet agent --onetime --no-daemonize --color=false --show_diff --verbose --no-splay' command
2016-04-29 16:52:59: hadoop-master1 schedule status: Started a Puppet run using the '/opt/puppetlabs/bin/puppet agent --onetime --no-daemonize --color=false --show_diff --verbose --no-splay' command
2016-04-29 16:52:59: Iteration complete. Initiated a Puppet run on 3 nodes.

[root@hadoop-master2 puppetlabs]# mco puppet status

 * [ ============================================================&gt; ] 3 / 3

   hadoop-master1: Currently stopped; last completed run 10 seconds ago
   hadoop-slaver1: Currently stopped; last completed run 15 seconds ago
   hadoop-slaver2: Currently stopped; last completed run 04 seconds ago
...
# 或者通过 puppetexplorer 查看节点最后的更新时间
</code></pre>

<h4>安装 package / service 插件</h4>

<p>为了更好的管理，再添加 package 和 service 两个插件</p>

<ul>
<li><a href="https://github.com/puppetlabs/mcollective-package-agent#readme">https://github.com/puppetlabs/mcollective-package-agent#readme</a></li>
<li><a href="https://github.com/puppetlabs/mcollective-service-agent#readme">https://github.com/puppetlabs/mcollective-service-agent#readme</a></li>
</ul>


<pre><code># http://stackoverflow.com/questions/8488253/how-to-force-cp-to-overwrite-without-confirmation
[root@hadoop-master2 mcollective]# unalias cp
[root@hadoop-master2 mcollective]# cp -rf mcollective-service-agent-3.1.3/* mcollective/   
[root@hadoop-master2 mcollective]# cp -rf mcollective-package-agent-4.4.0/* mcollective/

[root@hadoop-master2 mcollective]# rsync -az /usr/libexec/mcollective 172.17.0.2:/usr/libexec/

# 重启mcollective服务（或者 mco shell run service mcollective reload-agents 重新加载）

# updated 2016-5-11 17:15:08
# 还是重启比较好，reload-agents Data Plugins 没有重新加载
[root@hadoop-master1 puppet]# mco inventory hadoop-master2
Inventory for hadoop-master2:

   Server Statistics:
                      Version: 2.8.8
                   Start Time: 2016-05-11 17:12:45 +0800
                  Config File: /etc/puppetlabs/mcollective/server.cfg
                  Collectives: mcollective
              Main Collective: mcollective
                   Process ID: 39878
               Total Messages: 1
      Messages Passed Filters: 1
            Messages Filtered: 0
             Expired Messages: 0
                 Replies Sent: 0
         Total Processor Time: 1.17 seconds
                  System Time: 0.1 seconds

   Agents:
      discovery       package         puppet         
      rpcutil         service         shell          

   Data Plugins:
      agent           collective      fact           
      fstat           puppet          resource       
      service                                        

   Configuration Management Classes:
      No classes applied

   Facts:
      mcollective =&gt; 1
[root@hadoop-master1 puppet]# mco inventory hadoop-slaver2
Inventory for hadoop-slaver2:

   Server Statistics:
                      Version: 2.8.8
                   Start Time: 2016-05-11 16:56:09 +0800
                  Config File: /etc/puppetlabs/mcollective/server.cfg
                  Collectives: mcollective
              Main Collective: mcollective
                   Process ID: 14062
               Total Messages: 9
      Messages Passed Filters: 7
            Messages Filtered: 2
             Expired Messages: 0
                 Replies Sent: 6
         Total Processor Time: 1.31 seconds
                  System Time: 0.23 seconds

   Agents:
      discovery       package         puppet         
      rpcutil         service         shell          

   Data Plugins:
      agent           collective      fact           
      fstat                                          

   Configuration Management Classes:
      No classes applied

   Facts:
      mcollective =&gt; 1
</code></pre>

<p>验证下package的实力：</p>

<pre><code>[root@hadoop-master2 mcollective]# mco package lrzsz status

 * [ ============================================================&gt; ] 3 / 3

   hadoop-slaver1: lrzsz-0.12.20-27.1.el6.x86_64
   hadoop-master1: -purged.
   hadoop-slaver2: -purged.

Summary of Arch:

   x86_64 = 1

Summary of Ensure:

             purged = 2
   0.12.20-27.1.el6 = 1


Finished processing 3 / 3 hosts in 1488.41 ms

[root@hadoop-master2 mcollective]# mco rpc package install package=lrzsz
Discovering hosts using the mc method for 2 second(s) .... 3

 * [ ============================================================&gt; ] 3 / 3


hadoop-slaver1                           Unknown Request Status
   Package is already installed


Summary of Ensure:

   0.12.20-27.1.el6 = 3


Finished processing 3 / 3 hosts in 14525.03 ms
[root@hadoop-master2 mcollective]# mco package lrzsz status

 * [ ============================================================&gt; ] 3 / 3

   hadoop-master1: lrzsz-0.12.20-27.1.el6.x86_64
   hadoop-slaver2: lrzsz-0.12.20-27.1.el6.x86_64
   hadoop-slaver1: lrzsz-0.12.20-27.1.el6.x86_64

Summary of Arch:

   x86_64 = 3

Summary of Ensure:

   0.12.20-27.1.el6 = 3


Finished processing 3 / 3 hosts in 572.13 ms
</code></pre>

<p>还有很多的插件：</p>

<ul>
<li><a href="https://docs.puppet.com/mcollective/plugin_directory/index.html">https://docs.puppet.com/mcollective/plugin_directory/index.html</a></li>
<li>shell插件也不错，安装的时刻注意一下目录结构！<a href="https://github.com/puppetlabs/mcollective-shell-agent">https://github.com/puppetlabs/mcollective-shell-agent</a></li>
</ul>


<p>添加了 service，package，shell，puppet 插件后，用 mco 来执行管理集群太爽了！！</p>

<h4>后期统一安装</h4>

<pre><code>[root@hadoop-master1 mcollective]# ll
total 100
-rw-r--r-- 1 root root 17101 Apr 29 12:15 mcollective-package-agent-4.4.0.tar.gz
-rw-r--r-- 1 root root 44759 Apr 29 11:53 mcollective-puppet-agent-1.10.0.tar.gz
-rw-r--r-- 1 root root 12483 Apr 29 12:15 mcollective-service-agent-3.1.3.tar.gz
-rw-r--r-- 1 root root 17984 Apr 29 19:24 mcollective-shell-agent-0.0.2.tar.gz
[root@hadoop-master1 mcollective]# ls | xargs -I{} tar zxf {}

# TODO 可以考虑打包成rpm
[root@hadoop-master1 mcollective]# mkdir mcollective
[root@hadoop-master1 mcollective]# unalias cp
[root@hadoop-master1 mcollective]# cp -rf mcollective-package-agent-4.4.0/* mcollective/
[root@hadoop-master1 mcollective]# cp -rf mcollective-puppet-agent-1.10.0/* mcollective/
[root@hadoop-master1 mcollective]# cp -rf mcollective-service-agent-3.1.3/* mcollective/
[root@hadoop-master1 mcollective]# cp -rf mcollective-shell-agent-0.0.2/lib/mcollective/* mcollective/
[root@hadoop-master1 mcollective]# rm -rf mcollective-*

# 验证
[root@hadoop-master1 mcollective]# mco help
The Marionette Collective version 2.8.8

  completion      Helper for shell completion systems
  describe_filter Display human readable interpretation of filters
  facts           Reports on usage for a specific fact
  find            Find hosts using the discovery system matching filter criteria
  help            Application list and help
  inventory       General reporting tool for nodes, collectives and subcollectives
  package         Install, uninstall, update, purge and perform other actions to packages
  ping            Ping all nodes
  plugin          MCollective Plugin Application
  puppet          Schedule runs, enable, disable and interrogate the Puppet Agent
  rpc             Generic RPC agent client application
  service         Manages system services
  shell           Run shell commands

# 同步
[root@hadoop-master1 mcollective]# cd ..
[root@hadoop-master1 libexec]# rsync -az mcollective hadoop-master2:/usr/libexec/

# filter
[root@hadoop-master1 manifests]# mco shell run hostname -S "hostname=hadoop-master2"
[root@hadoop-master1 manifests]# mco ping -S "not hostname=hadoop-master1"
[root@hadoop-master1 manifests]# mco ping -S "! hostname=hadoop-master1"

[hadoop@hadoop-master1 ~]$ sudo mco shell --sort -I /cu-ud[1234]{1}$/ run -- ' ls /home/ud/ftpxdr | wc -l  '

# 少配置了
[root@hadoop-master1 manifests]# mco shell run "echo -e '\n\nplugin.puppet.command = /opt/puppetlabs/bin/puppet agent\nplugin.puppet.config = /etc/puppetlabs/puppet/puppet.conf' &gt;&gt; /etc/puppetlabs/mcollective/server.cfg" 


# 重启 mcollective 服务
[root@hadoop-master1 manifests]# mco shell run "echo service mcollective restart &gt;/tmp/mcollective_restart.sh ; nohup sh /tmp/mcollective_restart.sh "


# ---
[root@hadoop-master1 dtarepo]# mco rpc package install package=lrzsz -I cu-omc1
[root@hadoop-master1 dtarepo]# mco rpc package install package=gmetad -I cu-omc1

mco rpc service start service=puppet
=&gt; mco rpc --agent service --action start --argument service=puppet

mco plugin doc package

mco rpc service status service=puppet -S "environment=development"

mco puppet status
mco rpc puppet status


[root@hadoop-master1 gmond]# mco shell -I cu-ud2 run -- "/opt/puppetlabs/bin/puppet agent -t"
[root@hadoop-master1 production]# mco shell -I /^cu-omc2/ run -- "/opt/puppetlabs/bin/puppet agent -t"

# gmond 多网卡情况确认
[root@hadoop-master1 production]# route add -host 239.2.11.71 dev bond0

# puppet 基本语法
# https://docs.puppet.com/puppet/latest/reference/lang_conditional.html#if-statements
# https://docs.puppet.com/puppet/latest/reference/lang_relationships.html

# puppet使用tag可以更灵活的使用
# https://docs.puppet.com/puppet/latest/reference/lang_tags.html
apache::vhost {'docs.puppetlabs.com':
  port =&gt; 80,
  tag  =&gt; ['us_mirror1', 'us_mirror2'],
}

$ sudo puppet agent --test --tags apache,us_mirror1
</code></pre>

<p>再次强调Filter ： <a href="https://docs.puppet.com/mcollective/reference/basic/basic_cli_usage.html#selecting-request-targets-using-filters">Selecting Request Targets Using Filters</a></p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
