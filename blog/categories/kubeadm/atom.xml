<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kubeadm | Winse Blog]]></title>
  <link href="http://winse.github.io/blog/categories/kubeadm/atom.xml" rel="self"/>
  <link href="http://winse.github.io/"/>
  <updated>2023-12-07T21:28:24+08:00</updated>
  <id>http://winse.github.io/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[k8s-v1.23.5安装指南 - 使用kubeadm]]></title>
    <link href="http://winse.github.io/blog/2022/03/18/k8s-guide-use-kubeadm/"/>
    <updated>2022-03-18T11:58:48+08:00</updated>
    <id>http://winse.github.io/blog/2022/03/18/k8s-guide-use-kubeadm</id>
    <content type="html"><![CDATA[<p>注[2022-03-25]：如果后面需要用AWS，用Amazon的操作系统会便利一点。aws的命令这些都自带了。</p>

<p>本文是在 Amazon Linux 2 系统上安装部署的，和centos7.3基本相似。</p>

<p>和k8s软件依赖需要访问google的，已经在前面一篇文章中下载好，本文中会直接使用。依赖的软件可以在百度网盘下载：</p>

<pre><code>链接：https://pan.baidu.com/s/1P3ABqKGt1JhNkg-9yB22yQ 
提取码：k7af
</code></pre>

<h2>安装 amazon-linux-2 操作系统</h2>

<ul>
<li><a href="https://docs.amazonaws.cn/AWSEC2/latest/UserGuide/amazon-linux-2-virtual-machine.html#amazon-linux-2-virtual-machine-download">步骤 2：下载 Amazon Linux 2 VM 映像</a></li>
<li>下载 <a href="https://cdn.amazonlinux.com/os-images/2.0.20220218.3/">https://cdn.amazonlinux.com/os-images/2.0.20220218.3/</a></li>
</ul>


<p>这里下载vmware使用的镜像 <code>amzn2-vmware_esx-2.0.20220218.3-x86_64.xfs.gpt.ova</code> 和初始化配置 <code>Seed.iso</code> 。</p>

<p>这里简单说下，其实ova已经是可以直接用的，文档中讲的很多内容是辅助系统定制初始化的。user-data用于创建用户和修改文件内容，meta-data配置主机名和网络ip设置。为了本地开发测试，我们直接用默认提供 <code>Seed.iso</code> 即可，登录使用 <code>ec2-user:amazon</code> 。</p>

<p>然后双击 ova 文件，就可以导入创建一个虚拟机出来了。</p>

<ul>
<li>修改网络适配器为NAT模式；</li>
<li>添加CD/DVD设备，选择 <code>Seed.iso</code> ISO映射文件；</li>
<li>开机登录系统后，打开sshd的密码登录。</li>
</ul>


<pre><code>$ sudo ifup eth0
$ ip a

$ sudo vi /etc/ssh/sshd_config
#PasswordAuthentication no

$ sudo service sshd reload 
</code></pre>

<h2>安装docker</h2>

<p>k8s需要容器运行时软件，我们先安装好docker。</p>

<ul>
<li><a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker">https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker</a></li>
</ul>


<p>aws linux 2有它自己的docker源，使用docker官网文档的方式依赖有些找不到。直接按照aws官方文档中提供的方式安装。</p>

<h3>坑</h3>

<p>一开始是按照docker官网在centos的方式安装的，但yum repo的变量不对上，改了releasever后，然后依赖的版本找不到。</p>

<pre><code>## https://docs.docker.com/engine/install/centos/
[ec2-user@amazonlinux ~]$ cat /etc/issue
\S
Kernel \r on an \m

[ec2-user@amazonlinux ~]$ yum-debug-dump
Loaded plugins: langpacks, priorities, update-motd
Output written to: /home/ec2-user/yum_debug_dump-amazonlinux.onprem-2022-03-17_02:16:37.txt.gz
[ec2-user@amazonlinux ~]$ less /home/ec2-user/yum_debug_dump-amazonlinux.onprem-2022-03-17_02:16:37.txt.gz
[ec2-user@amazonlinux ~]$ 

$releasever的值,这个表示当前系统的发行版本，可以通过rpm -qi centos-release命令查看，结果如下：
$basearch是我们的系统硬件架构(CPU指令集),使用命令arch得到

[ec2-user@amazonlinux ~]$ sudo sed -i 's/$releasever/7/g' /etc/yum.repos.d/docker-ce.repo 

## 缺少依赖
</code></pre>

<h3>正式安装docker</h3>

<pre><code>## https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html
[ec2-user@amazonlinux ~]$ sudo yum update -y

[ec2-user@amazonlinux ~]$ sudo amazon-linux-extras install docker
Installing docker
Loaded plugins: langpacks, priorities, update-motd
Cleaning repos: amzn2-core amzn2extra-docker
12 metadata files removed
4 sqlite files removed
0 metadata files removed
Loaded plugins: langpacks, priorities, update-motd
amzn2-core                                                                                                                                        | 3.7 kB  00:00:00     
amzn2extra-docker                                                                                                                                 | 3.0 kB  00:00:00     
(1/5): amzn2-core/2/x86_64/group_gz                                                                                                               | 2.5 kB  00:00:00     
(2/5): amzn2extra-docker/2/x86_64/updateinfo                                                                                                      | 5.9 kB  00:00:00     
(3/5): amzn2-core/2/x86_64/updateinfo                                                                                                             | 452 kB  00:00:01     
(4/5): amzn2extra-docker/2/x86_64/primary_db                                                                                                      |  86 kB  00:00:00     
(5/5): amzn2-core/2/x86_64/primary_db                                                                                                             |  60 MB  00:01:42     
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package docker.x86_64 0:20.10.7-5.amzn2 will be installed
--&gt; Processing Dependency: runc &gt;= 1.0.0 for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Processing Dependency: libcgroup &gt;= 0.40.rc1-5.15 for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Processing Dependency: containerd &gt;= 1.3.2 for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Processing Dependency: pigz for package: docker-20.10.7-5.amzn2.x86_64
--&gt; Running transaction check
---&gt; Package containerd.x86_64 0:1.4.6-8.amzn2 will be installed
---&gt; Package libcgroup.x86_64 0:0.41-21.amzn2 will be installed
---&gt; Package pigz.x86_64 0:2.3.4-1.amzn2.0.1 will be installed
---&gt; Package runc.x86_64 0:1.0.0-2.amzn2 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

=========================================================================================================================================================================
 Package                               Arch                              Version                                      Repository                                    Size
=========================================================================================================================================================================
Installing:
 docker                                x86_64                            20.10.7-5.amzn2                              amzn2extra-docker                             42 M
Installing for dependencies:
 containerd                            x86_64                            1.4.6-8.amzn2                                amzn2extra-docker                             24 M
 libcgroup                             x86_64                            0.41-21.amzn2                                amzn2-core                                    66 k
 pigz                                  x86_64                            2.3.4-1.amzn2.0.1                            amzn2-core                                    81 k
 runc                                  x86_64                            1.0.0-2.amzn2                                amzn2extra-docker                            3.3 M

Transaction Summary
=========================================================================================================================================================================
Install  1 Package (+4 Dependent packages)

Total download size: 69 M
Installed size: 285 M
Is this ok [y/d/N]: y
Downloading packages:
(1/5): pigz-2.3.4-1.amzn2.0.1.x86_64.rpm                                                                                                          |  81 kB  00:00:00     
(2/5): libcgroup-0.41-21.amzn2.x86_64.rpm                                                                                                         |  66 kB  00:00:00     
(3/5): containerd-1.4.6-8.amzn2.x86_64.rpm                                                                                                        |  24 MB  00:01:14     
(4/5): runc-1.0.0-2.amzn2.x86_64.rpm                                                                                                              | 3.3 MB  00:00:10     
(5/5): docker-20.10.7-5.amzn2.x86_64.rpm                                                                                                          |  42 MB  00:01:50     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                                    641 kB/s |  69 MB  00:01:50     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : runc-1.0.0-2.amzn2.x86_64                                                                                                                             1/5 
  Installing : containerd-1.4.6-8.amzn2.x86_64                                                                                                                       2/5 
  Installing : libcgroup-0.41-21.amzn2.x86_64                                                                                                                        3/5 
  Installing : pigz-2.3.4-1.amzn2.0.1.x86_64                                                                                                                         4/5 
  Installing : docker-20.10.7-5.amzn2.x86_64                                                                                                                         5/5 
  Verifying  : docker-20.10.7-5.amzn2.x86_64                                                                                                                         1/5 
  Verifying  : containerd-1.4.6-8.amzn2.x86_64                                                                                                                       2/5 
  Verifying  : runc-1.0.0-2.amzn2.x86_64                                                                                                                             3/5 
  Verifying  : pigz-2.3.4-1.amzn2.0.1.x86_64                                                                                                                         4/5 
  Verifying  : libcgroup-0.41-21.amzn2.x86_64                                                                                                                        5/5 

Installed:
  docker.x86_64 0:20.10.7-5.amzn2                                                                                                                                        

Dependency Installed:
  containerd.x86_64 0:1.4.6-8.amzn2           libcgroup.x86_64 0:0.41-21.amzn2           pigz.x86_64 0:2.3.4-1.amzn2.0.1           runc.x86_64 0:1.0.0-2.amzn2          

Complete!
  0  ansible2                 available    \
        [ =2.4.2  =2.4.6  =2.8  =stable ]
  2  httpd_modules            available    [ =1.0  =stable ]
  3  memcached1.5             available    \
        [ =1.5.1  =1.5.16  =1.5.17 ]
  5  postgresql9.6            available    \
        [ =9.6.6  =9.6.8  =stable ]
  6  postgresql10             available    [ =10  =stable ]
  9  R3.4                     available    [ =3.4.3  =stable ]
 10  rust1                    available    \
        [ =1.22.1  =1.26.0  =1.26.1  =1.27.2  =1.31.0  =1.38.0
          =stable ]
 11  vim                      available    [ =8.0  =stable ]
 18  libreoffice              available    \
        [ =5.0.6.2_15  =5.3.6.1  =stable ]
 19  gimp                     available    [ =2.8.22 ]
 20  docker=latest            enabled      \
        [ =17.12.1  =18.03.1  =18.06.1  =18.09.9  =stable ]
 21  mate-desktop1.x          available    \
        [ =1.19.0  =1.20.0  =stable ]
 22  GraphicsMagick1.3        available    \
        [ =1.3.29  =1.3.32  =1.3.34  =stable ]
 23  tomcat8.5                available    \
        [ =8.5.31  =8.5.32  =8.5.38  =8.5.40  =8.5.42  =8.5.50
          =stable ]
 24  epel                     available    [ =7.11  =stable ]
 25  testing                  available    [ =1.0  =stable ]
 26  ecs                      available    [ =stable ]
 27  corretto8                available    \
        [ =1.8.0_192  =1.8.0_202  =1.8.0_212  =1.8.0_222  =1.8.0_232
          =1.8.0_242  =stable ]
 28  firecracker              available    [ =0.11  =stable ]
 29  golang1.11               available    \
        [ =1.11.3  =1.11.11  =1.11.13  =stable ]
 30  squid4                   available    [ =4  =stable ]
 32  lustre2.10               available    \
        [ =2.10.5  =2.10.8  =stable ]
 33  java-openjdk11           available    [ =11  =stable ]
 34  lynis                    available    [ =stable ]
 35  kernel-ng                available    [ =stable ]
 36  BCC                      available    [ =0.x  =stable ]
 37  mono                     available    [ =5.x  =stable ]
 38  nginx1                   available    [ =stable ]
 39  ruby2.6                  available    [ =2.6  =stable ]
 40  mock                     available    [ =stable ]
 41  postgresql11             available    [ =11  =stable ]
 42  php7.4                   available    [ =stable ]
 43  livepatch                available    [ =stable ]
 44  python3.8                available    [ =stable ]
 45  haproxy2                 available    [ =stable ]
 46  collectd                 available    [ =stable ]
 47  aws-nitro-enclaves-cli   available    [ =stable ]
 48  R4                       available    [ =stable ]
 49  kernel-5.4               available    [ =stable ]
 50  selinux-ng               available    [ =stable ]
 51  php8.0                   available    [ =stable ]
 52  tomcat9                  available    [ =stable ]
 53  unbound1.13              available    [ =stable ]
 54  mariadb10.5              available    [ =stable ]
 55  kernel-5.10              available    [ =stable ]
 56  redis6                   available    [ =stable ]
 57  ruby3.0                  available    [ =stable ]
 58  postgresql12             available    [ =stable ]
 59  postgresql13             available    [ =stable ]
 60  mock2                    available    [ =stable ]
 61  dnsmasq2.85              available    [ =stable ]
[ec2-user@amazonlinux ~]$ 

[ec2-user@amazonlinux ~]$ sudo service docker start
Redirecting to /bin/systemctl start docker.service
[ec2-user@amazonlinux ~]$ sudo systemctl enable docker
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.


[ec2-user@amazonlinux ~]$ sudo usermod -a -G docker ec2-user

[ec2-user@amazonlinux ~]$ docker info
Client:
 Context:    default
 Debug Mode: false

Server:
ERROR: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/info": dial unix /var/run/docker.sock: connect: permission denied
errors pretty printing info
[ec2-user@amazonlinux ~]$ exit
退出后再次连接：
[ec2-user@amazonlinux ~]$ docker info 
Client:
 Context:    default
 Debug Mode: false

Server:
 Containers: 0
  Running: 0
  Paused: 0
  Stopped: 0
 Images: 0
 Server Version: 20.10.7
 Storage Driver: overlay2
  Backing Filesystem: xfs
  Supports d_type: true
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: cgroupfs
 Cgroup Version: 1
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: d71fcd7d8303cbf684402823e425e9dd2e99285d
 runc version: 84113eef6fc27af1b01b3181f31bbaf708715301
 init version: de40ad0
 Security Options:
  seccomp
   Profile: default
 Kernel Version: 4.14.268-205.500.amzn2.x86_64
 Operating System: Amazon Linux 2
 OSType: linux
 Architecture: x86_64
 CPUs: 2
 Total Memory: 3.828GiB
 Name: amazonlinux.onprem
 ID: GENW:47BV:UJR2:247P:CPFE:PHSO:RA6Z:H4RK:HYEE:LXN3:XDIZ:SI6Q
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Registry: https://index.docker.io/v1/
 Labels:
 Experimental: false
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: false

[ec2-user@amazonlinux ~]$ sudo yum install telnet -y
</code></pre>

<h2>准备工作</h2>

<p>可以先了解一些基本概念
* <a href="https://www.jianshu.com/p/7bc34ff88d9d">Kubernetes in Action 笔记 —— 部署第一个应用</a></p>

<ul>
<li><a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B">准备开始</a></li>
</ul>


<pre><code>## 确保每个节点上 MAC 地址和 product_uuid 的唯一性
[ec2-user@amazonlinux ~]$ ip link
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
    link/ether 00:0c:29:a4:a6:fc brd ff:ff:ff:ff:ff:ff
[ec2-user@amazonlinux ~]$ ifconfig -a
eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 192.168.191.131  netmask 255.255.255.0  broadcast 192.168.191.255
        inet6 fe80::20c:29ff:fea4:a6fc  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether 00:0c:29:a4:a6:fc  txqueuelen 1000  (Ethernet)
        RX packets 1737  bytes 288390 (281.6 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 1704  bytes 139101 (135.8 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 625  bytes 57768 (56.4 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 625  bytes 57768 (56.4 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

[ec2-user@amazonlinux ~]$ sudo cat /sys/class/dmi/id/product_uuid
564DD81E-DEBE-B06D-CF35-D7E3DDA4A6FC

## 检查网络适配器
# 只有一个网卡，跳过

## 允许 iptables 检查桥接流量
[ec2-user@amazonlinux ~]$ lsmod | grep br_netfilter
[ec2-user@amazonlinux ~]$ 
[ec2-user@amazonlinux ~]$ sudo modprobe br_netfilter
[ec2-user@amazonlinux ~]$ lsmod | grep br_netfilter 
br_netfilter           24576  0
bridge                172032  1 br_netfilter
[ec2-user@amazonlinux ~]$ 

[ec2-user@amazonlinux ~]$ cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

[ec2-user@amazonlinux ~]$ cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

[ec2-user@amazonlinux ~]$ sudo sysctl --system
* Applying /etc/sysctl.d/00-defaults.conf ...
kernel.printk = 8 4 1 7
kernel.panic = 30
net.ipv4.neigh.default.gc_thresh1 = 0
net.ipv6.neigh.default.gc_thresh1 = 0
net.ipv4.neigh.default.gc_thresh2 = 15360
net.ipv6.neigh.default.gc_thresh2 = 15360
net.ipv4.neigh.default.gc_thresh3 = 16384
net.ipv6.neigh.default.gc_thresh3 = 16384
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
kernel.kptr_restrict = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-amazon.conf ...
kernel.sched_autogroup_enabled = 0
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
* Applying /etc/sysctl.conf ...

## SELINUX
sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config 
setenforce 0

## Docker
cat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

sudo systemctl enable docker
sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre>

<p>修改时区</p>

<pre><code>sudo rm -rf /etc/localtime 
sudo ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 
</code></pre>

<h2>主节点安装kubeadm并加载docker镜像</h2>

<pre><code>## 修改主机名
[ec2-user@amazonlinux ~]$ sudo hostnamectl --static set-hostname k8s 
[ec2-user@amazonlinux ~]$ sudo hostname k8s 

[ec2-user@k8s ~]$ rz
rz waiting to receive.
Starting zmodem transfer.  Press Ctrl+C to cancel.
Transferring ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm...
  100%    9253 KB    9253 KB/sec    00:00:01       0 Errors  
Transferring d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm...
  100%   21041 KB    21041 KB/sec    00:00:01       0 Errors  
Transferring 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm...
  100%    7228 KB    7228 KB/sec    00:00:01       0 Errors  
Transferring db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm...
  100%   19030 KB    19030 KB/sec    00:00:01       0 Errors  
Transferring 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm...
  100%    9689 KB    9689 KB/sec    00:00:01       0 Errors  

[ec2-user@k8s ~]$ sudo yum install -y *.rpm
Loaded plugins: langpacks, priorities, update-motd
Examining 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm: cri-tools-1.23.0-0.x86_64
Marking 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm to be installed
Examining 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm: kubectl-1.23.5-0.x86_64
Marking 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm to be installed
Examining ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm: kubeadm-1.23.5-0.x86_64
Marking ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm to be installed
Examining d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm: kubelet-1.23.5-0.x86_64
Marking d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm to be installed
Examining db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm: kubernetes-cni-0.8.7-0.x86_64
Marking db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm to be installed
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package cri-tools.x86_64 0:1.23.0-0 will be installed
---&gt; Package kubeadm.x86_64 0:1.23.5-0 will be installed
---&gt; Package kubectl.x86_64 0:1.23.5-0 will be installed
---&gt; Package kubelet.x86_64 0:1.23.5-0 will be installed
--&gt; Processing Dependency: conntrack for package: kubelet-1.23.5-0.x86_64
--&gt; Processing Dependency: ebtables for package: kubelet-1.23.5-0.x86_64
--&gt; Processing Dependency: socat for package: kubelet-1.23.5-0.x86_64
---&gt; Package kubernetes-cni.x86_64 0:0.8.7-0 will be installed
--&gt; Running transaction check
---&gt; Package conntrack-tools.x86_64 0:1.4.4-5.amzn2.2 will be installed
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.1)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.0)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cthelper.so.0(LIBNETFILTER_CTHELPER_1.0)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_queue.so.1()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cthelper.so.0()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
---&gt; Package ebtables.x86_64 0:2.0.10-16.amzn2.0.1 will be installed
---&gt; Package socat.x86_64 0:1.7.3.2-2.amzn2.0.1 will be installed
--&gt; Running transaction check
---&gt; Package libnetfilter_cthelper.x86_64 0:1.0.0-10.amzn2.1 will be installed
---&gt; Package libnetfilter_cttimeout.x86_64 0:1.0.0-6.amzn2.1 will be installed
---&gt; Package libnetfilter_queue.x86_64 0:1.0.2-2.amzn2.0.2 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

======================================================================================================================================================
 Package                Arch   Version             Repository                                                                                    Size
======================================================================================================================================================
Installing:
 cri-tools              x86_64 1.23.0-0            /4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64   34 M
 kubeadm                x86_64 1.23.5-0            /ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64     43 M
 kubectl                x86_64 1.23.5-0            /96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64     44 M
 kubelet                x86_64 1.23.5-0            /d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64    119 M
 kubernetes-cni         x86_64 0.8.7-0             /db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64
                                                                                                                                                 55 M
Installing for dependencies:
 conntrack-tools        x86_64 1.4.4-5.amzn2.2     amzn2-core                                                                                   186 k
 ebtables               x86_64 2.0.10-16.amzn2.0.1 amzn2-core                                                                                   122 k
 libnetfilter_cthelper  x86_64 1.0.0-10.amzn2.1    amzn2-core                                                                                    18 k
 libnetfilter_cttimeout x86_64 1.0.0-6.amzn2.1     amzn2-core                                                                                    18 k
 libnetfilter_queue     x86_64 1.0.2-2.amzn2.0.2   amzn2-core                                                                                    24 k
 socat                  x86_64 1.7.3.2-2.amzn2.0.1 amzn2-core                                                                                   291 k

Transaction Summary
======================================================================================================================================================
Install  5 Packages (+6 Dependent packages)

Total size: 296 M
Total download size: 658 k
Installed size: 298 M
Downloading packages:
(1/6): ebtables-2.0.10-16.amzn2.0.1.x86_64.rpm                                                                                 | 122 kB  00:00:10     
(2/6): libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64.rpm                                                                       |  18 kB  00:00:00     
(3/6): conntrack-tools-1.4.4-5.amzn2.2.x86_64.rpm                                                                              | 186 kB  00:00:10     
(4/6): libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64.rpm                                                                       |  18 kB  00:00:00     
(5/6): libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64.rpm                                                                         |  24 kB  00:00:00     
(6/6): socat-1.7.3.2-2.amzn2.0.1.x86_64.rpm                                                                                    | 291 kB  00:00:00     
------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                  45 kB/s | 658 kB  00:00:14     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64                                                                                     1/11 
  Installing : libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64                                                                                     2/11 
  Installing : libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64                                                                                       3/11 
  Installing : conntrack-tools-1.4.4-5.amzn2.2.x86_64                                                                                            4/11 
  Installing : ebtables-2.0.10-16.amzn2.0.1.x86_64                                                                                               5/11 
  Installing : cri-tools-1.23.0-0.x86_64                                                                                                         6/11 
  Installing : socat-1.7.3.2-2.amzn2.0.1.x86_64                                                                                                  7/11 
  Installing : kubelet-1.23.5-0.x86_64                                                                                                           8/11 
  Installing : kubernetes-cni-0.8.7-0.x86_64                                                                                                     9/11 
  Installing : kubectl-1.23.5-0.x86_64                                                                                                          10/11 
  Installing : kubeadm-1.23.5-0.x86_64                                                                                                          11/11 
  Verifying  : kubernetes-cni-0.8.7-0.x86_64                                                                                                     1/11 
  Verifying  : kubectl-1.23.5-0.x86_64                                                                                                           2/11 
  Verifying  : socat-1.7.3.2-2.amzn2.0.1.x86_64                                                                                                  3/11 
  Verifying  : cri-tools-1.23.0-0.x86_64                                                                                                         4/11 
  Verifying  : ebtables-2.0.10-16.amzn2.0.1.x86_64                                                                                               5/11 
  Verifying  : libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64                                                                                       6/11 
  Verifying  : conntrack-tools-1.4.4-5.amzn2.2.x86_64                                                                                            7/11 
  Verifying  : libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64                                                                                     8/11 
  Verifying  : kubeadm-1.23.5-0.x86_64                                                                                                           9/11 
  Verifying  : kubelet-1.23.5-0.x86_64                                                                                                          10/11 
  Verifying  : libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64                                                                                    11/11 

Installed:
  cri-tools.x86_64 0:1.23.0-0   kubeadm.x86_64 0:1.23.5-0   kubectl.x86_64 0:1.23.5-0   kubelet.x86_64 0:1.23.5-0   kubernetes-cni.x86_64 0:0.8.7-0  

Dependency Installed:
  conntrack-tools.x86_64 0:1.4.4-5.amzn2.2          ebtables.x86_64 0:2.0.10-16.amzn2.0.1           libnetfilter_cthelper.x86_64 0:1.0.0-10.amzn2.1  
  libnetfilter_cttimeout.x86_64 0:1.0.0-6.amzn2.1   libnetfilter_queue.x86_64 0:1.0.2-2.amzn2.0.2   socat.x86_64 0:1.7.3.2-2.amzn2.0.1               

Complete!
[ec2-user@k8s ~]$ 

[ec2-user@k8s ~]$ sudo yum install ebtables ethtool             
</code></pre>

<p>加载docker镜像：</p>

<pre><code>[ec2-user@k8s ~]$ docker load -i k8s.tar.gz 
194a408e97d8: Loading layer [==================================================&gt;]  68.57MB/68.57MB
2b8347a02bc5: Loading layer [==================================================&gt;]  1.509MB/1.509MB
618b3e11ccba: Loading layer [==================================================&gt;]  44.17MB/44.17MB
Loaded image: k8s.gcr.io/kube-proxy:v1.23.5
5b1fa8e3e100: Loading layer [==================================================&gt;]  3.697MB/3.697MB
83e216f0eb98: Loading layer [==================================================&gt;]  1.509MB/1.509MB
a70573edad24: Loading layer [==================================================&gt;]  121.1MB/121.1MB
Loaded image: k8s.gcr.io/kube-controller-manager:v1.23.5
46576c5a6a97: Loading layer [==================================================&gt;]  49.63MB/49.63MB
Loaded image: k8s.gcr.io/kube-scheduler:v1.23.5
6d75f23be3dd: Loading layer [==================================================&gt;]  3.697MB/3.697MB
b6e8c573c18d: Loading layer [==================================================&gt;]  2.257MB/2.257MB
d80003ff5706: Loading layer [==================================================&gt;]    267MB/267MB
664dd6f2834b: Loading layer [==================================================&gt;]  2.137MB/2.137MB
62ae031121b1: Loading layer [==================================================&gt;]  18.86MB/18.86MB
Loaded image: k8s.gcr.io/etcd:3.5.1-0
256bc5c338a6: Loading layer [==================================================&gt;]  336.4kB/336.4kB
80e4a2390030: Loading layer [==================================================&gt;]  46.62MB/46.62MB
Loaded image: k8s.gcr.io/coredns/coredns:v1.8.6
1021ef88c797: Loading layer [==================================================&gt;]  684.5kB/684.5kB
Loaded image: k8s.gcr.io/pause:3.6
50098fdfecae: Loading layer [==================================================&gt;]  131.3MB/131.3MB
Loaded image: k8s.gcr.io/kube-apiserver:v1.23.5

[ec2-user@k8s ~]$ docker images 
REPOSITORY                           TAG       IMAGE ID       CREATED        SIZE
k8s.gcr.io/kube-apiserver            v1.23.5   3fc1d62d6587   15 hours ago   135MB
k8s.gcr.io/kube-proxy                v1.23.5   3c53fa8541f9   15 hours ago   112MB
k8s.gcr.io/kube-controller-manager   v1.23.5   b0c9e5e4dbb1   15 hours ago   125MB
k8s.gcr.io/kube-scheduler            v1.23.5   884d49d6d8c9   15 hours ago   53.5MB
k8s.gcr.io/etcd                      3.5.1-0   25f8c7f3da61   4 months ago   293MB
k8s.gcr.io/coredns/coredns           v1.8.6    a4ca41631cc7   5 months ago   46.8MB
k8s.gcr.io/pause                     3.6       6270bb605e12   6 months ago   683kB
[ec2-user@k8s ~]$ 
</code></pre>

<h2>主节点(控制平面control-plane node)启动服务</h2>

<pre><code>[ec2-user@k8s ~]$ sudo su - 

[root@k8s ~]# kubeadm init 
[init] Using Kubernetes version: v1.23.5
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
        [WARNING Hostname]: hostname "k8s" could not be reached
        [WARNING Hostname]: hostname "k8s": lookup k8s on 192.168.191.2:53: no such host
        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.191.131]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s localhost] and IPs [192.168.191.131 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s localhost] and IPs [192.168.191.131 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[apiclient] All control plane components are healthy after 87.001525 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: sj6fff.bpak7gkd3hnyzcm5
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.191.131:6443 --token sj6fff.bpak7gkd3hnyzcm5 \
        --discovery-token-ca-cert-hash sha256:8e15649afc0771e80cce7f1dfdbb0933f4fdbd45ea1f9e03be1f3b78449a6d3c 
[root@k8s ~]# 
</code></pre>

<p>普通用户配置kubectl：</p>

<pre><code>
[ec2-user@k8s ~]$ mkdir -p $HOME/.kube
[ec2-user@k8s ~]$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[ec2-user@k8s ~]$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

[ec2-user@k8s ~]$ which kubectl
/usr/bin/kubectl
[ec2-user@k8s ~]$ 
[ec2-user@k8s ~]$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.191.131:6443
CoreDNS is running at https://192.168.191.131:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

[ec2-user@k8s ~]$ kubectl get nodes -o wide
NAME   STATUS     ROLES                  AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
k8s    NotReady   control-plane,master   14m   v1.23.5   192.168.191.131   &lt;none&gt;        Amazon Linux 2   4.14.268-205.500.amzn2.x86_64   docker://20.10.7

[ec2-user@k8s ~]$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE   IP                NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-64897985d-pcxpd       0/1     Pending   0          14m   &lt;none&gt;            &lt;none&gt;   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-64897985d-pfsj6       0/1     Pending   0          14m   &lt;none&gt;            &lt;none&gt;   &lt;none&gt;           &lt;none&gt;
kube-system   etcd-k8s                      1/1     Running   0          14m   192.168.191.131   k8s      &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-k8s            1/1     Running   0          14m   192.168.191.131   k8s      &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-k8s   1/1     Running   0          14m   192.168.191.131   k8s      &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-qj6lw              1/1     Running   0          14m   192.168.191.131   k8s      &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-k8s            1/1     Running   0          14m   192.168.191.131   k8s      &lt;none&gt;           &lt;none&gt;
[ec2-user@k8s ~]$ 
</code></pre>

<p>如果希望主节点（控制平面节点control-plane node)上也调度 Pod， 例如用于开发的单机 Kubernetes 集群，请运行：</p>

<pre><code>[root@k8s ~]# export KUBECONFIG=/etc/kubernetes/admin.conf
[root@k8s ~]# kubectl taint nodes --all node-role.kubernetes.io/master-
node/k8s untainted
[root@k8s ~]# 
</code></pre>

<h2>加入工作节点(nodes)</h2>

<p>先把docker安装好，以及系统基础配置，参考上面的步骤。然后安装kubeadm，以及加载gcr的docker镜像。</p>

<pre><code>[ec2-user@amazonlinux ~]$ ll
total 285480
-rw-r--r-- 1 ec2-user ec2-user   7401938 Mar 17 15:22 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm
-rw-r--r-- 1 ec2-user ec2-user   9921646 Mar 17 15:22 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm
-rw-r--r-- 1 ec2-user ec2-user   9475514 Mar 17 15:22 ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm
-rw-r--r-- 1 ec2-user ec2-user  21546750 Mar 17 15:22 d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm
-rw-r--r-- 1 ec2-user ec2-user  19487362 Mar 17 15:22 db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm
-rw-r--r-- 1 ec2-user ec2-user 224482960 Mar 17 15:22 k8s.tar.gz

[ec2-user@amazonlinux ~]$ sudo yum install *.rpm 
Loaded plugins: langpacks, priorities, update-motd
Examining 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm: cri-tools-1.23.0-0.x86_64
Marking 4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64.rpm to be installed
Examining 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm: kubectl-1.23.5-0.x86_64
Marking 96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64.rpm to be installed
Examining ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm: kubeadm-1.23.5-0.x86_64
Marking ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64.rpm to be installed
Examining d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm: kubelet-1.23.5-0.x86_64
Marking d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64.rpm to be installed
Examining db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm: kubernetes-cni-0.8.7-0.x86_64
Marking db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64.rpm to be installed
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package cri-tools.x86_64 0:1.23.0-0 will be installed
---&gt; Package kubeadm.x86_64 0:1.23.5-0 will be installed
---&gt; Package kubectl.x86_64 0:1.23.5-0 will be installed
---&gt; Package kubelet.x86_64 0:1.23.5-0 will be installed
--&gt; Processing Dependency: conntrack for package: kubelet-1.23.5-0.x86_64
amzn2-core                                                                                                                                  | 3.7 kB  00:00:00     
--&gt; Processing Dependency: ebtables for package: kubelet-1.23.5-0.x86_64
--&gt; Processing Dependency: socat for package: kubelet-1.23.5-0.x86_64
---&gt; Package kubernetes-cni.x86_64 0:0.8.7-0 will be installed
--&gt; Running transaction check
---&gt; Package conntrack-tools.x86_64 0:1.4.4-5.amzn2.2 will be installed
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.1)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1(LIBNETFILTER_CTTIMEOUT_1.0)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cthelper.so.0(LIBNETFILTER_CTHELPER_1.0)(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_queue.so.1()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cttimeout.so.1()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
--&gt; Processing Dependency: libnetfilter_cthelper.so.0()(64bit) for package: conntrack-tools-1.4.4-5.amzn2.2.x86_64
---&gt; Package ebtables.x86_64 0:2.0.10-16.amzn2.0.1 will be installed
---&gt; Package socat.x86_64 0:1.7.3.2-2.amzn2.0.1 will be installed
--&gt; Running transaction check
---&gt; Package libnetfilter_cthelper.x86_64 0:1.0.0-10.amzn2.1 will be installed
---&gt; Package libnetfilter_cttimeout.x86_64 0:1.0.0-6.amzn2.1 will be installed
---&gt; Package libnetfilter_queue.x86_64 0:1.0.2-2.amzn2.0.2 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

===================================================================================================================================================================
 Package                  Arch     Version                 Repository                                                                                         Size
===================================================================================================================================================================
Installing:
 cri-tools                x86_64   1.23.0-0                /4d300a7655f56307d35f127d99dc192b6aa4997f322234e754f16aaa60fd8906-cri-tools-1.23.0-0.x86_64        34 M
 kubeadm                  x86_64   1.23.5-0                /ab0e12925be5251baf5dd3b31493663d46e4a7b458c7a5b6b717f4ae87a81bd4-kubeadm-1.23.5-0.x86_64          43 M
 kubectl                  x86_64   1.23.5-0                /96b208380314a19ded917eaf125ed748f5e2b28a3cc8707a10a76a9f5b61c0df-kubectl-1.23.5-0.x86_64          44 M
 kubelet                  x86_64   1.23.5-0                /d39aa6eb38a6a8326b7e88c622107327dfd02ac8aaae32eceb856643a2ad9981-kubelet-1.23.5-0.x86_64         119 M
 kubernetes-cni           x86_64   0.8.7-0                 /db7cb5cb0b3f6875f54d10f02e625573988e3e91fd4fc5eef0b1876bb18604ad-kubernetes-cni-0.8.7-0.x86_64    55 M
Installing for dependencies:
 conntrack-tools          x86_64   1.4.4-5.amzn2.2         amzn2-core                                                                                        186 k
 ebtables                 x86_64   2.0.10-16.amzn2.0.1     amzn2-core                                                                                        122 k
 libnetfilter_cthelper    x86_64   1.0.0-10.amzn2.1        amzn2-core                                                                                         18 k
 libnetfilter_cttimeout   x86_64   1.0.0-6.amzn2.1         amzn2-core                                                                                         18 k
 libnetfilter_queue       x86_64   1.0.2-2.amzn2.0.2       amzn2-core                                                                                         24 k
 socat                    x86_64   1.7.3.2-2.amzn2.0.1     amzn2-core                                                                                        291 k

Transaction Summary
===================================================================================================================================================================
Install  5 Packages (+6 Dependent packages)

Total size: 296 M
Total download size: 658 k
Installed size: 298 M
Is this ok [y/d/N]: y
Downloading packages:
(1/6): ebtables-2.0.10-16.amzn2.0.1.x86_64.rpm                                                                                              | 122 kB  00:00:00     
(2/6): libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64.rpm                                                                                    |  18 kB  00:00:00     
(3/6): libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64.rpm                                                                                    |  18 kB  00:00:00     
(4/6): conntrack-tools-1.4.4-5.amzn2.2.x86_64.rpm                                                                                           | 186 kB  00:00:00     
(5/6): libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64.rpm                                                                                      |  24 kB  00:00:00     
(6/6): socat-1.7.3.2-2.amzn2.0.1.x86_64.rpm                                                                                                 | 291 kB  00:00:00     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                              1.0 MB/s | 658 kB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64                                                                                                  1/11 
  Installing : libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64                                                                                                  2/11 
  Installing : libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64                                                                                                    3/11 
  Installing : conntrack-tools-1.4.4-5.amzn2.2.x86_64                                                                                                         4/11 
  Installing : ebtables-2.0.10-16.amzn2.0.1.x86_64                                                                                                            5/11 
  Installing : cri-tools-1.23.0-0.x86_64                                                                                                                      6/11 
  Installing : socat-1.7.3.2-2.amzn2.0.1.x86_64                                                                                                               7/11 
  Installing : kubelet-1.23.5-0.x86_64                                                                                                                        8/11 
  Installing : kubernetes-cni-0.8.7-0.x86_64                                                                                                                  9/11 
  Installing : kubectl-1.23.5-0.x86_64                                                                                                                       10/11 
  Installing : kubeadm-1.23.5-0.x86_64                                                                                                                       11/11 
  Verifying  : kubernetes-cni-0.8.7-0.x86_64                                                                                                                  1/11 
  Verifying  : kubectl-1.23.5-0.x86_64                                                                                                                        2/11 
  Verifying  : socat-1.7.3.2-2.amzn2.0.1.x86_64                                                                                                               3/11 
  Verifying  : cri-tools-1.23.0-0.x86_64                                                                                                                      4/11 
  Verifying  : ebtables-2.0.10-16.amzn2.0.1.x86_64                                                                                                            5/11 
  Verifying  : libnetfilter_queue-1.0.2-2.amzn2.0.2.x86_64                                                                                                    6/11 
  Verifying  : conntrack-tools-1.4.4-5.amzn2.2.x86_64                                                                                                         7/11 
  Verifying  : libnetfilter_cttimeout-1.0.0-6.amzn2.1.x86_64                                                                                                  8/11 
  Verifying  : kubeadm-1.23.5-0.x86_64                                                                                                                        9/11 
  Verifying  : kubelet-1.23.5-0.x86_64                                                                                                                       10/11 
  Verifying  : libnetfilter_cthelper-1.0.0-10.amzn2.1.x86_64                                                                                                 11/11 

Installed:
  cri-tools.x86_64 0:1.23.0-0     kubeadm.x86_64 0:1.23.5-0     kubectl.x86_64 0:1.23.5-0     kubelet.x86_64 0:1.23.5-0     kubernetes-cni.x86_64 0:0.8.7-0    

Dependency Installed:
  conntrack-tools.x86_64 0:1.4.4-5.amzn2.2              ebtables.x86_64 0:2.0.10-16.amzn2.0.1               libnetfilter_cthelper.x86_64 0:1.0.0-10.amzn2.1      
  libnetfilter_cttimeout.x86_64 0:1.0.0-6.amzn2.1       libnetfilter_queue.x86_64 0:1.0.2-2.amzn2.0.2       socat.x86_64 0:1.7.3.2-2.amzn2.0.1                   

Complete!

[ec2-user@amazonlinux ~]$ sudo yum install ebtables ethtool  
Loaded plugins: langpacks, priorities, update-motd
Package ebtables-2.0.10-16.amzn2.0.1.x86_64 already installed and latest version
Package 2:ethtool-4.8-10.amzn2.x86_64 already installed and latest version
Nothing to do
</code></pre>

<p>加载docker镜像</p>

<pre><code>
[ec2-user@amazonlinux ~]$ docker load -i k8s.tar.gz 
194a408e97d8: Loading layer [==================================================&gt;]  68.57MB/68.57MB
2b8347a02bc5: Loading layer [==================================================&gt;]  1.509MB/1.509MB
618b3e11ccba: Loading layer [==================================================&gt;]  44.17MB/44.17MB
Loaded image: k8s.gcr.io/kube-proxy:v1.23.5
5b1fa8e3e100: Loading layer [==================================================&gt;]  3.697MB/3.697MB
83e216f0eb98: Loading layer [==================================================&gt;]  1.509MB/1.509MB
a70573edad24: Loading layer [==================================================&gt;]  121.1MB/121.1MB
Loaded image: k8s.gcr.io/kube-controller-manager:v1.23.5
46576c5a6a97: Loading layer [==================================================&gt;]  49.63MB/49.63MB
Loaded image: k8s.gcr.io/kube-scheduler:v1.23.5
6d75f23be3dd: Loading layer [==================================================&gt;]  3.697MB/3.697MB
b6e8c573c18d: Loading layer [==================================================&gt;]  2.257MB/2.257MB
d80003ff5706: Loading layer [==================================================&gt;]    267MB/267MB
664dd6f2834b: Loading layer [==================================================&gt;]  2.137MB/2.137MB
62ae031121b1: Loading layer [==================================================&gt;]  18.86MB/18.86MB
Loaded image: k8s.gcr.io/etcd:3.5.1-0
256bc5c338a6: Loading layer [==================================================&gt;]  336.4kB/336.4kB
80e4a2390030: Loading layer [==================================================&gt;]  46.62MB/46.62MB
Loaded image: k8s.gcr.io/coredns/coredns:v1.8.6
1021ef88c797: Loading layer [==================================================&gt;]  684.5kB/684.5kB
Loaded image: k8s.gcr.io/pause:3.6
50098fdfecae: Loading layer [==================================================&gt;]  131.3MB/131.3MB
Loaded image: k8s.gcr.io/kube-apiserver:v1.23.5

[ec2-user@amazonlinux ~]$ docker images 
REPOSITORY                           TAG       IMAGE ID       CREATED        SIZE
k8s.gcr.io/kube-apiserver            v1.23.5   3fc1d62d6587   15 hours ago   135MB
k8s.gcr.io/kube-proxy                v1.23.5   3c53fa8541f9   15 hours ago   112MB
k8s.gcr.io/kube-controller-manager   v1.23.5   b0c9e5e4dbb1   15 hours ago   125MB
k8s.gcr.io/kube-scheduler            v1.23.5   884d49d6d8c9   15 hours ago   53.5MB
k8s.gcr.io/etcd                      3.5.1-0   25f8c7f3da61   4 months ago   293MB
k8s.gcr.io/coredns/coredns           v1.8.6    a4ca41631cc7   5 months ago   46.8MB
k8s.gcr.io/pause                     3.6       6270bb605e12   6 months ago   683kB
[ec2-user@amazonlinux ~]$ 
</code></pre>

<p>中间出了个小插曲，一开始没有改主机名，导致加入节点的时刻用的是默认的，这样看起来不清晰，后面改了名称后就不认了。得重新弄一遍。</p>

<pre><code>## 加入节点
[ec2-user@amazonlinux ~]$ sudo su -
[root@amazonlinux ~]# kubeadm join 192.168.191.131:6443 --token sj6fff.bpak7gkd3hnyzcm5 \
         --discovery-token-ca-cert-hash sha256:8e15649afc0771e80cce7f1dfdbb0933f4fdbd45ea1f9e03be1f3b78449a6d3c 
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
        [WARNING Hostname]: hostname "amazonlinux.onprem" could not be reached
        [WARNING Hostname]: hostname "amazonlinux.onprem": lookup amazonlinux.onprem on 192.168.191.2:53: no such host
        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

[root@amazonlinux ~]# 

## 改下主机名称：
[root@amazonlinux ~]# hostnamectl --static set-hostname worker1
[root@amazonlinux ~]# hostname worker1
[root@amazonlinux ~]# exit

## 改了一下名，重启后不行了，重新加入
[ec2-user@worker1 ~]$ sudo su - 
Last login: Thu Mar 17 15:24:32 CST 2022 on pts/0

[root@worker1 ~]# kubeadm join 192.168.191.131:6443 --token sj6fff.bpak7gkd3hnyzcm5 \
          --discovery-token-ca-cert-hash sha256:8e15649afc0771e80cce7f1dfdbb0933f4fdbd45ea1f9e03be1f3b78449a6d3c 
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
        [WARNING Hostname]: hostname "worker1" could not be reached
        [WARNING Hostname]: hostname "worker1": lookup worker1 on 192.168.191.2:53: no such host
        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
        [ERROR Port-10250]: Port 10250 is in use
        [ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

## 直接重新加入不行，需要先重置再加入
[root@worker1 ~]# kubeadm reset 
[reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0317 17:42:03.050519    6887 removeetcdmember.go:80] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] No etcd config found. Assuming external etcd
[reset] Please, manually reset etcd to prevent further issues
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.

[root@worker1 ~]# kubeadm join 192.168.191.131:6443 --token sj6fff.bpak7gkd3hnyzcm5 \
         --discovery-token-ca-cert-hash sha256:8e15649afc0771e80cce7f1dfdbb0933f4fdbd45ea1f9e03be1f3b78449a6d3c 
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
        [WARNING Hostname]: hostname "worker1" could not be reached
        [WARNING Hostname]: hostname "worker1": lookup worker1 on 192.168.191.2:53: no such host
        [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
</code></pre>

<p>加入节点后，查看状态：</p>

<pre><code>[ec2-user@k8s ~]$ kubectl get nodes 
NAME      STATUS     ROLES                  AGE    VERSION
k8s       Ready      control-plane,master   166m   v1.23.5
worker1   NotReady   &lt;none&gt;                 30s    v1.23.5
</code></pre>

<h2>安装网络</h2>

<ul>
<li><a href="https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model">https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model</a></li>
<li><a href="https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/">https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/</a></li>
<li><a href="https://github.com/flannel-io/flannel#deploying-flannel-manually">https://github.com/flannel-io/flannel#deploying-flannel-manually</a></li>
</ul>


<p>github上的资源好像也不能下载了，打开后复制内容到新建的文件中。</p>

<pre><code># For Kubernetes v1.17+ kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
[ec2-user@k8s ~]$ vi kube-flannel.yml 

[ec2-user@k8s ~]$ kubectl apply -f kube-flannel.yml
</code></pre>

<p>由于初始化 <code>kubeadm init</code> 时没有添加网络参数，导致这里flannel网络插件一直处于 CrashLoopBackOff 状态，查看日志提示没有分配 cidr 报错查日志</p>

<pre><code>## https://cloud-atlas.readthedocs.io/zh_CN/latest/kubernetes/debug/k8s_crashloopbackoff.html
# 查看日志
[ec2-user@k8s ~]$ kubectl describe -n kube-system pod kube-flannel-ds-sbx86 
  Warning  BackOff    2m5s (x69 over 16m)  kubelet            Back-off restarting failed container

[ec2-user@k8s ~]$ kubectl logs -n kube-system kube-flannel-ds-sbx86 
E0317 09:19:27.915383       1 main.go:317] Error registering network: failed to acquire lease: node "k8s" pod cidr not assigned
W0317 09:19:27.915664       1 reflector.go:436] github.com/flannel-io/flannel/subnet/kube/kube.go:379: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding

可以再通过docker查看flannel日志
[root@test4 profile]# docker ps -l
[root@test4 profile]# docker logs f7be3ebe77fd 

## https://www.talkwithtrend.com/Article/251751
# kube-controller-manager 没有给新加入的节点分配 IP 段，init 的时候没有指定 IP 段
# 加最后两行，和 kube-flannel.yml 中的 net-conf.json/Network 对应：
[ec2-user@k8s ~]$ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml 
  - command:
    - kube-controller-manager
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --use-service-account-credentials=true
    - --allocate-node-cidrs=true
    - --cluster-cidr=10.244.0.0/16

# 重启服务
## https://stackoverflow.com/questions/51375940/kubernetes-master-node-is-down-after-restarting-host-machine
[ec2-user@k8s ~]$ sudo systemctl restart kubelet  

# 重新部署
# 然后删除flannel容器，重新部署
[ec2-user@k8s ~]$ kubectl delete -f kube-flannel.yml 
[ec2-user@k8s ~]$ kubectl apply -f kube-flannel.yml
</code></pre>

<p>注：还有可以临时编辑节点的配置 手动分配podCIDR。这里不做具体描述，参考： <a href="http://www.hyhblog.cn/2021/02/21/k8s-flannel-pod-cidr-not-assigned/">http://www.hyhblog.cn/2021/02/21/k8s-flannel-pod-cidr-not-assigned/</a> 。</p>

<p>再次查看pod状态，网络组件安装好后，dns组件也跑起来了。</p>

<pre><code>[ec2-user@k8s ~]$ kubectl get pods --all-namespaces -o wide 
NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
kube-system   coredns-64897985d-4d5rx       1/1     Running   0          2m30s   10.244.0.3        k8s       &lt;none&gt;           &lt;none&gt;
kube-system   coredns-64897985d-m9p9q       1/1     Running   0          2m30s   10.244.0.2        k8s       &lt;none&gt;           &lt;none&gt;
kube-system   etcd-k8s                      1/1     Running   0          166m    192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-k8s            1/1     Running   0          166m    192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-k8s   1/1     Running   0          12m     192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kube-flannel-ds-q4qkt         1/1     Running   0          60s     192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
kube-system   kube-flannel-ds-ttcwt         1/1     Running   0          6m1s    192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-pd77m              1/1     Running   0          60s     192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-qj6lw              1/1     Running   0          166m    192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-k8s            1/1     Running   0          166m    192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
</code></pre>

<h2>安装dashboard</h2>

<pre><code>## https://github.com/kubernetes/dashboard#kubernetes-dashboard
# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml

[ec2-user@k8s ~]$ kubectl apply -f dashboard-v2.5.1.yml 

[ec2-user@k8s ~]$ kubectl get pods -A -o wide
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE     IP                NODE      NOMINATED NODE   READINESS GATES
kube-system            coredns-64897985d-4d5rx                      1/1     Running   0          36m     10.244.0.3        k8s       &lt;none&gt;           &lt;none&gt;
kube-system            coredns-64897985d-m9p9q                      1/1     Running   0          36m     10.244.0.2        k8s       &lt;none&gt;           &lt;none&gt;
kube-system            etcd-k8s                                     1/1     Running   0          3h20m   192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system            kube-apiserver-k8s                           1/1     Running   0          3h19m   192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system            kube-controller-manager-k8s                  1/1     Running   0          46m     192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system            kube-flannel-ds-q4qkt                        1/1     Running   0          34m     192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
kube-system            kube-flannel-ds-ttcwt                        1/1     Running   0          39m     192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system            kube-proxy-pd77m                             1/1     Running   0          34m     192.168.191.132   worker1   &lt;none&gt;           &lt;none&gt;
kube-system            kube-proxy-qj6lw                             1/1     Running   0          3h20m   192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kube-system            kube-scheduler-k8s                           1/1     Running   0          3h20m   192.168.191.131   k8s       &lt;none&gt;           &lt;none&gt;
kubernetes-dashboard   dashboard-metrics-scraper-799d786dbf-q87wv   1/1     Running   0          59s     10.244.2.3        worker1   &lt;none&gt;           &lt;none&gt;
kubernetes-dashboard   kubernetes-dashboard-fb8648fd9-vprpt         1/1     Running   0          59s     10.244.2.2        worker1   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>安装还是很便捷和容易的，访问搞起来比较麻烦，由于是在虚拟机里面部署，kubectl命令也都在虚拟机操作，用 <code>kubectl proxy</code> 访问dashboard，如果不是localhost或者https的话不给访问的。</p>

<p>尝试了绑定网卡ip，但是由于不是https，还是不能访问dashboard：</p>

<pre><code>[ec2-user@k8s ~]$ kubectl proxy --address='0.0.0.0' --accept-hosts='.*'
Starting to serve on [::]:8001
</code></pre>

<p>访问dashboard方法一：kubectl proxy 加上 ssh的locally port forward，把本地的8001的请求转发到 远程服务器的localhost:8001。</p>

<pre><code>## https://github.com/kubernetes/dashboard#access
[ec2-user@k8s ~]$ kubectl proxy 
Starting to serve on 127.0.0.1:8001
</code></pre>

<p>在SecureCRT的ssh会话的配置 Session Options 的 Connection - Port Forwarding 增加 Local Port Forwarding 的端口转发。在Local和Remote的Port输入框中都填入8001即可。</p>

<p>重新连接，这样我们访问 <code>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login</code> 就能访问到dashboard页面了。</p>

<p>方法二：后面直接通过查看dashboard服务的ip，通过 ssh的socks5代理 来访问 使用内部地址的dashboard。</p>

<pre><code>[ec2-user@k8s ~]$ kubectl -n kubernetes-dashboard get service kubernetes-dashboard
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes-dashboard   ClusterIP   10.101.193.109   &lt;none&gt;        443/TCP   6h38m

## 通过服务ip访问（Locally Port Forwarding - socks5方式代理）：
https://10.101.193.109/#/login
https://10.101.193.109/#/pod?namespace=kube-system
</code></pre>

<p>方法三：或者网上说的使用端口转发：</p>

<pre><code>## https://kubernetes.io/zh/docs/tasks/access-application-cluster/port-forward-access-application-cluster/
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 8080:443 --address='0.0.0.0'
</code></pre>

<p>能访问了，接下来就是获取token进行登录。同样有两种方式，第一种暴力设置跳过登录，第二种方式从系统中获取/创建一个token来登录。</p>

<p>登录方法一：</p>

<pre><code>## https://www.cnblogs.com/tylerzhou/p/11117956.html
# 在1.10.1里面默认不再显然skip按钮,其实dashboard安装有很多坑,如果有读者按照以上设置仍然不能正常成功登陆,但是仍然想要体验dashboard,可以开启默认关闭的skip按钮,这样就可以进入到dashboard管理界面了.
      containers:
      - args:
        - --auto-generate-certificates
        - --enable-skip-login            # &lt;-- add this line
        image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
</code></pre>

<p>改了配置后记得重新加载。</p>

<p>方法二：另一种方式是从系统获取token，然后填到界面上然后登录。访问dashboard：</p>

<ul>
<li><a href="https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard">https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard</a></li>
<li><a href="https://jimmysong.io/kubernetes-handbook/guide/auth-with-kubeconfig-or-token.html">https://jimmysong.io/kubernetes-handbook/guide/auth-with-kubeconfig-or-token.html</a></li>
</ul>


<pre><code>[ec2-user@k8s ~]$ kubectl -n kube-system get secret
# 这些secrets中的大部分都可以用来访问dashboard的,只有不同的账户权限不同,很多账户被限制不能进行操作.

[ec2-user@k8s ~]$ kubectl -n kube-system describe secret deployment-controller-token

# 使用一条命令来显示token
[ec2-user@k8s ~]$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | awk '/^deployment-controller-token-/{print $1}') | awk '$1=="token:"{print $2}'
eyJhbGciOiJSUzI1NiIsImtpZCI6IjQzNllWOFFBYU5qaXdtUmdLelJQSDU5T2FVbGVpREJFZTlMQU12MXFhN1UifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4tejVwbWQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNTcwNWJiMzYtMTMyNi00MGY5LWI3ZWUtNzE3ZTAyMTM1NzA2Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.Av-RwOQGdEyZn56xmH_siz-7yU07OrhLhfiPqfJRaNJ5DL8wEDIZkxgNMzHrrthTsOJl7Tky3ABo5z3c_4xjgADGSqKqP0rvWtaLSHZFZR16c5S2c08aHdSH7KIAdoCy0muMiKHRw67QRf7zo5bPUyqfCyPY2vcB-pxqYnrTTAw71f34rgIPA-LACc5LIQwv8DT5O-KE1TopYF7lX5hXZIHOGP3sYpmbR7yIzO3MDNRUIfiZutYiQnHwXRQGBwHu1iUVk8Lu69gnqggkjp2cXa4d2ZUpCxrpeLGGdjPv6JPZEFLDhLbiBLF04b7IOdFQO4bH6BbXBNs9e0AGPbvp4Q
[ec2-user@k8s ~]$ 
</code></pre>

<p>方法三：当然，也可以创建一个dashboard的拥有完整权限的token：</p>

<pre><code>$ kubectl create serviceaccount cluster-admin-dashboard-sa -n kube-system

$ kubectl create clusterrolebinding cluster-admin-dashboard-sa \
  --clusterrole=cluster-admin \
  --serviceaccount=kube-system:cluster-admin-dashboard-sa -n kube-system

And then, you can use the token of just created cluster admin service account.
$ kubectl get secret | grep cluster-admin-dashboard-sa
cluster-admin-dashboard-sa-token-6xm8l   kubernetes.io/service-account-token   3         18m
$ kubectl describe secret cluster-admin-dashboard-sa-token-6xm8l

# Parse the token
$ TOKEN=$(kubectl describe secret -n kube-system $(kubectl get secret -n kube-system | awk '/^cluster-admin-dashboard-sa-token-/{print $1}') | awk '$1=="token:"{print $2}')
$ echo $TOKEN

## -OR-
[ec2-user@k8s ~]$ kubectl describe secret cluster-admin-dashboard-sa
## -OR-
[ec2-user@k8s ~]$ kubectl describe secret -n kube-system | grep deployment -A 12
</code></pre>

<p>如果使用token登录，一段事件没有操作就会有超时的困扰，可以修改token-ttl配置。</p>

<pre><code>##--&gt; Unauthorized (401): You have been logged out because your token has expired.

## https://blog.csdn.net/otoyix/article/details/118758736
# 增加一行参数 token-ttl=68400
  containers:
    - name: kubernetes-dashboard
      image: 'kubernetesui/dashboard:v2.0.0-rc5'
      args:
        - '--auto-generate-certificates'
        - '--namespace=kubernetes-dashboard'
        - '--token-ttl=68400'    -- 增加了此行
</code></pre>

<h2>安装metrics-server</h2>

<p>如果没有安装metrics-server，在dashboard中不能看到cpu/内存使用情况图形，kubectl top的命令也获取不到数据。</p>

<pre><code>[ec2-user@k8s ~]$ kubectl top nodes
error: Metrics API not available
[ec2-user@k8s ~]$ kubectl top pods -A
error: Metrics API not available
</code></pre>

<p>安装metrics-server会有镜像下载和证书的问题：</p>

<pre><code># 每台主机都导入一下该镜像
[ec2-user@k8s ~]$ docker load -i metrics-server-v0.6.1.tar.gz 
3dc34f14eb83: Loading layer [==================================================&gt;]  66.43MB/66.43MB
Loaded image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1

# kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
[ec2-user@k8s ~]$ vi metrics-server.yml
[ec2-user@k8s ~]$ kubectl apply -f metrics-server.yml 
</code></pre>

<p>还是启动不起来，由于metrics-server需要连服务端，证书不对，为了先跑起来，先忽略安全证书。在containers参数最后加上 <code>--kubelet-insecure-tls</code> ，然后删除后重新创建一次。</p>

<pre><code>## [k8s metrics-server 轻量化监控](https://www.jianshu.com/p/5fe108d70310)
## https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#cannot-use-the-metrics-server-securely-in-a-kubeadm-cluster

## 证书 https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs

## https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md#how-to-run-metrics-server-securely
## https://github.com/kubernetes-sigs/metrics-server/issues/196
## https://cloud.tencent.com/developer/article/1819955
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
        image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1

[ec2-user@k8s ~]$ kubectl delete -f metrics-server.yml 
[ec2-user@k8s ~]$ kubectl apply -f metrics-server.yml 
</code></pre>

<p>等一小会，再次查看top命令。</p>

<pre><code>[ec2-user@k8s ~]$ kubectl top nodes 
NAME      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s       91m          4%     913Mi           23%       
worker1   38m          1%     431Mi           11%       
[ec2-user@k8s ~]$ 
[ec2-user@k8s ~]$ kubectl top pods -n kube-system
NAME                              CPU(cores)   MEMORY(bytes)   
coredns-64897985d-4d5rx           1m           12Mi            
coredns-64897985d-m9p9q           1m           12Mi            
etcd-k8s                          11m          60Mi            
kube-apiserver-k8s                32m          312Mi           
kube-controller-manager-k8s       13m          46Mi            
kube-flannel-ds-q4qkt             2m           11Mi            
kube-flannel-ds-ttcwt             2m           11Mi            
kube-proxy-pd77m                  7m           16Mi            
kube-proxy-qj6lw                  2m           16Mi            
kube-scheduler-k8s                3m           17Mi            
metrics-server-7cf8b65d65-trtcj   33m          11Mi            
[ec2-user@k8s ~]$ 
</code></pre>

<p>同时dashboard web界面就能看到cpu/内存的性能图形了。</p>

<h2>Hello world</h2>

<p>编写一个配置，然后运行一个实例，看看两台机器上的pod网络是否互通。</p>

<pre><code>## docker run -it public.ecr.aws/amazonlinux/amazonlinux /bin/bash

[ec2-user@k8s ~]$ cat replicaset.yml 
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: hello-world
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hello-world
  template:
    metadata:
      labels:
        app: hello-world
    spec:
      containers:
      - name: hello-world
        image: amazonlinux:2
        command: ["/bin/sh"]
        args: ["-c", "while true; do echo hello; sleep 10;done"]
[ec2-user@k8s ~]$ 

[ec2-user@k8s ~]$ kubectl apply -f replicaset.yml  
replicaset.apps/hello-world created

[ec2-user@k8s ~]$ kubectl get pods -o wide
NAME                READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
hello-world-d2tss   1/1     Running   0          8s    10.244.0.7    k8s       &lt;none&gt;           &lt;none&gt;
hello-world-h9jxq   1/1     Running   0          8s    10.244.2.12   worker1   &lt;none&gt;           &lt;none&gt;
[ec2-user@k8s ~]$ 

[ec2-user@k8s ~]$ kubectl exec -ti hello-world-d2tss bash 
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
bash-4.2# cat /etc/hosts 

bash-4.2# yum install -y iputils net-tools 

bash-4.2# ping hello-world-h9jxq           
ping: hello-world-h9jxq: Name or service not known
服务service才有域名。后面试一下服务的，来ping域名，测试下dns。


bash-4.2# ping 10.244.0.7 
PING 10.244.0.7 (10.244.0.7) 56(84) bytes of data.
64 bytes from 10.244.0.7: icmp_seq=1 ttl=255 time=0.012 ms
64 bytes from 10.244.0.7: icmp_seq=2 ttl=255 time=0.021 ms
^C
--- 10.244.0.7 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1007ms
rtt min/avg/max/mdev = 0.012/0.016/0.021/0.006 ms

bash-4.2# ping 10.244.2.12
PING 10.244.2.12 (10.244.2.12) 56(84) bytes of data.
64 bytes from 10.244.2.12: icmp_seq=1 ttl=253 time=0.508 ms
64 bytes from 10.244.2.12: icmp_seq=2 ttl=253 time=0.425 ms
^C
--- 10.244.2.12 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1027ms
rtt min/avg/max/mdev = 0.425/0.466/0.508/0.046 ms
</code></pre>

<h2>Service domain</h2>

<p>配置启动容器和服务：</p>

<pre><code>[ec2-user@k8s ~]$ cat pg-db.yml 
---
apiVersion: v1
kind: Pod
metadata:
  name: db-op-1 
  labels:
    name: postgres
spec:
  hostname: db-op-1
  containers:
  - name: postgres
    image: postgis/postgis:9.6-2.5
    imagePullPolicy: IfNotPresent
---
apiVersion: v1
kind: Service
metadata:
  name: db-op-1
spec:
  ports:
  - protocol: TCP
    port: 5432
  selector:
    name: postgres

[ec2-user@k8s ~]$ kubectl apply -f pg-db.yml 
</code></pre>

<p>在默认的namespace中再启动一个postgis的容器，用来测试访问域名：</p>

<pre><code>[ec2-user@k8s ~]$ kubectl run busybox --image=postgis/postgis:9.6-2.5 -ti --restart=Never --rm  --command -- sh                
If you don't see a command prompt, try pressing enter.

# apt update ; apt-get install net-tools iproute2 iputils-ping -y

# cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local localdomain
options ndots:5

# ping db-op-1
PING db-op-1.default.svc.cluster.local (10.107.190.149) 56(84) bytes of data.

# psql -h db-op-1 -U postgres
Password for user postgres: 
psql (9.6.24)
Type "help" for help.

postgres=# \q
</code></pre>

<h2>参考</h2>

<ul>
<li><a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubeadm部署k8s(资源已有)]]></title>
    <link href="http://winse.github.io/blog/2017/08/13/kubeadm-install-k8s-on-centos7-with-resources/"/>
    <updated>2017-08-13T08:05:33+08:00</updated>
    <id>http://winse.github.io/blog/2017/08/13/kubeadm-install-k8s-on-centos7-with-resources</id>
    <content type="html"><![CDATA[<p>上一篇安装的文章操作过程中需要用到代理，期间会遇到穿插了各种问题，显的有点乱。在本地虚拟机安装调通后，今天把测试环境也升级了一下。再写一篇思路清晰一点的总结。</p>

<p>安装需要的rpm和docker images可以通过百度网盘下载：<a href="http://pan.baidu.com/s/1hrRs5MW">http://pan.baidu.com/s/1hrRs5MW</a> 。</p>

<p>预先需要做的工作，这些都已经配置好了的：  <br/>
* 时间同步， <br/>
* 主机名，<br/>
* /etc/hosts，
* 防火墙，<br/>
* selinux，  <br/>
* 无密钥登录，  <br/>
* 安装docker-1.12.6</p>

<p>主机集群的情况：
* 机器：cu[1-5]
* 主节点： cu3
* 跳板机： cu2（有外网IP）</p>

<h2>首先做YUM本地仓库，并把docker镜像导入到所有node节点</h2>

<p>首先在一台主机上部署YUM本地仓库</p>

<pre><code>[root@cu2 ~]# cd /var/www/html/kubernetes/
[root@cu2 kubernetes]# createrepo .
[root@cu2 kubernetes]# ll
total 42500
-rw-r--r-- 1 hadoop hadoop  8974214 Aug 10 15:22 1a6f5f73f43077a50d877df505481e5a3d765c979b89fda16b8b9622b9ebd9a4-kubeadm-1.7.2-0.x86_64.rpm
-rw-r--r-- 1 hadoop hadoop 17372710 Aug 10 15:22 1e508e26f2b02971a7ff5f034b48a6077d613e0b222e0ec973351117b4ff45ea-kubelet-1.7.2-0.x86_64.rpm
-rw-r--r-- 1 hadoop hadoop  9361006 Aug 10 15:22 dc8329515fc3245404fea51839241b58774e577d7736f99f21276e764c309db5-kubectl-1.7.2-0.x86_64.rpm
-rw-r--r-- 1 hadoop hadoop  7800562 Aug 10 15:22 e7a4403227dd24036f3b0615663a371c4e07a95be5fee53505e647fd8ae58aa6-kubernetes-cni-0.5.1-0.x86_64.rpm
drwxr-xr-x 2 root   root       4096 Aug 10 15:58 repodata
</code></pre>

<p></p>

<p>（所有node）导入新镜像</p>

<pre><code>在cu2上操作，导入docker镜像

docker load &lt;/home/hadoop/kubeadm.tar
ssh cu1 docker load &lt;/home/hadoop/kubeadm.tar 
ssh cu3 docker load &lt;/home/hadoop/kubeadm.tar
ssh cu4 docker load &lt;/home/hadoop/kubeadm.tar
ssh cu5 docker load &lt;/home/hadoop/kubeadm.tar

Loaded image: gcr.io/google_containers/etcd-amd64:3.0.17
Loaded image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.3
Loaded image: gcr.io/google_containers/kube-controller-manager-amd64:v1.7.2
Loaded image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4
Loaded image: gcr.io/google_containers/heapster-amd64:v1.3.0
Loaded image: gcr.io/google_containers/kube-scheduler-amd64:v1.7.2
Loaded image: gcr.io/google_containers/heapster-grafana-amd64:v4.4.1
Loaded image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4
Loaded image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4
Loaded image: centos:centos6
Loaded image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1
Loaded image: gcr.io/google_containers/pause-amd64:3.0
Loaded image: nginx:latest
Loaded image: gcr.io/google_containers/kube-apiserver-amd64:v1.7.2
Loaded image: gcr.io/google_containers/kube-proxy-amd64:v1.7.2
Loaded image: quay.io/coreos/flannel:v0.8.0-amd64
</code></pre>

<p>YUM仓库配置</p>

<pre><code>在cu2上操作

cat &gt; /etc/yum.repos.d/dta.repo  &lt;&lt;EOF
[K8S]
name=K8S Local
baseurl=http://cu2:801/kubernetes
enabled=1
gpgcheck=0
EOF

for h in cu{1,3:5} ; do scp /etc/yum.repos.d/dta.repo $h:/etc/yum.repos.d/ ; done
</code></pre>

<h2>安装kubeadm、kubelet</h2>

<pre><code>pdsh -w cu[1-5] "yum clean all; yum install -y kubelet kubeadm; systemctl enable kubelet "
</code></pre>

<h2>使用kubeadm部署集群</h2>

<h4>master节点</h4>

<p>初始化</p>

<pre><code>[root@cu3 ~]# kubeadm init --skip-preflight-checks --pod-network-cidr=10.244.0.0/16 --kubernetes-version=v1.7.2 
</code></pre>

<p>启动后会卡在了 <strong> Created API client, waiting for the control plane to become ready </strong> ， 不要关闭当前的窗口。新开一个窗口，查看并定位解决错误：</p>

<p>问题1</p>

<p>新打开一个窗口，查看 /var/log/messages 有如下错误：</p>

<pre><code>Aug 12 23:40:10 cu3 kubelet: error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: "systemd" is different from docker cgroup driver: "cgroupfs"
</code></pre>

<p>docker和kubelet的cgroup driver不一样，修改kubelet的配置。同时把docker启动参数 masq 一起改了。</p>

<pre><code>[root@cu3 ~]# sed -i 's/KUBELET_CGROUP_ARGS=--cgroup-driver=systemd/KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs/' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 
[root@cu3 ~]# sed -i 's#/usr/bin/dockerd.*#/usr/bin/dockerd --ip-masq=false#' /usr/lib/systemd/system/docker.service

[root@cu3 ~]# systemctl daemon-reload; systemctl restart docker kubelet 
</code></pre>

<p>多开几个窗口来解决问题，不会影响kubeadm运行的。就是说，由于其他的问题导致kubeadm中间卡住，只要你解决了问题，kubeadm就会继续配置直到成功。</p>

<p></p>

<p>初始化完后，窗口完整日志如下：</p>

<pre><code>[root@cu3 ~]# kubeadm init --skip-preflight-checks --pod-network-cidr=10.244.0.0/16 --kubernetes-version=v1.7.2 
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.7.2
[init] Using Authorization modes: [Node RBAC]
[preflight] Skipping pre-flight checks
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated CA certificate and key.
[certificates] Generated API server certificate and key.
[certificates] API Server serving cert is signed for DNS names [cu3 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.148]
[certificates] Generated API server kubelet client certificate and key.
[certificates] Generated service account token signing key and public key.
[certificates] Generated front-proxy CA certificate and key.
[certificates] Generated front-proxy client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[apiclient] Created API client, waiting for the control plane to become ready
 [apiclient] All control plane components are healthy after 494.001036 seconds
[token] Using token: ad430d.beff5be4b98dceec
[apiconfig] Created RBAC rules
[addons] Applied essential addon: kube-proxy
[addons] Applied essential addon: kube-dns

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular user):

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token ad430d.beff5be4b98dceec 192.168.0.148:6443
</code></pre>

<p>然后按照上面的提示，把kubectl要用的配置文件弄好：</p>

<pre><code>  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<p>到这里K8S的基础服务controller，apiserver，scheduler是起来了，但是dns还是有问题：</p>

<pre><code>[root@cu3 kubeadm]# kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-cu3                      1/1       Running   0          6m
kube-system   kube-apiserver-cu3            1/1       Running   0          5m
kube-system   kube-controller-manager-cu3   1/1       Running   0          6m
kube-system   kube-dns-2425271678-wwnkp     0/3       Pending   0          6m
kube-system   kube-proxy-ptnlx              1/1       Running   0          6m
kube-system   kube-scheduler-cu3            1/1       Running   0          6m
</code></pre>

<p>dns的容器是使用bridge网络，需要配置网络才能跑起来。有如下错误日志：</p>

<pre><code>Aug 12 23:54:04 cu3 kubelet: W0812 23:54:04.800316   12886 cni.go:189] Unable to update cni config: No networks found in /etc/cni/net.d
Aug 12 23:54:04 cu3 kubelet: E0812 23:54:04.800472   12886 kubelet.go:2136] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
</code></pre>

<p>下载 <a href="https://github.com/winse/docker-hadoop/tree/master/kube-deploy/kubeadm">https://github.com/winse/docker-hadoop/tree/master/kube-deploy/kubeadm</a> 目录下的 flannel 配置：</p>

<p>flannel配置文件稍微改了一下，在官网的文件基础上 cni-conf.json 增加了： <code>"ipMasq": false,</code></p>

<p></p>

<pre><code># 配置网络
[root@cu3 kubeadm]# kubectl apply -f kube-flannel.yml 
kubectl apply -f kube-flannel-rbac.yml 
serviceaccount "flannel" created
configmap "kube-flannel-cfg" created
daemonset "kube-flannel-ds" created
[root@cu3 kubeadm]# kubectl apply -f kube-flannel-rbac.yml 
clusterrole "flannel" created
clusterrolebinding "flannel" created

# 等待一段时间后，dns的pods也启动好了
[root@cu3 kubeadm]# kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE
kube-system   etcd-cu3                      1/1       Running   0          7m
kube-system   kube-apiserver-cu3            1/1       Running   0          7m
kube-system   kube-controller-manager-cu3   1/1       Running   0          7m
kube-system   kube-dns-2425271678-wwnkp     3/3       Running   0          8m
kube-system   kube-flannel-ds-dbvkj         2/2       Running   0          38s
kube-system   kube-proxy-ptnlx              1/1       Running   0          8m
kube-system   kube-scheduler-cu3            1/1       Running   0          7m
</code></pre>

<h2>Node节点部署</h2>

<p>配置kubelet、docker</p>

<pre><code>sed -i 's/KUBELET_CGROUP_ARGS=--cgroup-driver=systemd/KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs/' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 
sed -i 's#/usr/bin/dockerd.*#/usr/bin/dockerd --ip-masq=false#' /usr/lib/systemd/system/docker.service 

systemctl daemon-reload; systemctl restart docker kubelet 
</code></pre>

<p>注意：加了 ip-masq=false 后，docker0就不能上外网了。也就是说用docker命令单独起的docker容器不能上外网了！</p>

<pre><code>ExecStart=/usr/bin/dockerd --ip-masq=false
</code></pre>

<p>加入集群</p>

<pre><code>kubeadm join --token ad430d.beff5be4b98dceec 192.168.0.148:6443 --skip-preflight-checks

[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[preflight] Skipping pre-flight checks
[discovery] Trying to connect to API Server "192.168.0.148:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.0.148:6443"
[discovery] Cluster info signature and contents are valid, will use API Server "https://192.168.0.148:6443"
[discovery] Successfully established connection with API Server "192.168.0.148:6443"
[bootstrap] Detected server version: v1.7.2
[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)
[csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request
[csr] Received signed certificate from the API server, generating KubeConfig...
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"

Node join complete:
* Certificate signing request sent to master and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on the master to see this machine join.
</code></pre>

<p>CU2是跳板机，把kubectl的config配置拷贝过来，然后就可以在CU2上面运行命令：</p>

<p></p>

<pre><code>[root@cu2 kube-deploy]# kubectl get nodes
NAME      STATUS     AGE         VERSION
cu2       NotReady   &lt;invalid&gt;   v1.7.2
cu3       Ready      25m         v1.7.2

[root@cu2 kube-deploy]# kubectl proxy 
Starting to serve on 127.0.0.1:8001
</code></pre>

<p>我SecureCRT Socks代理做在这台机器上，本地浏览器访问 <a href="http://localhost:8001/ui">http://localhost:8001/ui</a>。。。咔咔</p>

<p>5台机器都添加成功后：</p>

<pre><code>[root@cu3 ~]# kubectl get nodes 
NAME      STATUS    AGE       VERSION
cu1       Ready     32s       v1.7.2
cu2       Ready     3m        v1.7.2
cu3       Ready     29m       v1.7.2
cu4       Ready     26s       v1.7.2
cu5       Ready     20s       v1.7.2
</code></pre>

<p>所有节点防火墙配置(由于是云主机，增加防火墙)：</p>

<pre><code>firewall-cmd --zone=trusted --add-source=192.168.0.0/16 --permanent 
firewall-cmd --zone=trusted --add-source=10.0.0.0/8 --permanent 
firewall-cmd --complete-reload
</code></pre>

<h2>SOURCE IP测试</h2>

<p>上次操作时有Sourceip的问题，现在应该不存在。。。看了iptables-save的信息，没有cni0/cbr0的相关的数据</p>

<p>还是再来测一遍：</p>

<p></p>

<pre><code>kubectl run centos --image=cu.esw.cn/library/java:jdk8 --command -- vi 
kubectl scale --replicas=4 deployment/centos

[root@cu2 kube-deploy]# pods
NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE         IP              NODE
default       centos-3954723268-62tpc                 1/1       Running   0          &lt;invalid&gt;   10.244.2.2      cu1
default       centos-3954723268-6cmf9                 1/1       Running   0          &lt;invalid&gt;   10.244.1.2      cu2
default       centos-3954723268-blfc4                 1/1       Running   0          &lt;invalid&gt;   10.244.3.2      cu4
default       centos-3954723268-tb1rn                 1/1       Running   0          &lt;invalid&gt;   10.244.4.2      cu5
default       nexus-djr9c                             1/1       Running   0          2m          192.168.0.37    cu1

# ping互通没问题 TEST

[root@cu2 hadoop]# ./pod_bash centos-3954723268-62tpc default
[root@centos-3024873821-4490r /]# ping 10.244.4.2 -c 1

# 源IP没问题 TEST

[root@centos-3954723268-62tpc opt]# yum install epel-release -y  
[root@centos-3954723268-62tpc opt]# yum install -y nginx 
[root@centos-3954723268-62tpc opt]# service nginx start

[root@centos-3954723268-blfc4 opt]# curl 10.244.2.2
[root@centos-3954723268-tb1rn opt]# curl 10.244.2.2

[root@centos-3954723268-62tpc opt]# less /var/log/nginx/access.log 
</code></pre>

<p></p>

<h4>DNS/heaspter</h4>

<p>奇了怪了，这次重新安装DNS时没遇到问题，heaspter安装也一次通过。</p>

<p>在cu3起的pods上执行 <code>nslookup kubernetes.default</code> 也是通的！</p>

<h4>监控</h4>

<pre><code># -- heaspter
[root@cu2 kubeadm]# kubectl apply -f heapster/influxdb/
deployment "monitoring-grafana" created
service "monitoring-grafana" created
serviceaccount "heapster" created
deployment "heapster" created
service "heapster" created
deployment "monitoring-influxdb" created
service "monitoring-influxdb" created
[root@cu2 kubeadm]# kubectl apply -f heapster/rbac/
clusterrolebinding "heapster" created

# -- dashboard
[root@cu2 kubeadm]# kubectl apply -f kubernetes-dashboard.yaml 
serviceaccount "kubernetes-dashboard" created
clusterrolebinding "kubernetes-dashboard" created
deployment "kubernetes-dashboard" created
service "kubernetes-dashboard" created

[root@cu2 kubeadm]# kubectl get service --all-namespaces
NAMESPACE     NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
default       kubernetes             10.96.0.1       &lt;none&gt;        443/TCP         18m
kube-system   kube-dns               10.96.0.10      &lt;none&gt;        53/UDP,53/TCP   18m
kube-system   kubernetes-dashboard   10.104.165.81   &lt;none&gt;        80/TCP          5m
</code></pre>

<p>等一小段时间，查看所有的服务：</p>

<pre><code>[root@cu2 kubeadm]# kubectl get services --all-namespaces
NAMESPACE     NAME                   CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
default       kubernetes             10.96.0.1        &lt;none&gt;        443/TCP         2h
kube-system   heapster               10.102.176.168   &lt;none&gt;        80/TCP          3m
kube-system   kube-dns               10.96.0.10       &lt;none&gt;        53/UDP,53/TCP   2h
kube-system   kubernetes-dashboard   10.110.2.118     &lt;none&gt;        80/TCP          2m
kube-system   monitoring-grafana     10.106.251.155   &lt;none&gt;        80/TCP          3m
kube-system   monitoring-influxdb    10.100.168.147   &lt;none&gt;        8086/TCP        3m
</code></pre>

<p>直接访问 10.106.251.155 或者查看 monitoring的pod 日志，查看heaspter的状态。dashboard上面出图要等一小段时间才行。</p>

<p>如果通过 monitoring-grafana 的IP访问能看到CLUSTER和POD的监控图，但是dashboard上的图就是出不来，可以重新部署dashboard：</p>

<pre><code>kubectl delete -f kubernetes-dashboard.yaml 
kubectl create -f kubernetes-dashboard.yaml 
</code></pre>

<p>到此整个K8S就在测试环境上重新运行起来了。</p>

<p>harbor就不安装了，平时没怎么用，也就5台机器直接save然后load工作量也不多。</p>

<h2>参考</h2>

<ul>
<li><a href="https://github.com/kubernetes/kubernetes/issues/40969">https://github.com/kubernetes/kubernetes/issues/40969</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzI4MTQyMDAxMA==&amp;mid=2247483665&amp;idx=1&amp;sn=d8b61666fe0a0965336d15250e2648cb&amp;scene=0">http://mp.weixin.qq.com/s?__biz=MzI4MTQyMDAxMA==&amp;mid=2247483665&amp;idx=1&amp;sn=d8b61666fe0a0965336d15250e2648cb&amp;scene=0</a></li>
<li><a href="http://cizixs.com/2017/05/23/container-network-cni">http://cizixs.com/2017/05/23/container-network-cni</a></li>
<li><a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration">https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubeadm部署kubernetes]]></title>
    <link href="http://winse.github.io/blog/2017/07/30/kubeadm-install-kubenetes-on-centos7/"/>
    <updated>2017-07-30T20:18:33+08:00</updated>
    <id>http://winse.github.io/blog/2017/07/30/kubeadm-install-kubenetes-on-centos7</id>
    <content type="html"><![CDATA[<ul>
<li>更新2018-03-26：这篇写的太杂了，说的也是稀里糊涂的。如果仅仅是k8s安装请直接参考 <a href="http://www.winseliu.com/blog/2017/08/13/kubeadm-install-k8s-on-centos7-with-resources/">Kubeadm部署k8s(镜像资源已有)</a></li>
</ul>


<p>官网文档差，删文档倒是不手软。使用脚本启动、安装的文档（docker-multinode）已经删掉了，现在都推荐使用kubeadm来进行安装。</p>

<p>本文使用代理在master上安装、并缓冲rpm、下载docker镜像，然后做本地YUM仓库和拷贝镜像到其他worker节点的方式来部署集群。下一篇再介绍在拥有kubelet/kubeadm rpm、以及k8s docker镜像的情况下怎么去部署一个新的k8s集群。</p>

<p>这里使用两台虚拟机做测试：</p>

<ul>
<li>k8s kube-master : 192.168.191.138</li>
<li>woker1 : 192.168.191.139</li>
</ul>


<h2>修改主机名，改时间、时区，防火墙</h2>

<pre><code>hostnamectl --static set-hostname k8s 
hostname k8s 

rm -rf /etc/localtime 
ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 

systemctl disable firewalld ; service firewalld stop
</code></pre>

<h2>安装docker</h2>

<ul>
<li><a href="https://docs.docker.com/v1.12/engine/installation/linux/rhel/">https://docs.docker.com/v1.12/engine/installation/linux/rhel/</a></li>
<li><a href="https://yum.dockerproject.org/repo/main/centos/7/Packages/">https://yum.dockerproject.org/repo/main/centos/7/Packages/</a> 打开看下1.12的具体版本</li>
<li><a href="https://docs.docker.com/v1.12/engine/admin/systemd/">https://docs.docker.com/v1.12/engine/admin/systemd/</a>  *</li>
</ul>


<pre><code>
tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'
[dockerrepo]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/7/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg
EOF

yum list docker-engine --showduplicates

yum install docker-engine-1.12.6 docker-engine-selinux-1.12.6 -y
systemctl enable docker ; systemctl start docker
</code></pre>

<h2>翻墙安装配置</h2>

<p>具体操作参考 <a href="/blog/2017/02/04/privoxy-http-proxy-for-shadowsocks">使用Privoxy把shadowsocks转换为Http代理</a></p>

<p></p>

<pre><code>[root@k8s ~]# yum install -y epel-release ; yum install -y python-pip 
[root@k8s ~]# pip install shadowsocks
[root@k8s ~]# vi /etc/shadowsocks.json 
[root@k8s ~]# sslocal -c /etc/shadowsocks.json 
[root@k8s ~]# curl --socks5-hostname 127.0.0.1:1080 www.google.com

[root@k8s ~]# yum install privoxy -y
[root@k8s ~]# vi /etc/privoxy/config 
...
forward-socks5 / 127.0.0.1:1080 .
listen-address 192.168.191.138:8118

[root@k8s ~]# systemctl enable privoxy
[root@k8s ~]# systemctl start privoxy

[root@k8s ~]# curl -x 192.168.191.138:8118 www.google.com

    等k8s安装启动好后，把privoxy的服务disable掉
    [root@k8s ~]# systemctl disable privoxy.service
</code></pre>

<h2>下载kubectl（怪了，这个竟然可以直接下载）</h2>

<p>变化好快，现在都1.7.2了！ <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a></p>

<p>在master机器（常用的操作机器）安装即可。</p>

<pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
mv ./kubectl /usr/local/bin/kubectl

# 启用shell的提示/自动完成autocompletion
echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc

[root@k8s ~]# kubectl version 
Client Version: version.Info{Major:"1", Minor:"7", GitVersion:"v1.7.2", GitCommit:"922a86cfcd65915a9b2f69f3f193b8907d741d9c", GitTreeState:"clean", BuildDate:"2017-07-21T08:23:22Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}
The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre>

<p></p>

<h2>通过VPN安装kubelet和kubeadm</h2>

<p>参考 <a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#installing-kubelet-and-kubeadm">https://kubernetes.io/docs/setup/independent/install-kubeadm/#installing-kubelet-and-kubeadm</a></p>

<p>You will install these packages on all of your machines:</p>

<ul>
<li>kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers.</li>
<li>kubeadm: the command to bootstrap the cluster.</li>
</ul>


<p>所有机器都要安装的，我们先在master节点上通过代理安装这两个软件，并把安装的所有rpm缓冲起来。</p>

<ul>
<li>配置kubernetes的仓库源：</li>
</ul>


<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config 
setenforce 0

yum-config-manager --enable kubernetes
</code></pre>

<ul>
<li>YUM配置socks5代理： <a href="https://unix.stackexchange.com/questions/43654/how-to-use-socks-proxy-with-yum">https://unix.stackexchange.com/questions/43654/how-to-use-socks-proxy-with-yum</a></li>
</ul>


<p></p>

<p>修改yum的配置，增加代理，并缓冲（用于其他机器安装）</p>

<pre><code>[root@k8s ~]# vi /etc/yum.conf 
keepcache=1
...
proxy=socks5://127.0.0.1:1080
</code></pre>

<ul>
<li>安装并启动kubelet：</li>
</ul>


<pre><code>yum install -y kubelet kubeadm

[root@k8s ~]# systemctl enable kubelet &amp;&amp; systemctl start kubelet
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/system/kubelet.service.
[root@k8s ~]# 
</code></pre>

<h2>通过VPN安装初始化集群</h2>

<p><strong>主要是配置代理下载docker容器</strong></p>

<p>由于是直接docker去获取镜像的，首先需要修改docker的配置。</p>

<p>参考 <a href="https://docs.docker.com/v1.12/engine/admin/systemd/#/http-proxy">https://docs.docker.com/v1.12/engine/admin/systemd/#/http-proxy</a></p>

<ul>
<li>配置代理并重启docker、kubelet</li>
</ul>


<pre><code>[root@k8s ~]# systemctl enable docker

[root@k8s ~]# mkdir -p /etc/systemd/system/docker.service.d/
[root@k8s ~]# vi /etc/systemd/system/docker.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://192.168.191.138:8118/" "HTTPS_PROXY=http://192.168.191.138:8118/" "NO_PROXY=localhost,127.0.0.1,10.0.0.0/8,192.168.191.138"

[root@k8s ~]# systemctl daemon-reload
[root@k8s ~]# systemctl restart docker
</code></pre>

<p>docker和kubelet的cgroup驱动方式不同，需要修复配置：<a href="https://github.com/kubernetes/kubeadm/issues/103">https://github.com/kubernetes/kubeadm/issues/103</a></p>

<pre><code>前面启动了一下kubelet，有如下的错误日志
[root@k8s ~]# journalctl -xeu kubelet
Jul 29 09:11:24 k8s kubelet[48557]: error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: "systemd" is different from docker cgroup driver: "cgr

修改配置
[root@k8s ~]# vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"

[root@k8s ~]# systemctl daemon-reload
[root@k8s ~]# service kubelet restart
Redirecting to /bin/systemctl restart  kubelet.service
</code></pre>

<ul>
<li>使用kubeadm进行初始化</li>
</ul>


<p><a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</a> （可以使用 &ndash;kubernetes-version 来指定k8s的版本）</p>

<pre><code># 配置代理，kubeadm有部分请求应该也是需要走代理的（前面用脚本安装过multinode on docker的经历猜测的）

export NO_PROXY="localhost,127.0.0.1,10.0.0.0/8,192.168.191.138"
export https_proxy=http://192.168.191.138:8118/
export http_proxy=http://192.168.191.138:8118/

# 使用reset重置，网络代理的配置修改了多次（kubeadm初始换过程失败过），还有前几次的初始化没有配置pod地址段

[root@k8s ~]# kubeadm reset
[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Removing kubernetes-managed containers
[reset] Deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/lib/etcd]
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

# 使用flannel需要指定pod的网卡地址段（文档要整体看一遍才能少踩坑，囧）

[root@k8s ~]# kubeadm init --skip-preflight-checks --pod-network-cidr=10.244.0.0/16
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.7.2
[init] Using Authorization modes: [Node RBAC]
[preflight] Skipping pre-flight checks
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated CA certificate and key.
[certificates] Generated API server certificate and key.
[certificates] API Server serving cert is signed for DNS names [k8s kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.191.138]
[certificates] Generated API server kubelet client certificate and key.
[certificates] Generated service account token signing key and public key.
[certificates] Generated front-proxy CA certificate and key.
[certificates] Generated front-proxy client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[apiclient] Created API client, waiting for the control plane to become ready  
&lt;-&gt; 这里会停的比较久，要去下载镜像，然后还得启动容器
[apiclient] All control plane components are healthy after 293.004469 seconds
[token] Using token: 2af779.b803df0b1effb3d9
[apiconfig] Created RBAC rules
[addons] Applied essential addon: kube-proxy
[addons] Applied essential addon: kube-dns

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular user):

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token 2af779.b803df0b1effb3d9 192.168.191.138:6443

[root@k8s ~]# 
</code></pre>

<p>监控安装情况命令有： <code>docker ps</code>, <code>docker images</code>, <code>journalctl -xeu kubelet</code> (/var/log/messages) 。</p>

<p>如果有镜像下载和容器新增，说明安装过程在进行中。否则得检查下你的代理是否正常工作了！</p>

<p>初始化完成后，配置kubectl的kubeconfig。一般都是主节点了，直接在节点执行下面命令：</p>

<pre><code>[root@k8s ~]# mkdir -p $HOME/.kube
[root@k8s ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@k8s ~]# chown $(id -u):$(id -g) $HOME/.kube/config
[root@k8s ~]# 
[root@k8s ~]# ll ~/.kube/
total 8
drwxr-xr-x. 3 root root   23 Jul 29 21:39 cache
-rw-------. 1 root root 5451 Jul 29 22:57 config
</code></pre>

<p></p>

<p><a href="http://tonybai.com/2016/12/30/install-kubernetes-on-ubuntu-with-kubeadm/">使用Kubeadm安装Kubernetes</a> 介绍了很多作者自己安装过程，以及遇到的问题，非常详细。安装的差不多才发现这篇文章，感觉好迟，如果早点找到，至少安装的时刻心安一点啊。</p>

<p>OK，服务启动了，但是 dns容器 还没有正常启动。由于我们的网络组建还没有安装好啊。其实官网也有说明，但是这安装的顺序也是醉了。</p>

<p> <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</a></p>

<h2>安装flannel</h2>

<p>参考： <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network</a></p>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml
</code></pre>

<p>flannel启动了后，再等一阵，dns才会启动好。</p>

<p></p>

<h2>安装dashboard</h2>

<pre><code>现在就一台机器，得让master也能跑pods。 
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#master-isolation

[root@k8s ~]# kubectl taint nodes --all node-role.kubernetes.io/master-
node "k8s" untainted

# https://lukemarsden.github.io/docs/user-guide/ui/
# 部署dashboard

[root@k8s ~]# kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml

[root@k8s ~]# kubectl get pods --all-namespaces 看看dashboard的情况

[root@k8s ~]# kubectl get services --all-namespaces
NAMESPACE     NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
default       kubernetes             10.96.0.1       &lt;none&gt;        443/TCP         1h
kube-system   kube-dns               10.96.0.10      &lt;none&gt;        53/UDP,53/TCP   1h
kube-system   kubernetes-dashboard   10.107.103.17   &lt;none&gt;        80/TCP          9m
</code></pre>

<p>用 <a href="https://master:6443/ui">https://master:6443/ui</a> 访问不了，可以直接用k8s的service地址访问 <a href="http://10.107.103.17/#!/overview?namespace=kube-system">http://10.107.103.17/#!/overview?namespace=kube-system</a></p>

<p></p>

<p>或者通过 <strong> proxy </strong> 访问UI：<a href="https://github.com/kubernetes/kubernetes/issues/44275">https://github.com/kubernetes/kubernetes/issues/44275</a></p>

<p>先运行proxy，启动代理程序：</p>

<pre><code>[root@k8s ~]# kubectl proxy
Starting to serve on 127.0.0.1:8001
</code></pre>

<p>然后访问： <a href="http://localhost:8001/ui">http://localhost:8001/ui</a></p>

<h2>所有的pods、镜像、容器</h2>

<p>基本的东西都跑起来，还是挺激动啊！！第N次安装部署K8S了啊，每次都还是得像坐过山车一样啊！</p>

<p></p>

<pre><code>[root@k8s ~]# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE       IP                NODE
kube-system   etcd-k8s                                1/1       Running   0          9h        192.168.191.138   k8s
kube-system   kube-apiserver-k8s                      1/1       Running   0          9h        192.168.191.138   k8s
kube-system   kube-controller-manager-k8s             1/1       Running   0          9h        192.168.191.138   k8s
kube-system   kube-dns-2425271678-qwx9f               3/3       Running   0          9h        10.244.0.2        k8s
kube-system   kube-flannel-ds-s5f63                   2/2       Running   0          9h        192.168.191.138   k8s
kube-system   kube-proxy-4pjkg                        1/1       Running   0          9h        192.168.191.138   k8s
kube-system   kube-scheduler-k8s                      1/1       Running   0          9h        192.168.191.138   k8s
kube-system   kubernetes-dashboard-3313488171-xl25m   1/1       Running   0          8h        10.244.0.3        k8s
[root@k8s ~]# docker images
REPOSITORY                                               TAG                 IMAGE ID            CREATED             SIZE
gcr.io/google_containers/kubernetes-dashboard-amd64      v1.6.3              691a82db1ecd        35 hours ago        139 MB
gcr.io/google_containers/kube-apiserver-amd64            v1.7.2              4935105a20b1        8 days ago          186.1 MB
gcr.io/google_containers/kube-proxy-amd64                v1.7.2              13a7af96c7e8        8 days ago          114.7 MB
gcr.io/google_containers/kube-controller-manager-amd64   v1.7.2              2790e95830f6        8 days ago          138 MB
gcr.io/google_containers/kube-scheduler-amd64            v1.7.2              5db1f9874ae0        8 days ago          77.18 MB
quay.io/coreos/flannel                                   v0.8.0-amd64        9db3bab8c19e        2 weeks ago         50.73 MB
gcr.io/google_containers/k8s-dns-sidecar-amd64           1.14.4              38bac66034a6        4 weeks ago         41.81 MB
gcr.io/google_containers/k8s-dns-kube-dns-amd64          1.14.4              a8e00546bcf3        4 weeks ago         49.38 MB
gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64     1.14.4              f7f45b9cb733        4 weeks ago         41.41 MB
gcr.io/google_containers/etcd-amd64                      3.0.17              243830dae7dd        5 months ago        168.9 MB
gcr.io/google_containers/pause-amd64                     3.0                 99e59f495ffa        15 months ago       746.9 kB
[root@k8s ~]# docker ps 
CONTAINER ID        IMAGE                                                                                                                            COMMAND                  CREATED             STATUS              PORTS               NAMES
631dc2cab02e        gcr.io/google_containers/kubernetes-dashboard-amd64@sha256:2c4421ed80358a0ee97b44357b6cd6dc09be6ccc27dfe9d50c9bfc39a760e5fe      "/dashboard --insecur"   7 hours ago         Up 7 hours                              k8s_kubernetes-dashboard_kubernetes-dashboard-3313488171-xl25m_kube-system_0e41b8ce-747a-11e7-befb-000c2944b96c_0
8f5e4d044a6e        gcr.io/google_containers/pause-amd64:3.0                                                                                         "/pause"                 8 hours ago         Up 8 hours                              k8s_POD_kubernetes-dashboard-3313488171-xl25m_kube-system_0e41b8ce-747a-11e7-befb-000c2944b96c_0
65881f9dd2dd        gcr.io/google_containers/k8s-dns-sidecar-amd64@sha256:97074c951046e37d3cbb98b82ae85ed15704a290cce66a8314e7f846404edde9           "/sidecar --v=2 --log"   9 hours ago         Up 9 hours                              k8s_sidecar_kube-dns-2425271678-qwx9f_kube-system_ebffa28d-746d-11e7-befb-000c2944b96c_0
994c2ec99663        gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64@sha256:aeeb994acbc505eabc7415187cd9edb38cbb5364dc1c2fc748154576464b3dc2     "/dnsmasq-nanny -v=2 "   9 hours ago         Up 9 hours                              k8s_dnsmasq_kube-dns-2425271678-qwx9f_kube-system_ebffa28d-746d-11e7-befb-000c2944b96c_0
5b181a0ed809        gcr.io/google_containers/k8s-dns-kube-dns-amd64@sha256:40790881bbe9ef4ae4ff7fe8b892498eecb7fe6dcc22661402f271e03f7de344          "/kube-dns --domain=c"   9 hours ago         Up 9 hours                              k8s_kubedns_kube-dns-2425271678-qwx9f_kube-system_ebffa28d-746d-11e7-befb-000c2944b96c_0
a0d3f166e992        gcr.io/google_containers/pause-amd64:3.0                                                                                         "/pause"                 9 hours ago         Up 9 hours                              k8s_POD_kube-dns-2425271678-qwx9f_kube-system_ebffa28d-746d-11e7-befb-000c2944b96c_0
9cc7d6faf0b0        quay.io/coreos/flannel@sha256:a8116d095a1a2c4e5a47d5fea20ef82bd556bafe15bb2e6aa2c79f8f22f9586f                                   "/bin/sh -c 'set -e -"   9 hours ago         Up 9 hours                              k8s_install-cni_kube-flannel-ds-s5f63_kube-system_7ba88f5a-7470-11e7-befb-000c2944b96c_0
2f41276df8e1        quay.io/coreos/flannel@sha256:a8116d095a1a2c4e5a47d5fea20ef82bd556bafe15bb2e6aa2c79f8f22f9586f                                   "/opt/bin/flanneld --"   9 hours ago         Up 9 hours                              k8s_kube-flannel_kube-flannel-ds-s5f63_kube-system_7ba88f5a-7470-11e7-befb-000c2944b96c_0
bc25b0c70264        gcr.io/google_containers/pause-amd64:3.0                                                                                         "/pause"                 9 hours ago         Up 9 hours                              k8s_POD_kube-flannel-ds-s5f63_kube-system_7ba88f5a-7470-11e7-befb-000c2944b96c_0
dc3e5641c273        gcr.io/google_containers/kube-proxy-amd64@sha256:d455480e81d60e0eff3415675278fe3daec6f56c79cd5b33a9b76548d8ab4365                "/usr/local/bin/kube-"   9 hours ago         Up 9 hours                              k8s_kube-proxy_kube-proxy-4pjkg_kube-system_ebee4211-746d-11e7-befb-000c2944b96c_0
6b8b9515f562        gcr.io/google_containers/pause-amd64:3.0                                                                                         "/pause"                 9 hours ago         Up 9 hours                              k8s_POD_kube-proxy-4pjkg_kube-system_ebee4211-746d-11e7-befb-000c2944b96c_0
72418ca8e94f        gcr.io/google_containers/kube-apiserver-amd64@sha256:a9ccc205760319696d2ef0641de4478ee90fb0b75fbe6c09b1d64058c8819f97            "kube-apiserver --ser"   9 hours ago         Up 9 hours                              k8s_kube-apiserver_kube-apiserver-k8s_kube-system_b69ae39bcc54d7b75c2e7325359f8f87_0
9c9a3f5d8919        gcr.io/google_containers/pause-amd64:3.0                                                                                         "/pause"                 9 hours ago         Up 9 hours                              k8s_POD_kube-apiserver-k8s_kube-system_b69ae39bcc54d7b75c2e7325359f8f87_0
43a1751ff2bb        gcr.io/google_containers/etcd-amd64@sha256:d83d3545e06fb035db8512e33bd44afb55dea007a3abd7b17742d3ac6d235940                      "etcd --listen-client"   9 hours ago         Up 9 hours                              k8s_etcd_etcd-k8s_kube-system_9fb4ea9ba2043e46f75eec93827c4ce3_0
b110fff29f66        gcr.io/google_containers/pause-amd64:3.0                                                                                         "/pause"                 9 hours ago         Up 9 hours                              k8s_POD_etcd-k8s_kube-system_9fb4ea9ba2043e46f75eec93827c4ce3_0
66ae85500128        gcr.io/google_containers/kube-scheduler-amd64@sha256:b2e897138449e7a00508dc589b1d4b71e56498a4d949ff30eb07b1e9d665e439            "kube-scheduler --add"   9 hours ago         Up 9 hours                              k8s_kube-scheduler_kube-scheduler-k8s_kube-system_16c371efb8946190c917cd90c2ede8ca_0
d4343be2f2d0        gcr.io/google_containers/pause-amd64:3.0                                                                                         "/pause"                 9 hours ago         Up 9 hours                              k8s_POD_kube-scheduler-k8s_kube-system_16c371efb8946190c917cd90c2ede8ca_0
9934cd83f6b3        gcr.io/google_containers/kube-controller-manager-amd64@sha256:2b268ab9017fadb006ee994f48b7222375fe860dc7bd14bf501b98f0ddc2961b   "kube-controller-mana"   9 hours ago         Up 9 hours                              k8s_kube-controller-manager_kube-controller-manager-k8s_kube-system_6b826c4e872a9635472113953c4538f0_0
acc1d7d90180        gcr.io/google_containers/pause-amd64:3.0                                                                                         "/pause"                 9 hours ago         Up 9 hours                              k8s_POD_kube-controller-manager-k8s_kube-system_6b826c4e872a9635472113953c4538f0_0
[root@k8s ~]# 
</code></pre>

<h2>Woker节点部署</h2>

<p>时间，主机名，/etc/hosts，防火墙，selinux, 无密钥登录，安装docker-1.12.6就不再赘述了。</p>

<p>直接用master的yum缓冲，还有docker镜像直接拷贝：</p>

<pre><code># master机器已安装httpd服务

[root@k8s html]# ln -s /var/cache/yum/x86_64/7/kubernetes/packages/ k8s 
[root@k8s k8s]# createrepo .          

# 把镜像全部拷到worker节点

[root@k8s ~]# docker save $( echo $( docker images | grep -v REPOSITORY | awk '{print $1}' ) ) | ssh worker1 docker load 

# 配置私有仓库源

[root@worker1 yum.repos.d]# vi k8s.repo
[k8s]
name=Kubernetes
baseurl=http://master/k8s
enabled=1
gpgcheck=0
[root@worker1 yum.repos.d]# yum list | grep k8s 
kubeadm.x86_64                             1.7.2-0                     k8s      
kubectl.x86_64                             1.7.2-0                     k8s      
kubelet.x86_64                             1.7.2-0                     k8s      
kubernetes-cni.x86_64                      0.5.1-0                     k8s      

[root@worker1 yum.repos.d]# yum install -y kubelet kubeadm                          

# 修改cgroup-driver

[root@worker1 yum.repos.d]# vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf  
[root@worker1 yum.repos.d]# 
[root@worker1 yum.repos.d]# service docker restart
Redirecting to /bin/systemctl restart  docker.service

[root@worker1 yum.repos.d]# systemctl daemon-reload
[root@worker1 yum.repos.d]# systemctl enable kubelet.service
[root@worker1 yum.repos.d]# service kubelet restart
Redirecting to /bin/systemctl restart  kubelet.service

# worker节点加入集群（初始化）

[root@worker1 yum.repos.d]# kubeadm join --token 2af779.b803df0b1effb3d9 192.168.191.138:6443 --skip-preflight-checks
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[preflight] Skipping pre-flight checks
[discovery] Trying to connect to API Server "192.168.191.138:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.191.138:6443"
[discovery] Cluster info signature and contents are valid, will use API Server "https://192.168.191.138:6443"
[discovery] Successfully established connection with API Server "192.168.191.138:6443"
[bootstrap] Detected server version: v1.7.2
[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)
[csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request
[csr] Received signed certificate from the API server, generating KubeConfig...
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"

Node join complete:
* Certificate signing request sent to master and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on the master to see this machine join.

[root@k8s ~]# kubectl get nodes
NAME      STATUS    AGE       VERSION
k8s       Ready     10h       v1.7.2
worker1   Ready     57s       v1.7.2
</code></pre>

<p>主节点运行的flannel网络组件是个 daemonset 的pod，只要加入到集群就会在每个节点上启动。不需要额外的操作。</p>

<h2>关于重启：</h2>

<p>使用RPM安装的好处是：程序系统都帮你管理了：</p>

<ul>
<li>worker节点重启后，kubelet会把所有的服务都带起来。</li>
<li>master重启后，需要等一段时间，因为pods启动有顺序/依赖：dns需要等flannel，dashboard需要等dns。</li>
</ul>


<h2>POD间连通性测试</h2>

<pre><code>[root@k8s ~]# kubectl run hello-nginx --image=nginx --port=80
deployment "hello-nginx" created
[root@k8s ~]# kubectl get pods
NAME                           READY     STATUS              RESTARTS   AGE
hello-nginx-1507731416-qh3fx   0/1       ContainerCreating   0          8s

# 脚本启动新的dockerd并配置加速器，下载好然后save导入都本地docker实例
# https://github.com/winse/docker-hadoop/blob/master/kube-deploy/hadoop/docker-download-mirror.sh

[root@k8s ~]# ./docker-download-mirror.sh nginx 
Using default tag: latest
latest: Pulling from library/nginx

94ed0c431eb5: Pull complete 
9406c100a1c3: Pull complete 
aa74daafd50c: Pull complete 
Digest: sha256:788fa27763db6d69ad3444e8ba72f947df9e7e163bad7c1f5614f8fd27a311c3
Status: Downloaded newer image for nginx:latest
eb78099fbf7f: Loading layer [==================================================&gt;] 58.42 MB/58.42 MB
29f11c413898: Loading layer [==================================================&gt;] 52.74 MB/52.74 MB
af5bd3938f60: Loading layer [==================================================&gt;] 3.584 kB/3.584 kB
Loaded image: nginx:latest

# 拷贝镜像到其他的worker节点，就几台机器搭建register服务感觉太重了

[root@k8s ~]# docker save nginx | ssh worker1 docker load
Loaded image: nginx:latest

# 查看效果

[root@k8s ~]# kubectl get pods
NAME                           READY     STATUS    RESTARTS   AGE
hello-nginx-1507731416-qh3fx   1/1       Running   0          1m

# 扩容

[root@k8s ~]# kubectl scale --replicas=4 deployment/hello-nginx  
deployment "hello-nginx" scaled
[root@k8s ~]# kubectl get pods -o wide
NAME                           READY     STATUS    RESTARTS   AGE       IP           NODE
hello-nginx-1507731416-h39f0   1/1       Running   0          34s       10.244.0.6   k8s
hello-nginx-1507731416-mnj3m   1/1       Running   0          34s       10.244.1.3   worker1
hello-nginx-1507731416-nsdr2   1/1       Running   0          34s       10.244.0.7   k8s
hello-nginx-1507731416-qh3fx   1/1       Running   0          5m        10.244.1.2   worker1
[root@k8s ~]# kubectl delete deployment hello-nginx

这容器太简洁了，PING都没有啊！！搞个熟悉的linux版本，再跑一遍

kubectl run centos --image=centos:centos6 --command -- vi 
kubectl scale --replicas=4 deployment/centos

[root@k8s ~]# kubectl get pods  -o wide 
NAME                      READY     STATUS    RESTARTS   AGE       IP            NODE
centos-3024873821-4490r   1/1       Running   0          49s       10.244.1.6    worker1
centos-3024873821-k74gn   1/1       Running   0          11s       10.244.0.11   k8s
centos-3024873821-l27xs   1/1       Running   0          11s       10.244.0.10   k8s
centos-3024873821-pbg52   1/1       Running   0          11s       10.244.1.7    worker1

[root@k8s ~]# kubectl exec -ti centos-3024873821-4490r bash
[root@centos-3024873821-4490r /]# yum install -y iputils
[root@centos-3024873821-4490r /]# ping 10.244.0.11 -c 1

以上IP都是互通的，从master节点PING这些IP也是通的。

# 查看pod状态的命令
kubectl -n ${NAMESPACE} describe pod ${POD_NAME}
kubectl -n ${NAMESPACE} logs ${POD_NAME} -c ${CONTAINER_NAME}
</code></pre>

<h2>源IP问题</h2>

<p>原来部署hadoop的时刻，已经遇到过了。知道根源所在，但是这次使用的cni（直接改 <code>dockerd --ip-masq=false</code> 配置仅修改的是docker0）。</p>

<p>先来重现下源ip问题：</p>

<pre><code>./pod_bash centos-3024873821-t3k3r 

yum install epel-release -y ; yum install nginx -y ;
service nginx start

ifconfig

# nginx安装后，访问查看access_log

less /var/log/nginx/access.log 
</code></pre>

<p>在 kube-flannel.yml 中添加 cni-conf.json 网络配置为 <code>"ipMasq": false,</code>，没啥效果，在iptables上面还是有cni的cbr0的MASQUERADE（SNAT）。</p>

<p>注意：重启后，发现一切都正常了。可能是通过apply修改的，没有生效！在配置flannel之前就修改属性应该就ok了！！后面的可以不要看了，方法还比较挫。</p>

<p></p>

<p>用比较极端点的方式，删掉docker0，替换成cni0。 <a href="https://kubernetes.io/docs/getting-started-guides/scratch/#docker">https://kubernetes.io/docs/getting-started-guides/scratch/#docker</a></p>

<p></p>

<p>把docker的网卡设置成cni0(flannel会创建cni0的网卡) :</p>

<pre><code># 清空原来的策略
iptables -t nat -F
ip link set docker0 down
ip link delete docker0

[root@worker1 ~]# cat /usr/lib/systemd/system/docker.service  | grep dockerd
ExecStart=/usr/bin/dockerd --bridge=cni0 --ip-masq=false 
</code></pre>

<p>但是机器重启后cni0这个网卡设备就没有了，导致机器重启后docker启动失败！（cni-conf.json的"ipMasq": false是有效果的，但是好像得是新建的网卡设备才行！）</p>

<p></p>

<pre><code>&gt; Aug 01 08:36:10 k8s dockerd[943]: time="2017-08-01T08:36:10.017266292+08:00" level=fatal msg="Error starting daemon: Error initializing network controller: Error creating default \"bridge\" network: bridge device with non default name cni0 must be created manually"

ip link add name cni0 type bridge
ip link set dev cni0 mtu 1460
# 让flannel来设置IP地址
# ip addr add $NODE_X_BRIDGE_ADDR dev cni0
ip link set dev cni0 up

systemctl restart docker kubelet
</code></pre>

<p></p>

<p>另一种网络部署方式 kubenet + hostroutes ： <a href="https://jishu.io/kubernetes/deploy-production-ready-kubernetes-cluster-on-aliyun/">https://jishu.io/kubernetes/deploy-production-ready-kubernetes-cluster-on-aliyun/</a></p>

<h2>DNS</h2>

<p><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/</a></p>

<pre><code># cat busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always

kubectl create -f busybox.yaml
kubectl exec -ti busybox -- nslookup kubernetes.default
kubectl exec busybox cat /etc/resolv.conf
</code></pre>

<p></p>

<h2>DNS问题</h2>

<p>在master节点上的POD容器内访问DNS（service）服务，但是返回数据却是域名服务内部POD的IP，而不是Service服务的IP地址。</p>

<pre><code>[root@k8s ~]# kubectl describe services kube-dns -n kube-system
Name:                   kube-dns
Namespace:              kube-system
Labels:                 k8s-app=kube-dns
                        kubernetes.io/cluster-service=true
                        kubernetes.io/name=KubeDNS
Annotations:            &lt;none&gt;
Selector:               k8s-app=kube-dns
Type:                   ClusterIP
IP:                     10.96.0.10
Port:                   dns     53/UDP
Endpoints:              10.244.0.30:53
Port:                   dns-tcp 53/TCP
Endpoints:              10.244.0.30:53
Session Affinity:       None
Events:                 &lt;none&gt;

[root@k8s ~]# kubectl exec -ti centos-3024873821-b6d48 -- nslookup kubernetes.default
;; reply from unexpected source: 10.244.0.30#53, expected 10.96.0.10#53
;; reply from unexpected source: 10.244.0.30#53, expected 10.96.0.10#53
</code></pre>

<p></p>

<h4>相关问题的一些资源：</h4>

<ul>
<li>*<a href="https://stackoverflow.com/questions/41574846/kubernetes-pods-replying-with-unexpected-source-for-dns-queries">kubernetes pods replying with unexpected source for DNS queries</a></li>
<li><a href="https://stackoverflow.com/questions/34001758/kube-proxy-in-iptables-mode-is-not-working/34008477#34008477">https://stackoverflow.com/questions/34001758/kube-proxy-in-iptables-mode-is-not-working/34008477#34008477</a></li>
<li><p><a href="https://github.com/coreos/coreos-kubernetes/issues/572">cni plugin + flannel on v1.3: pods can&rsquo;t route to service IPs</a></p></li>
<li><p><a href="https://www.slideshare.net/kubecon/container-network-interface-network-plugins-for-kubernetes-and-beyond">Container Network Interface: Network Plugins for Kubernetes and beyond</a></p></li>
<li><a href="http://www.dasblinkenlichten.com/understanding-cni-container-networking-interface/">Understanding CNI (Container Networking Interface)</a></li>
<li><a href="https://feisky.gitbooks.io/kubernetes/network/flannel/">Kubernetes指南 - flannel</a></li>
<li>Pod to external traffic is not masqueraded <a href="https://github.com/kubernetes/kubernetes/issues/40761">https://github.com/kubernetes/kubernetes/issues/40761</a></li>
</ul>


<h4>解决方法：</h4>

<p><strong> kube-proxy加上 &ndash;masquerade-all 解决了。</strong></p>

<h4>处理方法：</h4>

<blockquote><p><a href="https://kubernetes.io/docs/admin/kubeadm/">https://kubernetes.io/docs/admin/kubeadm/</a>
kubeadm installs add-on components via the API server. Right now this is the internal DNS server and the kube-proxy DaemonSet.</p></blockquote>

<p>修改有技巧，正如官网文档所说：kube-proxy是内部容器启动的。没找到yaml配置，不能直接改配置文件，这里有如下两种方式修改：</p>

<ul>
<li>通过Dashboard页面的编辑对配置进行修改</li>
<li>通过edit命令对配置进行修改：<code>kubectl edit daemonset kube-proxy -n=kube-system</code> 命令添加 <code>- --masquerade-all</code></li>
</ul>


<h2>Heapster</h2>

<p>参考</p>

<ul>
<li><a href="https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md">https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md</a></li>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/">https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/</a></li>
</ul>


<pre><code>[root@k8s ~]# git clone https://github.com/kubernetes/heapster.git
Cloning into 'heapster'...
remote: Counting objects: 26084, done.
remote: Total 26084 (delta 0), reused 0 (delta 0), pack-reused 26084
Receiving objects: 100% (26084/26084), 36.33 MiB | 2.66 MiB/s, done.
Resolving deltas: 100% (13084/13084), done.
Checking out files: 100% (2531/2531), done.

[root@k8s ~]# cd heapster/
[root@k8s heapster]# kubectl create -f deploy/kube-config/influxdb/
deployment "monitoring-grafana" created
service "monitoring-grafana" created
serviceaccount "heapster" created
deployment "heapster" created
service "heapster" created
deployment "monitoring-influxdb" created
service "monitoring-influxdb" created
[root@k8s heapster]# kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml 
clusterrolebinding "heapster" created
</code></pre>

<p>其他资源：</p>

<ul>
<li><a href="http://codingwater.org/2016/08/18/Kubernetes%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7-Heapster/">http://codingwater.org/2016/08/18/Kubernetes%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7-Heapster/</a></li>
<li><a href="http://www.pangxie.space/docker/727">http://www.pangxie.space/docker/727</a></li>
<li><a href="http://jerrymin.blog.51cto.com/3002256/1904460">http://jerrymin.blog.51cto.com/3002256/1904460</a></li>
<li><a href="http://blog.takipi.com/graphite-vs-grafana-build-the-best-monitoring-architecture-for-your-application/?utm_content=buffer607cd&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">http://blog.takipi.com/graphite-vs-grafana-build-the-best-monitoring-architecture-for-your-application/?utm_content=buffer607cd&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer</a></li>
</ul>


<p>DNS的问题耗了比较多的时间。弄好了DNS后，以及heapster的docker镜像的下载都OK的话，就万事俱备了。最后重新启动下dashboard就行了：</p>

<pre><code>[root@k8s ~]# kubectl delete -f kubernetes-dashboard.yaml 
[root@k8s ~]# kubectl create -f kubernetes-dashboard.yaml 
</code></pre>

<p>然后就可以在dashboard上看到美美的曲线图了。</p>

<p></p>

<h2>harbor</h2>

<p>参考 <a href="https://github.com/vmware/harbor/blob/master/docs/kubernetes_deployment.md">https://github.com/vmware/harbor/blob/master/docs/kubernetes_deployment.md</a></p>

<p>日新月异啊，1.1.2版本了！！ 用迅雷直接下载 <a href="https://github.com/vmware/harbor/releases/download/v1.1.2/harbor-offline-installer-v1.1.2.tgz">https://github.com/vmware/harbor/releases/download/v1.1.2/harbor-offline-installer-v1.1.2.tgz</a>  这个地址。</p>

<p>操作方式还是和原来的版本一样。也就是说可以用原来简化的脚本来安装！</p>

<p>搭建好了后，会基本的使用就差不多了。测试环境资源有限，并且其实用save和load也能解决（咔咔）。</p>

<h2>livenessProbe - Nexus的无响应处理</h2>

<p>在 <a href="https://github.com/winse/docker-hadoop/blob/master/kube-deploy/nexus-rc.yaml">github仓库</a> 上有一份开发环境的NEXUS的启动脚本，从一开始的单pods，改成replicationcontroller。觉得万事大吉了。</p>

<p>但，现在又出现一个问题，就是容器还在，但是8081不提供服务了。这很尴尬，其他开发人员说nexus又不能访问了，我想不对，不是已经改成rc了么，容器应该不会挂才对啊。上环境一看，容器是在，但是服务是真没响应。</p>

<p>怎么办？</p>

<p>搞定时任务，觉得有点low。后面想如果判断一下服务不能访问了就重启，其实k8s已经想到了这一点了，提供了<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-a-liveness-http-request">存活探针livenessProbe</a> 。直接按照官网给的http的例子写就行了。等过几天看效果。</p>

<h2>参考</h2>

<p>官方的一些资源</p>

<ul>
<li><a href="https://kubernetes.io/docs/getting-started-guides/scratch/#kube-proxy">https://kubernetes.io/docs/getting-started-guides/scratch/#kube-proxy</a></li>
<li><a href="https://kubernetes.io/docs/admin/kubeadm/">https://kubernetes.io/docs/admin/kubeadm/</a></li>
<li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/">https://kubernetes.io/docs/setup/independent/install-kubeadm/</a></li>
<li><a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</a></li>
<li><a href="https://lukemarsden.github.io/docs/getting-started-guides/kubeadm/">https://lukemarsden.github.io/docs/getting-started-guides/kubeadm/</a></li>
<li><a href="https://kubernetes.io/docs/admin/kubeadm/#running-kubeadm-without-an-internet-connection">https://kubernetes.io/docs/admin/kubeadm/#running-kubeadm-without-an-internet-connection</a></li>
<li><a href="https://kubernetes.io/docs/admin/kubeadm/#environment-variables">https://kubernetes.io/docs/admin/kubeadm/#environment-variables</a></li>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm-upgrade-1-7/">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm-upgrade-1-7/</a> 怎么升级，以及如何制定特定的k8s版本</li>
</ul>


<p>使用kubeadm安装集群</p>

<ul>
<li><a href="http://tonybai.com/2016/12/30/install-kubernetes-on-ubuntu-with-kubeadm/">http://tonybai.com/2016/12/30/install-kubernetes-on-ubuntu-with-kubeadm/</a> 参考</li>
<li><a href="http://tonybai.com/2016/12/30/install-kubernetes-on-ubuntu-with-kubeadm-2/">http://tonybai.com/2016/12/30/install-kubernetes-on-ubuntu-with-kubeadm-2/</a>  weave net网络</li>
<li><a href="https://www.kubernetes.org.cn/1165.html">https://www.kubernetes.org.cn/1165.html</a> 就是上面第一篇，但是排版看起来跟舒服点</li>
<li><a href="http://hairtaildai.com/blog/11">http://hairtaildai.com/blog/11</a> 安装似乎太顺利了，都没有遇到啥问题？</li>
<li><a href="https://my.oschina.net/xdatk/blog/895645">https://my.oschina.net/xdatk/blog/895645</a> 这篇不推荐，太繁琐了。很多贴的是内容，不知道改过啥！</li>
</ul>


<p>DNS问题参考</p>

<ul>
<li><a href="https://stackoverflow.com/questions/41574846/kubernetes-pods-replying-with-unexpected-source-for-dns-queries">https://stackoverflow.com/questions/41574846/kubernetes-pods-replying-with-unexpected-source-for-dns-queries</a></li>
<li><a href="https://kubernetes.io/docs/admin/kube-proxy/">https://kubernetes.io/docs/admin/kube-proxy/</a></li>
<li><p><a href="https://docs.docker.com/engine/admin/systemd/#httphttps-proxy">https://docs.docker.com/engine/admin/systemd/#httphttps-proxy</a></p></li>
<li><p><a href="https://coreos.com/matchbox/docs/latest/bootkube-upgrades.html">https://coreos.com/matchbox/docs/latest/bootkube-upgrades.html</a> 命令行编辑的方法在这里看到的
</p></li>
<li><p><a href="https://github.com/kubernetes/kubernetes/issues/34101">https://github.com/kubernetes/kubernetes/issues/34101</a>
Ok, so it turns out that this flag is not enough, we still have an issue reaching kubernetes service IP. The simplest solution to this is to run kube-proxy with &ndash;proxy-mode=userspace. To enable this, you can use kubectl -n kube-system edit ds kube-proxy-amd64 &amp;&amp; kubectl -n kube-system delete pods -l name=kube-proxy-amd64.</p></li>
<li><p><a href="https://github.com/kubernetes/kubernetes/issues/36835">https://github.com/kubernetes/kubernetes/issues/36835</a> To enable off-cluster bridging when &ndash;proxy-mode=iptables, also set &ndash;cluster-cidr.</p></li>
<li><a href="https://github.com/kubernetes/kubeadm/issues/102">https://github.com/kubernetes/kubeadm/issues/102</a> proxy: clusterCIDR not specified, unable to distinguish between internal and external traffic</li>
</ul>


<p>其他一些资源</p>

<ul>
<li><a href="https://github.com/cookeem/kubeadm-ha/blob/master/README_CN.md">https://github.com/cookeem/kubeadm-ha/blob/master/README_CN.md</a></li>
<li><p><a href="https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/">https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/</a>
</p></li>
<li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/</a> Replication Controllers</p></li>
<li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy</a> RestartPolicy</li>
</ul>


<p>&mdash;END</p>
]]></content>
  </entry>
  
</feed>
