<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Redis | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/redis/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2018-01-20T17:52:57+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Codis使用进阶]]></title>
    <link href="http://winseliu.com/blog/2017/03/23/codis-usage2/"/>
    <updated>2017-03-23T17:48:55+08:00</updated>
    <id>http://winseliu.com/blog/2017/03/23/codis-usage2</id>
    <content type="html"><![CDATA[<p>去年年中的时刻有安装过codis。当时因为任务紧就使用jedis的ShardedJedisPool功能粗略的解决，由于是自己手动路由和管理，维护起来太难，特别是当初设置的实例数不够用时，相当麻烦。</p>

<p>年初项目各种测试，于是有些闲暇的时间，重新弄一弄redis cluster。算是搭建一个环境来测试：</p>

<p>版本：</p>

<ul>
<li>codis-3.2</li>
<li>centos6</li>
</ul>


<h2>测试环境编译安装</h2>

<p>现在的版本已经有了全部的依赖，直接编译即可。（centos6和官网提供的编译版本不兼容）</p>

<ul>
<li><a href="https://github.com/CodisLabs/codis/blob/release3.2/doc/tutorial_zh.md#0-%E4%B8%8B%E8%BD%BD%E4%B8%8E%E7%BC%96%E8%AF%91">官网文档</a></li>
</ul>


<pre><code>tar zxvf go1.6.2.linux-amd64.tar.gz 

/etc/profile
export GOROOT=/opt/go
export GOPATH=/opt/gopath
export PATH=$GOPATH/bin:$GOROOT/bin:$PATH

-

[root@cu2 CodisLabs]# pwd
/opt/go/src/github.com/CodisLabs

# @2017-06-05
# 如果下载的是tar.gz，直接在CodisLabs目录下解压，然后做个软链接
# cd $GOPATH ; mkdir -p src/github.com/CodisLabs/
# cd src/github.com/CodisLabs/; ln -s codis-3.2-rc2 codis
[root@cu2 CodisLabs]# git clone --branch release3.2  https://github.com/CodisLabs/codis.git 

[root@cu2 CodisLabs]# cd codis/

# 安装一些依赖
# # ./autogen.sh: line 5: autoconf: command not found
# yum install autoconf 
[root@cu2 codis]# make 

[root@cu2 codis]# ll bin/
total 101292
drwxr-xr-x 4 root root     4096 Mar 15 12:58 assets
-rwxr-xr-x 1 root root 21036930 Mar 15 12:58 codis-admin
-rwxr-xr-x 1 root root 22343059 Mar 15 12:58 codis-dashboard
-rwxr-xr-x 1 root root 18378506 Mar 15 12:58 codis-fe
-rwxr-xr-x 1 root root 22675153 Mar 15 12:58 codis-proxy
-rwxr-xr-x 1 root root  7982967 Mar 15 12:58 codis-server
-rwxr-xr-x 1 root root  5580431 Mar 15 12:58 redis-benchmark
-rwxr-xr-x 1 root root  5712419 Mar 15 12:58 redis-cli
-rw-r--r-- 1 root root      170 Mar 15 12:58 version
[root@cu2 codis]# cat bin/version 
version = 2017-03-15 00:40:41 +0800 @be9ee25c63a64396b5fb0076447be560497b909d @3.2-beta-10-gbe9ee25
compile = 2017-03-15 12:58:23 +0800 by go version go1.6.2 linux/amd64

# 生成默认配置
[root@cu2 codis]# bin/codis-dashboard --default-config | tee dashboard.toml
[root@cu2 codis]# bin/codis-proxy --default-config | tee proxy.toml
</code></pre>

<h2>生产部署</h2>

<p>把测试环境的GOPATH和GOROOT全部拷贝到生产即可。这里上面已经生成了dashboard和proxy的配置了哦！！</p>

<pre><code>[ud@cu-ud6 opt]$ ll
drwxrwxr-x.  2 ud   ud   4096 3月  18 00:10 bin
drwxr-xr-x. 11 ud   ud   4096 4月  20 2016 go
drwxr-xr-x.  4 ud   ud   4096 3月  15 12:58 gopath
drwxr-xr-x.  8 ud   ud   4096 3月  17 20:13 jdk1.8.0_92
drwxr-xr-x. 10 ud   ud   4096 2月  20 2014 zookeeper-3.4.6

[ud@cu-ud6 opt]$ ll bin
总用量 24
-rw-rw-r--. 1 ud ud 234 3月  17 20:36 codis.profile
lrwxrwxrwx. 1 ud ud  54 3月  17 20:34 redis-cli -&gt; ../gopath/src/github.com/CodisLabs/codis/bin/redis-cli
-rwxrwxr-x. 1 ud ud 487 3月  17 20:54 start-codis-dashboard.sh
-rwxrwxr-x. 1 ud ud 310 3月  18 00:10 start-codis-proxy.sh
-rwxrwxr-x. 1 ud ud 335 3月  17 21:17 start-redis.sh
-rwxrwxr-x. 1 ud ud 323 3月  17 20:55 start-zoo.sh

[ud@cu-ud6 opt]$ for f in $( find bin -type f ) ; do echo " =============== $f ================= "; cat "$f" ; done
 =============== bin/codis.profile ================= 
#!/bin/sh

export GOROOT=/opt/go
export GOPATH=/opt/gopath
export CODIS_HOME=$GOPATH/src/github.com/CodisLabs/codis/
export LOG_DIR=/var/log

export JAVA_HOME=/opt/jdk1.8.0_92

export PATH=$JAVA_HOME/bin:$GOPATH/bin:$GOROOT/bin:$PATH

 =============== bin/start-zoo.sh ================= 
#!/bin/sh

CODIS_BIN="${BASH_SOURCE-$0}"
CODIS_BIN="$(dirname "${CODIS_BIN}")"
CODIS_BINDIR="$(cd "${CODIS_BIN}"; pwd)"

source $CODIS_BINDIR/codis.profile

export ZOO_LOG_DIR=$LOG_DIR

cd /opt/zookeeper-3.4.6
sed 's@dataDir=/tmp/zookeeper@dataDir=/data/zookeeper@' conf/zoo_sample.cfg &gt;conf/zoo.cfg

bin/zkServer.sh start

 =============== bin/start-codis-dashboard.sh ================= 
#!/bin/sh

CODIS_BIN="${BASH_SOURCE-$0}"
CODIS_BIN="$(dirname "${CODIS_BIN}")"
CODIS_BINDIR="$(cd "${CODIS_BIN}"; pwd)"

source $CODIS_BINDIR/codis.profile

cd $CODIS_HOME
nohup bin/codis-dashboard \
  --ncpu=4 \
  --config=dashboard.toml \
  --log=$LOG_DIR/codis_dashboard.log \
  --log-level=INFO \
  &gt;/dev/null 2&gt;&amp;1 &amp;

nohup bin/codis-fe \
  --ncpu=4 \
  --zookeeper=127.0.0.1:2181 \
  --listen=0.0.0.0:28080 \
  --log=$LOG_DIR/codis_fe.log \
  --log-level=INFO \
  &gt;/dev/null 2&gt;&amp;1 &amp;

 =============== bin/start-codis-proxy.sh ================= 
#!/bin/sh

CODIS_BIN="${BASH_SOURCE-$0}"
CODIS_BIN="$(dirname "${CODIS_BIN}")"
CODIS_BINDIR="$(cd "${CODIS_BIN}"; pwd)"

source $CODIS_BINDIR/codis.profile

cd $CODIS_HOME
nohup bin/codis-proxy \
  --ncpu=24 \
  --config=proxy.toml \
  --log=$LOG_DIR/codis_proxy.log \
  --log-level=INFO \
  &gt;/dev/null 2&gt;&amp;1 &amp;

 =============== bin/start-redis.sh ================= 
#!/bin/sh

CODIS_BIN="${BASH_SOURCE-$0}"
CODIS_BIN="$(dirname "${CODIS_BIN}")"
CODIS_BINDIR="$(cd "${CODIS_BIN}"; pwd)"

source $CODIS_BINDIR/codis.profile

PORT=${1:-6379}

cd $CODIS_HOME
bin/codis-server --daemonize yes --port $PORT --pidfile /var/run/redis_$PORT.pid --logfile $LOG_DIR/redis_$PORT.log --save "" --bind $(hostname) 
</code></pre>

<p>环境：</p>

<ul>
<li>zookeeper: cu-ud6</li>
<li>dashboard: cu-ud6</li>
<li>fa: cu-ud6</li>
<li>proxy: cu-ud6/7/8</li>
<li>redis: cu-ud6/7/8:6378/6379</li>
<li>nginx代理: cu-ud9</li>
</ul>


<p>web界面添加步骤：</p>

<ul>
<li>界面上添加proxy : cu6/7/8:11080</li>
<li>再添加group，填数字: &frac12;/&frac34;/5/6</li>
<li>然后添加server : cu-ud6/7/8:6378/6379</li>
<li>最后分配slots</li>
</ul>


<p><img src="/images/blogs/codis.jpg" alt="" /></p>

<p>nginx1.11新版本已经支持tcp的代理，可以实现proxy的负载均衡：</p>

<pre><code># 编译Nginx
./configure --with-stream --with-http_ssl_module --with-pcre=src/pcre --with-zlib=src/zlib --prefix=/usr/local/nginx
make &amp;&amp; make install

[ud@cu-ud9 nginx]$ cat conf/nginx.conf

#user  nobody;
worker_processes  1;

#error_log  logs/error.log;
error_log  /var/log/nginx_error.log  notice;
#error_log  logs/error.log  info;

#pid        logs/nginx.pid;

events {
    worker_connections  1024;
}

stream {
  upstream proxy {
    hash   $remote_addr;
    server cu-ud6:19000;
    server cu-ud7:19000;
    server cu-ud8:19000;
  }

  server {
    listen cu-ud9:19000;
    proxy_timeout 600s;
    proxy_pass proxy;
  }
}

# 测试获取数据
[ud@cu-ud6 opt]$ bin/redis-cli -h cu-ud6 -p 6379 scan 0 # 样本Key
[ud@cu-ud6 opt]$ bin/redis-cli -h cu-ud9 -p 19000
&gt; get XXX
</code></pre>

<p>重置统计量：</p>

<ul>
<li><a href="https://github.com/CodisLabs/codis/issues/1049">https://github.com/CodisLabs/codis/issues/1049</a></li>
</ul>


<pre><code>[ud@cu-ud6 codis]$ bin/codis-admin --proxy=cu-ud6:11080 --reset-stats
</code></pre>

<p></p>

<h2>问题</h2>

<p>pipeline量太大，修改proxy的 backend_max_pipeline/session_max_pipeline 。同时在客户端代码里面执行一定量的pipe后执行sync。</p>

<pre><code>2017/03/18 00:01:23 session.go:79: [INFO] session [0xc839888d80] create: {"ops":0,"create":1489766483,"remote":"192.168.32.182:57029"}
2017/03/18 00:01:24 session.go:86: [INFO] session [0xc834a06d80] closed: {"ops":39601,"create":1489766483,"lastop":1489766484,"remote":"192.168.32.182:57028"}, error: too many pipelined r
equests
</code></pre>

<p>sync还是会超时，修改nginx的proxy_timeout以及客户端初始化的timeout参数。</p>

<pre><code>new JedisPool(new GenericObjectPoolConfig(), "cu-ud9", 19000, 10 * 60 * 1000)
</code></pre>

<p>W：感觉proxy还是会有停顿，sync后有时会出现几分钟时间没响应。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redis批量操作]]></title>
    <link href="http://winseliu.com/blog/2016/08/17/redis-batch-operate/"/>
    <updated>2016-08-17T15:33:47+08:00</updated>
    <id>http://winseliu.com/blog/2016/08/17/redis-batch-operate</id>
    <content type="html"><![CDATA[<p>紧接上一篇优化的文章，在生产进行shard分库/分片操作后，部分大量zsort键值对拆散到多个redis实例。原来统计总量的命令现在需要汇总后才行。</p>

<p>今天就来说说Redis里面的批量操作，批量操作其实就是循环，把大部分的工作让机器做了。逻辑比较复杂的用比较实现，功能简单的用脚本即可。</p>

<ul>
<li>单机用lua脚本</li>
</ul>


<pre><code># redis-cli

eval "local aks=redis.call('keys', '*'); local res=0; for i,r in ipairs(aks) do res=res+redis.call('hlen', r) end; return res" 0
</code></pre>

<p>通过keys获取所有的键（都是hash类型），然后计算出匹配的hash包括的键值对总数。</p>

<p>在<strong>同一个实例</strong>的小数据量的统计，lua脚本优势还是比较明显的：redis自带的还能编程。</p>

<ul>
<li>shell脚本</li>
</ul>


<p>看官网的文档：<a href="http://redis.io/topics/rediscli">the Redis command line interface</a></p>

<pre><code>## shell

cat commands | redis-cli --raw

## redis-cli

127.0.0.1:6379&gt; CONFIG RESETSTAT
OK
...
127.0.0.1:6379&gt; info stats
# Stats
total_connections_received:2
...
</code></pre>

<p>注意：不要用循环然后调用<code>redis-cli COMMAND</code>，这种方式会产生很多的TCP连接，如果要执行的命令太多，可能导致TCP端口不够用。</p>

<p>还有 <code>redis-cli --stat</code>，<code>redis-cli --scan</code>，<code>redis-cli monitor</code> 这些命令挺有意思的。</p>

<ul>
<li>直接tcp连接管道操作</li>
</ul>


<pre><code>## shell

sed 's/$/^M/' commands &gt; test.redis.cmd
cat test.redis.cmd | nc localhost 6379

## redis-cli

127.0.0.1:6379&gt; info stats
# Stats
total_connections_received:1
...
</code></pre>

<p>通过tcp连接操作，一批次的操作就一个连接。但是，操作的时刻需要主要，linux的换行符是 <code>\n</code>，而redis需要的是 <code>\r\n</code>；还有通过tcp连接返回的结果是redis协议原始数据，没有经过处理，需要稍微看看协议规范<a href="http://redis.io/topics/protocol">Redis Protocol specification</a></p>

<ul>
<li>pipelining</li>
</ul>


<p>redis自带的管道功能，性能提升相当明显（<a href="http://redis.io/topics/pipelining#some-benchmark">官网数据</a>）。</p>

<p>原来看到过觉得高大上，准备试试。但是返回结果却只有一个成功数而已！！</p>

<pre><code>$ echo 119.84.100.68 | xargs -I{} echo "1:ips:2016-08-16:{}" | while read cmd ; do echo -e "*2\r\n\$5\r\nzcard\r\n\$${#cmd}\r\n$cmd\r\n" ; done  | redis-cli -p 26379 --pipe
All data transferred. Waiting for the last reply...
Last reply received from server.
errors: 0, replies: 1
</code></pre>

<p>注意：redis-cli带的pipe就比较坑：最后只返回操作成功失败的统计数量，只适合用来做更新操作<a href="http://redis.io/topics/mass-insert">Redis Mass Insertion</a>！！</p>

<p>要用pipeline来实现查询的话，用java/scala等语言来弄吧。</p>

<p>最后帖一下用shell脚本：</p>

<pre><code>day=${day:-`date "+%Y-%m-%d"`}
key="1:ips:$day"

for h in hadoop-master{1..4} ; do 
  exists=$(redis-cli -h $h -p 6372 exists $key)
  if [ $exists -gt 0 ] ; then
    redis-cli -h $h -p 6372 zrange $key 0 -1 &gt; activeresourceip$day.data
    for h in hadoop-master{1..4} ; do
      cat activeresourceip$day.data | while read ip ; do echo -e "zcard $key:$ip\r" ; done | redis-cli -h $h -p 6372
    done
  fi
done | awk '{s+=$1} END {print s}' 
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redis使用优化]]></title>
    <link href="http://winseliu.com/blog/2016/07/28/redis-optimise/"/>
    <updated>2016-07-28T08:22:26+08:00</updated>
    <id>http://winseliu.com/blog/2016/07/28/redis-optimise</id>
    <content type="html"><![CDATA[<p>最近对生产的Redis做了两个优化：Redis扩展、以及对简单键值对的存储优化（string改成hash形式）</p>

<h2>Redis扩展</h2>

<p>上一篇介绍的Codis安装。但是使用Pipeline操作时间比较长、连接数比较多的情况下，经常出现连接重置的情况。感觉不踏实，go也不懂感觉短时间处理不了这种问题。</p>

<p>寻求它法。前期是把不同业务数据写入不同的redis实例，根据业务来分。对于同一个业务来说，得根据key的hash来写入不同的实例，但是自己写的话得包装一堆东西。</p>

<p>jedis工具包括Shared的功能，根据写入key的hash映射到不同的redis实例。截取了部分Shared的主要代码:</p>

<pre><code>public class Sharded&lt;R, S extends ShardInfo&lt;R&gt;&gt; {
...
    private void initialize(List&lt;S&gt; shards) {
    nodes = new TreeMap&lt;Long, S&gt;();

    for (int i = 0; i != shards.size(); ++i) {
        final S shardInfo = shards.get(i);
        if (shardInfo.getName() == null)
        for (int n = 0; n &lt; 160 * shardInfo.getWeight(); n++) {
            nodes.put(this.algo.hash("SHARD-" + i + "-NODE-" + n),
                shardInfo);
        }
        else
        for (int n = 0; n &lt; 160 * shardInfo.getWeight(); n++) {
            nodes.put(
                this.algo.hash(shardInfo.getName() + "*"
                    + shardInfo.getWeight() + n), shardInfo);
        }
        resources.put(shardInfo, shardInfo.createResource());
    }
    }
... 
    public S getShardInfo(byte[] key) {
    SortedMap&lt;Long, S&gt; tail = nodes.tailMap(algo.hash(key));
    if (tail.isEmpty()) {
        return nodes.get(nodes.firstKey());
    }
    return tail.get(tail.firstKey());
    }

    public S getShardInfo(String key) {
    return getShardInfo(SafeEncoder.encode(getKeyTag(key)));
    }
...
</code></pre>

<p>使用的时刻很简单，通过ShardedJedis来进读写，大部分的操作与Jedis类似。只是有部分整个集群的操作不能用：keys/scan等。</p>

<pre><code>  public List&lt;JedisShardInfo&gt; getShards(String sValue) {
    String[] servers = sValue.split(",");

    List&lt;JedisShardInfo&gt; shards = new ArrayList&lt;&gt;();
    for (String server : servers) {
      Pair&lt;String, Integer&gt; hp = parseServer(server);
      shards.add(new JedisShardInfo(hp.getLeft(), hp.getRight(), Integer.MAX_VALUE));
    }
    return shards;
  }
  private ShardedJedisPool createRedisPool(String server) {
    return new ShardedJedisPool(new GenericObjectPoolConfig(), getShards(server));
  }
</code></pre>

<p>如果使用过程中要使用keys，可以通过getAllShards得到所有Jedis实例的键再进行处理：</p>

<pre><code>  public Double zscore(String key, String member) {
    try (ShardedJedis redis = getRedis()) {
      return redis.zscore(key, member);
    }
  }

  public void expires(List&lt;String&gt; patterns, int seconds) {
    try (ShardedJedis shardedJedis = getRedis()) {
      Set&lt;String&gt; keys = new HashSet&lt;&gt;();

      for (Jedis redis : shardedJedis.getAllShards()) {
        for (String p : patterns) {
          keys.addAll(redis.keys(p)); // 调用单独实例的keys命令获取匹配的键
        }
      }

      ShardedJedisPipeline pipeline = shardedJedis.pipelined();
      for (String key : keys) {
        pipeline.expire(key, seconds);
      }
      pipeline.sync();
    }
  }
</code></pre>

<p>进行多实例(集群)切分后，效果还是挺明显的。写入高峰期分流效果显著，负载均摊，可使用的内存也翻翻，键也基本平均分布（ <code>--maxmemory-policy volatile-lru</code> ）。生产实际效果：</p>

<pre><code>[hadoop@hadoop-master1 redis]$ sh stat_cluster.sh 

 * [ ============================================================&gt; ] 4 / 4

hadoop-master1:
# Memory
used_memory:44287785776
used_memory_human:41.25G
used_memory_rss:67458658304
used_memory_peak:67981990576
used_memory_peak_human:63.31G
used_memory_lua:33792
mem_fragmentation_ratio:1.52
mem_allocator:jemalloc-3.6.0
# Keyspace
db0:keys=72729777,expires=11967,avg_ttl=63510023

hadoop-master2:
# Memory
used_memory:50667945344
used_memory_human:47.19G
used_memory_rss:66036752384
used_memory_peak:64424543672
used_memory_peak_human:60.00G
used_memory_lua:33792
mem_fragmentation_ratio:1.30
mem_allocator:jemalloc-3.6.0
# Keyspace
db0:keys=100697581,expires=13426,avg_ttl=63509903

hadoop-master3:
# Memory
used_memory:56763389184
used_memory_human:52.87G
used_memory_rss:66324045824
used_memory_peak:64424546136
used_memory_peak_human:60.00G
used_memory_lua:33792
mem_fragmentation_ratio:1.17
mem_allocator:jemalloc-3.6.0
# Keyspace
db0:keys=94363547,expires=13544,avg_ttl=63505693

hadoop-master4:
# Memory
used_memory:54513952832
used_memory_human:50.77G
used_memory_rss:67257393152
used_memory_peak:64820124928
used_memory_peak_human:60.37G
used_memory_lua:33792
mem_fragmentation_ratio:1.23
mem_allocator:jemalloc-3.6.0
# Keyspace
db0:keys=83297543,expires=12418,avg_ttl=63507046


Finished processing 4 / 4 hosts in 298.89 ms
</code></pre>

<h2>存储优化</h2>

<p>实际环境中存在会大量的用到简单string键值对，挺耗内存的。其实使用hash（内部存储ziplist）能更有效的利用内存。</p>

<p>注意是ziplist形式的hash才能省内存！！如果是skiplist的hash会浪费内存。</p>

<ul>
<li><a href="http://webcache.googleusercontent.com/search?q=cache:yr96Qf3F0e4J:heylinux.com/archives/1920.html+&amp;cd=1&amp;hl=zh-CN&amp;ct=clnk&amp;gl=jp">内存优化之Redis数据结构的设计优化实践</a> heylinux.com/archives/1920.html 这篇文章可能访问不了，可以通过google/baidu的快照来查看</li>
<li><a href="http://redis4you.com/articles.php?id=008&amp;name=Understanding+hash-max-zipmap-entries+and+">Understanding hash-max-zipmap-entries and &ldquo;hash of hashes&rdquo; optimization</a></li>
<li><a href="http://redis.io/topics/memory-optimization">http://redis.io/topics/memory-optimization</a></li>
<li><a href="http://redis.io/topics/lru-cache">http://redis.io/topics/lru-cache</a></li>
<li><a href="https://segmentfault.com/a/1190000004708270">Redis内存优化</a></li>
</ul>


<p>下面引用官网对简单键值对和Hash的一个比较（Redis中key的相关特性不关注）: 对于小数据量的hash进行了优化</p>

<blockquote><p> a few keys use a lot more memory than a single key containing a hash with a few fields.</p>

<p> We use a trick.</p>

<p> But many times hashes contain just a few fields. When hashes are small we can instead just encode them in an O(N) data structure, like a linear array with length-prefixed key value pairs. Since we do this only when N is small</p>

<p> This does not work well just from the point of view of time complexity, but also from the point of view of constant times, since a linear array of key value pairs happens to play very well with the CPU cache (it has a better cache locality than a hash table).</p></blockquote>

<p>优化主要涉及到ziplist的两个参数，是一个cpu/memory之间的均衡关系。entries直接用默认的就好了，value最好不要大于254（ziplist节点entry大于254需要增加4个到5字节，来存储前一个entry的长度）。</p>

<pre><code>hash-max-zipmap-entries 512 (hash-max-ziplist-entries for Redis &gt;= 2.6)
hash-max-zipmap-value 64  (hash-max-ziplist-value for Redis &gt;= 2.6)
</code></pre>

<p>简单列几条数据：</p>

<pre><code>3:0dc46077dfaa4970a1ec9f38cfc29277fa9e1012.ime.galileo.baidu.com  -&gt;  1469584847
3:co4hk52ia0b1.5buzd.com                                          -&gt;  1468859527
1:119.84.110.82_39502                                             -&gt;  1469666877
</code></pre>

<p>原始key内容可以不需要，鉴于包括域名的key太长，直接对数据key取md5。以1亿键值对来进行估算，取md5的前五位作为key，后27位作为hash键值对的key。</p>

<p>扫描原始redis实例，然后把键值对转换后存储到新的实例。转换Scala代码如下：</p>

<pre><code>import java.util.{List =&gt; JList}
import org.apache.commons.codec.digest.DigestUtils
import redis.clients.jedis._
import scala.collection.JavaConversions._

trait RedisUtils {

  def md5(data: String): String = {
    DigestUtils.md5Hex(data)
  }

  def Type(redis: Jedis, key: String) = redis.`type`(key)

  def scan(redis: Jedis)(action: JList[String] =&gt; Unit): Unit = {
    import scala.util.control.Breaks._

    var cursor = "0"
    breakable {
      while (true) {
        val res = redis.scan(cursor)

        action(res.getResult())

        cursor = res.getStringCursor
        if (cursor.equals("0")) {
          break
        }
      }
    }
  }

  def printInfo(redis: Jedis): Unit = {
    println(redis.info())
  }

  // 验证：
  //  打印 **总共** 的键值对数量
  //  eval "local aks=redis.call('keys', '*'); local res=0; for i,r in ipairs(aks) do res=res+redis.call('hlen', r) end; return res" 0
  //  打印 **每个** hash包括的键值对个数
  //  eval "local aks=redis.call('keys', '*'); local res={}; for i,r in ipairs(aks) do res[i]=redis.call('hlen', r) end; return res" 0
  //

}

Object RedisTransfer extends RedisUtils {

  def handle(key: String, value: String, tp: Pipeline): Unit = {
    val m5 = md5(key)
    tp.hset(m5.substring(0, 5), m5.substring(5), value)
  }

  def main(args: Array[String]) {
    val Array(sHost, sPort, tHost, tPort) = args

    val timeout = 60 * 1000
    val source = new Jedis(sHost, sPort.toInt, timeout)
    val sp = source.pipelined()
    val target = new Jedis(tHost, tPort.toInt, timeout)
    val tp = target.pipelined()

    scan(source) { keys =&gt;
      // 仅处理 string类型 的记录
      val requests = for (key &lt;- keys) yield Some((key, sp.get(key)))
      sp.sync()

      for (
        request &lt;- requests;
        (key, resp) &lt;- request
      ) {
        try {
          handle(key, resp.get(), tp)
        } catch {
          case e: Exception =&gt; println(s"fetch $key with exception, ${e.getMessage}")
        }
      }
    }

    tp.sync()

    printInfo(target)

    target.close()
    source.close()
  }

}
</code></pre>

<p>由于对数据进行了处理，对比不是很清晰，不能直接说省了多少空间。但是添加上面的处理后，原来30G（大概3亿多）的实例变成了15G。</p>

<h2>另一个案例</h2>

<p>另外对域名的实例做了下测试，6.4百万的键值对：707.29M内存：</p>

<p>md5前4个字符作为key，总共产生65536个键值对。每个hash大概包括100个kv。</p>

<ul>
<li>hash的key使用原来的键

<ul>
<li>不调ziplist_value的值，实际的转换成hash(skiplist)：939.6M，</li>
<li>ziplist_value修改成1024，转换成hash(ziplist)：513.78M</li>
</ul>
</li>
<li>md5的作为hash的新key：344.7M</li>
<li>md5的后28位作为hash的新key： 259.09M</li>
</ul>


<p>如：</p>

<pre><code>MD5:
  3:0dc46077dfaa4970a1ec9f38cfc29277fa9e1012.ime.galileo.baidu.com
  1356de078028ddf266c962533760b27c

1356 -&gt; hash( 3:0dc46077dfaa4970a1ec9f38cfc29277fa9e1012.ime.galileo.baidu.com -&gt; 1469584847 )
1356 -&gt; hash( 1356de078028ddf266c962533760b27c -&gt; 1469584847 )
1356 -&gt; hash( de078028ddf266c962533760b27c -&gt; 1469584847 )
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Codis简单使用]]></title>
    <link href="http://winseliu.com/blog/2016/07/14/codis-guide/"/>
    <updated>2016-07-14T19:35:23+08:00</updated>
    <id>http://winseliu.com/blog/2016/07/14/codis-guide</id>
    <content type="html"><![CDATA[<p>总有单机搞不定的时刻，并且手动切分很麻烦的时刻。不得不开始redis集群，官网的redis3不支持pipeline首先就排除了。</p>

<p>安装codis，需要先安装go。(官网入门文档](<a href="https://github.com/CodisLabs/codis/blob/master/doc/tutorial_zh.md">https://github.com/CodisLabs/codis/blob/master/doc/tutorial_zh.md</a>)</p>

<pre><code>[root@cu2 local]# tar zxvf go1.6.2.linux-amd64.tar.gz 

[hadoop@cu2 ~]$ vi .bash_profile
...
export GOROOT=/usr/local/go
export GOPATH=$HOME/codis

PATH=$GOPATH/bin:$GOROOT/bin:$HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$PATH

[hadoop@cu2 codis]$ source ~/.bash_profile 
</code></pre>

<p>git,make 这些要提前安装好。测试环境都编译过hadoop、spark肯定齐全的。</p>

<p>通过go在线安装（如果生产不能上网，可以先在测试环境安装好后，然后打包复制过去）：</p>

<pre><code>[hadoop@cu2 codis]$ go get -u -d github.com/CodisLabs/codis
package github.com/CodisLabs/codis: no buildable Go source files in /home/hadoop/codis/src/github.com/CodisLabs/codis
[hadoop@cu2 codis]$ 

&lt;&lt;安装依赖的工具
[hadoop@cu2 codis]$ go get github.com/tools/godep

[hadoop@cu2 codis]$ make
GO15VENDOREXPERIMENT=0 GOPATH=`godep path` godep restore
godep: [WARNING]: godep should only be used inside a valid go package directory and
godep: [WARNING]: may not function correctly. You are probably outside of your $GOPATH.
godep: [WARNING]:       Current Directory: /home/hadoop/codis/src/github.com/CodisLabs/codis
godep: [WARNING]:       $GOPATH: /home/hadoop/codis/src/github.com/CodisLabs/codis/Godeps/_workspace

&lt;&lt;这里要等一段时间

GOPATH=`godep path`:$GOPATH go build -o bin/codis-proxy ./cmd/proxy
godep: WARNING: Godep workspaces (./Godeps/_workspace) are deprecated and support for them will be removed when go1.8 is released.
godep: WARNING: Go version (go1.6) &amp; $GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling because a Godep workspace (Godeps/_workspace) exists
GOPATH=`godep path`:$GOPATH go build -o bin/codis-config ./cmd/cconfig
godep: WARNING: Godep workspaces (./Godeps/_workspace) are deprecated and support for them will be removed when go1.8 is released.
godep: WARNING: Go version (go1.6) &amp; $GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling because a Godep workspace (Godeps/_workspace) exists
make -j4 -C extern/redis-2.8.21/
make[1]: Entering directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21'
cd src &amp;&amp; make all
make[2]: Entering directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/src'
rm -rf redis-server redis-sentinel redis-cli redis-benchmark redis-check-dump redis-check-aof *.o *.gcda *.gcno *.gcov redis.info lcov-html
(cd ../deps &amp;&amp; make distclean)
make[3]: Entering directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps'
(cd hiredis &amp;&amp; make clean) &gt; /dev/null || true
(cd linenoise &amp;&amp; make clean) &gt; /dev/null || true
(cd lua &amp;&amp; make clean) &gt; /dev/null || true
(cd jemalloc &amp;&amp; [ -f Makefile ] &amp;&amp; make distclean) &gt; /dev/null || true
(rm -f .make-*)
make[3]: Leaving directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps'
(rm -f .make-*)
echo STD=-std=c99 -pedantic &gt;&gt; .make-settings
echo WARN=-Wall -W &gt;&gt; .make-settings
echo OPT=-O2 &gt;&gt; .make-settings
echo MALLOC=jemalloc &gt;&gt; .make-settings
echo CFLAGS= &gt;&gt; .make-settings
echo LDFLAGS= &gt;&gt; .make-settings
echo REDIS_CFLAGS= &gt;&gt; .make-settings
echo REDIS_LDFLAGS= &gt;&gt; .make-settings
echo PREV_FINAL_CFLAGS=-std=c99 -pedantic -Wall -W -O2 -g -ggdb   -I../deps/hiredis -I../deps/linenoise -I../deps/lua/src -DUSE_JEMALLOC -I../deps/jemalloc/include &gt;&gt; .make-settings
echo PREV_FINAL_LDFLAGS=  -g -ggdb -rdynamic &gt;&gt; .make-settings
(cd ../deps &amp;&amp; make hiredis linenoise lua jemalloc)
make[3]: Entering directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps'
(cd hiredis &amp;&amp; make clean) &gt; /dev/null || true
(cd linenoise &amp;&amp; make clean) &gt; /dev/null || true
(cd lua &amp;&amp; make clean) &gt; /dev/null || true
(cd jemalloc &amp;&amp; [ -f Makefile ] &amp;&amp; make distclean) &gt; /dev/null || true
(rm -f .make-*)
(echo "" &gt; .make-ldflags)
(echo "" &gt; .make-cflags)
MAKE hiredis
cd hiredis &amp;&amp; make static
MAKE linenoise
cd linenoise &amp;&amp; make
MAKE lua
cd lua/src &amp;&amp; make all CFLAGS="-O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL " MYLDFLAGS="" AR="ar rcu"
MAKE jemalloc
cd jemalloc &amp;&amp; ./configure --with-jemalloc-prefix=je_ --enable-cc-silence CFLAGS="-std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops " LDFLAGS=""
make[4]: Entering directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps/linenoise'
cc  -Wall -Os -g  -c linenoise.c
make[4]: Entering directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps/lua/src'
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lapi.o lapi.c
make[4]: Entering directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps/hiredis'
cc -std=c99 -pedantic -c -O3 -fPIC  -Wall -W -Wstrict-prototypes -Wwrite-strings -g -ggdb  net.c
checking for xsltproc... /usr/bin/xsltproc
checking for gcc... gcc
checking whether the C compiler works... yes
checking for C compiler default output file name... a.out
checking for suffix of executables... 
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C compiler... cc -std=c99 -pedantic -c -O3 -fPIC  -Wall -W -Wstrict-prototypes -Wwrite-strings -g -ggdb  hiredis.c
yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking how to run the C preprocessor... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lcode.o lcode.c
make[4]: Leaving directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps/linenoise'
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o ldebug.o ldebug.c
gcc -E
checking for grep that handles long lines and -e... /bin/grep
checking for egrep... /bin/grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o ldo.o ldo.c
yes
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o ldump.o ldump.c
checking for stdlib.h... ldo.c: In function ‘f_parser’:
ldo.c:496: warning: unused variable ‘c’
yes
checking for string.h... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lfunc.o lfunc.c
yes
checking for memory.h... yes
checking for strings.h... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lgc.o lgc.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o llex.o llex.c
yes
cc -std=c99 -pedantic -c -O3 -fPIC  -Wall -W -Wstrict-prototypes -Wwrite-strings -g -ggdb  sds.c
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking whether byte ordering is bigendian... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lmem.o lmem.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lobject.o lobject.c
no
checking size of void *... cc -std=c99 -pedantic -c -O3 -fPIC  -Wall -W -Wstrict-prototypes -Wwrite-strings -g -ggdb  async.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lopcodes.o lopcodes.c
8
checking size of int... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lparser.o lparser.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lstate.o lstate.c
4
checking size of long... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lstring.o lstring.c
8
checking size of intmax_t... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o ltable.o ltable.c
8
checking build system type... ar rcs libhiredis.a net.o hiredis.o sds.o async.o
x86_64-unknown-linux-gnu
checking host system type... x86_64-unknown-linux-gnu
checking whether pause instruction is compilable... make[4]: Leaving directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps/hiredis'
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o ltm.o ltm.c
yes
checking whether SSE2 intrinsics is compilable... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lundump.o lundump.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lvm.o lvm.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lzio.o lzio.c
yes
checking for ar... ar
checking whether __attribute__ syntax is compilable... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o strbuf.o strbuf.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o fpconv.o fpconv.c
yes
checking whether compiler supports -fvisibility=hidden... yes
checking whether compiler supports -Werror... yes
checking whether tls_model attribute is compilable... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lauxlib.o lauxlib.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lbaselib.o lbaselib.c
yes
checking for a BSD-compatible install... /usr/bin/install -c
checking for ranlib... ranlib
checking for ld... /usr/bin/ld
checking for autoconf... /usr/bin/autoconf
checking for memalign... yes
checking for valloc... yes
checking configured backtracing method... N/A
checking for sbrk... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o ldblib.o ldblib.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o liolib.o liolib.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lmathlib.o lmathlib.c
yes
checking whether utrace(2) is compilable... no
checking whether valgrind is compilable... no
checking STATIC_PAGE_SHIFT... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o loslib.o loslib.c
12
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o ltablib.o ltablib.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lstrlib.o lstrlib.c
checking pthread.h usability... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o loadlib.o loadlib.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o linit.o linit.c
yes
checking pthread.h presence... yes
checking for pthread.h... yes
checking for pthread_create in -lpthread... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lua_cjson.o lua_cjson.c
yes
checking for _malloc_thread_cleanup... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lua_struct.o lua_struct.c
no
checking for _pthread_mutex_init_calloc_cb... no
checking for TLS... yes
checking whether a program using ffsl is compilable... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lua_cmsgpack.o lua_cmsgpack.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lua_bit.o lua_bit.c
yes
checking whether atomic(9) is compilable... no
checking whether Darwin OSAtomic*() is compilable... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o lua.o lua.c
cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o luac.o luac.c
no
checking whether to force 32-bit __sync_{add,sub}_and_fetch()... no
checking whether to force 64-bit __sync_{add,sub}_and_fetch()... no
checking whether Darwin OSSpin*() is compilable... no
checking for stdbool.h that conforms to C99... cc -O2 -Wall -DLUA_ANSI -DENABLE_CJSON_GLOBAL    -c -o print.o print.c
yes
checking for _Bool... ar rcu liblua.a lapi.o lcode.o ldebug.o ldo.o ldump.o lfunc.o lgc.o llex.o lmem.o lobject.o lopcodes.o lparser.o lstate.o lstring.o ltable.o ltm.o lundump.o lvm.o lzio.o strbuf.o fpconv.o lauxlib.o lbaselib.o ldblib.o liolib.o lmathlib.o loslib.o ltablib.o lstrlib.o loadlib.o linit.o lua_cjson.o lua_struct.o lua_cmsgpack.o lua_bit.o        # DLL needs all object files
ranlib liblua.a
cc -o lua  lua.o liblua.a -lm 
liblua.a(loslib.o): In function `os_tmpname':
loslib.c:(.text+0x35): warning: the use of `tmpnam' is dangerous, better use `mkstemp'
cc -o luac  luac.o print.o liblua.a -lm 
yes
make[4]: Leaving directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps/lua/src'
configure: creating ./config.status
config.status: creating Makefile
config.status: creating doc/html.xsl
config.status: creating doc/manpages.xsl
config.status: creating doc/jemalloc.xml
config.status: creating include/jemalloc/jemalloc_macros.h
config.status: creating include/jemalloc/jemalloc_protos.h
config.status: creating include/jemalloc/internal/jemalloc_internal.h
config.status: creating test/test.sh
config.status: creating test/include/test/jemalloc_test.h
config.status: creating config.stamp
config.status: creating bin/jemalloc.sh
config.status: creating include/jemalloc/jemalloc_defs.h
config.status: creating include/jemalloc/internal/jemalloc_internal_defs.h
config.status: creating test/include/test/jemalloc_test_defs.h
config.status: executing include/jemalloc/internal/private_namespace.h commands
config.status: executing include/jemalloc/internal/private_unnamespace.h commands
config.status: executing include/jemalloc/internal/public_symbols.txt commands
config.status: executing include/jemalloc/internal/public_namespace.h commands
config.status: executing include/jemalloc/internal/public_unnamespace.h commands
config.status: executing include/jemalloc/internal/size_classes.h commands
config.status: executing include/jemalloc/jemalloc_protos_jet.h commands
config.status: executing include/jemalloc/jemalloc_rename.h commands
config.status: executing include/jemalloc/jemalloc_mangle.h commands
config.status: executing include/jemalloc/jemalloc_mangle_jet.h commands
config.status: executing include/jemalloc/jemalloc.h commands
===============================================================================
jemalloc version   : 3.6.0-0-g46c0af68bd248b04df75e4f92d5fb804c3d75340
library revision   : 1

CC                 : gcc
CPPFLAGS           :  -D_GNU_SOURCE -D_REENTRANT
CFLAGS             : -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -fvisibility=hidden
LDFLAGS            : 
EXTRA_LDFLAGS      : 
LIBS               :  -lpthread
RPATH_EXTRA        : 

XSLTPROC           : /usr/bin/xsltproc
XSLROOT            : 

PREFIX             : /usr/local
BINDIR             : /usr/local/bin
INCLUDEDIR         : /usr/local/include
LIBDIR             : /usr/local/lib
DATADIR            : /usr/local/share
MANDIR             : /usr/local/share/man

srcroot            : 
abs_srcroot        : /home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps/jemalloc/
objroot            : 
abs_objroot        : /home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps/jemalloc/

JEMALLOC_PREFIX    : je_
JEMALLOC_PRIVATE_NAMESPACE
                   : je_
install_suffix     : 
autogen            : 0
experimental       : 1
cc-silence         : 1
debug              : 0
code-coverage      : 0
stats              : 1
prof               : 0
prof-libunwind     : 0
prof-libgcc        : 0
prof-gcc           : 0
tcache             : 1
fill               : 1
utrace             : 0
valgrind           : 0
xmalloc            : 0
mremap             : 0
munmap             : 0
dss                : 0
lazy_lock          : 0
tls                : 1
===============================================================================
cd jemalloc &amp;&amp; make CFLAGS="-std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops " LDFLAGS="" lib/libjemalloc.a
make[4]: Entering directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps/jemalloc'
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/jemalloc.o src/jemalloc.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/arena.o src/arena.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/atomic.o src/atomic.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/base.o src/base.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/bitmap.o src/bitmap.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/chunk.o src/chunk.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/chunk_dss.o src/chunk_dss.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/chunk_mmap.o src/chunk_mmap.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/ckh.o src/ckh.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/ctl.o src/ctl.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/extent.o src/extent.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/hash.o src/hash.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/huge.o src/huge.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/mb.o src/mb.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/mutex.o src/mutex.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/prof.o src/prof.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/quarantine.o src/quarantine.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/rtree.o src/rtree.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/stats.o src/stats.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/tcache.o src/tcache.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/util.o src/util.c
gcc -std=gnu99 -Wall -pipe -g3 -O3 -funroll-loops  -c -D_GNU_SOURCE -D_REENTRANT -Iinclude -Iinclude -o src/tsd.o src/tsd.c
ar crus lib/libjemalloc.a src/jemalloc.o src/arena.o src/atomic.o src/base.o src/bitmap.o src/chunk.o src/chunk_dss.o src/chunk_mmap.o src/ckh.o src/ctl.o src/extent.o src/hash.o src/huge.o src/mb.o src/mutex.o src/prof.o src/quarantine.o src/rtree.o src/stats.o src/tcache.o src/util.o src/tsd.o
make[4]: Leaving directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps/jemalloc'
make[3]: Leaving directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/deps'
    CC adlist.o
    CC ae.o
    CC anet.o
    CC dict.o
anet.c: In function ‘anetSockName’:
anet.c:565: warning: dereferencing pointer ‘s’ does break strict-aliasing rules
anet.c:563: note: initialized from here
anet.c:569: warning: dereferencing pointer ‘s’ does break strict-aliasing rules
anet.c:567: note: initialized from here
anet.c: In function ‘anetPeerToString’:
anet.c:543: warning: dereferencing pointer ‘s’ does break strict-aliasing rules
anet.c:541: note: initialized from here
anet.c:547: warning: dereferencing pointer ‘s’ does break strict-aliasing rules
anet.c:545: note: initialized from here
anet.c: In function ‘anetTcpAccept’:
anet.c:511: warning: dereferencing pointer ‘s’ does break strict-aliasing rules
anet.c:509: note: initialized from here
anet.c:515: warning: dereferencing pointer ‘s’ does break strict-aliasing rules
anet.c:513: note: initialized from here
    CC redis.o
    CC sds.o
    CC zmalloc.o
    CC lzf_c.o
    CC lzf_d.o
    CC pqsort.o
    CC zipmap.o
    CC ziplist.o
    CC sha1.o
    CC release.o
    CC networking.o
    CC util.o
    CC object.o
    CC db.o
    CC replication.o
    CC rdb.o
db.c: In function ‘scanGenericCommand’:
db.c:454: warning: ‘pat’ may be used uninitialized in this function
db.c:455: warning: ‘patlen’ may be used uninitialized in this function
    CC t_string.o
    CC t_list.o
    CC t_set.o
    CC t_zset.o
    CC t_hash.o
    CC config.o
    CC aof.o
    CC pubsub.o
    CC multi.o
    CC debug.o
    CC sort.o
    CC intset.o
    CC syncio.o
    CC migrate.o
    CC endianconv.o
    CC slowlog.o
    CC scripting.o
    CC bio.o
    CC rio.o
    CC rand.o
    CC memtest.o
    CC crc64.o
    CC crc32.o
    CC bitops.o
    CC sentinel.o
    CC notify.o
    CC setproctitle.o
    CC hyperloglog.o
    CC latency.o
    CC sparkline.o
    CC slots.o
    CC redis-cli.o
    CC redis-benchmark.o
    CC redis-check-dump.o
    CC redis-check-aof.o
    LINK redis-benchmark
    LINK redis-check-dump
    LINK redis-check-aof
    LINK redis-server
    INSTALL redis-sentinel
    LINK redis-cli

Hint: It's a good idea to run 'make test' ;)

make[2]: Leaving directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/src'
make[1]: Leaving directory `/home/hadoop/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21'
[hadoop@cu2 codis]$ 
</code></pre>

<p>简单使用：</p>

<pre><code>[hadoop@cu2 codis]$ pwd
/home/hadoop/codis/src/github.com/CodisLabs/codis
[hadoop@cu2 codis]$ ll bin
total 39856
drwxrwxr-x 4 hadoop hadoop     4096 Jul 14 15:49 assets
-rwxrwxr-x 1 hadoop hadoop 17329904 Jul 14 15:49 codis-config
-rwxrwxr-x 1 hadoop hadoop 17151864 Jul 14 15:49 codis-proxy
-rwxrwxr-x 1 hadoop hadoop  6313083 Jul 14 15:49 codis-server

&lt;&lt;配置
[hadoop@cu2 codis]$ vi config.ini 
zk=cu3:2181
dashboard_addr=cu2:18087
session_max_timeout=0
session_max_bufsize=1310720
session_max_pipeline=10240000

&lt;&lt;启动dashboard（大部分命令其实都可以通过网页来完成）
[hadoop@cu2 codis]$ nohup bin/codis-config dashboard &gt;log/dashboard.log 2&gt;&amp;1 &amp;

&lt;&lt;初始化1024 slot
[hadoop@cu2 codis]$ bin/codis-config slot init
{
  "msg": "OK",
  "ret": 0
}

&lt;&lt;启动redis server
[hadoop@cu2 codis]$ bin/codis-server --port 16379 --daemonize yes

&lt;&lt;在网页上分配好slot，或者：

$ bin/codis-config slot range-set 0 511 1 online
$ bin/codis-config slot range-set 512 1023 2 online

&lt;&lt;然后启动proxy
[hadoop@hadoop-master1 codis]$ nohup bin/codis-proxy -c config.ini -L proxy.log  --cpu=64 --addr=0.0.0.0:6372 --http-addr=0.0.0.0:11000 &gt;&gt;proxy.log 2&gt;&amp;1 &amp;

&lt;&lt;客户端连接proxy
[hadoop@cu2 codis]$ ~/redis/bin/redis-cli -p 19000
&lt;&lt;不支持的命令
127.0.0.1:19000&gt; keys *
Error: Server closed the connection
127.0.0.1:19000&gt; scan 0
Error: Server closed the connection

127.0.0.1:19000&gt; get a
"b"

127.0.0.1:19000&gt; select 2
(error) ERR invalid DB index, only accept DB 0

&lt;&lt;也可以单独连接到redis server，进行操作
[hadoop@cu2 codis]$ ~/redis/bin/redis-cli -p 16378
# Keyspace
db0:keys=6,expires=0,avg_ttl=0
127.0.0.1:16378&gt; keys *
1) "7"
2) "1"
3) "2"
4) "4"
5) "5"
6) "a"
</code></pre>

<p><a href="https://github.com/CodisLabs/codis/blob/master/doc/unsupported_cmds.md">codis不支持的命令</a>，基本上都是全局操作的命令。不影响使用，如果一定要使用，可以通过客户端单独连server执行。</p>

<p>有个疑问，怎么查看proxy写的数据放在那个slot？要学学go才行。</p>

<h2>安装参考</h2>

<ul>
<li><a href="https://lvs071103.gitbooks.io/codis/content/install.html">https://lvs071103.gitbooks.io/codis/content/install.html</a></li>
<li><a href="http://jicki.blog.51cto.com/1323993/1748549">http://jicki.blog.51cto.com/1323993/1748549</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Build redis-2.8]]></title>
    <link href="http://winseliu.com/blog/2015/01/22/build-redis/"/>
    <updated>2015-01-22T09:59:13+08:00</updated>
    <id>http://winseliu.com/blog/2015/01/22/build-redis</id>
    <content type="html"><![CDATA[<h2>jemalloc</h2>

<p>默认make使用的libc，在内存方面会产生比较多的碎片。可以使用jemalloc要进行内存的分配管理。</p>

<p>如果报<code>make cc Command not found</code>，需要先安装gcc。</p>

<pre><code>tar zxvf redis-2.8.13.bin.tar.gz 
cd redis-2.8.13
cd deps/jemalloc/
# 用于产生h头文件
./configure 

cd redis-2.8.13
make MALLOC=jemalloc
src/redis-server 
</code></pre>

<p>查看jemalloc的include的内容如下：</p>

<pre><code>[hadoop@localhost jemalloc]$ cd include/jemalloc/
internal/              jemalloc_defs.h.in     jemalloc_macros.h      jemalloc_mangle.h      jemalloc_mangle.sh     jemalloc_protos.h.in   jemalloc_rename.h      jemalloc.sh
jemalloc_defs.h        jemalloc.h             jemalloc_macros.h.in   jemalloc_mangle_jet.h  jemalloc_protos.h      jemalloc_protos_jet.h  jemalloc_rename.sh  
</code></pre>

<p>查看内存使用：</p>

<pre><code>[hadoop@localhost redis-2.8.13]$ src/redis-cli info
...
# Memory
used_memory:503576
used_memory_human:491.77K
used_memory_rss:2158592
used_memory_peak:503576
used_memory_peak_human:491.77K
used_memory_lua:33792
mem_fragmentation_ratio:4.29
mem_allocator:jemalloc-3.6.0
...
</code></pre>

<p>redis在使用过程中，会产生碎片。重启以及libc和jemalloc的对比如下：</p>

<pre><code># 运行中实例
# Memory
used_memory:4623527744
used_memory_human:4.31G
used_memory_rss:48304705536
used_memory_peak:38217543280
used_memory_peak_human:35.59G
used_memory_lua:33792
mem_fragmentation_ratio:10.45
mem_allocator:libc

51616 hadoop    20   0 45.1g  44g 1136 S  0.0 35.7   3410:42 /home/hadoop/redis-2.8.13/src/redis-server *:6371

# 序列化为rdb的文件大小
[hadoop@hadoop-master1 18111]$ ll
总用量 1183116
-rw-rw-r--. 1 hadoop hadoop 1210319541 1月  14 11:28 dump.rdb

# 重启后的实例
[hadoop@hadoop-master1 18111]$  ~/redis-2.8.13/src/redis-server --port 18111
[77484] 14 Jan 14:33:17.910 * DB loaded from disk: 218.337 seconds

# Memory
used_memory:4763158704
used_memory_human:4.44G
used_memory_rss:6217580544
used_memory_peak:4763158704
used_memory_peak_human:4.44G
used_memory_lua:33792
mem_fragmentation_ratio:1.31
mem_allocator:libc

77484 hadoop    20   0 6052m 5.8g 1200 S  0.0  4.6   3:38.39 /home/hadoop/redis-2.8.13/src/redis-server *:18111

# 使用jemalloc替换libc的实例
[hadoop@hadoop-master1 18111]$ ~/redis-jemalloc/redis-2.8.13/src/redis-server --port 18888
[14793] 14 Jan 14:50:11.250 * DB loaded from disk: 209.839 seconds

# Memory
used_memory:4527760088
used_memory_human:4.22G
used_memory_rss:4625887232
used_memory_peak:4527760088
used_memory_peak_human:4.22G
used_memory_lua:33792
mem_fragmentation_ratio:1.02
mem_allocator:jemalloc-3.6.0

14793 hadoop    20   0 4538m 4.3g 1360 S  0.0  3.4   3:28.10 /home/hadoop/redis-jemalloc/redis-2.8.13/src/redis-server *:18888                                                                                                                       
</code></pre>

<h2>tcmalloc</h2>

<ul>
<li>root安装</li>
</ul>


<p>如果有root用户的话操作比较简单。现在<a href="https://code.google.com/p/gperftools/">gperftools</a>和<a href="http://download.savannah.gnu.org/releases/libunwind/libunwind-0.99-beta.tar.gz">libunwind-0.99-beta</a></p>

<pre><code>cd libunwind-0.99-beta
./configure 
make &amp;&amp; make install
cd /home/hadoop/gperftools-2.4
./configure 
make &amp;&amp; make install

cd redis-2.8.13
make MALLOC=tcmalloc
</code></pre>

<p>如果出现<strong>./libtool: line 1125: g++: command not found</strong>的错误，缺少编译环境；</p>

<pre><code>[root@localhost gperftools-2.4]# yum -y install gcc+ gcc-c++
</code></pre>

<p>编译后，运行报错<strong>src/redis-server: error while loading shared libraries: libtcmalloc.so.4: cannot open shared object file: No such file or directory</strong>，需要配置环境变量：</p>

<pre><code>[hadoop@localhost redis-2.8.13]$ export LD_LIBRARY_PATH=/usr/local/lib
[hadoop@localhost redis-2.8.13]$ src/redis-server 
</code></pre>

<p>或者按照网上的做法：</p>

<pre><code>echo "/usr/local/lib" &gt; /etc/ld.so.conf.d/usr_local_lib.conf  
/sbin/ldconfig  
</code></pre>

<p>检查tcmalloc是否生效<code>lsof -n | grep tcmalloc</code>，出现以下信息说明生效</p>

<pre><code>redis-ser 1716    hadoop  mem       REG  253,0  2201976  936349 /usr/local/lib/libtcmalloc.so.4.2.6
</code></pre>

<p>修改配置文件找到daemonize，将后面的no改为yes，让其可以以服务方式运行。</p>

<ul>
<li>普通用户安装</li>
</ul>


<p>考虑到可以各台机器上面复制，指定编译目录这种方式会比较方便。</p>

<pre><code>cd libunwind-0.99-beta
CFLAGS=-fPIC ./configure --prefix=/home/hadoop/redis
make &amp;&amp; make install

cd gperftools-2.4
./configure -h
export LDFLAGS="-L/home/hadoop/redis/lib"
export CPPFLAGS="-I/home/hadoop/redis/include"
./configure --prefix=/home/hadoop/redis
make &amp;&amp; make install
</code></pre>

<p>编译好后，把东西redis目录内容移到redis-2.8.13/src下。然后修改src/Makefile：</p>

<pre><code>[hadoop@master1 redis-2.8.13]$ vi src/Makefile
# Include paths to dependencies
FINAL_CFLAGS+= -I../deps/hiredis -I../deps/linenoise -I../deps/lua/src

ifeq ($(MALLOC),tcmalloc)
        #FINAL_CFLAGS+= -DUSE_TCMALLOC
        #FINAL_LIBS+= -ltcmalloc
        FINAL_CFLAGS+= -DUSE_TCMALLOC -I./include
        FINAL_LIBS+= -L./lib  -ltcmalloc -ldl

endif

ifeq ($(MALLOC),tcmalloc_minimal)
        FINAL_CFLAGS+= -DUSE_TCMALLOC
        FINAL_LIBS+= -ltcmalloc_minimal
endif
</code></pre>

<p>然后编译：</p>

<pre><code>[hadoop@master1 redis-2.8.13]$ export LD_LIBRARY_PATH=/home/hadoop/redis-2.8.13/src/lib
[hadoop@master1 redis-2.8.13]$ make MALLOC=tcmalloc
cd src &amp;&amp; make all
make[1]: Entering directory `/home/hadoop/redis-2.8.13/src'
    LINK redis-server
    INSTALL redis-sentinel
    CC redis-cli.o
In file included from zmalloc.h:40,
                 from redis-cli.c:50:
./include/google/tcmalloc.h:35:2: warning: #warning is a GCC extension
./include/google/tcmalloc.h:35:2: warning: #warning "google/tcmalloc.h is deprecated. Use gperftools/tcmalloc.h instead"
    LINK redis-cli
    CC redis-benchmark.o
In file included from zmalloc.h:40,
                 from redis-benchmark.c:47:
./include/google/tcmalloc.h:35:2: warning: #warning is a GCC extension
./include/google/tcmalloc.h:35:2: warning: #warning "google/tcmalloc.h is deprecated. Use gperftools/tcmalloc.h instead"
    LINK redis-benchmark
    CC redis-check-dump.o
    LINK redis-check-dump
    CC redis-check-aof.o
    LINK redis-check-aof

Hint: To run 'make test' is a good idea ;)

make[1]: Leaving directory `/home/hadoop/redis-2.8.13/src'
[hadoop@master1 redis-2.8.13]$ 
</code></pre>

<h2>redis3集群安装cluster</h2>

<p>编译安装和2.8一样，configuration/make/makeinstall即可。</p>

<pre><code>[hadoop@umcc97-44 cluster-test]$ cat cluster.conf 
port .
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
</code></pre>

<p>比较苦逼的是需要安装ruby，服务器不能上网！其实ruby在能访问的机器上面安装就可以了！初始化集群的脚本其实就是客户端连接服务端，初始化集群而已。
还有就是在调用命令的时刻要加上<code>-c</code>，这样才是使用集群模式，不然仅仅连单机，读写其他集群服务会报错！</p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVx5-Ab4jaAAA9_Lg7l-I862.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVyXSAC5iEAABfrrHCfuI114.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVzBSAc3KOAADvQfFIPrs908.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCc-AXZ3EAAHoKZnb1nQ426.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCkWAZYuLAAAWM5VoXJI861.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCuSAAuXxAABB-LpH1nQ340.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/05/5F/wKhkA1PZBrSAPaMTAAAcSnjmhXE093.png" alt="" /></p>

<h2>Cygwin</h2>

<p>开发环境系统都是在windows，想调试一步步的看源码就得编译下redis。由于cygwin环境，模拟的linux，有部分的变量没有定义，需要进行修改。修改如下:</p>

<pre><code>$ git log -1
commit 0c211a1953afeda3d0d45126653e2d4c38bd88cb
Author: antirez &lt;antirez@gmail.com&gt;
Date:   Fri Dec 5 10:51:09 2014 +010

$ git branch
* 2.8

$ git diff
diff --git a/deps/hiredis/net.c b/deps/hiredis/net.c
index bdb84ce..6e95f22 100644
--- a/deps/hiredis/net.c
+++ b/deps/hiredis/net.c
@@ -51,6 +51,13 @@
 #include "net.h"
 #include "sds.h"

+/* Cygwin Fix */
+#ifdef __CYGWIN__
+#define TCP_KEEPCNT 8
+#define TCP_KEEPINTVL 150
+#define TCP_KEEPIDLE 14400
+#endif
+
 /* Defined in hiredis.c */
 void __redisSetError(redisContext *c, int type, const char *str);

diff --git a/src/Makefile b/src/Makefile
index 8b3e959..a72b2f2 100644
--- a/src/Makefile
+++ b/src/Makefile
@@ -63,6 +63,9 @@ else
 ifeq ($(uname_S),Darwin)
        # Darwin (nothing to do)
 else
+ifeq ($(uname_S),CYGWIN_NT-6.3-WOW64)
+       # cygwin (nothing to do)
+else
 ifeq ($(uname_S),AIX)
         # AIX
         FINAL_LDFLAGS+= -Wl,-bexpall
@@ -75,6 +78,7 @@ else
 endif
 endif
 endif
+endif
 # Include paths to dependencies
 FINAL_CFLAGS+= -I../deps/hiredis -I../deps/linenoise -I../deps/lua/src
</code></pre>

<p>然后编译：</p>

<pre><code>cd deps/
make lua hiredis linenoise

cd ..
make
</code></pre>

<p>编译成功后，把程序导入eclipse CDT环境进行运行调试。导入后需要重新构建一下，不然调试的时刻会按照/cygwin的路径来查找源码。</p>

<ul>
<li>Import，然后选择C/C++目录下的[Existing Code as Makefile project]</li>
<li>在[Existing Code Location]填入redis程序对应的目录，在[Toolchain for Indexer Settings]选择<strong>Cygwin GCC</strong></li>
<li>导入完成后，右键选择[Build Configuration]->[Build All]</li>
<li>Run然后选择执行redis-server即可。</li>
</ul>


<p>好像也可以远程调试</p>

<pre><code>[root@Frankzfz]$gdbserver 10.27.10.48:9000 ./test_seg_fault
</code></pre>

<h2>参考</h2>

<ul>
<li><a href="http://blog.sina.com.cn/s/blog_71954d8a0100nixe.html">tcp Keepalive</a></li>
<li><a href="http://jiangzhixiang123.blog.163.com/blog/static/2780206220115643822896/">setsockopt之 TCP_KEEPIDLE/TCP_KEEPINTVL/TCP_KEEPCNT - [Linux]</a></li>
<li><a href="http://blog.csdn.net/ce123_zhouwei/article/details/6625486">GDB+GdbServer: ARM程序调试</a></li>
<li><a href="http://my.oschina.net/shelllife/blog/167914">使用gdbserver远程调试</a></li>
<li><p><a href="http://qingfengju.com/article.asp?id=303">用gdb,gdbserver,eclipse+cdt在windows上远程调试linux程序</a></p></li>
<li><p><a href="http://www.cnblogs.com/kernel_hcy/archive/2011/05/15/2046963.html">redis源码分析（1）内存管理</a></p></li>
<li><a href="http://blog.csdn.net/unix21/article/details/12119059">利用TCMalloc替换Nginx和Redis默认glibc库的malloc内存分配</a>)</li>
<li><a href="http://blog.nosqlfan.com/html/3490.html">Redis采用不同内存分配器碎片率对比</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
