<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Winse Blog]]></title>
  <link href="http://winse.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://winse.github.io/"/>
  <updated>2014-07-29T22:10:40+08:00</updated>
  <id>http://winse.github.io/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Use ShortCircuit Local Reading]]></title>
    <link href="http://winse.github.io/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading/"/>
    <updated>2014-07-29T20:11:58+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading</id>
    <content type="html"><![CDATA[<p>hadoop一直以来认为是本地读写文件的，但是其实也是通过TCP端口去获取数据，只是都在同一台机器。在hivetuning调优hive的文档中看到了ShortCircuit的HDFS配置属性，查看了ShortCircuit的来由，真正的实现了本地读取文件。蒙查查表示看的不是很明白，最终大致就是通过linux的<strong>文件描述符</strong>来实现功能同时保证文件的权限。</p>

<p>由于仅在自己的机器上面配置来查询hbase的数据，性能方面提升感觉不是很明显。等以后整到正式环境再对比对比。</p>

<p>配置如下。</p>

<p>1 修改hdfs-site.xml</p>

<pre><code>&lt;property&gt;
        &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
        &lt;value&gt;/home/hadoop/data/sockets/dn_socket&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>注意：socket路径的权限控制的比较严格。dn_socket<strong>所有的父路径</strong>要么仅有当前启动用户的读权限，要么仅root可读。</p>

<p><img src="http://file.bmob.cn/M00/05/52/wKhkA1PXfbKANLOrAADWJQ5taVs391.png" alt="" /></p>

<p>2 修改hbase的配置，并添加HADOOP_HOME（hbase查找hadoop-native）</p>

<p><img src="http://file.bmob.cn/M00/05/52/wKhkA1PXhRKAZDs6AAChrEauBoU738.png" alt="" /></p>

<p>hbase的脚本找到hadoop命令后，会把hadoop的java.library.path的路径加入到hbase的启动脚本中。</p>

<pre><code>[hadoop@master1 ~]$ tail -15 hbase-0.98.3-hadoop2/conf/hbase-site.xml 
    &lt;name&gt;hbase.tmp.dir&lt;/name&gt;
    &lt;value&gt;/home/hadoop/data/hbase&lt;/value&gt;
  &lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
        &lt;value&gt;/home/hadoop/data/sockets/dn_socket&lt;/value&gt;
&lt;/property&gt;

&lt;/configuration&gt;

[hadoop@master1 ~]$ cat hbase-0.98.3-hadoop2/conf/hbase-env.sh
...
export HADOOP_HOME=/home/hadoop/hadoop-2.2.0
...
</code></pre>

<p>3 同步到其他节点，然后重启hdfs,hbase</p>

<h2>参考</h2>

<ul>
<li><a href="http://vdisk.weibo.com/s/z_44nz36hNM3Z">hive-tuning</a></li>
<li><a href="http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/">How Improved Short-Circuit Local Reads Bring Better Performance and Security to Hadoop</a></li>
<li><a href="http://hbase.apache.org/book/perf.hdfs.html">HDFS&ndash;Apache HBase Performance Tuning</a></li>
<li><a href="http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">HDFS Short-Circuit Local Reads</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop安全的关闭datanode节点]]></title>
    <link href="http://winse.github.io/blog/2014/07/29/safely-remove-datanode/"/>
    <updated>2014-07-29T15:08:41+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/29/safely-remove-datanode</id>
    <content type="html"><![CDATA[<p>hadoop默认就有冗余（dfs.replication）的机制，所以一般情况下，一台机器挂了也没所谓。集群会自动的进行复制均衡处理。</p>

<p>作为测试，如果dfs.replication设置为1的情况下，怎么安全的把datanode节点服务关闭呢？例如说，刚刚开始搭建环境是把namenode、datanode放在一台机器上，后面增加了机器如何把datanode分离出来呢？</p>

<p>借助于<strong>dfs.hosts.exclude</strong>即可完成顺序的完成此项任务。</p>

<p>修改hdfs-site.xml配置。我操作的时刻仅修改了master1上的hdfs-site.xml。把<strong>master1</strong>值写入到对应的文件中。</p>

<pre><code>    [hadoop@master1 hadoop]$ cat hdfs-site.xml 
    ...
    &lt;property&gt;
            &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;
            &lt;value&gt;/home/hadoop/hadoop-2.2.0/etc/hadoop/exclude&lt;/value&gt;
    &lt;/property&gt;

    &lt;/configuration&gt;
    [hadoop@master1 hadoop]$ cat /home/hadoop/hadoop-2.2.0/etc/hadoop/exclude
    master1
</code></pre>

<p>修改完成后，刷新节点即可(完全没有必要重启集dfs)。</p>

<pre><code>hadoop dfsadmin -refreshNodes
</code></pre>

<p>可以通过<code>dfsadmin -report</code>或者网页查看master1已经变成<em>Decommission In Progress</em>了。</p>

<p><img src="http://file.bmob.cn/M00/05/4C/wKhkA1PXUMOAVvvWAAED6CN-3Rg187.png" alt="" /></p>

<p>注：</p>

<p>问题一： 在新建节点是slaver1的防火墙没关闭，由于master1已经被exclude，而slaver1不能提供服务，上传文件时报错：</p>

<pre><code>[hadoop@master1 hadoop]$ hadoop fs -put slaves  /
14/07/29 15:18:21 WARN hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /slaves._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1384)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2477)
</code></pre>

<p>关闭防火墙一样再次上传，还是报同样的错误。此时，也可以通过刷新节点<code>hadoop dfsadmin -refreshNodes</code>来解决。</p>

<p>问题二： 设置备份数量</p>

<pre><code>[hadoop@master1 hadoop]$ hadoop fs -setrep 3 /slaves 
Replication 3 set: /slaves
</code></pre>

<p>问题三： 新增节点</p>

<p>拷贝程序到新增节点，然后启动</p>

<pre><code>[hadoop@master1 ~]$ tar zc hadoop-2.2.0 --exclude=logs | ssh slaver2 'cat | tar zx'

[hadoop@slaver2 ~]$ cd hadoop-2.2.0/
[hadoop@slaver2 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start datanode
</code></pre>

<p>也可以修改master上的slavers文件再<code>sbin/start-dfs.sh</code>启动。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Ganglia on Redhat5+]]></title>
    <link href="http://winse.github.io/blog/2014/07/18/install-ganglia-on-redhat/"/>
    <updated>2014-07-18T14:53:44+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/18/install-ganglia-on-redhat</id>
    <content type="html"><![CDATA[<p>对使用C写的复杂的程序安装心里有阴影，还有本来可以上网的话，使用yum安装会省很多的事情。
但是没办法，环境是这样，正式环境没有提供网络环境，搭建的本地yum环境也不知道行不行。</p>

<p>上次在自己电脑的虚拟机上面成功安装过ganglia，但apache、rrdtool依赖使用yum安装的，安装过程比较揪心。把ganglia安装到正式环境就不了了之的。
上个星期生产环境出现了用户查询数据久久不能返回的问题，由于查询程序写的比较差的缘故。但同时也给自己敲了警钟，都不知道集群机器运行情况，终究是大隐患；安装后监测集群同时为以后程序的调优工作带来便利。</p>

<p>本次安装全部使用源码包安装，有部分lib有重复编译。</p>

<p>总结下，原来安装ganglia就仅是按照网络的步骤一步步的弄，同时各个程序的版本又有可能不一致，每一步都胆战心惊！没有重点重心，以至于浪费了很多的事情。
分步骤有条不紊的操作就可以踏实多了，安装ganglia主要涉及三个核心部分(安装程序包<a href="http://yunpan.cn/QCFUiuyAWSyZI">下载</a>（提取码：0ec4）)：</p>

<ul>
<li>rrdtool</li>
<li>gmetad / gmond</li>
<li>apache / web</li>
<li>集群子节点部署</li>
<li>配置hadoop metrics监控hadoop集群</li>
</ul>


<p>按照顺序一个个的安装就可以了。无需为一个个依赖的版本不一致问题而忧心，同时可以更好的参考网络上的实践。</p>

<h2>安装rrdtool</h2>

<p>推荐按照<a href="http://oss.oetiker.ch/rrdtool/doc/rrdbuild.en.html#IBUILDING_DEPENDENCIES">官网教程</a>步骤操作，很&amp;非常的详细。
教程中环境变量必须得设置！这个很重点！</p>

<p>下面是安装rrdtool过程中用到的软件，列出的顺序即为安装的次序：</p>

<pre><code>[hadoop@umcc97-44 rrdbuild]$ ll -tr | grep -v 'tar'
总计 24132
drwxrwxrwx  6   1000          1000    4096 07-17 12:12 pkg-config-0.23
drwxr-xr-x 11 hadoop            80    4096 07-17 12:28 zlib-1.2.3
drwxr-xr-x  7   1004 avahi-autoipd    4096 07-17 12:29 libpng-1.2.18
drwxr-xr-x  8   1000 users            4096 07-17 12:31 freetype-2.3.5
drwxrwxrwx 15  50138 vcsa            12288 07-17 16:37 libxml2-2.6.32
drwxrwxrwx 15   1488 users            4096 07-17 16:53 fontconfig-2.4.2
drwxrwxrwx  4 sjyw   sjyw             4096 07-17 16:56 pixman-0.10.0
drwxrwsrwx  8   1000 ftp              4096 07-17 16:59 cairo-1.6.4
drwxrwxrwx 12 sjyw   sjyw             4096 07-17 17:01 glib-2.15.4
drwxrwxrwx  9 sjyw   sjyw             4096 07-17 17:16 pango-1.21.1
drwxr-xr-x 11   1003          1001    4096 07-17 17:36 rrdtool-1.4.8
</code></pre>

<p>具体操作的步骤（原来包括操作步骤，发现太累赘了重新调整了一下）：</p>

<pre><code># 下面几个环境变量时基础！
BUILD_DIR=/home/ganglia/rrdbuild
INSTALL_DIR=/opt/rrdtool-1.4.8

export PKG_CONFIG_PATH=${INSTALL_DIR}/lib/pkgconfig
export PATH=$INSTALL_DIR/bin:$PATH

export LDFLAGS="-Wl,--rpath -Wl,${INSTALL_DIR}/lib" 

[root@umcc97-44 rrdbuild]# tar zxvf pkg-config-0.23.tar.gz 
[root@umcc97-44 rrdbuild]# cd pkg-config-0.23
[root@umcc97-44 pkg-config-0.23]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
[root@umcc97-44 pkg-config-0.23]# make &amp;&amp; make install

# 这个环境变量也很重要
[root@umcc97-44 pkg-config-0.23]# export PKG_CONFIG=$INSTALL_DIR/bin/pkg-config
[root@umcc97-44 pkg-config-0.23]# cd ..

[root@umcc97-44 rrdbuild]# tar zxvf zlib-1.2.3.tar.gz 
[root@umcc97-44 rrdbuild]# cd zlib-1.2.3
# 修改了下官网的命令; 64位问题 recompile with -fPIC
[root@umcc97-44 zlib-1.2.3]# CFLAGS="-O3 -fPIC" ./configure
[root@umcc97-44 zlib-1.2.3]# make &amp;&amp; make install

[root@umcc97-44 rrdbuild]# tar zxvf libpng-1.2.18.tar.gz 
[root@umcc97-44 rrdbuild]# cd libpng-1.2.18
[root@umcc97-44 zlib-1.2.3]# cd ../libpng-1.2.18
[root@umcc97-44 libpng-1.2.18]# env CFLAGS="-O3 -fPIC" ./configure --prefix=$INSTALL_DIR
[root@umcc97-44 libpng-1.2.18]# make &amp;&amp; make install

[root@umcc97-44 libpng-1.2.18]# cd ..
[root@umcc97-44 rrdbuild]# tar zxvf freetype-2.3.5.tar.gz 
[root@umcc97-44 rrdbuild]# cd freetype-2.3.5
[root@umcc97-44 freetype-2.3.5]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
[root@umcc97-44 freetype-2.3.5]# make &amp;&amp; make install

[root@umcc97-44 rrdbuild]# tar zxvf libxml2-2.6.32.tar.gz 
[root@umcc97-44 rrdbuild]# cd libxml2-2.6.32
[root@umcc97-44 libxml2-2.6.32]#  ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
[root@umcc97-44 libxml2-2.6.32]# make &amp;&amp; make install

[root@umcc97-44 libxml2-2.6.32]# cd ..
[root@umcc97-44 rrdbuild]# tar zxvf fontconfig-2.4.2.tar.gz 
[root@umcc97-44 rrdbuild]# cd fontconfig-2.4.2
[root@umcc97-44 fontconfig-2.4.2]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC" --with-freetype-config=$INSTALL_DIR/bin/freetype-config
[root@umcc97-44 fontconfig-2.4.2]# make &amp;&amp; make install

[root@umcc97-44 fontconfig-2.4.2]# cd ..
[root@umcc97-44 rrdbuild]# cd pixman-0.10.0
[root@umcc97-44 pixman-0.10.0]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
[root@umcc97-44 pixman-0.10.0]# make &amp;&amp; make install

[root@umcc97-44 pixman-0.10.0]# cd ../cairo-1.6.4
[root@umcc97-44 cairo-1.6.4]# ./configure --prefix=$INSTALL_DIR \
&gt;     --enable-xlib=no \
&gt;     --enable-xlib-render=no \
&gt;     --enable-win32=no \
&gt;     CFLAGS="-O3 -fPIC"
[root@umcc97-44 cairo-1.6.4]# make &amp;&amp; make install

[root@umcc97-44 cairo-1.6.4]# cd ..
[root@umcc97-44 rrdbuild]# tar zxvf glib-2.15.4.tar.gz 
[root@umcc97-44 rrdbuild]# cd glib-2.15.4
[root@umcc97-44 glib-2.15.4]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
[root@umcc97-44 glib-2.15.4]# make &amp;&amp; make install

[root@umcc97-44 rrdbuild]# bunzip2 -c pango-1.21.1.tar.bz2 | tar xf -
[root@umcc97-44 rrdbuild]# ll
[root@umcc97-44 rrdbuild]# cd pango-1.21.1
[root@umcc97-44 pango-1.21.1]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC" --without-x
[root@umcc97-44 pango-1.21.1]# export PATH=$INSTALL_DIR/bin:$PATH
[root@umcc97-44 pango-1.21.1]# make &amp;&amp; make install

[root@umcc97-44 rrdbuild]# cd rrdtool-1.4.8/
[root@umcc97-44 rrdtool-1.4.8]#  ./configure --prefix=$INSTALL_DIR --disable-tcl --disable-python
[root@umcc97-44 rrdtool-1.4.8]# make clean
[root@umcc97-44 rrdtool-1.4.8]# make 
[root@umcc97-44 rrdtool-1.4.8]# make install

## 安装完后，搞个例子玩玩   
[root@umcc97-44 rrdtool-1.4.8]# cd /opt/rrdtool-1.4.8/share/rrdtool/examples/
[root@umcc97-44 examples]# ll
[root@umcc97-44 examples]# ./4charts.pl 
This script has created 4charts.png in the current directory
This demonstrates the use of the TIME and % RPN operators
# 运行完后，会在当前目录生成不同尺寸的png的图片

[hadoop@umcc97-44 ~]$ /opt/rrdtool-1.4.8/bin/rrdtool -v
RRDtool 1.4.8  Copyright 1997-2013 by Tobias Oetiker &lt;tobi@oetiker.ch&gt;
               Compiled Jul 17 2014 17:37:58

Usage: rrdtool [options] command command_options
Valid commands: create, update, updatev, graph, graphv,  dump, restore,
        last, lastupdate, first, info, fetch, tune,
        resize, xport, flushcached

RRDtool is distributed under the Terms of the GNU General
Public License Version 2. (www.gnu.org/copyleft/gpl.html)

For more information read the RRD manpages
</code></pre>

<p>到这里rrd安装好，遇到zlib的CFLAGS变量设置的问题，以及终端断了必须重新设置<strong>环境变量</strong>两个大点的问题！其他如果按照官网的顺序安装基本顺顺利利了。</p>

<p>同时认识到了pkg，其实类似于java的jar嘛，依赖包不一定非要安装在系统的默认位置，自己管理也是一种简单易行的方式。接下来安装gmetad/gmond也使用这样方式，为后面部署gmond带来便利：所有依赖的包都放在一个目录下嘛！
接下来ganglia程序。</p>

<h2>gmetad安装</h2>

<p>需要用到的软件包：</p>

<pre><code>./gangliabuild/ganglia-web-3.5.12
./gangliabuild/apr-1.5.1
./gangliabuild/apr-util-1.5.3
./gangliabuild/confuse-2.7
./gangliabuild/expat-2.0.1
./gangliabuild/ganglia-3.6.0
</code></pre>

<p>整个安装过程，除了make的时刻rrd的库找不到的问题（通过LD_LIBRARY_PATH解决），其他都安装的很顺。</p>

<pre><code># 把下载来的tar全部解压
[root@umcc97-44 gangliabuild]# find . -name "*.tar.gz" -exec tar zxvf {} \;

[root@umcc97-44 gangliabuild]# cd expat-2.0.1
[root@umcc97-44 expat-2.0.1]# INSTALL_DIR=/opt/ganglia
[root@umcc97-44 expat-2.0.1]# ./configure --prefix=$INSTALL_DIR 
[root@umcc97-44 expat-2.0.1]# make &amp;&amp; make install

[root@umcc97-44 expat-2.0.1]# cd ../apr-1.5.1
[root@umcc97-44 apr-1.5.1]# ./configure --prefix=$INSTALL_DIR 
[root@umcc97-44 apr-1.5.1]# make &amp;&amp; make install

[root@umcc97-44 apr-1.5.1]# cd ../apr-util-1.5.3
[root@umcc97-44 apr-util-1.5.3]# ./configure --with-apr=/opt/ganglia --with-expat=/opt/ganglia --prefix=$INSTALL_DIR 
[root@umcc97-44 apr-util-1.5.3]# make &amp;&amp; make install

[root@umcc97-44 apr-util-1.5.3]# cd ../confuse-2.7
[root@umcc97-44 confuse-2.7]# ./configure CFLAGS=-fPIC --disable-nls --prefix=$INSTALL_DIR 
[root@umcc97-44 confuse-2.7]# make &amp;&amp; make install

[root@umcc97-44 confuse-2.7]# cd ../ganglia-3.6.0
[root@umcc97-44 ganglia-3.6.0]# export LDFLAGS="-Wl,--rpath -Wl,${INSTALL_DIR}/lib" 
[root@umcc97-44 ganglia-3.6.0]# export PKG_CONFIG_PATH=${INSTALL_DIR}/lib/pkgconfig
# 注意sysconfdir，运行程序配置所在的目录
[root@umcc97-44 ganglia-3.6.0]# ./configure --prefix=$INSTALL_DIR --with-librrd=/opt/rrdtool-1.4.8 --with-libexpat=/opt/ganglia --with-libconfuse=/opt/ganglia --with-libpcre=no  --with-gmetad --enable-gexec --enable-status -sysconfdir=/etc/ganglia
...
Welcome to..
     ______                  ___
    / ____/___ _____  ____ _/ (_)___ _
   / / __/ __ `/ __ \/ __ `/ / / __ `/
  / /_/ / /_/ / / / / /_/ / / / /_/ /
  \____/\__,_/_/ /_/\__, /_/_/\__,_/
                   /____/

Copyright (c) 2005 University of California, Berkeley

Version: 3.6.0
Library: Release 3.6.0 0:0:0

Type "make" to compile.

[root@umcc97-44 ganglia-3.6.0]# 
# 设置rrd的LIB路径
[root@umcc97-44 ganglia-3.6.0]# export LD_LIBRARY_PATH=/opt/rrdtool-1.4.8/lib
[root@umcc97-44 ganglia-3.6.0]# make
[root@umcc97-44 ganglia-3.6.0]# make install
</code></pre>

<p>接下来是配置gmetad</p>

<pre><code>[root@umcc97-44 ganglia-3.6.0]#  cd gmetad
[root@umcc97-44 gmetad]# cp gmetad.init /etc/init.d/gmetad
[root@umcc97-44 gmetad]# chkconfig gmetad on

[root@umcc97-44 gmetad]# chkconfig --list gmetad
gmetad          0:off   1:off   2:on    3:on    4:on    5:on    6:off

[root@umcc97-44 gmetad]# mkdir -p /var/lib/ganglia/rrds
[root@umcc97-44 gmetad]# chown nobody:nobody /var/lib/ganglia/rrds
[root@umcc97-44 gmetad]# 
# 没有启动起来，程序的路径不对
[root@umcc97-44 gmetad]# service gmetad start
Starting GANGLIA gmetad: 
[root@umcc97-44 gmetad]# 
[root@umcc97-44 gmetad]# ln -s /opt/ganglia/sbin/gmetad /usr/sbin/gmetad
[root@umcc97-44 gmetad]# service gmetad start
Starting GANGLIA gmetad: [  OK  ]

# 配置
[root@umcc97-44 gmetad]# cp gmetad.conf /etc/ganglia/gmetad.conf
[root@umcc97-44 gmetad]# vi /etc/ganglia/gmetad.conf 
 datasource "hadoop" localhost
 rrd_rootdir "/var/lib/ganglia/rrds"

[root@umcc97-44 gmetad]# service gmetad restart
Shutting down GANGLIA gmetad: [  OK  ]
Starting GANGLIA gmetad: [  OK  ]

# 测试下
[root@umcc97-44 gmetad]# telnet localhost 8651
</code></pre>

<h2>gmond安装</h2>

<pre><code>[root@umcc97-44 gmetad]# pwd
/home/ganglia/gangliabuild/ganglia-3.6.0/gmetad
[root@umcc97-44 gmetad]# cd ..
[root@umcc97-44 ganglia-3.6.0]# ./configure --prefix=$INSTALL_DIR  --with-libpcre=no
...
Welcome to..
     ______                  ___
    / ____/___ _____  ____ _/ (_)___ _
   / / __/ __ `/ __ \/ __ `/ / / __ `/
  / /_/ / /_/ / / / / /_/ / / / /_/ /
  \____/\__,_/_/ /_/\__, /_/_/\__,_/
                   /____/

Copyright (c) 2005 University of California, Berkeley

Version: 3.6.0
Library: Release 3.6.0 0:0:0

Type "make" to compile.

# 尽管检查通过了，但是make会报错
# 需要指定lib包位置
[root@umcc97-44 ganglia-3.6.0]# ./configure --prefix=$INSTALL_DIR  --with-libpcre=no  --with-libexpat=/opt/ganglia --with-libconfuse=/opt/ganglia -sysconfdir=/etc/ganglia
[root@umcc97-44 ganglia-3.6.0]# make &amp;&amp; make install

[root@umcc97-44 ganglia-3.6.0]# cd gmond/
[root@umcc97-44 gmond]# ./gmond -t &gt; /etc/ganglia/gmond.conf

# 和gmetad一样，需要把路径把程序做个软连接
[root@umcc97-44 gmond]# cat gmond.init
    #!/bin/sh
    #
    # chkconfig: 2345 70 40
    # description: gmond startup script
    #
    GMOND=/usr/sbin/gmond

...
[root@umcc97-44 gmond]# ln -s /opt/ganglia/sbin/gmond /usr/sbin/gmond

[root@umcc97-44 gmond]# cp gmond.init /etc/init.d/gmond
[root@umcc97-44 gmond]# chkconfig --add gmond
[root@umcc97-44 gmond]# chkconfig --list gmond
gmond           0:off   1:off   2:on    3:on    4:on    5:on    6:off

[root@umcc97-44 ganglia-3.6.0]# vi /etc/ganglia/gmond.conf 
 cluster-name

[root@umcc97-44 ganglia-3.6.0]# service gmond start
Starting GANGLIA gmond: [  OK  ]

# 测试下
[root@umcc97-44 ganglia-3.6.0]# telnet localhost 8649
</code></pre>

<p>查看运行情况：</p>

<pre><code>[root@umcc97-44 ganglia-3.6.0]# ldconfig -v
[root@umcc97-44 ganglia-3.6.0]# /opt/ganglia/bin/gstat -a
</code></pre>

<h2>apache和php环境安装</h2>

<pre><code>[root@umcc97-44 webbuild]# tar zxvf httpd-2.4.9.tar.gz 
[root@umcc97-44 webbuild]# cd httpd-2.4.9
[root@umcc97-44 httpd-2.4.9]# ./configure -with-enable-so -sysconfdir=/etc/httpd
...
checking for APR... no
configure: error: APR not found.  Please read the documentation.

# 前面安装ganglia时也安装过APR但是安装的目录指定的，混用不是很好。查看官方安装2.4的安装文档，可以直接把apr放到srclib下，编译时会同时编译这些依赖
[root@umcc97-44 httpd-2.4.9]# cd srclib/
[root@umcc97-44 srclib]# cp -r /home/ganglia/gangliabuild/apr-1.5.1 ./
[root@umcc97-44 srclib]# cp -r /home/ganglia/gangliabuild/apr-util-1.5.3 ./
[root@umcc97-44 srclib]# mv apr-1.5.1 apr
[root@umcc97-44 srclib]# mv apr-util-1.5.3 apr-util
[root@umcc97-44 srclib]# ll
[root@umcc97-44 srclib]# cd ..
[root@umcc97-44 httpd-2.4.9]#  cd ../
[root@umcc97-44 webbuild]# tar zxvf pcre-8.35.tar.gz 
# 正则表达式的包，这里安装默认位置
[root@umcc97-44 webbuild]# cd pcre-8.35
[root@umcc97-44 pcre-8.35]# ./configure 
[root@umcc97-44 pcre-8.35]# make &amp;&amp; make install

[root@umcc97-44 pcre-8.35]# cd ../httpd-2.4.9
[root@umcc97-44 httpd-2.4.9]# ./configure --with-included-apr -with-enable-so -sysconfdir=/etc/httpd
[root@umcc97-44 httpd-2.4.9]# make &amp;&amp; make install

[root@umcc97-44 httpd-2.4.9]# cd /usr/local/apache2/
[root@umcc97-44 apache2]# cd /etc/httpd

[root@umcc97-44 httpd]# cd /home/ganglia/webbuild/
[root@umcc97-44 webbuild]# tar zxvf php-5.5.14\ \(2\).tar.gz 
[root@umcc97-44 webbuild]# cd php-5.5.14
# 用了安装rrd时的libxml
[root@umcc97-44 php-5.5.14]# ./configure -with-apxs2=/usr/local/apache2/bin/apxs --with-libxml-dir=/opt/rrdtool-1.4.8/ -sysconfdir=/etc -with-config-file-path=/etc -with-config-file-scan-dir=/usr/etc/php.d -with-zlib
[root@umcc97-44 php-5.5.14]# make &amp;&amp; make install

[root@umcc97-44 php-5.5.14]#  
[root@umcc97-44 php-5.5.14]#  vi /etc/httpd/httpd.conf

    LoadModule php5_module        modules/libphp5.so #这个安装php后自动加上了

    DocumentRoot "/var/www/html"
    &lt;Directory "/var/www/html"&gt;

    AddType application/x-httpd-php .php

[root@umcc97-44 php-5.5.14]# /usr/local/apache2/bin/apachectl start
AH00526: Syntax error on line 215 of /etc/httpd/httpd.conf:
DocumentRoot must be a directory

[root@umcc97-44 php-5.5.14]# mkdir -p /var/www/html
[root@umcc97-44 php-5.5.14]# /usr/local/apache2/bin/apachectl start
AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.18.97.44. Set the 'ServerName' directive globally to suppress this message

[root@umcc97-44 php-5.5.14]#  vi /etc/httpd/httpd.conf
    ServerName
[root@umcc97-44 php-5.5.14]#/usr/local/apache2/bin/apachectl start
httpd (pid 31416) already running

[root@umcc97-44 php-5.5.14]# cp /usr/local/apache2/bin/apachectl /etc/init.d/httpd
[root@umcc97-44 php-5.5.14]# chkconfig --add httpd
service httpd does not support chkconfig

[root@umcc97-44 ~]# vi /etc/init.d/httpd 
 #chkconfig: 2345 10 90
 #description: Activates/Deactivates Apache Web Server

[root@umcc97-44 ~]# service httpd start

[root@umcc97-44 ~]# cd /var/www/html/
[root@umcc97-44 ~]# vi index.php
# http://umcc97-44 浏览器查看下结果

# /usr/local/apache2/bin/apachectl -k stop
[root@umcc97-44 ganglia-web]# service httpd -k stop 
# 等apache结束
[root@umcc97-44 ganglia-web]# tail -f /usr/local/apache2/logs/error_log 
</code></pre>

<p>部署ganglia-web：</p>

<pre><code>[root@umcc97-44 ~]# cd /home/ganglia/gangliabuild/ganglia-web-3.5.12
[root@umcc97-44 ganglia-web-3.5.12]# ls
[root@umcc97-44 ganglia-web-3.5.12]# make install
rsync --exclude "rpmbuild" --exclude "*.gz" --exclude "Makefile" --exclude "*debian*" --exclude "ganglia-web-3.5.12" --exclude ".git*" --exclude "*.in" --exclude "*~" --exclude "#*#" --exclude "ganglia-web.spec" --exclude "apache.conf" -a . ganglia-web-3.5.12
mkdir -p //var/lib/ganglia-web/dwoo/compiled &amp;&amp; \
    mkdir -p //var/lib/ganglia-web/dwoo/cache &amp;&amp; \
    mkdir -p //var/lib/ganglia-web &amp;&amp; \
    rsync -a ganglia-web-3.5.12/conf //var/lib/ganglia-web &amp;&amp; \
    mkdir -p //usr/share/ganglia-webfrontend &amp;&amp; \
    rsync --exclude "conf" -a ganglia-web-3.5.12/* //usr/share/ganglia-webfrontend &amp;&amp; \
    chown -R root:root //var/lib/ganglia-web

[root@umcc97-44 ganglia-web-3.5.12]# mv /usr/share/ganglia-webfrontend /var/www/html/ganglia
[root@umcc97-44 ganglia-web-3.5.12]# cd /var/www/html/ganglia/  

# 修改配置，在安装完gmetad后有新建/var/lib/ganglia/rrds其实和conf中的配置是一致的
[root@umcc97-44 ganglia]# cp conf_default.php conf.php  

[root@umcc97-44 ganglia]# cd /var/lib/ganglia-web/
[root@umcc97-44 ganglia-web]# cd dwoo/
[root@umcc97-44 dwoo]# ll
total 8
drwxr-xr-x 2 root root 4096 Jul 17 21:34 cache
drwxr-xr-x 2 root root 4096 Jul 17 21:34 compiled
[root@umcc97-44 dwoo]# chmod 777 *  
# http://umcc97-44/ganglia
</code></pre>

<p>部署gmond到其他集群节点</p>

<pre><code>
[root@umcc97-44 opt]# cat /etc/init.d/gmond 
    #!/bin/sh
    #
    # chkconfig: 2345 70 40
    # description: gmond startup script
    #
    GMOND=/usr/sbin/gmond   

[root@umcc97-44 opt]# vi /etc/ganglia/gmetad.conf   
 data_source
 # 重启gmetad
[root@umcc97-44 opt]# ssh-copy-id -i ~/.ssh/id_rsa.pub umcc97-144
[root@umcc97-44 opt]# scp /etc/init.d/gmond umcc97-144:/etc/init.d/
[root@umcc97-44 opt]# ssh umcc97-144 'mkdir /etc/ganglia' 
[root@umcc97-44 opt]# scp /etc/ganglia/gmond.conf  umcc97-144:/etc/ganglia/
[root@umcc97-44 opt]# rsync -vaz ganglia umcc97-144:/opt/
[root@umcc97-44 opt]# ssh umcc97-144
Last login: Tue Jun 10 12:08:47 2014

[root@umcc97-144 ~]# ln -s /opt/ganglia/sbin/gmond /usr/sbin/gmond
[root@umcc97-144 ~]# chkconfig --add gmond
[root@umcc97-144 ~]# service gmond start
Starting GANGLIA gmond: [  OK  ]

[root@umcc97-144 ~]# 
</code></pre>

<h2>Hadoop/Hbase Metrics配置</h2>

<pre><code>[hadoop@umcc97-44 ~]$ cat hadoop-2.2.0/etc/hadoop/hadoop-metrics*
#
#   Licensed to the Apache Software Foundation (ASF) under one or more
#   contributor license agreements.  See the NOTICE file distributed with
#   this work for additional information regarding copyright ownership.
#   The ASF licenses this file to You under the Apache License, Version 2.0
#   (the "License"); you may not use this file except in compliance with
#   the License.  You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#

# syntax: [prefix].[source|sink].[instance].[options]
# See javadoc of package-info.java for org.apache.hadoop.metrics2 for details

# @changed
#*.sink.file.class=org.apache.hadoop.metrics2.sink.FileSink
# default sampling period, in seconds
#*.period=10

# The namenode-metrics.out will contain metrics from all context
#namenode.sink.file.filename=namenode-metrics.out
# Specifying a special sampling period for namenode:
#namenode.sink.*.period=8

#datanode.sink.file.filename=datanode-metrics.out

# the following example split metrics of different
# context to different sinks (in this case files)
#jobtracker.sink.file_jvm.context=jvm
#jobtracker.sink.file_jvm.filename=jobtracker-jvm-metrics.out
#jobtracker.sink.file_mapred.context=mapred
#jobtracker.sink.file_mapred.filename=jobtracker-mapred-metrics.out

#tasktracker.sink.file.filename=tasktracker-metrics.out

#maptask.sink.file.filename=maptask-metrics.out

#reducetask.sink.file.filename=reducetask-metrics.out



*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31
*.sink.ganglia.period=10

*.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both
*.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40

namenode.sink.ganglia.servers=umcc97-44:8649
resourcemanager.sink.ganglia.servers=umcc97-44:8649

datanode.sink.ganglia.servers=umcc97-44:8649
nodemanager.sink.ganglia.servers=umcc97-44:8649

maptask.sink.ganglia.servers=umcc97-44:8649
reducetask.sink.ganglia.servers=umcc97-44:8649



# Configuration of the "dfs" context for null
dfs.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the "dfs" context for file
#dfs.class=org.apache.hadoop.metrics.file.FileContext
#dfs.period=10
#dfs.fileName=/tmp/dfsmetrics.log

# Configuration of the "dfs" context for ganglia
# Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
# dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# dfs.period=10
# dfs.servers=localhost:8649


# Configuration of the "mapred" context for null
mapred.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the "mapred" context for file
#mapred.class=org.apache.hadoop.metrics.file.FileContext
#mapred.period=10
#mapred.fileName=/tmp/mrmetrics.log

# Configuration of the "mapred" context for ganglia
# Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
# mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# mapred.period=10
# mapred.servers=localhost:8649


# Configuration of the "jvm" context for null
#jvm.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the "jvm" context for file
#jvm.class=org.apache.hadoop.metrics.file.FileContext
#jvm.period=10
#jvm.fileName=/tmp/jvmmetrics.log

# Configuration of the "jvm" context for ganglia
# jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# jvm.period=10
# jvm.servers=localhost:8649

# Configuration of the "rpc" context for null
rpc.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the "rpc" context for file
#rpc.class=org.apache.hadoop.metrics.file.FileContext
#rpc.period=10
#rpc.fileName=/tmp/rpcmetrics.log

# Configuration of the "rpc" context for ganglia
# rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# rpc.period=10
# rpc.servers=localhost:8649


# Configuration of the "ugi" context for null
ugi.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the "ugi" context for file
#ugi.class=org.apache.hadoop.metrics.file.FileContext
#ugi.period=10
#ugi.fileName=/tmp/ugimetrics.log

# Configuration of the "ugi" context for ganglia
# ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# ugi.period=10
# ugi.servers=localhost:8649

[hadoop@umcc97-44 ~]$ cat hbase-0.98.3-hadoop2/conf/hadoop-metrics2-hbase.properties 
# syntax: [prefix].[source|sink].[instance].[options]
# See javadoc of package-info.java for org.apache.hadoop.metrics2 for details

#*.sink.file*.class=org.apache.hadoop.metrics2.sink.FileSink
# default sampling period
#*.period=10

# Below are some examples of sinks that could be used
# to monitor different hbase daemons.

# hbase.sink.file-all.class=org.apache.hadoop.metrics2.sink.FileSink
# hbase.sink.file-all.filename=all.metrics

# hbase.sink.file0.class=org.apache.hadoop.metrics2.sink.FileSink
# hbase.sink.file0.context=hmaster
# hbase.sink.file0.filename=master.metrics

# hbase.sink.file1.class=org.apache.hadoop.metrics2.sink.FileSink
# hbase.sink.file1.context=thrift-one
# hbase.sink.file1.filename=thrift-one.metrics

# hbase.sink.file2.class=org.apache.hadoop.metrics2.sink.FileSink
# hbase.sink.file2.context=thrift-two
# hbase.sink.file2.filename=thrift-one.metrics

# hbase.sink.file3.class=org.apache.hadoop.metrics2.sink.FileSink
# hbase.sink.file3.context=rest
# hbase.sink.file3.filename=rest.metrics


*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31
*.sink.ganglia.period=10

hbase.sink.ganglia.period=10
hbase.sink.ganglia.servers=umcc97-44:8649
</code></pre>

<p>然后properties配置同步到集群的从节点（datanode/regionserver），重启集群。等一会儿就能在ganglia-web界面看到多了很多很多的指标量。</p>

<h2>参考</h2>

<h3>ganglia</h3>

<ul>
<li><a href="http://oss.oetiker.ch/rrdtool/doc/rrdbuild.en.html#IBUILDING_DEPENDENCIES">RRDTool安装</a></li>
<li><a href="http://www.cnblogs.com/qq78292959/archive/2012/05/30/2526761.html">CFLAGS=&ldquo;-O3 -fPIC"为64位编译参数</a></li>
<li><a href="http://www.codesky.net/article/201107/174186.html">pkgconfig作用处理包依赖</a></li>
<li><a href="http://blog.chinaunix.net/uid-23916356-id-3290237.html">gmetad和gmond安装以及配置</a></li>
<li><a href="http://wenku.baidu.com/link?url=RH4EhSP3U_dp4I7goEVA_DFkb0DrgZ3uWw_mSt2hhaRb6mQJLtWxaa75RrwETwtY5e8BvOCI_p9RNrmXn_qbEexTE-PGlgtf6f5T3cGglKq">gmond节点拷贝安装</a></li>
<li><a href="http://blog.chinaunix.net/uid-11121450-id-3147002.html">http://blog.chinaunix.net/uid-11121450-id-3147002.html</a></li>
<li><a href="http://blog.chinaunix.net/uid-23916356-id-3290237.html">http://blog.chinaunix.net/uid-23916356-id-3290237.html</a></li>
<li><a href="http://wenku.baidu.com/link?url=qY7vCTyodgSCsoIg6c2UiHXWv0nEGkS9nd0DbQERxFGEaTvgvi7FMQTKv5Sn1L9H8CX5_gDgAbJJ5jaQh3KhZED7PoB2Bgr2I6mS-vDc1LS">虚拟机操作从零开始弄, 搭了个本地源, 配置</a></li>
<li><a href="http://www.linuxidc.com/Linux/2014-01/95804p2.htm">Hadoop/Hbase metrics2配置</a></li>
<li><a href="https://github.com/cbuchner1/CudaMiner/issues/23">https://github.com/cbuchner1/CudaMiner/issues/23</a></li>
<li><a href="http://bbs.csdn.net/topics/390546319">http://bbs.csdn.net/topics/390546319</a> LIBRARY_PATH是编译时使用的，LD_LIBRARY_PATH是运行时使用的。</li>
</ul>


<h3>apache web</h3>

<ul>
<li><a href="http://blog.sina.com.cn/s/blog_70121e200100lq0h.html">apache程序安装</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_5d15305b0101ceft.html">apache服务安装配置</a></li>
<li><a href="http://www.cnblogs.com/yuboyue/archive/2011/07/18/2109875.html">apache关闭服务</a></li>
<li><a href="http://www.soadmin.com/zonghe/operating-system/1008085.htm">目录权限处理</a></li>
<li><a href="http://blog.163.com/figo_2007@126/blog/static/2318076520111149413935/">http://blog.163.com/figo_2007@126/blog/static/2318076520111149413935/</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_70121e200100lq0h.html">http://blog.sina.com.cn/s/blog_70121e200100lq0h.html</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_815611fb0101cxnl.html">http://blog.sina.com.cn/s/blog_815611fb0101cxnl.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tez编译及使用]]></title>
    <link href="http://winse.github.io/blog/2014/06/18/hadoop-tez-firststep/"/>
    <updated>2014-06-18T04:22:58+08:00</updated>
    <id>http://winse.github.io/blog/2014/06/18/hadoop-tez-firststep</id>
    <content type="html"><![CDATA[<h2>初步了解</h2>

<p>hadoop2自带的mapreduce任务中间只能传递一次，也即一个任务只能聚合一次。而tez项目是对原有yarn架构的一个拓展，使用DAG（无环有向图）实现MRR的任务框架。</p>

<p><img src="http://farm6.staticflickr.com/5571/14256993179_4990fc86d5_o.png" alt="" /></p>

<p>上图中，左边的MR任务完成一个步骤后，需要进行<strong>数据存储</strong>后再执行另一个任务来进行第二个“reduce”； 而tez则可以在reduce后继续执行reduce，减少了中间过程的IO以及mapreduce的启动时间。</p>

<h2>环境整合</h2>

<ul>
<li><a href="http://tez.incubator.apache.org/install.html">Install/Deploy</a></li>
<li>hadoop-2.2.0（umcc97-44：hdfs， umcc97-79：yarn）</li>
<li>windows下使用Cygwin编译</li>
</ul>


<h3>下载编译tez</h3>

<p>首先下载<a href="http://apache.fayea.com/apache-mirror/incubator/tez/tez-0.4.0-incubating/">tez-0.4.0-incubating.tar.gz</a>，同时还需要<a href="http://code.google.com/p/protobuf">protoc</a>的程序支持（编译<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html">hadoop源码</a>也需要这个的）。
解压后，使用mvn编译。</p>

<pre><code>Administrator@winseliu /cygdrive/e/local/libs/big
$ tar zxvf tez-0.4.0-incubating.tar.gz

Administrator@winseliu /cygdrive/e/local/libs/big
$ cd tez-0.4.0-incubating/

Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating
$ mvn install -DskipTests -Dmaven.javadoc.skip
...
[INFO] Reactor Summary:
[INFO]
[INFO] tez ............................................... SUCCESS [1.518s]
[INFO] tez-api ........................................... SUCCESS [8.890s]
[INFO] tez-common ........................................ SUCCESS [0.725s]
[INFO] tez-runtime-internals ............................. SUCCESS [2.529s]
[INFO] tez-runtime-library ............................... SUCCESS [5.100s]
[INFO] tez-mapreduce ..................................... SUCCESS [3.666s]
[INFO] tez-mapreduce-examples ............................ SUCCESS [2.692s]
[INFO] tez-dag ........................................... SUCCESS [13.943s]
[INFO] tez-tests ......................................... SUCCESS [1.691s]
[INFO] tez-dist .......................................... SUCCESS [14.370s]
[INFO] Tez ............................................... SUCCESS [0.245s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 55.791s
[INFO] Finished at: Tue Jun 17 17:33:45 CST 2014
[INFO] Final Memory: 35M/151M
[INFO] ------------------------------------------------------------------------
</code></pre>

<h3>上传tez程序的jars到HDFS</h3>

<p>为了简单我直接把tez放到开发环境的集群上面去测试了。放到本地环境应该也类似。</p>

<pre><code>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating
$ cd tez-dist/

Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist
$ cd target/

Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist/target
$ export HADOOP_USER_NAME=hadoop

Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist/target
$  hadoop dfs -put tez-0.4.0-incubating/tez-0.4.0-incubating/ hdfs://umcc97-44:9000/apps/
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.
</code></pre>

<h3>配置集群环境</h3>

<p>首先看下原来集群的classpath路径，由于已经包括了etc/hadoop目录，所以这里我直接把<code>tez-site.xml</code>放到该目录下。把所有的tez-lib上传到share目录下，并添加到HADOOP_CLASSPATH。</p>

<pre><code>  [hadoop@umcc97-79 hadoop]$ hadoop classpath
  /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.2.0/contrib/capacity-scheduler/*.jar

  # 用于map/reduce
  [hadoop@umcc97-79 hadoop]$ cat tez-site.xml 
  &lt;?xml version="1.0"?&gt;
  &lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;

  &lt;!-- Put site-specific property overrides in this file. --&gt;

  &lt;configuration&gt;
    &lt;property&gt;
      &lt;name&gt;tez.lib.uris&lt;/name&gt;
      &lt;value&gt;${fs.default.name}/apps/tez-0.4.0-incubating,${fs.default.name}/apps/tez-0.4.0-incubating/lib/&lt;/value&gt;
    &lt;/property&gt;
  &lt;/configuration&gt;
  [hadoop@umcc97-79 hadoop]$ 

  [hadoop@umcc97-79 hadoop]$ cd ~/hadoop-2.2.0/share/hadoop/tez/
  [hadoop@umcc97-79 tez]$ ll
  total 9616
  -rw-r--r-- 1 hadoop hadoop  303139 Jun 17 17:33 avro-1.7.4.jar
  -rw-r--r-- 1 hadoop hadoop   41123 Jun 17 17:33 commons-cli-1.2.jar
  -rw-r--r-- 1 hadoop hadoop  610259 Jun 17 17:33 commons-collections4-4.0.jar
  -rw-r--r-- 1 hadoop hadoop 1648200 Jun 17 17:33 guava-11.0.2.jar
  -rw-r--r-- 1 hadoop hadoop  710492 Jun 17 17:33 guice-3.0.jar
  -rw-r--r-- 1 hadoop hadoop  656365 Jun 17 17:33 hadoop-mapreduce-client-common-2.2.0.jar
  -rw-r--r-- 1 hadoop hadoop 1455001 Jun 17 17:33 hadoop-mapreduce-client-core-2.2.0.jar
  -rw-r--r-- 1 hadoop hadoop   21537 Jun 17 17:33 hadoop-mapreduce-client-shuffle-2.2.0.jar
  -rw-r--r-- 1 hadoop hadoop   81743 Jun 17 17:33 jettison-1.3.4.jar
  -rw-r--r-- 1 hadoop hadoop  533455 Jun 17 17:33 protobuf-java-2.5.0.jar
  -rw-r--r-- 1 hadoop hadoop  995968 Jun 17 17:33 snappy-java-1.0.4.1.jar
  -rw-r--r-- 1 hadoop hadoop  749917 Jun 17 17:33 tez-api-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop   34049 Jun 17 17:33 tez-common-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop  970987 Jun 17 17:33 tez-dag-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop  246409 Jun 17 17:33 tez-mapreduce-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop  199934 Jun 17 17:33 tez-mapreduce-examples-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop  114692 Jun 17 17:33 tez-runtime-internals-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop  352177 Jun 17 17:33 tez-runtime-library-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop    6845 Jun 17 17:33 tez-tests-0.4.0-incubating.jar
  [hadoop@umcc97-79 tez]$ 

  # 用于client任务提交
  [hadoop@umcc97-79 hadoop]$ grep HADOOP_CLASSPATH hadoop-env.sh
  export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tez/*:${HADOOP_HOME}/share/hadoop/tez/lib/*:$HADOOP_CLASSPATH

  [hadoop@umcc97-79 hadoop]$ sed -n 19,23p mapred-site.xml
  &lt;configuration&gt;
    &lt;property&gt;
      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
      &lt;value&gt;yarn-tez&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<h3>同步，重启yarn</h3>

<pre><code>for h in `cat hadoop-2.2.0/etc/hadoop/slaves ` ; do rsync -vaz --exclude=logs --exclude=pid --exclude=tmp  hadoop-2.2.0 $h:~/ ; done

rsync -vaz --exclude=logs --exclude=pid --exclude=tmp  hadoop-2.2.0 umcc97-44:~/
</code></pre>

<h3>测试效果</h3>

<pre><code>  [hadoop@umcc97-79 ~]$ hadoop classpath
  /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.2.0/share/hadoop/tez/*:/home/hadoop/hadoop-2.2.0/share/hadoop/tez/lib/*:/home/hadoop/hadoop-2.2.0/contrib/capacity-scheduler/*.jar
  [hadoop@umcc97-79 ~]$ cd hadoop-2.2.0/share/hadoop/mapreduce/
  [hadoop@umcc97-79 mapreduce]$ hadoop jar hadoop-mapreduce-client-jobclient-2.2.0-tests.jar sleep -mt 1 -rt 1 -m 1 -r 1

    hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount  /hello/in /hello/out
    hadoop fs -put hadoop-2.2.0/logs/yarn-hadoop-resourcemanager-umcc97-79.* /hello/in
    hadoop fs -rmr /hello/out
    hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount  /hello/in /hello/out
</code></pre>

<h3>回滚，使用时临时修改环境变量即可</h3>

<p>使用了tez后，使用hive-0.12.0不能运行了。由于其他同事需要用hive，得把配置全部修改回去【<a href="/blog/2014/06/21/upgrade-hive/">hive-0.13中使用tez</a>】。</p>

<p>其实在<strong>提交任务</strong>时指定配置参数即可。</p>

<pre><code>[hadoop@umcc97-79 ~]$ export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tez/*:${HADOOP_HOME}/share/hadoop/tez/lib/*:$HADOOP_CLASSPATH
[hadoop@umcc97-79 ~]$ hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount -Dmapreduce.framework.name=yarn-tez  /hello/in /hello/out
</code></pre>

<p>org.apache.tez.mapreduce.examples.OrderedWordCount不仅计算出了结果，同时按个数大小进行了排序。</p>

<p>问题： tez的任务的history还不知道怎么弄的，启动historyserver没作用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[远程调试hadoop2以及错误处理方法]]></title>
    <link href="http://winse.github.io/blog/2014/04/22/remote-debug-hadoop2/"/>
    <updated>2014-04-22T06:47:48+08:00</updated>
    <id>http://winse.github.io/blog/2014/04/22/remote-debug-hadoop2</id>
    <content type="html"><![CDATA[<p>了解程序运行过程，除了一行行代码的扫射源代码。更快捷的方式是运行调试源码，通过F6/F7来一步步的带领我们熟悉程序。针对特定细节具体数据，打个断点调试则是水到渠成的方式。</p>

<h2>Java远程调试</h2>

<pre><code> * JDK 1.3 or earlier -Xnoagent -Djava.compiler=NONE -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=6006
 * JDK 1.4(linux ok) -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=6006
 * newer JDK(win7 &amp; jdk7) -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=6006
</code></pre>

<h2>同一操作系统任务提交</h2>

<p>windows提交到windows，linux提交到linux，可以直接通过命令行添加参数调试wordcount任务：</p>

<pre><code>E:\local\dotfile&gt;hdfs dfs -rmr /out # native-lib放在非path路径下，cmd脚本中有对其进行处理

E:\local\dotfile&gt;hadoop org.apache.hadoop.examples.WordCount  "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090 -Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native -Dmapreduce.reduce.java.opts=-Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native"  /in /out
</code></pre>

<p><strong>suspend设置为y，会等待客户端连接再运行</strong>。在eclipse中在WordCount$TokenizerMapper#map打个断点，然后再使用<code>Remote Java Application</code>就可以调试程序了。</p>

<h2>Hadoop集群环境下调试任务</h2>

<p>hadoop有很多的程序，同样有对应的环境变量选项来进行设置！</p>

<ul>
<li>主程序-调试Job提交

<ul>
<li><code>set HADOOP_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"</code></li>
<li>可以在配置文件中进行设置。需要注意可能会覆盖已经设置的该参数的值。</li>
</ul>
</li>
<li>Nodemanager调试

<ul>
<li><code>set HADOOP_NODEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8092"</code></li>
<li>(linux下需要定义在文件中)<code>YARN_NODEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8092"</code></li>
</ul>
</li>
<li>ResourceManager调试

<ul>
<li>HADOOP_RESOURCEMANAGER_OPTS</li>
<li><code>export YARN_RESOURCEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8091"</code></li>
</ul>
</li>
</ul>


<p>Linux上的设置略有不同，通过SSH再调用的进程(如NodeManager)需要把其OPTS写到命令行脚本文件中！！
linux需要远程调试NodeManager的话，需要写到etc/hadoop/yarn-env.sh文件中！不然，nodemanger不生效（通过ssh去执行的）！</p>

<h3>其他调试技巧</h3>

<p>调试测试集群环境，比本地windows开发环境复杂点。毕竟本地windows的就一个主一个从。而把<strong>任务放到分布式集群</strong>上时，例如调试分布式缓存的！
那么就需要一些小技巧来获取任务运行所在的机器！下面的步骤中有具体操作命令。</p>

<h3>任务配置及运行</h3>

<p>eclipse下windows提交job到linux的补丁，查阅<a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655">[MAPREDUCE-5655]</a></p>

<pre><code># 配置
    &lt;property&gt;
        &lt;name&gt;mapred.remote.os&lt;/name&gt;
        &lt;value&gt;Linux&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.job.jar&lt;/name&gt;
        &lt;value&gt;dta-analyser-all.jar&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;
        &lt;value&gt;-Xmx1024m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;mapred.task.timeout&lt;/name&gt;
        &lt;value&gt;1800000&lt;/value&gt;
    &lt;/property&gt;

# 代码，map/reduce数都设置为1   
job.setNumReduceTasks(1);
job.getConfiguration().setInt(MRJobConfig.NUM_MAPS, 1);
</code></pre>

<ul>
<li>调试的时刻把超时时间设置的久一点，否则：</li>
</ul>


<pre><code> Got exception: java.net.SocketTimeoutException: Call From winseliu/127.0.0.1 to winse.com:2850 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch :
</code></pre>

<ul>
<li>调试main方法参数设置</li>
</ul>


<p>调试main（转瞬即逝的把suspend设置为true！），map的调试选项的语句写在配置文件里面</p>

<pre><code>export HADOOP_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8073"

Administrator@winseliu ~/hadoop
$ sh -x bin/hadoop org.apache.hadoop.examples.WordCount /in /out 
</code></pre>

<h3>遍历所有子节点，查找节点运行map程序的信息</h3>

<p>map调试的端口配置为18090，根据这个选项来查找程序运行的机器。</p>

<pre><code>[hadoop@umcc97-44 ~]$ for h in `cat hadoop-2.2.0/etc/hadoop/slaves` ; do ssh $h 'ps aux|grep java | grep 18090'; echo $h;  done
hadoop    8667  0.0  0.0  63888  1268 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
umcc97-142
hadoop   12686  0.0  0.0  63868  1260 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
umcc97-143
hadoop   23516  0.0  0.0  63856  1108 ?        Ss   18:11   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1605/container_1397006359464_1605_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 57576 attempt_1397006359464_1605_m_000000_0 2 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002/stderr 
hadoop   23522  0.0  0.0 605136 15728 ?        Sl   18:11   0:00 /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1605/container_1397006359464_1605_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 57576 attempt_1397006359464_1605_m_000000_0 2
hadoop   23665  0.0  0.0  63856  1264 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
umcc97-144
</code></pre>

<p>仅打印运行map的节点名称</p>

<pre><code>[hadoop@umcc97-44 ~]$ for h in `cat hadoop-2.2.0/etc/hadoop/slaves` ; do ssh $h 'if ps aux|grep -v grep | grep java | grep 18090 | grep -v bash 2&gt;&amp;1 1&gt;/dev/null ; then echo `hostname`; fi'; done
umcc97-142
[hadoop@umcc97-44 ~]$ 
</code></pre>

<p>后面的操作就和普通的java程序调试步骤一样了。不再赘述。</p>

<h2>任务运行过程中的数据</h2>

<h4>辅助运行的两个bash程序</h4>

<p>运行的第一个程序（000001）为AppMaster，第二程序（000002）才是我们提交job的map任务。</p>

<pre><code>[hadoop@umcc97-143 ~]$ cd hadoop-2.2.0/tmp/nm-local-dir/nmPrivate
[hadoop@umcc97-143 nmPrivate]$ ls -Rl
.:
total 12
drwxrwxr-x 4 hadoop hadoop 4096 Apr 21 18:34 application_1397006359464_1606
-rw-rw-r-- 1 hadoop hadoop    6 Apr 21 18:34 container_1397006359464_1606_01_000001.pid
-rw-rw-r-- 1 hadoop hadoop    6 Apr 21 18:34 container_1397006359464_1606_01_000002.pid

./application_1397006359464_1606:
total 8
drwxrwxr-x 2 hadoop hadoop 4096 Apr 21 18:34 container_1397006359464_1606_01_000001
drwxrwxr-x 2 hadoop hadoop 4096 Apr 21 18:34 container_1397006359464_1606_01_000002

./application_1397006359464_1606/container_1397006359464_1606_01_000001:
total 8
-rw-r--r-- 1 hadoop hadoop   95 Apr 21 18:34 container_1397006359464_1606_01_000001.tokens
-rw-r--r-- 1 hadoop hadoop 3121 Apr 21 18:34 launch_container.sh

./application_1397006359464_1606/container_1397006359464_1606_01_000002:
total 8
-rw-r--r-- 1 hadoop hadoop  129 Apr 21 18:34 container_1397006359464_1606_01_000002.tokens
-rw-r--r-- 1 hadoop hadoop 3532 Apr 21 18:34 launch_container.sh
[hadoop@umcc97-143 nmPrivate]$ 
[hadoop@umcc97-143 nmPrivate]$ jps
4692 NodeManager
4173 DataNode
13497 YarnChild
7538 HRegionServer
13376 MRAppMaster
13574 Jps
[hadoop@umcc97-143 nmPrivate]$ cat *.pid
13366
13491
[hadoop@umcc97-143 nmPrivate]$ ps aux | grep 13366
hadoop   13366  0.0  0.0  63868  1088 ?        Ss   18:34   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001/stderr 
hadoop   13594  0.0  0.0  61204   760 pts/2    S+   18:36   0:00 grep 13366
[hadoop@umcc97-143 nmPrivate]$ ps aux | grep 13491
hadoop   13491  0.0  0.0  63868  1100 ?        Ss   18:34   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/container_1397006359464_1606_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 52046 attempt_1397006359464_1606_m_000000_0 2 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002/stderr 
hadoop   13599  0.0  0.0  61204   760 pts/2    S+   18:37   0:00 grep 13491
[hadoop@umcc97-143 nmPrivate]$ 
</code></pre>

<h4>程序运行本地缓存数据</h4>

<pre><code>[hadoop@umcc97-143 container_1397006359464_1606_01_000002]$ ls -l
total 28
-rw-r--r-- 1 hadoop hadoop  129 Apr 21 18:34 container_tokens
-rwx------ 1 hadoop hadoop  516 Apr 21 18:34 default_container_executor.sh
lrwxrwxrwx 1 hadoop hadoop   65 Apr 21 18:34 filter.io -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/filecache/10/filter.io
lrwxrwxrwx 1 hadoop hadoop  120 Apr 21 18:34 job.jar -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/filecache/10/job.jar
lrwxrwxrwx 1 hadoop hadoop  120 Apr 21 18:34 job.xml -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/filecache/13/job.xml
-rwx------ 1 hadoop hadoop 3532 Apr 21 18:34 launch_container.sh
drwx--x--- 2 hadoop hadoop 4096 Apr 21 18:34 tmp
[hadoop@umcc97-143 container_1397006359464_1606_01_000002]$ 
</code></pre>

<h2>处理问题方法</h2>

<ul>
<li>打印DEBUG日志：<code>export HADOOP_ROOT_LOGGER=DEBUG,console</code>

<ul>
<li>日志文件放置在nodemanager节点的logs/userlogs目录下。</li>
</ul>
</li>
<li>打印DEBUG日志也搞不定时，可以在源码里面sysout信息然后把<strong>class覆盖</strong>，来进行定位配置的问题。</li>
<li>如果不清楚shell的执行过程，可以通过<code>sh -x [CMD]</code>，或者在脚本文件的操作前加上<code>set -x</code>。相当于windows-batch的<code>echo on</code>功能。</li>
</ul>


<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/975271/remote-debugging-a-java-application">remote debugger opts</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
