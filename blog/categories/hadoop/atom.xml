<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2018-06-10T16:49:37+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Zookeeper ACL]]></title>
    <link href="http://winseliu.com/blog/2017/09/02/zookeeper-acl/"/>
    <updated>2017-09-02T23:14:55+08:00</updated>
    <id>http://winseliu.com/blog/2017/09/02/zookeeper-acl</id>
    <content type="html"><![CDATA[<p>集群又一次进行安检，SSH躲不过需要升级的，这次还加了hadoop security和zookeeper acl的bug。以前没太在意这些内容，既然安全检查出来了，还是需要处理的。</p>

<pre><code>ZooKeeper 未授权访问【原理扫描】
详细描述    ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 
ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 
在通常情况下，zookeeper允许未经授权的访问。
解决办法    为ZooKeeper配置相应的访问权限。 

方式一： 
1）增加一个认证用户 
addauth digest 用户名:密码明文 
eg. addauth digest user1:password1 
2）设置权限 
setAcl /path auth:用户名:密码明文:权限 
eg. setAcl /test auth:user1:password1:cdrwa 
3）查看Acl设置 
getAcl /path 

方式二： 
setAcl /path digest:用户名:密码密文:权限

威胁分值    5.0
危险插件    否
发现日期    2015-02-10
</code></pre>

<h2>Zookeeper权限基本知识点、操作</h2>

<ul>
<li><a href="https://zookeeper.apache.org/doc/r3.3.3/zookeeperProgrammers.html#sc_ZooKeeperAccessControl">https://zookeeper.apache.org/doc/r3.3.3/zookeeperProgrammers.html#sc_ZooKeeperAccessControl</a></li>
<li><a href="https://my.oschina.net/guol/blog/1358538">https://my.oschina.net/guol/blog/1358538</a></li>
<li><a href="http://blog.csdn.net/xyang81/article/details/53147894">http://blog.csdn.net/xyang81/article/details/53147894</a></li>
<li><a href="https://ihong5.wordpress.com/2014/07/24/apache-zookeeper-setting-acl-in-zookeeper-client/">https://ihong5.wordpress.com/2014/07/24/apache-zookeeper-setting-acl-in-zookeeper-client/</a></li>
<li><a href="https://zookeeper.apache.org/doc/r3.3.3/zookeeperStarted.html">https://zookeeper.apache.org/doc/r3.3.3/zookeeperStarted.html</a></li>
</ul>


<p>Note also that an ACL pertains only to a specific znode. In particular it does not apply to children. ACL在znode上无继承性，也就是说子znode不会继承父znode的ACL权限.</p>

<ul>
<li>world has a single id, anyone, that represents anyone.</li>
<li>auth doesn&rsquo;t use any id, represents any authenticated user.</li>
<li>digest uses a username:password string to generate MD5 hash which is then used as an ACL ID identity. Authentication is done by sending the username:password in clear text. When used in the ACL the expression will be the username:base64 encoded SHA1 password digest.</li>
<li>ip uses the client host IP as an ACL ID identity. The ACL expression is of the form addr/bits(3.5+) where the most significant bits of addr are matched against the most significant bits of the client host IP.</li>
</ul>


<p>zookeeper的ACL格式为 schema:id:permissions 。模式就是上面列的几种，再加一个super。创建的节点默认权限为 world:anyone:rwadc 表示所有人都对这个节点有rwadc的权限。</p>

<ul>
<li>Create：允许对子节点Create 操作</li>
<li>Read：允许对本节点GetChildren 和GetData 操作</li>
<li>Write ：允许对本节点SetData 操作</li>
<li>Delete ：允许对子节点Delete 操作</li>
<li>Admin ：允许对本节点setAcl 操作</li>
</ul>


<h2>Auth授权</h2>

<p>不需要id，当前 &ldquo;登录&rdquo; 的所有users都有权限（sasl、kerberos这些授权方式不懂，囧)。虽然不需要id，但是格式还得按照 scheme:id:perm 的写法。</p>

<pre><code>[zk: localhost:2181(CONNECTED) 15] setAcl /c auth:rwadc  
auth:rwadc does not have the form scheme:id:perm
Acl is not valid : /c

[zk: k8s(CONNECTED) 13] addauth digest a:a
[zk: k8s(CONNECTED) 14] addauth digest b:b
[zk: k8s(CONNECTED) 15] addauth digest c:c
[zk: k8s(CONNECTED) 16] create /e e
Created /e
[zk: k8s(CONNECTED) 17] setAcl /e auth::cdrwa
...省略节点输出信息

[zk: k8s(CONNECTED) 18] getAcl /e
'digest,'a:mDmPUap4qvYwm+PZOtJ/scGyHLY=
: cdrwa
'digest,'b:+F8zPn3x1CLx3qpYHEaRwIheWcc=
: cdrwa
'digest,'c:K7CO7OxIfBOQxczG+7FI9BdZ6/s=
: cdrwa
</code></pre>

<p>id随便写也可以，zookeeper都不记录的。</p>

<pre><code>[zk: localhost:2181(CONNECTED) 9] addauth digest hdfs:hdfs    
[zk: localhost:2181(CONNECTED) 10] setAcl /c auth:x:x:rwadc
...
[zk: localhost:2181(CONNECTED) 11] getAcl /c               
'digest,'user:tpUq/4Pn5A64fVZyQ0gOJ8ZWqkY=
: cdrwa
'digest,'hdfs:0wpra2yK6RCUB9sbo0BkElpzcl8=
: cdrwa
</code></pre>

<p>也可以对根 / 授权，这样客户端就不能随便在根下面新建节点了。</p>

<pre><code>[zk: localhost:2181(CONNECTED) 9] addauth digest user:password    
[zk: localhost:2181(CONNECTED) 21] setAcl / auth::rawdc

重新登录
[zk: localhost:2181(CONNECTED) 0] ls /
Authentication is not valid : /
[zk: localhost:2181(CONNECTED) 1] getAcl /
'digest,'user:tpUq/4Pn5A64fVZyQ0gOJ8ZWqkY=
: cdrwa
</code></pre>

<p>还原</p>

<p>使用有权限的用户/实例，如果都忘了那就只能放绝招：使用超级管理员登录，重新设置权限为world即可。</p>

<pre><code>[zk: localhost:2181(CONNECTED) 26] setAcl / world:anyone:cdrwa
</code></pre>

<h2>Digest</h2>

<p>直接用起来比 auth 简单，直接把密文交给zookeeper。首先得生成对应用户的密码。</p>

<pre><code>[root@k8s zookeeper-3.4.10]# java -cp zookeeper-3.4.10.jar:lib/* org.apache.zookeeper.server.auth.DigestAuthenticationProvider user:password
user:password-&gt;user:tpUq/4Pn5A64fVZyQ0gOJ8ZWqkY=

[root@k8s zookeeper-3.4.10]# java -cp zookeeper-3.4.10.jar:lib/* org.apache.zookeeper.server.auth.DigestAuthenticationProvider es:es
es:es-&gt;es:KiHfMOSWCTgPKpz78IL/6qO8AEE=
</code></pre>

<p>scheme是digest的时候，id需要密文。通过Zookeeper的客户端编码方式添加认证（登录），digest对应的auth数据是明文。</p>

<p>ACL授权一样使用 setAcl ：</p>

<pre><code>$$ A实例
[zk: localhost:2181(CONNECTED) 17] setAcl /b digest:user:tpUq/4Pn5A64fVZyQ0gOJ8ZWqkY=:cdrwa
和md5密码类似，数据库被盗了，如果是常用的密码会被猜出来
[zk: localhost:2181(CONNECTED) 18] getAcl /b
'digest,'user:tpUq/4Pn5A64fVZyQ0gOJ8ZWqkY=
: cdrwa

$$ B实例
重新登录：
[zk: k8s:2181(CONNECTED) 2] ls /b
Authentication is not valid : /b

$$ A实例
[zk: localhost:2181(CONNECTED) 20] create /b/bb ''
Authentication is not valid : /b/bb
[zk: localhost:2181(CONNECTED) 21] addauth digest user:tpUq/4Pn5A64fVZyQ0gOJ8ZWqkY=
[zk: localhost:2181(CONNECTED) 22] create /b/bb ''                                 
Authentication is not valid : /b/bb

# 需要使用明文登录
[zk: localhost:2181(CONNECTED) 23] addauth digest user:password
[zk: localhost:2181(CONNECTED) 24] create /b/bb '' 
Created /b/bb 

# 权限没有继承性
[zk: localhost:2181(CONNECTED) 25] getAcl /b/bb
'world,'anyone
: cdrwa
</code></pre>

<h1>IP</h1>

<p>ip的权限配置更简单些。逻辑就是匹配客户端的IP地址，在权限IP地址段范围内的才能访问。</p>

<pre><code>$$ A实例
[zk: localhost:2181(CONNECTED) 18] setAcl /i ip:127.0.0.1:cdrwa
...
[zk: localhost:2181(CONNECTED) 19] getAcl /i
'ip,'127.0.0.1
: cdrwa
[zk: localhost:2181(CONNECTED) 24] get /i
Authentication is not valid : /i

咋回事呢，就是本地还没权限？有时可localhost不一定对应127.0.0.1的。。。

$$ B实例
[root@k8s zookeeper-3.4.10]# bin/zkCli.sh -server 127.0.0.1
[zk: 127.0.0.1(CONNECTED) 0] get /i
i
...
改成另一个网卡的ip地址
[zk: 127.0.0.1(CONNECTED) 1] setAcl /i ip:192.168.191.138:cdrwa
...
[zk: 127.0.0.1(CONNECTED) 2] getAcl /i
'ip,'192.168.191.138
: cdrwa
[zk: 127.0.0.1(CONNECTED) 3] get /i
Authentication is not valid : /i

$$ C实例
用主机名(191.138)登录的实例
[zk: k8s(CONNECTED) 19] get /i
i
</code></pre>

<h2>超级管理员</h2>

<p>如果权限设置错了，咋办？</p>

<pre><code>[zk: k8s(CONNECTED) 21] setAcl /i ip:192.168.191.0/24:cdrwa                   
Acl is not valid : /i

[zk: k8s(CONNECTED) 25] setAcl /i ip:192.168.191.0:cdrwa

[zk: k8s(CONNECTED) 26] getAcl /i
'ip,'192.168.191.0
: cdrwa
[zk: k8s(CONNECTED) 27] get /i
Authentication is not valid : /i
</code></pre>

<p>除非把客户端的ip地址换成 192.168.191.0 否则就访问不了了。</p>

<p>此时需要超级管理员才行，不然真没办法折腾了。（不知道为啥）是可以删掉（特指我当前的环境啊），但是这样数据就没有了啊！！</p>

<pre><code>[zk: localhost:2181(CONNECTED) 26] getAcl /i
'ip,'192.168.191.0
: cdrwa
[zk: localhost:2181(CONNECTED) 27] delete /i
[zk: localhost:2181(CONNECTED) 28] ls /
[a, b, c, zookeeper, d, e]
[zk: localhost:2181(CONNECTED) 29] ls /i
Node does not exist: /i
</code></pre>

<p>如果数据很重要，重启后用超级管理员的方式找回密码还是很划的来的。</p>

<ul>
<li><a href="https://community.hortonworks.com/articles/29900/zookeeper-using-superdigest-to-gain-full-access-to.html">https://community.hortonworks.com/articles/29900/zookeeper-using-superdigest-to-gain-full-access-to.html</a></li>
</ul>


<p>用 DigestAuthenticationProvider 加密就不操作了，直接用 es:es 对应的 es:es->es:KiHfMOSWCTgPKpz78IL/6qO8AEE= 作为管理员的账号密码。</p>

<pre><code>export SERVER_JVMFLAGS=-Dzookeeper.DigestAuthenticationProvider.superDigest=es:KiHfMOSWCTgPKpz78IL/6qO8AEE=

[root@k8s zookeeper-3.4.10]# bin/zkServer.sh stop
[root@k8s zookeeper-3.4.10]# bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/zookeeper-3.4.10/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED

$$ A实例
[root@k8s zookeeper-3.4.10]# bin/zkCli.sh 
[zk: localhost:2181(CONNECTED) 0] get /i
Authentication is not valid : /i
[zk: localhost:2181(CONNECTED) 1] getAcl /i
'ip,'192.168.191.0
: cdrwa
[zk: localhost:2181(CONNECTED) 2] addauth digest es:es
[zk: localhost:2181(CONNECTED) 3] get /i
i
...
[zk: localhost:2181(CONNECTED) 4] setAcl /i world:anyone:cdrwa
...

$$ B实例
[zk: localhost:2181(CONNECTED) 0] get /i
i
[zk: localhost:2181(CONNECTED) 1] getAcl /i
'world,'anyone
: cdrwa
</code></pre>

<h2>实践&mdash;好玩</h2>

<p>权限可以直接在创建的时刻指定：</p>

<pre><code>create /mynode content digest:user:tpUq/4Pn5A64fVZyQ0gOJ8ZWqkY=:cdrwa
</code></pre>

<p>也可以一次性设置N个权限：</p>

<p>注：以下操作都是超级管理员登录的窗口，所以不存在权限的问题。想怎么改就怎么改</p>

<pre><code>setAcl /i ip:192.168.191.0:cdrwa,ip:127.0.0.1:cdrwa,ip:192.168.191.138:cdrwa

getAcl /i
'ip,'192.168.191.0
: cdrwa
'ip,'127.0.0.1
: cdrwa
'ip,'192.168.191.138
: cdrwa
</code></pre>

<p>但是，使用ip、digest、word重设权限后，会覆盖旧的：</p>

<pre><code>[zk: localhost:2181(CONNECTED) 7] setAcl /i ip:0.0.0.0:cdrwa
[zk: localhost:2181(CONNECTED) 8] getAcl /i
'ip,'0.0.0.0
: cdrwa

[zk: localhost:2181(CONNECTED) 15] setAcl /i world:anyone:cdraw
[zk: localhost:2181(CONNECTED) 16] getAcl /i
'world,'anyone
: cdrwa
</code></pre>

<p>3.4的版本不支持ip段（3.5应该是ok的）： <a href="https://github.com/apache/zookeeper/blob/release-3.4.10/src/java/main/org/apache/zookeeper/server/auth/IPAuthenticationProvider.java#L114">IPAuthenticationProvider</a></p>

<pre><code>    public boolean isValid(String id) {
        return addr2Bytes(id) != null;
    }
</code></pre>

<p>可以找对应版本的源码（远程）调试下：</p>

<pre><code>[root@k8s zookeeper-3.4.10]# export SERVER_JVMFLAGS="-Dzookeeper.DigestAuthenticationProvider.superDigest=es:KiHfMOSWCTgPKpz78IL/6qO8AEE= -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005"
[root@k8s zookeeper-3.4.10]# bin/zkServer.sh start
</code></pre>

<p>auth的权限比较有意思：自家兄弟添加、排除异己；permission按最新的算</p>

<pre><code>[zk: localhost:2181(CONNECTED) 21] setAcl /i auth::cdrwa,ip:0.0.0.0:cd
...
[zk: localhost:2181(CONNECTED) 22] getAcl /i
'ip,'0.0.0.0
: cd
'digest,'es:KiHfMOSWCTgPKpz78IL/6qO8AEE=
: cdrwa

# auth add
[zk: localhost:2181(CONNECTED) 27] addauth digest m:m
[zk: localhost:2181(CONNECTED) 28] addauth digest n:n
[zk: localhost:2181(CONNECTED) 29] setAcl /i auth::cdrwa
...
[zk: localhost:2181(CONNECTED) 30] getAcl /i
'digest,'es:KiHfMOSWCTgPKpz78IL/6qO8AEE=
: cdrwa
'digest,'m:WZiIgWqJgd8EQVBh55Bslf/7JRc=
: cdrwa
'digest,'n:TZ3f1UF7B75EF5g6qWR0VmEvb/s=
: cdrwa

# perm
[zk: localhost:2181(CONNECTED) 31] addauth digest z:z
[zk: localhost:2181(CONNECTED) 32] addauth digest l:l
[zk: localhost:2181(CONNECTED) 33] setAcl /i auth:z:z:cd
...
[zk: localhost:2181(CONNECTED) 34] getAcl /i
'digest,'es:KiHfMOSWCTgPKpz78IL/6qO8AEE=
: cd
'digest,'m:WZiIgWqJgd8EQVBh55Bslf/7JRc=
: cd
'digest,'n:TZ3f1UF7B75EF5g6qWR0VmEvb/s=
: cd
'digest,'z:cOgtYxFOAwKiTCMigcN2j2fFI3c=
: cd
'digest,'l:gdlgatwJdq7uG8kFfIjcIZj0tnQ=
: cd

可以看到全部变成cd了

[zk: localhost:2181(CONNECTED) 35] setAcl /i auth:z:z:cdraw
...
[zk: localhost:2181(CONNECTED) 36] getAcl /i               
'digest,'es:KiHfMOSWCTgPKpz78IL/6qO8AEE=
: cdrwa
'digest,'m:WZiIgWqJgd8EQVBh55Bslf/7JRc=
: cdrwa
'digest,'n:TZ3f1UF7B75EF5g6qWR0VmEvb/s=
: cdrwa
'digest,'z:cOgtYxFOAwKiTCMigcN2j2fFI3c=
: cdrwa
'digest,'l:gdlgatwJdq7uG8kFfIjcIZj0tnQ=
: cdrwa

全部变成cdrwa
</code></pre>

<p>我觉得用 auth 设置权限是最保险的，不会搞错了出现自己都访问不了的情况。</p>

<h2>后记</h2>

<p>ok，到此基本的知识点算大概了解了。还有自定义实现授权的provider，这有点高级了有兴趣的自己去看官方文档了。</p>

<p>但是因为权限没有继承关系，像一些开源项目用到zookeeper的话，怎么进行加密呢？所有子目录都一个个的加？或者自定义根路径（chroot）让别人猜不到？</p>

<p>还有像zookeeper自己的目录 /zookeeper ，怎么进行权限管理呢？</p>

<p></p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K8s Hadoop Deploy]]></title>
    <link href="http://winseliu.com/blog/2017/04/14/k8s-hadoop-deploy/"/>
    <updated>2017-04-14T10:56:39+08:00</updated>
    <id>http://winseliu.com/blog/2017/04/14/k8s-hadoop-deploy</id>
    <content type="html"><![CDATA[<p>折磨了一个多星期，最后还是调通了。折磨源于不自知，源于孤单，源于自负，后来通过扩展、查阅资料、请教同事顺利解决。简单部署可以查看<a href="https://github.com/winse/docker-hadoop">README.md</a> 。</p>

<pre><code>yum install docker-engine-1.12.6 docker-engine-selinux-1.12.6 -y

cd kube-deploy
vi hosts
vi k8s.profile
# 把deploy同步到其他实体机，同时把k8s.profile映射到/etc/profile.d
./rsync-deploy.sh

cd docker-multinode/
./master.sh or ./worker.sh

docker save gcr.io/google_containers/etcd-amd64:3.0.4 | docker-bs load
docker save quay.io/coreos/flannel:v0.6.1-amd64 | docker-bs load

cd kube-deploy/hadoop/kubenetes/
./prepare.sh
kubectl create -f hadoop-master2.yaml
kubectl create -f hadoop-slaver.yaml 
</code></pre>

<p>Tip：其实使用一套配置就可以启动多个集群，在 <code>kubectl create</code> 后面加上 <code>-n namespace</code> 即可。</p>

<p>比如：</p>

<pre><code>[root@cu2 kubenetes]# kubectl create namespace hd1
[root@cu2 kubenetes]# kubectl create namespace hd2

[root@cu2 kubenetes]# ./prepare.sh hd1
[root@cu2 kubenetes]# kubectl create -f hadoop-master2.yaml -n hd1
[root@cu2 kubenetes]# kubectl create -f hadoop-slaver.yaml -n hd1
[root@cu2 kubenetes]# ./prepare.sh hd2
[root@cu2 kubenetes]# kubectl create -f hadoop-master2.yaml -n hd2
[root@cu2 kubenetes]# kubectl create -f hadoop-slaver.yaml -n hd2

[root@cu2 kubenetes]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE
hd1           hadoop-master2                          1/1       Running   0          28s
hd1           slaver-rc-fdcsw                         1/1       Running   0          18s
hd1           slaver-rc-qv964                         1/1       Running   0          18s
hd2           hadoop-master2                          1/1       Running   0          26s
hd2           slaver-rc-0vdfk                         1/1       Running   0          17s
hd2           slaver-rc-r7g84                         1/1       Running   0          17s
...
</code></pre>

<p>现在想来其实就是 <strong> dockerd &ndash;ip-masq=false </strong>的问题（所有涉及的dockerd都需要加）。 还有就是一台机器单机下的容器互相访问，源IP都错也是安装了openvpn所导致，对所有过eth0的都加了MASQUERADE。</p>

<p>根源就在于请求的源地址被替换，也就是iptables的转发进行了SNAT。关于iptables转发这篇文章讲的非常清晰；<a href="http://fancyxinyu.blog.163.com/blog/static/18232136620136185434661/">IPtables之四：NAT原理和配置  </a> 。</p>

<h2>所遇到的问题</h2>

<p>没加ip-masq之前，namenode收到datanode的请求后，源地址是flannel.0的ip: 10.1.98.0。</p>

<p>namenode对应的日志为：</p>

<pre><code>2017-04-09 07:22:06,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.1.98.0, datanodeUuid=5086c549-f3bb-4ef6-8f56-05b1f7adb7d3, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-522174fa-6e7b-4c3f-ae99-23c3018e35d7;nsid=1613705851;c=0) storage 5086c549-f3bb-4ef6-8f56-05b1f7adb7d3
2017-04-09 07:22:06,920 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/10.1.98.0:50010
2017-04-09 07:22:06,921 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.1.98.0:50010
</code></pre>

<p>一开始以为是flannel的问题，换成yum安装，然后同时flannel把backend切换成vxlan后，还是一样的问题。</p>

<p>最后请教搞网络的同事，应该是请求的源地址被替换了，也就定位到iptables。然后通过查看文档，其实前面也有看到过对应的文章，但是看不明白不知道缘由。</p>

<ul>
<li><a href="https://groups.google.com/d/msg/kubernetes-users/P4uh7y383oo/bPzIRaxhs5gJ">Networking Problem in creating HDFS cluster. - Eugene Yakubovich </a></li>
<li><a href="https://groups.google.com/d/msg/kubernetes-users/P4uh7y383oo/a1GIV4hcAgAJ">Networking Problem in creating HDFS cluster. - Huihui He </a></li>
<li><a href="https://developer.ibm.com/recipes/tutorials/networking-your-docker-containers-using-docker0-bridge/">Networking your docker containers using docker0 bridge</a></li>
</ul>


<p>iptables的部分相关信息：</p>

<pre><code>[root@cu2 ~]# iptables -S -t nat
...
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A PREROUTING -j PREROUTING_direct
-A PREROUTING -j PREROUTING_ZONES_SOURCE
-A PREROUTING -j PREROUTING_ZONES
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT -j OUTPUT_direct
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -s 10.1.34.0/24 ! -o docker0 -j MASQUERADE
-A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
-A POSTROUTING -j POSTROUTING_direct
-A POSTROUTING -j POSTROUTING_ZONES_SOURCE
-A POSTROUTING -j POSTROUTING_ZONES
-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -m mark --mark 0x4000/0x4000 -j MASQUERADE
-A KUBE-SEP-75CPIAPDB4MAVFWI -s 10.1.40.3/32 -m comment --comment "kube-system/kube-dns:dns-tcp" -j KUBE-MARK-MASQ
-A KUBE-SEP-75CPIAPDB4MAVFWI -p tcp -m comment --comment "kube-system/kube-dns:dns-tcp" -m tcp -j DNAT --to-destination 10.1.40.3:53
-A KUBE-SEP-IWNPEB4T46P6VG5J -s 192.168.0.148/32 -m comment --comment "default/kubernetes:https" -j KUBE-MARK-MASQ
-A KUBE-SEP-IWNPEB4T46P6VG5J -p tcp -m comment --comment "default/kubernetes:https" -m recent --set --name KUBE-SEP-IWNPEB4T46P6VG5J --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 192.168.0.148:6443
-A KUBE-SEP-UYUINV25NDNSKNUW -s 10.1.40.3/32 -m comment --comment "kube-system/kube-dns:dns" -j KUBE-MARK-MASQ
-A KUBE-SEP-UYUINV25NDNSKNUW -p udp -m comment --comment "kube-system/kube-dns:dns" -m udp -j DNAT --to-destination 10.1.40.3:53
-A KUBE-SEP-XDHL2OHX2ICPQHKI -s 10.1.40.2/32 -m comment --comment "kube-system/kubernetes-dashboard:" -j KUBE-MARK-MASQ
-A KUBE-SEP-XDHL2OHX2ICPQHKI -p tcp -m comment --comment "kube-system/kubernetes-dashboard:" -m tcp -j DNAT --to-destination 10.1.40.2:9090
-A KUBE-SERVICES -d 10.0.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y
-A KUBE-SERVICES -d 10.0.0.95/32 -p tcp -m comment --comment "kube-system/kubernetes-dashboard: cluster IP" -m tcp --dport 80 -j KUBE-SVC-XGLOHA7QRQ3V22RZ
-A KUBE-SERVICES -d 10.0.0.10/32 -p udp -m comment --comment "kube-system/kube-dns:dns cluster IP" -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU
-A KUBE-SERVICES -d 10.0.0.10/32 -p tcp -m comment --comment "kube-system/kube-dns:dns-tcp cluster IP" -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4
-A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
-A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment "kube-system/kube-dns:dns-tcp" -j KUBE-SEP-75CPIAPDB4MAVFWI
-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment "default/kubernetes:https" -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-IWNPEB4T46P6VG5J --mask 255.255.255.255 --rsource -j KUBE-SEP-IWNPEB4T46P6VG5J
-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment "default/kubernetes:https" -j KUBE-SEP-IWNPEB4T46P6VG5J
-A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment "kube-system/kube-dns:dns" -j KUBE-SEP-UYUINV25NDNSKNUW
-A KUBE-SVC-XGLOHA7QRQ3V22RZ -m comment --comment "kube-system/kubernetes-dashboard:" -j KUBE-SEP-XDHL2OHX2ICPQHKI
</code></pre>

<p>在dockerd服务脚本加上 <code>--ip-masq=false</code> 后，<code>-A POSTROUTING -s 10.1.34.0/24 ! -o docker0 -j MASQUERADE</code> 这一句就没有了，也就是不会进行源地址重写了，这样请求发送到namenode后还是datanode容器的IP。问题解决，原因简单的让人欲哭无泪啊。</p>

<p>写yaml遇到的一些其他问题：</p>

<ul>
<li><a href="http://andykdocs.de/development/Docker/Fixing+the+Docker+TERM+variable+issue">Fixing the Docker TERM variable issue</a></li>
<li><a href="http://stackoverflow.com/questions/27195466/hdfs-datanode-denied-communication-with-namenode-because-hostname-cannot-be-reso">hdfs Datanode denied communication with namenode because hostname cannot be resolved</a></li>
</ul>


<p>当然还有很多其他的问题，这篇就写这么多，优化工作后面的弄好了再写。</p>

<h2>中间过程步骤记录</h2>

<p>主要就是记录心路历程，如果以后遇到同样的问题能让自己快速回想起来。如果仅仅为了部署，可以跳过该部分，直接后最后的常用命令。</p>

<p>记录下中间 <strong>通过yum安装etcd和flanneld</strong> 的过程。物理机安装flanneld会把配置docker环境变量（/run/flannel/subnet.env）加入启动脚本。</p>

<pre><code>安装docker-v1.12
https://docs.docker.com/v1.12/
https://docs.docker.com/v1.12/engine/installation/linux/centos/

# 删掉原来的
yum-config-manager --disable docker-ce*
yum remove -y docker-ce*

sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'
[dockerrepo]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/7/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg
EOF

https://yum.dockerproject.org/repo/main/centos/7/Packages/
[root@cu3 ~]# yum --showduplicates list docker-engine | expand
docker-engine.x86_64             1.12.6-1.el7.centos                  dockerrepo

[root@cu3 yum.repos.d]# yum install docker-engine-1.12.6 docker-engine-selinux-1.12.6


https://kubernetes.io/docs/getting-started-guides/centos/centos_manual_config/

cat &gt; /etc/yum.repos.d/virt7-docker-common-release.repo &lt;&lt;EOF
[virt7-docker-common-release]
name=virt7-docker-common-release
baseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/
gpgcheck=0
EOF

yum -y install --enablerepo=virt7-docker-common-release etcd flannel
yum -y install --enablerepo=virt7-docker-common-release flannel

- ETCD配置
[root@cu3 docker-multinode]# 
etcdctl mkdir /kube-centos/network
etcdctl set /kube-centos/network/config "{ \"Network\": \"10.1.0.0/16\", \"SubnetLen\": 24, \"Backend\": { \"Type\": \"vxlan\" } }"

- FlANNEL
[root@cu3 ~]# cat /etc/sysconfig/flanneld
# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS="http://cu3:2379"

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX="/kube-centos/network"

# Any additional options that you want to pass
#FLANNEL_OPTIONS=""

[root@cu2 yum.repos.d]# systemctl daemon-reload

[root@cu2 yum.repos.d]# cat /run/flannel/subnet.env

[root@cu2 ~]# systemctl cat docker
...
# /usr/lib/systemd/system/docker.service.d/flannel.conf
[Service]
EnvironmentFile=-/run/flannel/docker 
</code></pre>

<p>测试过程中有yaml配置中启动sshd，然后启动容器后，通过手动启动namenode、datanode的方式来测试：</p>

<pre><code>cd hadoop-2.6.5
gosu hadoop mkdir /data/bigdata
gosu hadoop sbin/hadoop-daemon.sh start datanode 

cd hadoop-2.6.5/
gosu hadoop  bin/hadoop namenode -format 
gosu hadoop sbin/hadoop-daemon.sh start namenode
</code></pre>

<p>后来发现问题出在iptables后，又回到原来的docker-bootstrap启动，需要删除flannel.1的网络：</p>

<pre><code># yum安装flanneld后停止 https://kubernetes.io/docs/getting-started-guides/scratch/
ip link set flannel.1 down
ip link delete flannel.1
route -n

rm /usr/lib/systemd/system/docker.service.d/flannel.conf 
</code></pre>

<p>开了防火墙的话，把容器的端加入到信任列表：</p>

<pre><code>systemctl enable firewalld &amp;&amp; systemctl start firewalld

firewall-cmd --zone=trusted --add-source=10.0.0.0/8 --permanent 
firewall-cmd --zone=trusted --add-source=192.168.0.0/16 --permanent 
firewall-cmd --reload
</code></pre>

<h2>一些有趣的命令</h2>

<pre><code>查看用了哪些镜像

[root@cu2 /]# kubectl get pods --all-namespaces -o jsonpath="{..image}" |\
 tr -s '[[:space:]]' '\n' |\
 sort |\
 uniq -c
      2 gcr.io/google_containers/dnsmasq-metrics-amd64:1.0
      2 gcr.io/google_containers/exechealthz-amd64:1.2
     12 gcr.io/google_containers/hyperkube-amd64:v1.5.5
      2 gcr.io/google_containers/kube-addon-manager-amd64:v6.1
      2 gcr.io/google_containers/kubedns-amd64:1.9
      2 gcr.io/google_containers/kube-dnsmasq-amd64:1.4
      2 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.0


修改默认kubectl的配置

[root@cu2 ~]# vi $KUBECONFIG 
apiVersion: v1
kind: Config
preferences: {}
current-context: default
clusters:
- cluster:
    server: http://localhost:8080
  name: default
contexts:
- context:
    cluster: default
    user: ""
    namespace: kube-system
  name: default
users: {}

如果kubectl没有下载，可以从镜像启动的容器里面获取

[root@cu2 docker-multinode]# docker exec -ti 0c0360bcc2c3 bash
root@cu2:/# cp kubectl /var/run/

[root@cu2 run]# mv kubectl /data/kubernetes/kube-deploy/docker-multinode/

获取容器IP

https://kubernetes.io/docs/user-guide/jsonpath/
[root@cu2 ~]# kubectl get pods -o wide -l run=redis -o jsonpath={..podIP}
10.1.75.2 10.1.75.3 10.1.58.3 10.1.58.2 10.1.33.3

网络共用: --net

docker run -ti --entrypoint=sh --net=container:8e9f21956469f4ef7e5b9d91798788ab83f380795d2825cdacae0ed28f5ba03b gcr.io/google_containers/skydns-amd64:1.0


格式化输出

kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}"  

[root@cu2 ~]# export POD_COL="custom-columns=NAME:.metadata.name,RESTARTS:.status.containerStatuses[*].restartCount,CONTAINERS:.spec.containers[*].name,IP:.status.podIP,HOST:.spec.nodeName"
[root@cu2 ~]# kubectl get pods -o $POD_COL 

kubectl get po -l k8s-app=kube-dns -o=custom-columns=NAME:.metadata.name,CONTAINERS:.spec.containers[*].name

[root@cu2 kubernetes]# kubectl get po --all-namespaces -o=custom-columns=NAME:.metadata.name,CONTAINERS:.spec.containers[*].name

kubectl get po --all-namespaces {range .items[*]}{.metadata.name}{“\t”}{end}

备份

echo "$(docker ps  | grep -v IMAGE | awk '{print $2}' )
$(docker-bs ps | grep -v IMAGE | awk '{print $2}' )" | sort -u | while read image ; do docker save $image&gt;$(echo $image | tr '[/:]' _).tar ; done

加Label

cat /etc/hosts | grep -E "\scu[0-9]\s" | awk '{print "kubectl label nodes "$1" hostname="$2}' | while read line ; do sh -c "$line" ; done

扩容

[root@cu2 kubernetes]# kubectl run redis --image=redis:3.2.8 
[root@cu2 kubernetes]# kubectl scale --replicas=9 deployment/redis

 echo " $( kubectl describe pods hadoop-master2 | grep -E "Node|Container ID" | awk -F/ '{print $NF}' | tr '\n' ' ' | awk '{print "ssh "$1" \rdocker exec -ti "$2" bash"}' ) "
</code></pre>

<p>测试DNS是否成功：</p>

<pre><code>[root@cu2 kube-deploy]# vi busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always

[root@cu3 kube-deploy]# kubectl create -f busybox.yaml 
pod "busybox" created
[root@cu3 kube-deploy]# kubectl get pods 
NAME      READY     STATUS              RESTARTS   AGE
busybox   0/1       ContainerCreating   0          11s
[root@cu3 kube-deploy]# kubectl get pods 
NAME      READY     STATUS    RESTARTS   AGE
busybox   1/1       Running   0          1m
[root@cu3 kube-deploy]# kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local

用容器的MYSQL的做客户端

kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h mysql -ppassword
</code></pre>

<p>小结一点：日志的重要性！</p>

<pre><code>[root@cu2 kubenetes]# docker ps -a | grep kubelet
[root@cu2 kubenetes]# docker logs --tail=200 7432da457558

E0417 11:39:40.194844   22528 configmap.go:174] Couldn't get configMap hadoop/dta-hadoop-config: configmaps "dta-hadoop-config" not found
E0417 11:39:40.194910   22528 configmap.go:174] Couldn't get configMap hadoop/dta-bin-config: configmaps "dta-bin-config" not found
</code></pre>

<p>监控heapster的一些错误，还没调好</p>

<pre><code>[root@cu2 ~]# kubectl exec -ti heapster-564189836-shn2q -n kube-system -- sh
/ # 
/ # 
没pod的数据
/ # /heapster --source=https://kubernetes.default --sink=log --heapster-port=8083 -v 10

E0329 10:11:53.823641       1 reflector.go:203] k8s.io/heapster/metrics/processors/node_autoscaling_enricher.go:100: Failed to list *api.Node: Get https://kubernetes.default/api/v1/nodes?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout


$heapster/metrics
$heapster/api/v1/model/debug/allkeys
</code></pre>

<p>其他一些配置</p>

<pre><code>
other_args=" --registry-mirror=https://docker.mirrors.ustc.edu.cn "

--insecure-registry gcr.io 

iptables -S -t nat
</code></pre>

<h2>其他一些资源</h2>

<ul>
<li><a href="https://kubernetes.io/docs/concepts/cluster-administration/resource-usage-monitoring/">https://kubernetes.io/docs/concepts/cluster-administration/resource-usage-monitoring/</a></li>
<li><p><a href="https://github.com/kubernetes/heapster/tree/v1.3.0/deploy/kube-config/influxdb">https://github.com/kubernetes/heapster/tree/v1.3.0/deploy/kube-config/influxdb</a></p></li>
<li><p><a href="https://github.com/kubernetes/heapster/blob/master/docs/debugging.md">https://github.com/kubernetes/heapster/blob/master/docs/debugging.md</a></p></li>
<li><p><a href="https://docs.docker.com/v1.12/engine/installation/linux/centos/">https://docs.docker.com/v1.12/engine/installation/linux/centos/</a></p></li>
<li><p><a href="https://github.com/CodisLabs/codis/blob/release3.2/Dockerfile">https://github.com/CodisLabs/codis/blob/release3.2/Dockerfile</a></p></li>
<li><a href="https://github.com/sporkmonger/redis-k8s/blob/master/redis.yaml">https://github.com/sporkmonger/redis-k8s/blob/master/redis.yaml</a></li>
<li><a href="https://github.com/sobotklp/kubernetes-redis-cluster/blob/master/redis-cluster.yml">https://github.com/sobotklp/kubernetes-redis-cluster/blob/master/redis-cluster.yml</a></li>
</ul>


<p>statefulset</p>

<ul>
<li><a href="https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/">https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/stateful-application/run-stateful-application/">https://kubernetes.io/docs/tutorials/stateful-application/run-stateful-application/</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/stateful-application/run-replicated-stateful-application/">https://kubernetes.io/docs/tutorials/stateful-application/run-replicated-stateful-application/</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hdfs异构存储实操]]></title>
    <link href="http://winseliu.com/blog/2016/05/05/hdfs-heterogeneous-storage/"/>
    <updated>2016-05-05T21:41:39+08:00</updated>
    <id>http://winseliu.com/blog/2016/05/05/hdfs-heterogeneous-storage</id>
    <content type="html"><![CDATA[<p>[注意] 查看官方文档一定要和自己使用的环境对应！操作 storagepolicies 不同版本对应的命令不同（2.6.3<->2.7.2）！</p>

<p>我这里测试环境使用的是 2.6.3 <a href="https://hadoop.apache.org/docs/r2.6.3/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Heterogeneous Storage: Archival Storage, SSD &amp; Memory</a></p>

<h2>配置</h2>

<p>直接把内存盘放到 /dev/shm 下，单独挂载一个 tmpfs 的效果也差不多。<a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">r2.7.2 Memory Storage Support in HDFS</a> 2.6.3没有这个文档 概念都适应的。</p>

<p>1 调节系统参数</p>

<pre><code>vi /etc/security/limits.conf

    hadoop           -       nofile          65535
    hadoop           -       nproc           65535
    hadoop           -       memlock         268435456
</code></pre>

<p>需要调节memlock的大小，否则启动datanode报错。</p>

<pre><code>2016-05-05 19:22:22,674 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.lang.RuntimeException: Cannot start datanode because the configured max locked memory size (dfs.datanode.max.locked.memory) of 134217728 bytes is more than the datanode's available RLIMIT_MEMLOCK ulimit of 65536 bytes.
        at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1067)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.&lt;init&gt;(DataNode.java:417)
</code></pre>

<p>2 添加RAM_DISK</p>

<pre><code>vi hdfs-site.xml

    &lt;property&gt;
    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
    &lt;value&gt;/data/bigdata/hadoop/dfs/data,[RAM_DISK]/dev/shm/dfs/data&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
    &lt;name&gt;dfs.datanode.max.locked.memory&lt;/name&gt;
    &lt;value&gt;134217728&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<p>注意内存盘的写法，<code>[RAM_DISK]</code> 必须这些写，不然datanode不知道指定路径的storage的类型(默认是 DISK )。<a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html#Storage_Types_and_Storage_Policies">Storage_Types_and_Storage_Policies</a></p>

<blockquote><p>The default storage type of a datanode storage location will be DISK if it does not have a storage type tagged explicitly.</p></blockquote>

<p>3 同步配置并重启dfs</p>

<pre><code>[root@cu2 ~]# scp /etc/security/limits.conf cu3:/etc/security/
[hadoop@cu2 hadoop-2.6.3]$ rsync -vaz etc cu3:~/hadoop-2.6.3/ 

[hadoop@cu2 hadoop-2.6.3] sbin/stop-dfs.sh
[hadoop@cu2 hadoop-2.6.3] sbin/start-dfs.sh
</code></pre>

<p>可以去到datanode查看日志，可以看到 /dev/shm/dfs/data 路径 <strong>StorageType</strong> 为 <strong>RAM_DISK</strong> ：</p>

<pre><code>2016-05-05 19:33:39,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /data/bigdata/hadoop/dfs/data/current
2016-05-05 19:33:39,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /data/bigdata/hadoop/dfs/data/current, StorageType: DISK
2016-05-05 19:33:39,863 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /dev/shm/dfs/data/current
2016-05-05 19:33:39,863 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /dev/shm/dfs/data/current, StorageType: RAM_DISK
</code></pre>

<p>同时查看 内存盘 的路径内容：</p>

<pre><code>[hadoop@cu2 hadoop-2.6.3]$ ssh cu3 tree /dev/shm/dfs
/dev/shm/dfs
└── data
    ├── current
    │   ├── BP-1108852639-192.168.0.148-1452322889531
    │   │   ├── current
    │   │   │   ├── finalized
    │   │   │   ├── rbw
    │   │   │   └── VERSION
    │   │   └── tmp
    │   └── VERSION
    └── in_use.lock

7 directories, 3 files
</code></pre>

<h2>测试使用</h2>

<p>通过三个例子对比，简单描述下使用。首先，使用默认的方式(主要用于对比)，第二个例子写文件是添加参数，第三个设置目录的存储类型（目录/文件会继承父目录的存储类型）</p>

<p>1 测试1</p>

<pre><code>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -put README.txt /tmp/

[hadoop@cu2 hadoop-2.6.3]$ hdfs fsck /tmp/README.txt -files -blocks -locations
...
/tmp/README.txt 1366 bytes, 1 block(s):  OK
0. BP-1108852639-192.168.0.148-1452322889531:blk_1073752574_11776 len=1366 repl=1 [192.168.0.148:50010]

[hadoop@cu3 hadoop-2.6.3]$ find /data/bigdata/hadoop/dfs/data/ /dev/shm/dfs/data/ -name "*1073752574*"
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir41/blk_1073752574_11776.meta
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir41/blk_1073752574
</code></pre>

<p>2 写文件时添加 lazy_persist 标识</p>

<pre><code># 添加 -l 参数，后台代码会加上 LAZY_PERSIST 标识。
[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -help put 
-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt; :
  Copy files from the local file system into fs. Copying fails if the file already
  exists, unless the -f flag is given.
  Flags:

  -p  Preserves access and modification times, ownership and the mode. 
  -f  Overwrites the destination if it already exists.                 
  -l  Allow DataNode to lazily persist the file to disk. Forces        
         replication factor of 1. This flag will result in reduced
         durability. Use with care.
</code></pre>

<p><img src="/images/blogs/storage-lazy.png" alt="" /></p>

<pre><code># -l 参数会把 replication 强制设置成数字1 ！
[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -put -l README.txt /tmp/readme.txt2

# 查看namenode的日志，可以看到文件写入到 RAM_DISK 类型的存储
[hadoop@cu2 hadoop-2.6.3]$ less logs/hadoop-hadoop-namenode-cu2.log 

    2016-05-05 20:38:36,465 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/readme.txt2._COPYING_. BP-1108852639-192.168.0.148-1452322889531 blk_1073752578_11780{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-dcb2673f-3297-4bd7-af1c-ac0ee3eebaf9:NORMAL:192.168.0.30:50010|RBW]]}
    2016-05-05 20:38:36,592 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 192.168.0.30:50010 is added to blk_1073752578_11780{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[RAM_DISK]DS-bf1ab64f-7eb3-41e0-8466-43287de9893d:NORMAL:192.168.0.30:50010|FINALIZED]]} size 0
    2016-05-05 20:38:36,594 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/readme.txt2._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1388277364_1

# 具体的内容所在位置
[hadoop@cu4 ~]$ tree /dev/shm/dfs/data/
/dev/shm/dfs/data/
├── current
│   ├── BP-1108852639-192.168.0.148-1452322889531
│   │   ├── current
│   │   │   ├── finalized
│   │   │   │   └── subdir0
│   │   │   │       └── subdir42
│   │   │   │           ├── blk_1073752578
│   │   │   │           └── blk_1073752578_11780.meta
│   │   │   ├── rbw
│   │   │   └── VERSION
│   │   └── tmp
│   └── VERSION
└── in_use.lock
</code></pre>

<p>3 设置目录的存储类型</p>

<pre><code>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -mkdir /ramdisk
[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -setStoragePolicy /ramdisk LAZY_PERSIST 
Set storage policy LAZY_PERSIST on /ramdisk

[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -put README.txt /ramdisk

[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk
The storage policy of /ramdisk:
BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}

# 不支持通配符
[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk/*
getStoragePolicy: File/Directory does not exist: /ramdisk/*

[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk/README.txt
The storage policy of /ramdisk/README.txt:
BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}


# 添加replication参数，再测试多个备份只有一个写ram_disk
[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -Ddfs.replication=3 -put README.txt /ramdisk/readme.txt2

[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk/readme.txt2
The storage policy of /ramdisk/readme.txt2:
BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}

[hadoop@cu2 hadoop-2.6.3]$ hdfs fsck /ramdisk/readme.txt2 -files -blocks -locations

    /ramdisk/readme.txt2 1366 bytes, 1 block(s):  OK
    0. BP-1108852639-192.168.0.148-1452322889531:blk_1073752580_11782 len=1366 repl=3 [192.168.0.30:50010, 192.168.0.174:50010, 192.168.0.148:50010]

[hadoop@cu3 ~]$ find /data/bigdata/hadoop/dfs/data /dev/shm/dfs/data -name "*1073752580*"
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580_11782.meta

# 已经把ram_disk的内容持久化到磁盘了("Lazy_Persist")
[hadoop@cu4 ~]$ find /data/bigdata/hadoop/dfs/data /dev/shm/dfs/data -name "*1073752580*"
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/lazypersist/subdir0/subdir42/blk_1073752580
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/lazypersist/subdir0/subdir42/blk_1073752580_11782.meta
/dev/shm/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580
/dev/shm/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580_11782.meta

[hadoop@cu5 ~]$ find /data/bigdata/hadoop/dfs/data /dev/shm/dfs/data -name "*1073752580*"
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580_11782.meta
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580
</code></pre>

<p>[设想] 对于那些处理完就删除的临时文件，可以把持久化的时间设置的久一点 <code>dfs.datanode.lazywriter.interval.sec</code>。这样就不需要写磁盘了。</p>

<p>不要妄想了，反正都会持久化！就是缓冲的效果，其他没有了！！一次性存储并且不需要持久化的还是用alluxio吧。</p>

<pre><code>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.LazyWriter#saveNextReplica
    org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService#submitLazyPersistTask
</code></pre>

<h2>参考</h2>

<ul>
<li>挺详细的<a href="http://blog.csdn.net/androidlushangderen/article/details/51105876">HDFS异构存储</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[整理] Hadoop入门]]></title>
    <link href="http://winseliu.com/blog/2016/04/23/hadoop-guide-catalog/"/>
    <updated>2016-04-23T15:45:34+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/23/hadoop-guide-catalog</id>
    <content type="html"><![CDATA[<h2>1. 环境准备</h2>

<p>工欲善事其必先利其器。不要吝啬硬件上投入，找一个适合自己的环境！</p>

<ul>
<li>Windows

<ul>
<li><a href="/blog/2014/02/23/quickly-open-program-in-windows/">快速打开程序</a></li>
<li>Cygwin：Windows本地编译需要，执行命令比 cmd 更方便</li>
</ul>
</li>
<li><a href="/blog/2011/02/28/win7-install-fedora-linux/">Windows + Linux双系统</a></li>
<li>Linux

<ul>
<li><a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a></li>
<li><a href="/blog/2015/09/13/review-linux-101-hacks/">【linux 101 Hacks】读后感</a>

<ul>
<li><a href="/images/blogs/linux-101-hacks-review-securecrt-config.png">Socket5代理</a></li>
</ul>
</li>
<li><a href="/blog/2016/03/11/install-and-config-openvpn/">OpenVPN</a></li>
<li>docker

<ul>
<li><a href="/blog/2014/09/27/docker-start-guide-on-centos/">Docker入门</a></li>
<li><a href="/blog/2014/09/30/docker-ssh-on-centos/">配置ssh</a></li>
<li><a href="/blog/2014/10/18/docker-dnsmasq-handler-hosts-build-hadoop-cluster/">Dnsmasq</a></li>
</ul>
</li>
</ul>
</li>
</ul>


<h2>2. 安装部署hadoop/spark</h2>

<h4>编译安装</h4>

<ul>
<li>Hadoop安装与升级:

<ul>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-1-install-in-docker/">Docker中安装</a></li>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-2-hadoop-upgrade/">2.2升级到2.6</a></li>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-3-ha/">HA配置</a></li>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-4-ha-upgrade/">HA升级</a></li>
</ul>
</li>
<li><a href="/blog/2015/03/08/vmware-build-hadoop2-dot-6/">Centos6 Build hadoop2.6</a></li>
<li><a href="/blog/2015/03/09/windows-build-hadoop-2-dot-6/">Windows Build hadoop2.6</a></li>
<li><a href="/blog/2014/10/16/spark-build-and-configuration/">各版本Spark编译/搭建环境</a></li>
</ul>


<h4>功能优化</h4>

<ul>
<li><a href="/blog/2014/09/01/hadoop2-mapreduce-compress/">Hadoop2 Mapreduce输入输出压缩</a></li>
<li><a href="/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading/">Hadoop2 ShortCircuit Local Reading</a></li>
<li><a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>

<ul>
<li><a href="/blog/2016/04/08/snappy-centos5-on-hive-on-spark/">Hive-on-spark Snappy on Centos5</a></li>
</ul>
</li>
<li><a href="/blog/2016/05/05/hdfs-heterogeneous-storage">HDFS RamDisk内存缓冲</a></li>
</ul>


<h4>维护</h4>

<ul>
<li><a href="/blog/2013/02/22/hadoop-cluster-increases-nodes/">Hadoop集群增加节点</a></li>
<li><a href="/blog/2014/07/29/safely-remove-datanode/">安全的关闭datanode</a></li>
<li><a href="/blog/2015/03/25/deploy-separated-yarn-on-exists-hdfs-cluster/">已有HDFS上部署yarn</a></li>
<li><a href="/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs/">Hadoop不同版本yarn和hdfs混搭，spark-yarn环境配置</a></li>
</ul>


<h4>旧版本安装</h4>

<ul>
<li><a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a></li>
<li><a href="/blog/2013/03/24/pseudo-distributed-hadoop-in-windows/"><del>Windows配置hadoop伪分布式环境(续)</del></a> 不再推荐cygwin下部署Hadoop。</li>
<li><a href="/blog/2013/03/02/quickly-build-a-second-hadoop-cluster/">快速搭建第二个hadoop分布式集群环境</a></li>
<li><a href="/blog/2013/03/27/run-on-hadoop-on-ant/"><del>Ant实现hadoop插件Run-on-Hadoop</del></a></li>
</ul>


<h2>3. 进阶</h2>

<h4>配置深入理解</h4>

<ul>
<li><a href="/blog/2014/08/02/hadoop-datanode-config-should-equals/">Hadoop的datanode数据节点机器配置</a></li>
<li><a href="/blog/2016/03/17/hadoop-memory-opts-and-args/">Hadoop内存环境变量和参数</a></li>
<li><a href="/blog/2016/04/11/spark-on-yarn-memory-allocate/">Spark-on-yarn内存分配</a></li>
<li><a href="/blog/2016/03/25/spark-sql-executors-dynamic-on-yarn/">SparkSQL-on-YARN的Executors池(动态)配置</a></li>
</ul>


<h4>问题定位</h4>

<ul>
<li><a href="/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/">在windows开发测试mapreduce几种方式</a></li>
<li><a href="/blog/2014/04/22/remote-debug-hadoop2/">远程调试hadoop2以及错误处理方法</a></li>
<li><a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">逐步定位Java程序OOM的异常</a></li>
</ul>


<h4>读码</h4>

<ul>
<li>Hadoop2 Balancer磁盘空间平衡

<ul>
<li><a href="/blog/2014/08/06/read-hadoop-balancer-source-part1/">上</a></li>
<li><a href="/blog/2014/09/05/read-hadoop-balancer-source-part2/">中</a></li>
<li><a href="/blog/2014/09/05/read-hadoop-balancer-source-part3/">下</a></li>
</ul>
</li>
<li><a href="/blog/2015/03/13/hadoop-distcp/">Hadoop Distcp</a></li>
</ul>


<h4>其他</h4>

<ul>
<li><a href="/blog/2014/09/12/scala-wordcount-on-hadoop/">Scala Wordcount on Hadoop2</a></li>
<li><a href="/blog/2014/12/07/hadoop-mr-rest-api/">MR Rest接口</a></li>
</ul>


<h2>4. Hadoop平台</h2>

<ul>
<li>zookeeper</li>
<li>hive

<ul>
<li><a href="/blog/2014/06/21/upgrade-hive/">Upgrade Hive: 0.12.0 to 0.13.1</a></li>
<li>tez:

<ul>
<li><a href="/blog/2014/06/18/hadoop-tez-firststep/">Tez编译及使用</a></li>
<li><a href="/blog/2016/01/12/tez-ui-config-and-run/">配置TEZ-UI</a></li>
</ul>
</li>
<li>hive on spark

<ul>
<li><a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a></li>
<li><a href="/blog/2016/04/08/snappy-centos5-on-hive-on-spark/">Hive-on-spark Snappy on Centos5</a></li>
<li><a href="/blog/2016/03/29/limit-on-sparksql-and-hive/">Limit on Sparksql and Hive</a></li>
</ul>
</li>
<li><a href="/blog/2016/04/08/dbcp-parameters/">DBCP参数在Hive JDBC上的实践</a></li>
<li><a href="/blog/2016/04/13/hiveserver2-ui-and-upgrade-hive2-dot-0-0/">Hiveserver2 Ui and Upgrade hive2.0.0</a></li>
</ul>
</li>
<li>kafka

<ul>
<li><a href="/blog/2015/01/08/kafka-guide/">Kafka快速入门</a></li>
</ul>
</li>
<li>alluxio(tachyon)

<ul>
<li><a href="/blog/2015/04/15/tachyon-quickstart/">Tachyon入门指南</a></li>
<li><a href="/blog/2015/04/18/tachyon-deep-source/">Tachyon剖析</a></li>
<li><a href="/blog/2016/04/15/alluxio-quickstart2/">Alluxio入门大全2</a></li>
</ul>
</li>
</ul>


<h2>5. 监控与自动化部署</h2>

<h4>监控</h4>

<ul>
<li><a href="/blog/2013/02/26/linux-top-command-mannual/">top</a></li>
<li><del>nagios</del>

<ul>
<li><a href="/blog/2015/09/25/nagios-start-guide/">Nagios监控主机</a></li>
</ul>
</li>
<li><del>cacti</del>    Ganglia更简单

<ul>
<li><a href="/blog/2015/09/22/cacti-start-guide/"><del>Cacti监控主机</del></a></li>
<li><a href="/blog/2015/10/13/cacti-batch-adding-configurations/"><del>Cacti批量添加配置</del></a></li>
</ul>
</li>
<li>ganglia

<ul>
<li><a href="/blog/2014/07/18/install-ganglia-on-redhat/"><del>Install Ganglia on Redhat5+</del></a> 手动安装依赖太麻烦了！</li>
<li><a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a></li>
<li><a href="/blog/2016/06/17/ganglia-install-on-centos-with-puppet/">使用Puppet安装配置Ganglia</a></li>
<li><a href="/blog/2016/02/01/ganglia-python-extension/">Ganglia扩展-Python</a></li>
<li><a href="/blog/2016/02/25/ganglia-web-ui-views/">Ganglia页自定义视图</a></li>
</ul>
</li>
</ul>


<h4>自动化</h4>

<ul>
<li>git:

<ul>
<li><a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a></li>
<li><a href="/blog/2014/02/19/maven-package-dependent-git-projects/">打包依赖的git项目</a></li>
<li><a href="/blog/2013/05/27/handle-git-conflict/">处理git冲突</a></li>
</ul>
</li>
<li><a href="/blog/2014/09/07/expect-automate-and-batch-config-ssh/">expect-批量实现SSH无密钥登录</a></li>
<li>puppet

<ul>
<li><a href="/blog/2016/04/08/puppet-install/">puppet4.4.1入门安装</a></li>
<li><a href="/blog/2016/04/21/puppet-domain-fdqn/">puppet入门之域名证书</a></li>
<li><a href="/blog/2016/04/21/puppetdb-install-and-config/">puppetdb安装配置</a>

<ul>
<li><a href="/blog/2015/12/13/postgresql-start-guide/">postgresql入门</a></li>
</ul>
</li>
<li>puppet-ui

<ul>
<li><a href="/blog/2016/05/05/puppetboard-install/">puppetboard安装</a></li>
<li><a href="/blog/2016/04/21/puppetexplorer-setting/">puppetexplorer设置</a></li>
<li>foreman</li>
</ul>
</li>
<li><a href="/blog/2016/04/04/rpm-build-your-package/">RPM打包</a></li>
<li>puppet基本使用以及配置集群</li>
<li>mcollective

<ul>
<li><a href="/blog/2016/04/28/mcollective-quick-start/">安装配置</a></li>
<li><a href="/blog/2016/04/28/mcollective-plugins/">插件安装</a></li>
</ul>
</li>
<li><a href="/blog/2016/05/03/hiera-and-facts/">Hiera</a></li>
</ul>
</li>
</ul>


<p>&hellip;</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark-on-yarn内存分配]]></title>
    <link href="http://winseliu.com/blog/2016/04/11/spark-on-yarn-memory-allocate/"/>
    <updated>2016-04-11T19:44:51+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/11/spark-on-yarn-memory-allocate</id>
    <content type="html"><![CDATA[<p>上次写了一篇关于配置参数是如何影响mapreduce的实际调度的<a href="/blog/2016/03/17/hadoop-memory-opts-and-args/">参考</a>：</p>

<ul>
<li>opts（yarn.app.mapreduce.am.command-opts、mapreduce.map.java.opts、mapreduce.reduce.java.opts）是实际运行程序是内存参数。</li>
<li>memory（yarn.app.mapreduce.am.resource.mb、mapreduce.map.memory.mb、mapreduce.reduce.memory.mb）是用于ResourceManager计算集群资源使用和调度。</li>
</ul>


<p>了解参数区别，就没有再深究task内存的问题了。</p>

<h2>新问题-内存分配</h2>

<p>这次又遇到内存问题：spark使用yarn-client的方式运行时，spark有memoryOverhead的设置，但是加了额外的内存后，再经过集群调度内存浪费严重，对于本来就小内存的集群来说完全无法接受。</p>

<ul>
<li>am默认是512加上384 overhead，也就是896m。但是调度后am分配内存资源为1024。</li>
<li>executor默认是1024加上384，等于1408M。单调度后executor分配内存资源为2048。</li>
</ul>


<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-0.png" alt="" /></p>

<p>从appmaster的日志可以看出来请求的内存大小是1408：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-1.png" alt="" /></p>

<p><strong>一个executor就浪费了500M，本来可以跑4个executor的但现在只能执行3个！</strong></p>

<p>关于内存参数的具体含义查看官网： <a href="http://spark.apache.org/docs/latest/running-on-yarn.html">spark-on-yarn</a> 和 <a href="http://hadoop.apache.org/docs/r2.6.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a></p>

<table>
<thead>
<tr>
<th></th>
<th style="text-align:center;"> <em>参数</em>                                  </th>
<th></th>
<th style="text-align:left;"> <em>值</em></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.am.memory                    </td>
<td></td>
<td style="text-align:left;"> 512m</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.driver.memory                     </td>
<td></td>
<td style="text-align:left;"> 1g</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.executor.memoryOverhead      </td>
<td></td>
<td style="text-align:left;"> executorMemory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.driver.memoryOverhead        </td>
<td></td>
<td style="text-align:left;"> driverMemory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.am.memoryOverhead            </td>
<td></td>
<td style="text-align:left;"> AM memory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.nodemanager.resource.memory-mb     </td>
<td></td>
<td style="text-align:left;"> 8192</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.scheduler.minimum-allocation-mb    </td>
<td></td>
<td style="text-align:left;"> 1024</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.scheduler.maximum-allocation-mb    </td>
<td></td>
<td style="text-align:left;"> 8192</td>
</tr>
</tbody>
</table>


<p>分配的内存看着像是 <strong>最小分配内存</strong> 的整数倍。把 <code>yarn.scheduler.minimum-allocation-mb</code> 修改为512，重启yarn再运行，executor的分配的内存果真减少到1536(<strong>512*3</strong>)。</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-3.png" alt="" /></p>

<p>同时 <a href="http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html">http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html</a> 这篇文章也讲 <strong>在YARN中，Container申请的内存大小必须为yarn.scheduler.minimum-allocation-mb的整数倍</strong> 。我们不去猜，调试下调度代码，看看究竟是什么情况。</p>

<pre><code>[hadoop@cu2 hadoop-2.6.3]$ sbin/yarn-daemon.sh stop resourcemanager 

[hadoop@cu2 hadoop]$ grep "minimum-allocation-mb" -1 yarn-site.xml 
&lt;property&gt;
&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;&lt;value&gt;512&lt;/value&gt;
&lt;/property&gt;

[hadoop@cu2 hadoop-2.6.3]$ export YARN_RESOURCEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8000"
[hadoop@cu2 hadoop-2.6.3]$ sbin/yarn-daemon.sh start resourcemanager 
</code></pre>

<p>本地eclipse在 <code>CapacityScheduler#allocate</code> 打断点，然后跑任务：</p>

<pre><code>hive&gt; set hive.execution.engine=spark;
hive&gt; select count(*) from t_ods_access_log2 where month=201512;
</code></pre>

<p>AppMaster内存分配：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-appmaster.png" alt="" /></p>

<p>Executor内存分配：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-executor.png" alt="" /></p>

<p>request进到allocate后，最终调用 <code>DefaultResourceCalculator.normalize</code> 重新计算了一遍请求需要的资源，把内存调整了。默认的DefaultResourceCalculator可以通过 capacity-scheduler.xml 的 <code>yarn.scheduler.capacity.resource-calculator</code> 来修改。</p>

<p>具体代码调度过程如下：</p>

<pre><code>  public Allocation allocate(ApplicationAttemptId applicationAttemptId,
      List&lt;ResourceRequest&gt; ask, List&lt;ContainerId&gt; release, 
      List&lt;String&gt; blacklistAdditions, List&lt;String&gt; blacklistRemovals) {
    ...
    // Sanity check
    SchedulerUtils.normalizeRequests(
        ask, getResourceCalculator(), getClusterResource(),
        getMinimumResourceCapability(), maximumAllocation);
...

  public static void normalizeRequest(
      ResourceRequest ask, 
      ResourceCalculator resourceCalculator, 
      Resource clusterResource,
      Resource minimumResource,
      Resource maximumResource,
      Resource incrementResource) {
    Resource normalized = 
        Resources.normalize(
            resourceCalculator, ask.getCapability(), minimumResource,
            maximumResource, incrementResource);
    ask.setCapability(normalized);
  } 
...

  public static Resource normalize(
      ResourceCalculator calculator, Resource lhs, Resource min,
      Resource max, Resource increment) {
    return calculator.normalize(lhs, min, max, increment);
  }
...

  public Resource normalize(Resource r, Resource minimumResource,
      Resource maximumResource, Resource stepFactor) {
    int normalizedMemory = Math.min(
        roundUp(
            Math.max(r.getMemory(), minimumResource.getMemory()),
            stepFactor.getMemory()),
            maximumResource.getMemory());
    return Resources.createResource(normalizedMemory);
  }
...

  public static int roundUp(int a, int b) {
    return divideAndCeil(a, b) * b;
  }
</code></pre>

<h2>小结</h2>

<p>今天又重新认识一个yarn参数 <code>yarn.scheduler.minimum-allocation-mb</code> ，不仅仅是最小分配的内存，同时分配的资源也是minimum-allocation-mb的整数倍，还告诉我们 <code>yarn.nodemanager.resource.memory-mb</code> 也最好是minimum-allocation-mb的整数倍。</p>

<p>间接的学习了新的参数，可以通过 <code>yarn.scheduler.capacity.resource-calculator</code> 参数 来修改 CapacityScheduler 调度器的资源计算类。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
