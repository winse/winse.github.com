<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Winse Blog]]></title>
  <link href="http://winse.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://winse.github.io/"/>
  <updated>2014-09-03T16:34:19+08:00</updated>
  <id>http://winse.github.io/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Mapreduce输入输出压缩]]></title>
    <link href="http://winse.github.io/blog/2014/09/01/hadoop2-mapreduce-compress/"/>
    <updated>2014-09-01T16:05:13+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/01/hadoop2-mapreduce-compress</id>
    <content type="html"><![CDATA[<p>当数据达到一定量时，自然就想到了对数据进行压缩来降低存储压力。在Hadoop的任务中提供了5个参数来控制输入输出的数据的压缩格式。添加map输出数据压缩可以降低集群间的网络传输，最终reduce输出压缩可以减低hdfs的集群存储空间。</p>

<p>如果是使用hive等工具的话，效果会更加明显。因为hive的查询结果是临时存储在hdfs中，然后再通过一个<strong>Fetch Operator</strong>来获取数据，最后清理掉，压缩存储临时的数据可以减少磁盘的读写。</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;Should the job outputs be compressed?
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.output.fileoutputformat.compress.type&lt;/name&gt;
  &lt;value&gt;RECORD&lt;/value&gt;
  &lt;description&gt;If the job outputs are to compressed as SequenceFiles, how should
               they be compressed? Should be one of NONE, RECORD or BLOCK.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;
  &lt;description&gt;If the job outputs are compressed, how should they be compressed?
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;Should the outputs of the maps be compressed before being
               sent across the network. Uses SequenceFile compression.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;
  &lt;description&gt;If the map outputs are compressed, how should they be 
               compressed?
  &lt;/description&gt;
&lt;/property&gt;
</code></pre>

<p>上面5个属性弄好，在core-sitem.xml加下<code>io.compression.codecs</code>基本就完成配置了。</p>

<p>这里主要探究下mapreduce（下面全部简称MR）过程中自动解压缩。刚刚接触Hadoop一般都不会去了解什么压缩不压缩的，先把hdfs-api，MR-api弄一遭。配置的TextInputFormat竟然能正确的读取tar.gz文件的内容，觉得不可思议，TextInputFormat不是直接读取txt行记录的输入嘛？难道还能读取压缩文件，先解压再&hellip;？？</p>

<p>先说下OutputFormat，在MR中调用context.write写入数据的方法时，最终使用OutputFormat创建的RecordWriter进行持久化。在TextOutputFormat创建RecordWriter时，如果使用压缩会在结果文件名上<strong>加对应压缩库的后缀</strong>，如gzip压缩对应的后缀gz、snappy压缩对应后缀snappy等。对应下面代码的<code>getDefaultWorkFile</code>。</p>

<p><img src="http://file.bmob.cn/M00/0B/03/wKhkA1QEjD2ASHiZAAExjXuQ25Y062.png" alt="" /></p>

<p>同样对应的TextInputFormat的RecordReader也进行类似的处理：根据<strong>文件的后缀</strong>来判定该文件是否使用压缩，并使用对应的输入流InputStream来解码。</p>

<p><img src="http://file.bmob.cn/M00/0B/03/wKhkA1QEjZaAdBeJAAEvRVMKVWY059.png" alt="" /></p>

<p>此处的关键代码为<code>CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);</code>，根据分块（split）的文件名来判断使用的压缩算法。
初始化Codec实现、以及根据文件名来获取压缩算法的实现还是挺有意思的：通过反转字符串然后最近匹配（headMap）来获取对应的结果。</p>

<pre><code>  private void addCodec(CompressionCodec codec) {
    String suffix = codec.getDefaultExtension();
    codecs.put(new StringBuilder(suffix).reverse().toString(), codec);
    codecsByClassName.put(codec.getClass().getCanonicalName(), codec);

    String codecName = codec.getClass().getSimpleName();
    codecsByName.put(codecName.toLowerCase(), codec);
    if (codecName.endsWith("Codec")) {
      codecName = codecName.substring(0, codecName.length() - "Codec".length());
      codecsByName.put(codecName.toLowerCase(), codec);
    }
  }

  public CompressionCodec getCodec(Path file) {
    CompressionCodec result = null;
    if (codecs != null) {
      String filename = file.getName();
      String reversedFilename = new StringBuilder(filename).reverse().toString();
      SortedMap&lt;String, CompressionCodec&gt; subMap = 
        codecs.headMap(reversedFilename);
      if (!subMap.isEmpty()) {
        String potentialSuffix = subMap.lastKey();
        if (reversedFilename.startsWith(potentialSuffix)) {
          result = codecs.get(potentialSuffix);
        }
      }
    }
    return result;
  }
</code></pre>

<p>了解了这些，MR（TextInputFormat）的输入文件可以比较随意些：各种压缩文件、原始文件都可以，只要文件有对应压缩算法的后缀即可。hive的解压缩功能也很容易了，如果使用hive存储text形式的文件，进行压缩无需进行额外的程序代码修改，仅仅修改MR的配置即可，注意下<strong>文件后缀</strong>！！</p>

<p>如，在MR中生成了snappy压缩的文件，此时<strong>不能</strong>在文件的后面添加东西。否则在hive查询时，根据<strong>后缀</strong>进行解压会导致结果乱码/不正确。</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;hive.exec.compress.output&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt; This controls whether the final outputs of a query (to a local/hdfs file or a hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.exec.compress.intermediate&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt; This controls whether intermediate files produced by hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* &lt;/description&gt;
&lt;/property&gt;
</code></pre>

<p>hive也弄了两个参数来控制它自己的MR的输出输入压缩控制属性。其他的配置使用mapred-site.xml的配置即可。</p>

<p><img src="http://file.bmob.cn/M00/0B/1D/wKhkA1QFQLmAfyZSAAIOx4UEIbY016.png" alt="" /></p>

<p>网上一些资料有<code>hive.intermediate.compression.codec</code>和<code>hive.intermediate.compression.type</code>两个参数能调整中间过程的压缩算法。其实和mapreduce的参数功能是一样的。</p>

<p><img src="http://file.bmob.cn/M00/0B/1F/wKhkA1QFQWyAUDMLAAGyNqR_X-c417.png" alt="" /></p>

<p>附上解压缩的全部配置：</p>

<pre><code>$#core-site.xml
    &lt;property&gt;
        &lt;name&gt;io.compression.codecs&lt;/name&gt;
        &lt;value&gt;
    org.apache.hadoop.io.compress.GzipCodec,
    org.apache.hadoop.io.compress.DefaultCodec,
    org.apache.hadoop.io.compress.BZip2Codec,
    org.apache.hadoop.io.compress.SnappyCodec
        &lt;/value&gt;
    &lt;/property&gt;

$#mapred-site.xml
    &lt;property&gt;
        &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; 
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapred.output.compression.codec&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
    &lt;/property&gt;

$#hive-site.xml
    &lt;property&gt;
        &lt;name&gt;hive.exec.compress.output&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<p>运行hive后，临时存储在HDFS的结果数据，注意文件的后缀。</p>

<p><img src="http://file.bmob.cn/M00/0B/20/wKhkA1QFRjSACnLfAABVdoK0f1c803.png" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://www.geek521.com/?p=4814">深入学习《Programing Hive》：数据压缩</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop的datanode数据节点软/硬件配置应该一致]]></title>
    <link href="http://winse.github.io/blog/2014/08/02/hadoop-datanode-config-should-equals/"/>
    <updated>2014-08-02T22:21:12+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/02/hadoop-datanode-config-should-equals</id>
    <content type="html"><![CDATA[<p>最好的就是集群的所有的datanode的节点的<strong>硬件配置一样</strong>！当然系统时间也的一致，hosts等等这些。机器配置一样时可以使用脚本进行批量处理，给维护带来很大的便利性。</p>

<p>今天收到运维的信息，说集群的一台机器硬盘爆了！上到环境查看<code>df -h</code>发现硬盘配置和其他datanode的不同！但是hadoop hdfs-site.xml的<code>dfs.datanode.data.dir</code>却是一样的！</p>

<p>经验： dir的配置应该是一个系统设备对应一个路径，而不是一个系统目录对应dir的一个路径！</p>

<h2>问题现象以及根源</h2>

<p>问题机器A的磁盘情况：</p>

<pre><code>[hadoop@hadoop-slaver8 ~]$ df -h
文件系统              容量  已用  可用 已用%% 挂载点
/dev/sda3             2.7T  2.5T   53G  98% /
tmpfs                  32G  260K   32G   1% /dev/shm
/dev/sda1              97M   32M   61M  35% /boot

[hadoop@hadoop-slaver8 /]$ ll
总用量 170
dr-xr-xr-x.   2 root   root    4096 2月  12 19:39 bin
dr-xr-xr-x.   5 root   root    1024 2月  13 02:40 boot
drwxr-xr-x.   2 root   root    4096 2月  23 2012 cgroup
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data1
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data10
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data11
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data12
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data13
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data14
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data15
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data2
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data3
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data4
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data5
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data6
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data7
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data8
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data9
</code></pre>

<p>再看集群其他机器：</p>

<pre><code>[hadoop@hadoop-slaver1 ~]$ df -h
文件系统              容量  已用  可用 已用%% 挂载点
/dev/sda3             2.7T   32G  2.5T   2% /
tmpfs                  32G   88K   32G   1% /dev/shm
/dev/sda1              97M   32M   61M  35% /boot
/dev/sdb1             1.8T  495G  1.3T  29% /data1
/dev/sdb2             1.8T  485G  1.3T  28% /data2
/dev/sdb3             1.8T  492G  1.3T  29% /data3
/dev/sdb4             1.8T  488G  1.3T  28% /data4
/dev/sdb5             1.8T  486G  1.3T  28% /data5
/dev/sdb6             1.8T  480G  1.3T  28% /data6
/dev/sdb7             1.8T  479G  1.3T  28% /data7
/dev/sdb8             1.8T  474G  1.3T  28% /data8
/dev/sdb9             1.8T  480G  1.3T  28% /data9
/dev/sdb10            1.8T  478G  1.3T  28% /data10
/dev/sdb11            1.8T  475G  1.3T  28% /data11
/dev/sdb12            1.8T  489G  1.3T  29% /data12
/dev/sdb13            1.8T  475G  1.3T  28% /data13
/dev/sdb14            1.8T  476G  1.3T  28% /data14
/dev/sdb15            1.8T  469G  1.3T  27% /data15
</code></pre>

<p>出问题机器没有挂存储，仅仅是建立了对应的目录结构，并不是把目录挂载到单独的存储设备上。</p>

<p>同时查看50070的前面的信息，hadoop把每个逗号分隔后的路径默认都做一个磁盘设备来计算！</p>

<pre><code>Node              Address             ..Admin State CC    Used  NU    RU(%) R(%)      Blocks Block  Pool Used Block Pool Used (%)
hadoop-slaver1  192.168.32.21:50010 2   In Service  26.86   7.05    1.37    18.44   26.25       68.66   264844  7.05    26.25   
hadoop-slaver8  192.168.32.28:50010 1   In Service  37.94   2.46    34.71   0.77    6.48        2.03    29637   2.46    6.48    
</code></pre>

<p>配置容量是所有配置的路径所在盘容量的<strong>累加</strong>。总的剩余空间（余量）也是各个dir配置路径的剩余空间<strong>累加</strong>的！这样很容易出现问题！
最好的就是集群的所有的datanode的节点的<strong>硬件配置一样</strong>！当然系统时间也的一致，hosts等等这些。</p>

<h2>问题处理</h2>

<p>首先得把问题解决啊：</p>

<ul>
<li>把<code>dfs.datanode.data.dir</code>路径个数调整为磁盘个数！</li>
<li>修改该datanode的hdfs-site的配置，添加<code>dfs.datanode.du.reserved</code>，留给系统的空间设置为400多G。</li>
<li>冗余份数也没有必要3份，浪费空间。如果两台机器同时出现问题，还是同一份数据，那只能说是天意！你可以去趟澳门赌一圈了！</li>
</ul>


<pre><code>&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&lt;value&gt;/data1/hadoop/data&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;
&lt;value&gt;437438953472&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;2&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>设置了reserved保留空间后，再看LIVE页面slaver8的容量变少了且正好等于(盘的容量2.7T-430G~=2.26T 计算容量的hdfs源码在<code>FsVolumeImpl.getCapacity()</code>)。</p>

<pre><code>hadoop-slaver8  192.168.32.28:50010 1   In Service  2.26    2.23    0.00    0.03    98.66
</code></pre>

<p>datanode和blockpool的平衡处理，可以参考<a href="http://hadoop-master1:50070/dfsnodelist.jsp?whatNodes=LIVE">Live Datanodes</a>的容量和进行！</p>

<pre><code>[hadoop@hadoop-master1 ~]$ hdfs balancer -help
Usage: java Balancer
        [-policy &lt;policy&gt;]      the balancing policy: datanode or blockpool
        [-threshold &lt;threshold&gt;]        Percentage of disk capacity

[hadoop@hadoop-slaver8 ~]$ hadoop-2.2.0/bin/hdfs getconf -confkey dfs.datanode.du.reserved
137438953472
</code></pre>

<p>删除一些没用的备份数据。配置好以后，重启当前slaver8节点，并进行数据平衡（如果觉得麻烦，直接丢掉原来的一个目录下的数据也行，可能更快！均衡器运行的太慢！！）</p>

<pre><code>[hadoop@hadoop-slaver8 ~]$  ~/hadoop-2.2.0/sbin/hadoop-daemon.sh stop datanode

[hadoop@hadoop-slaver8 ~]$  for i in 6 7 8 9 10 11 12 13 14 15; do  cd /data$i/hadoop/data/current/BP-1414312971-192.168.32.11-1392479369615/current/finalized;  find . -type f -exec mv {} /data1/hadoop/data/current/BP-1414312971-192.168.32.11-1392479369615/current/finalized/{} \;; done

[hadoop@hadoop-slaver8 ~]$  ~/hadoop-2.2.0/sbin/hadoop-daemon.sh start datanode


[hadoop@hadoop-master1 ~]$ hdfs dfsadmin -setBalancerBandwidth 10485760
[hadoop@hadoop-master1 ~]$ hdfs balancer -threshold 60
</code></pre>

<p>查看datanode的日志，由于移动数据，有些blk的id一样，会清理一些数据。对于均衡器程序的阀值越小集群越平衡！默认是10（%），会移动很多的数据（准备看下均衡器的源码，了解各个参数以及运行的逻辑）！</p>

<h2>参考</h2>

<ul>
<li><a href="http://blog.csdn.net/lingzihan1215/article/details/8700532">hadoop的datanode多磁盘空间处理</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Snappy Compress]]></title>
    <link href="http://winse.github.io/blog/2014/07/30/hadoop2-snappy-compress/"/>
    <updated>2014-07-30T00:25:39+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/30/hadoop2-snappy-compress</id>
    <content type="html"><![CDATA[<p>网上查了很多资料说的很复杂，要叉叉叉叉叉！其实hadoop2已经集成了hadoop-snappy，只要安装snappy即可。但是也没有一些文章说的只要编译snappy然后放到lib/native路径下即可，还需要重新编译libhadoop的library包。</p>

<p>查找hadoop-snappy的源码的时刻，在C代码里面定义了<code>HADOOP_SNAPPY_LIBRARY</code>，然后理着这个思路去查找，发现在CMakeFile文件中也定义了对应的变量，然后再查找pom.xml的native profile中定义了snappy.prefix的属性。最后就有了下面的步骤。</p>

<p>1) build snappy</p>

<p>编译Snappy，并把lib拷贝/同步到hadoop的native目录下。</p>

<pre><code>tar zxf snappy-1.1.1.tar.gz 
cd snappy-1.1.1
./configure --prefix=/home/hadoop/snappy
make
make install

cd snappy
cd lib/
rysnc -vaz * ~/hadoop-2.2.0/lib/native/
</code></pre>

<p>2) rebuild hadoop common project</p>

<p>重新编译hadoop的lib，覆盖原来的文件。</p>

<pre><code>[hadoop@master1 hadoop-common]$ mvn package -Dmaven.javadoc.skip=true -DskipTests -Dsnappy.prefix=/home/hadoop/snappy -Drequire.snappy=true -Pnative 

[hadoop@master1 hadoop-common]$ cd ~/hadoop-2.2.0-src/hadoop-common-project/hadoop-common/
[hadoop@master1 hadoop-common]$ cd target/native/target/usr/local/lib/
[hadoop@master1 lib]$ ll
total 1252
-rw-rw-r--. 1 hadoop hadoop 820824 Jul 30 00:18 libhadoop.a
lrwxrwxrwx. 1 hadoop hadoop     18 Jul 30 00:18 libhadoop.so -&gt; libhadoop.so.1.0.0
-rwxrwxr-x. 1 hadoop hadoop 455542 Jul 30 00:18 libhadoop.so.1.0.0
[hadoop@master1 lib]$ rsync -vaz * ~/hadoop-2.2.0/lib/native/
sending incremental file list
libhadoop.a
libhadoop.so.1.0.0

sent 409348 bytes  received 53 bytes  818802.00 bytes/sec
total size is 1276384  speedup is 3.12
[hadoop@master1 lib]$ 
</code></pre>

<p>3) check</p>

<p>检查程序snappy是否已经配置成功</p>

<pre><code>[hadoop@master1 ~]$ hadoop checknative -a
14/07/30 00:22:14 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
14/07/30 00:22:14 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library
Native library checking:
hadoop: true /home/hadoop/hadoop-2.2.0/lib/native/libhadoop.so.1.0.0
zlib:   true /lib64/libz.so.1
snappy: true /home/hadoop/hadoop-2.2.0/lib/native/libsnappy.so.1
lz4:    true revision:43
bzip2:  false 
14/07/30 00:22:14 INFO util.ExitUtil: Exiting with status 1
[hadoop@master1 ~]$ 
</code></pre>

<p>4) 跑一个压缩程序</p>

<p>先参考网上的，直接用hbase的带的测试类运行（前提：需要在hbase-env.sh中配置HADOOP_HOME，这样hbase才能找到hadoop下的lib本地库）</p>

<pre><code>[hadoop@master1 ~]$ hbase-0.98.3-hadoop2/bin/hbase org.apache.hadoop.hbase.util.CompressionTest file:///tmp/abc.snappy snappy
2014-07-30 08:50:42,617 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/hadoop/hbase-0.98.3-hadoop2/lib/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2014-07-30 08:50:44,515 INFO  [main] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-30 08:50:44,522 INFO  [main] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
2014-07-30 08:50:45,388 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
2014-07-30 08:50:45,408 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
2014-07-30 08:50:45,430 ERROR [main] hbase.KeyValue: Unexpected getShortMidpointKey result, fakeKey:testkey, firstKeyInBlock:testkey
2014-07-30 08:50:47,088 INFO  [main] compress.CodecPool: Got brand-new decompressor [.snappy]
SUCCESS
[hadoop@master1 ~]$ 
</code></pre>

<p>看到最后的<strong>SUCCESS</strong>就说明安装配置成功了！</p>

<p>接下来自己写程序测试压缩/解压缩，首先编写java类：</p>

<pre><code>import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionInputStream;
import org.apache.hadoop.io.compress.CompressionOutputStream;
import org.apache.hadoop.io.compress.SnappyCodec;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.zookeeper.common.IOUtils;

public class SnappyCompressTest {

        public static void main(String[] args) throws FileNotFoundException, IOException {
                try {
                        execute(args);
                } catch (Exception e) {
                        System.out.println("Usage: $0 read|write file[.snappy]");
                }
        }

        private static void execute(String[] args) throws FileNotFoundException, IOException {
                String op = args[0];
                String file = args[1];
                String snappyFile = file + ".snappy";

                Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
                CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());

                if (StringUtils.equalsIgnoreCase(op, "read")) {
                        FileInputStream fin = new FileInputStream(snappyFile);
                        CompressionInputStream in = codec.createInputStream(fin);
                        FileOutputStream fout = new FileOutputStream(file);
                        IOUtils.copyBytes(in, fout, 4096, true);
                } else {
                        FileInputStream fin = new FileInputStream(file);
                        CompressionOutputStream out = codec.createOutputStream(new FileOutputStream(snappyFile));
                        IOUtils.copyBytes(fin, out, 4096, true);
                }
        }

}
</code></pre>

<p>编译运行，测试读写功能。使用hadoop命令可以简化很多工作，把当前路径加入到<code>HADOOP_CLASSPATH</code>。</p>

<pre><code>[hadoop@master1 test]$ javac -cp `hadoop classpath` SnappyCompressTest.java 
[hadoop@master1 test]$ export HADOOP_CLASSPATH=$PWD
[hadoop@master1 test]$ hadoop SnappyCompressTest 
Usage: $0 read|write file[.snappy]
[hadoop@master1 test]$ hadoop SnappyCompressTest write test.txt 
[hadoop@master1 test]$ ll
total 16
-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:23 test.txt
-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
[hadoop@master1 test]$ rm test.txt
[hadoop@master1 test]$ hadoop SnappyCompressTest read test.txt 
[hadoop@master1 test]$ ll
total 16
-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:28 test.txt
-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
[hadoop@master1 test]$ cat test.txt
abc
abc
abc
</code></pre>

<p>5) hbase中添加压缩</p>

<p>把所有library，以及hbase的配置同步其他所有从节点。对hbase的表使用Snappy压缩。</p>

<pre><code>hbase(main):001:0&gt; create 'st1', 'f1'
hbase(main):005:0&gt; alter 'st1', {NAME=&gt;'f1', COMPRESSION=&gt;'snappy'}
Updating all regions with the new schema...
0/1 regions updated.
1/1 regions updated.
Done.
0 row(s) in 2.7880 seconds

hbase(main):010:0&gt; create 'sst1','f1'
0 row(s) in 0.5730 seconds

=&gt; Hbase::Table - sst1
hbase(main):011:0&gt; flush 'sst1'
0 row(s) in 2.5380 seconds

hbase(main):012:0&gt; flush 'st1'
0 row(s) in 7.5470 seconds
</code></pre>

<p>对于hbase来说，使用压缩消耗还是挺大的。插入10w数据中间进行compaction时停顿比较久。最后flush写数据的时间也长了很多！
下面是文件写入后的文件大小对比（由于是进行简单的测试，写入的数据重复比较多。具体比例没有参考价值）：</p>

<p><img src="http://file.bmob.cn/M00/05/5A/wKhkA1PYz9CAB-TdAAEWX8LGpUo149.png" alt="" /></p>

<p>6) 正式环境下解压snappy文件</p>

<pre><code>
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionInputStream;
import org.apache.hadoop.io.compress.SnappyCodec;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.zookeeper.common.IOUtils;

public class DecompressTest {
    public static void main(String[] args) throws IOException {

        Configuration conf = new Configuration();
        Path path = new Path(args[0]);
        FileSystem fs = path.getFileSystem(conf);

        Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
        CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());

        InputStream fin = fs.open(path);
        CompressionInputStream in = codec.createInputStream(fin);

        IOUtils.copyBytes(in, System.out, 4096, true);

        fin.close();

        System.out.println("SUCCESS");

    }
}

// build &amp; run

&gt;DecompressTest.java 
vi DecompressTest.java 
javac -cp `hadoop classpath`  DecompressTest.java 
export HADOOP_CLASSPATH=.
# snappyfile on hdfs
hadoop DecompressTest /user/hive/t_ods_access_log2/month=201408/day=20140828/hour=2014082808/t_ods_access_log2-2014082808.our.snappy.1409187524328
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 ShortCircuit Local Reading]]></title>
    <link href="http://winse.github.io/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading/"/>
    <updated>2014-07-29T20:11:58+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading</id>
    <content type="html"><![CDATA[<p>hadoop一直以来认为是本地读写文件的，但是其实也是通过TCP端口去获取数据，只是都在同一台机器。在hivetuning调优hive的文档中看到了ShortCircuit的HDFS配置属性，查看了ShortCircuit的来由，真正的实现了本地读取文件。蒙查查表示看的不是很明白，最终大致就是通过linux的<strong>文件描述符</strong>来实现功能同时保证文件的权限。</p>

<p>由于仅在自己的机器上面配置来查询hbase的数据，性能方面提升感觉不是很明显。等以后整到正式环境再对比对比。</p>

<p>配置如下。</p>

<p>1 修改hdfs-site.xml</p>

<pre><code>&lt;property&gt;
        &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
        &lt;value&gt;/home/hadoop/data/sockets/dn_socket&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>注意：socket路径的权限控制的比较严格。dn_socket<strong>所有的父路径</strong>要么仅有当前启动用户的写权限，要么仅root可写。</p>

<p><img src="http://file.bmob.cn/M00/05/52/wKhkA1PXfbKANLOrAADWJQ5taVs391.png" alt="" /></p>

<p>2 修改hbase的配置，并添加HADOOP_HOME（hbase查找hadoop-native）</p>

<p><img src="http://file.bmob.cn/M00/05/52/wKhkA1PXhRKAZDs6AAChrEauBoU738.png" alt="" /></p>

<p>hbase的脚本找到hadoop命令后，会把hadoop的java.library.path的路径加入到hbase的启动脚本中。</p>

<pre><code>[hadoop@master1 ~]$ tail -15 hbase-0.98.3-hadoop2/conf/hbase-site.xml 
    &lt;name&gt;hbase.tmp.dir&lt;/name&gt;
    &lt;value&gt;/home/hadoop/data/hbase&lt;/value&gt;
  &lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
        &lt;value&gt;/home/hadoop/data/sockets/dn_socket&lt;/value&gt;
&lt;/property&gt;

&lt;/configuration&gt;

[hadoop@master1 ~]$ cat hbase-0.98.3-hadoop2/conf/hbase-env.sh
...
export HADOOP_HOME=/home/hadoop/hadoop-2.2.0
...
</code></pre>

<p>3 同步到其他节点，然后重启hdfs,hbase</p>

<h2>参考</h2>

<ul>
<li><a href="http://vdisk.weibo.com/s/z_44nz36hNM3Z">hive-tuning</a></li>
<li><a href="http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/">How Improved Short-Circuit Local Reads Bring Better Performance and Security to Hadoop</a></li>
<li><a href="http://hbase.apache.org/book/perf.hdfs.html">HDFS&ndash;Apache HBase Performance Tuning</a></li>
<li><a href="http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">HDFS Short-Circuit Local Reads</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop安全的关闭datanode节点]]></title>
    <link href="http://winse.github.io/blog/2014/07/29/safely-remove-datanode/"/>
    <updated>2014-07-29T15:08:41+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/29/safely-remove-datanode</id>
    <content type="html"><![CDATA[<p>hadoop默认就有冗余（dfs.replication）的机制，所以一般情况下，一台机器挂了也没所谓。集群会自动的进行复制均衡处理。</p>

<p>作为测试，如果dfs.replication设置为1的情况下，怎么安全的把datanode节点服务关闭呢？例如说，刚刚开始搭建环境是把namenode、datanode放在一台机器上，后面增加了机器如何把datanode分离出来呢？</p>

<p>借助于<strong>dfs.hosts.exclude</strong>即可完成顺序的完成此项任务。</p>

<p>修改hdfs-site.xml配置。我操作的时刻仅修改了master1上的hdfs-site.xml。把<strong>master1</strong>值写入到对应的文件中。</p>

<pre><code>    [hadoop@master1 hadoop]$ cat hdfs-site.xml 
    ...
    &lt;property&gt;
            &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;
            &lt;value&gt;/home/hadoop/hadoop-2.2.0/etc/hadoop/exclude&lt;/value&gt;
    &lt;/property&gt;

    &lt;/configuration&gt;
    [hadoop@master1 hadoop]$ cat /home/hadoop/hadoop-2.2.0/etc/hadoop/exclude
    master1
</code></pre>

<p>修改完成后，刷新节点即可(完全没有必要重启集dfs)。</p>

<pre><code>hadoop dfsadmin -refreshNodes
</code></pre>

<p>可以通过<code>dfsadmin -report</code>或者网页查看master1已经变成<em>Decommission In Progress</em>了。</p>

<p><img src="http://file.bmob.cn/M00/05/4C/wKhkA1PXUMOAVvvWAAED6CN-3Rg187.png" alt="" /></p>

<p>注：</p>

<p>问题一： 在新建节点是slaver1的防火墙没关闭，由于master1已经被exclude，而slaver1不能提供服务，上传文件时报错：</p>

<pre><code>[hadoop@master1 hadoop]$ hadoop fs -put slaves  /
14/07/29 15:18:21 WARN hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /slaves._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1384)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2477)
</code></pre>

<p>关闭防火墙一样再次上传，还是报同样的错误。此时，也可以通过刷新节点<code>hadoop dfsadmin -refreshNodes</code>来解决。</p>

<p>问题二： 设置备份数量</p>

<pre><code>[hadoop@master1 hadoop]$ hadoop fs -setrep 3 /slaves 
Replication 3 set: /slaves
</code></pre>

<p>问题三： 新增节点</p>

<p>拷贝程序到新增节点，然后启动</p>

<pre><code>[hadoop@master1 ~]$ tar zc hadoop-2.2.0 --exclude=logs | ssh slaver2 'cat | tar zx'

[hadoop@slaver2 ~]$ cd hadoop-2.2.0/
[hadoop@slaver2 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start datanode
</code></pre>

<p>也可以修改master上的slavers文件再<code>sbin/start-dfs.sh</code>启动。</p>
]]></content>
  </entry>
  
</feed>
