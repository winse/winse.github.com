<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Winse Blog]]></title>
  <link href="http://winse.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://winse.github.io/"/>
  <updated>2014-07-30T11:05:43+08:00</updated>
  <id>http://winse.github.io/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Snappy Compress]]></title>
    <link href="http://winse.github.io/blog/2014/07/30/hadoop2-snappy-compress/"/>
    <updated>2014-07-30T00:25:39+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/30/hadoop2-snappy-compress</id>
    <content type="html"><![CDATA[<p>网上查了很多资料说的很复杂，要叉叉叉叉叉！其实hadoop2已经集成了hadoop-snappy，只要安装snappy即可。但是也没有一些文章说的只要编译snappy然后放到lib/native路径下即可，还需要重新编译libhadoop的library包。</p>

<p>查找hadoop-snappy的源码的时刻，在C代码里面定义了<code>HADOOP_SNAPPY_LIBRARY</code>，然后理着这个思路去查找，发现在CMakeFile文件中也定义了对应的变量，然后再查找pom.xml的native profile中定义了snappy.prefix的属性。最后就有了下面的步骤。</p>

<p>1) build snappy</p>

<p>编译Snappy，并把lib拷贝/同步到hadoop的native目录下。</p>

<pre><code>tar zxf snappy-1.1.1.tar.gz 
cd snappy-1.1.1
./configure --prefix=/home/hadoop/snappy
make
make install

cd snappy
cd lib/
rysnc -vaz * ~/hadoop-2.2.0/lib/native/
</code></pre>

<p>2) rebuild hadoop common project</p>

<p>重新编译hadoop的lib，覆盖原来的文件。</p>

<pre><code>[hadoop@master1 hadoop-common]$ mvn package -Dmaven.javadoc.skip=true -DskipTests -Dsnappy.prefix=/home/hadoop/snappy -Drequire.snappy=true -Pnative 

[hadoop@master1 hadoop-common]$ cd ~/hadoop-2.2.0-src/hadoop-common-project/hadoop-common/
[hadoop@master1 hadoop-common]$ cd target/native/target/usr/local/lib/
[hadoop@master1 lib]$ ll
total 1252
-rw-rw-r--. 1 hadoop hadoop 820824 Jul 30 00:18 libhadoop.a
lrwxrwxrwx. 1 hadoop hadoop     18 Jul 30 00:18 libhadoop.so -&gt; libhadoop.so.1.0.0
-rwxrwxr-x. 1 hadoop hadoop 455542 Jul 30 00:18 libhadoop.so.1.0.0
[hadoop@master1 lib]$ rsync -vaz * ~/hadoop-2.2.0/lib/native/
sending incremental file list
libhadoop.a
libhadoop.so.1.0.0

sent 409348 bytes  received 53 bytes  818802.00 bytes/sec
total size is 1276384  speedup is 3.12
[hadoop@master1 lib]$ 
</code></pre>

<p>3) check</p>

<p>检查程序snappy是否已经配置成功</p>

<pre><code>[hadoop@master1 ~]$ hadoop checknative -a
14/07/30 00:22:14 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
14/07/30 00:22:14 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library
Native library checking:
hadoop: true /home/hadoop/hadoop-2.2.0/lib/native/libhadoop.so.1.0.0
zlib:   true /lib64/libz.so.1
snappy: true /home/hadoop/hadoop-2.2.0/lib/native/libsnappy.so.1
lz4:    true revision:43
bzip2:  false 
14/07/30 00:22:14 INFO util.ExitUtil: Exiting with status 1
[hadoop@master1 ~]$ 
</code></pre>

<p>4) 跑一个压缩程序</p>

<p>先参考网上的，直接用hbase的带的测试类运行（前提：需要在hbase-env.sh中配置HADOOP_HOME，这样hbase才能找到hadoop下的lib本地库）</p>

<pre><code>[hadoop@master1 ~]$ hbase-0.98.3-hadoop2/bin/hbase org.apache.hadoop.hbase.util.CompressionTest file:///tmp/abc.snappy snappy
2014-07-30 08:50:42,617 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/hadoop/hbase-0.98.3-hadoop2/lib/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2014-07-30 08:50:44,515 INFO  [main] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-30 08:50:44,522 INFO  [main] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
2014-07-30 08:50:45,388 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
2014-07-30 08:50:45,408 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
2014-07-30 08:50:45,430 ERROR [main] hbase.KeyValue: Unexpected getShortMidpointKey result, fakeKey:testkey, firstKeyInBlock:testkey
2014-07-30 08:50:47,088 INFO  [main] compress.CodecPool: Got brand-new decompressor [.snappy]
SUCCESS
[hadoop@master1 ~]$ 
</code></pre>

<p>看到最后的<strong>SUCCESS</strong>就说明安装配置成功了！</p>

<p>接下来自己写程序测试压缩/解压缩，首先编写java类：</p>

<pre><code>import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionInputStream;
import org.apache.hadoop.io.compress.CompressionOutputStream;
import org.apache.hadoop.io.compress.SnappyCodec;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.zookeeper.common.IOUtils;

public class SnappyCompressTest {

        public static void main(String[] args) throws FileNotFoundException, IOException {
                try {
                        execute(args);
                } catch (Exception e) {
                        System.out.println("Usage: $0 read|write file[.snappy]");
                }
        }

        private static void execute(String[] args) throws FileNotFoundException, IOException {
                String op = args[0];
                String file = args[1];
                String snappyFile = file + ".snappy";

                Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
                CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());

                if (StringUtils.equalsIgnoreCase(op, "read")) {
                        FileInputStream fin = new FileInputStream(snappyFile);
                        CompressionInputStream in = codec.createInputStream(fin);
                        FileOutputStream fout = new FileOutputStream(file);
                        IOUtils.copyBytes(in, fout, 4096, true);
                } else {
                        FileInputStream fin = new FileInputStream(file);
                        CompressionOutputStream out = codec.createOutputStream(new FileOutputStream(snappyFile));
                        IOUtils.copyBytes(fin, out, 4096, true);
                }
        }

}
</code></pre>

<p>编译运行，测试读写功能。使用hadoop命令可以简化很多工作，把当前路径加入到<code>HADOOP_CLASSPATH</code>。</p>

<pre><code>[hadoop@master1 test]$ javac -cp `hadoop classpath` SnappyCompressTest.java 
[hadoop@master1 test]$ export HADOOP_CLASSPATH=$PWD
[hadoop@master1 test]$ hadoop SnappyCompressTest 
Usage: $0 read|write file[.snappy]
[hadoop@master1 test]$ hadoop SnappyCompressTest write test.txt 
[hadoop@master1 test]$ ll
total 16
-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:23 test.txt
-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
[hadoop@master1 test]$ rm test.txt
[hadoop@master1 test]$ hadoop SnappyCompressTest read test.txt 
[hadoop@master1 test]$ ll
total 16
-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:28 test.txt
-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
[hadoop@master1 test]$ cat test.txt
abc
abc
abc
</code></pre>

<p>5) hbase中添加压缩</p>

<p>把所有library，以及hbase的配置同步其他所有从节点。对hbase的表使用Snappy压缩。</p>

<pre><code>hbase(main):001:0&gt; create 'st1', 'f1'
hbase(main):005:0&gt; alter 'st1', {NAME=&gt;'f1', COMPRESSION=&gt;'snappy'}
Updating all regions with the new schema...
0/1 regions updated.
1/1 regions updated.
Done.
0 row(s) in 2.7880 seconds

hbase(main):010:0&gt; create 'sst1','f1'
0 row(s) in 0.5730 seconds

=&gt; Hbase::Table - sst1
hbase(main):011:0&gt; flush 'sst1'
0 row(s) in 2.5380 seconds

hbase(main):012:0&gt; flush 'st1'
0 row(s) in 7.5470 seconds
</code></pre>

<p>对于hbase来说，使用压缩消耗还是挺大的。插入10w数据中间进行compaction时停顿比较久。最后flush写数据的时间也长了很多！
下面是文件写入后的文件大小对比（由于是进行简单的测试，写入的数据重复比较多。具体比例没有参考价值）：</p>

<p><img src="http://file.bmob.cn/M00/05/5A/wKhkA1PYz9CAB-TdAAEWX8LGpUo149.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 ShortCircuit Local Reading]]></title>
    <link href="http://winse.github.io/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading/"/>
    <updated>2014-07-29T20:11:58+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading</id>
    <content type="html"><![CDATA[<p>hadoop一直以来认为是本地读写文件的，但是其实也是通过TCP端口去获取数据，只是都在同一台机器。在hivetuning调优hive的文档中看到了ShortCircuit的HDFS配置属性，查看了ShortCircuit的来由，真正的实现了本地读取文件。蒙查查表示看的不是很明白，最终大致就是通过linux的<strong>文件描述符</strong>来实现功能同时保证文件的权限。</p>

<p>由于仅在自己的机器上面配置来查询hbase的数据，性能方面提升感觉不是很明显。等以后整到正式环境再对比对比。</p>

<p>配置如下。</p>

<p>1 修改hdfs-site.xml</p>

<pre><code>&lt;property&gt;
        &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
        &lt;value&gt;/home/hadoop/data/sockets/dn_socket&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>注意：socket路径的权限控制的比较严格。dn_socket<strong>所有的父路径</strong>要么仅有当前启动用户的写权限，要么仅root可写。</p>

<p><img src="http://file.bmob.cn/M00/05/52/wKhkA1PXfbKANLOrAADWJQ5taVs391.png" alt="" /></p>

<p>2 修改hbase的配置，并添加HADOOP_HOME（hbase查找hadoop-native）</p>

<p><img src="http://file.bmob.cn/M00/05/52/wKhkA1PXhRKAZDs6AAChrEauBoU738.png" alt="" /></p>

<p>hbase的脚本找到hadoop命令后，会把hadoop的java.library.path的路径加入到hbase的启动脚本中。</p>

<pre><code>[hadoop@master1 ~]$ tail -15 hbase-0.98.3-hadoop2/conf/hbase-site.xml 
    &lt;name&gt;hbase.tmp.dir&lt;/name&gt;
    &lt;value&gt;/home/hadoop/data/hbase&lt;/value&gt;
  &lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
        &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
        &lt;value&gt;/home/hadoop/data/sockets/dn_socket&lt;/value&gt;
&lt;/property&gt;

&lt;/configuration&gt;

[hadoop@master1 ~]$ cat hbase-0.98.3-hadoop2/conf/hbase-env.sh
...
export HADOOP_HOME=/home/hadoop/hadoop-2.2.0
...
</code></pre>

<p>3 同步到其他节点，然后重启hdfs,hbase</p>

<h2>参考</h2>

<ul>
<li><a href="http://vdisk.weibo.com/s/z_44nz36hNM3Z">hive-tuning</a></li>
<li><a href="http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/">How Improved Short-Circuit Local Reads Bring Better Performance and Security to Hadoop</a></li>
<li><a href="http://hbase.apache.org/book/perf.hdfs.html">HDFS&ndash;Apache HBase Performance Tuning</a></li>
<li><a href="http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">HDFS Short-Circuit Local Reads</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop安全的关闭datanode节点]]></title>
    <link href="http://winse.github.io/blog/2014/07/29/safely-remove-datanode/"/>
    <updated>2014-07-29T15:08:41+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/29/safely-remove-datanode</id>
    <content type="html"><![CDATA[<p>hadoop默认就有冗余（dfs.replication）的机制，所以一般情况下，一台机器挂了也没所谓。集群会自动的进行复制均衡处理。</p>

<p>作为测试，如果dfs.replication设置为1的情况下，怎么安全的把datanode节点服务关闭呢？例如说，刚刚开始搭建环境是把namenode、datanode放在一台机器上，后面增加了机器如何把datanode分离出来呢？</p>

<p>借助于<strong>dfs.hosts.exclude</strong>即可完成顺序的完成此项任务。</p>

<p>修改hdfs-site.xml配置。我操作的时刻仅修改了master1上的hdfs-site.xml。把<strong>master1</strong>值写入到对应的文件中。</p>

<pre><code>    [hadoop@master1 hadoop]$ cat hdfs-site.xml 
    ...
    &lt;property&gt;
            &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;
            &lt;value&gt;/home/hadoop/hadoop-2.2.0/etc/hadoop/exclude&lt;/value&gt;
    &lt;/property&gt;

    &lt;/configuration&gt;
    [hadoop@master1 hadoop]$ cat /home/hadoop/hadoop-2.2.0/etc/hadoop/exclude
    master1
</code></pre>

<p>修改完成后，刷新节点即可(完全没有必要重启集dfs)。</p>

<pre><code>hadoop dfsadmin -refreshNodes
</code></pre>

<p>可以通过<code>dfsadmin -report</code>或者网页查看master1已经变成<em>Decommission In Progress</em>了。</p>

<p><img src="http://file.bmob.cn/M00/05/4C/wKhkA1PXUMOAVvvWAAED6CN-3Rg187.png" alt="" /></p>

<p>注：</p>

<p>问题一： 在新建节点是slaver1的防火墙没关闭，由于master1已经被exclude，而slaver1不能提供服务，上传文件时报错：</p>

<pre><code>[hadoop@master1 hadoop]$ hadoop fs -put slaves  /
14/07/29 15:18:21 WARN hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /slaves._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1384)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2477)
</code></pre>

<p>关闭防火墙一样再次上传，还是报同样的错误。此时，也可以通过刷新节点<code>hadoop dfsadmin -refreshNodes</code>来解决。</p>

<p>问题二： 设置备份数量</p>

<pre><code>[hadoop@master1 hadoop]$ hadoop fs -setrep 3 /slaves 
Replication 3 set: /slaves
</code></pre>

<p>问题三： 新增节点</p>

<p>拷贝程序到新增节点，然后启动</p>

<pre><code>[hadoop@master1 ~]$ tar zc hadoop-2.2.0 --exclude=logs | ssh slaver2 'cat | tar zx'

[hadoop@slaver2 ~]$ cd hadoop-2.2.0/
[hadoop@slaver2 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start datanode
</code></pre>

<p>也可以修改master上的slavers文件再<code>sbin/start-dfs.sh</code>启动。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Ganglia on Redhat5+]]></title>
    <link href="http://winse.github.io/blog/2014/07/18/install-ganglia-on-redhat/"/>
    <updated>2014-07-18T14:53:44+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/18/install-ganglia-on-redhat</id>
    <content type="html"><![CDATA[<p>对使用C写的复杂的程序安装心里有阴影，还有本来可以上网的话，使用yum安装会省很多的事情。
但是没办法，环境是这样，正式环境没有提供网络环境，搭建的本地yum环境也不知道行不行。</p>

<p>上次在自己电脑的虚拟机上面成功安装过ganglia，但apache、rrdtool依赖使用yum安装的，安装过程比较揪心。把ganglia安装到正式环境就不了了之的。
上个星期生产环境出现了用户查询数据久久不能返回的问题，由于查询程序写的比较差的缘故。但同时也给自己敲了警钟，都不知道集群机器运行情况，终究是大隐患；安装后监测集群同时为以后程序的调优工作带来便利。</p>

<p>本次安装全部使用源码包安装，有部分lib有重复编译。</p>

<p>总结下，原来安装ganglia就仅是按照网络的步骤一步步的弄，同时各个程序的版本又有可能不一致，每一步都胆战心惊！没有重点重心，以至于浪费了很多的事情。
分步骤有条不紊的操作就可以踏实多了，安装ganglia主要涉及三个核心部分(安装程序包<a href="http://yunpan.cn/QCFUiuyAWSyZI">下载</a>（提取码：0ec4）)：</p>

<ul>
<li>rrdtool</li>
<li>gmetad / gmond</li>
<li>apache / web</li>
<li>集群子节点部署</li>
<li>配置hadoop metrics监控hadoop集群</li>
</ul>


<p>按照顺序一个个的安装就可以了。无需为一个个依赖的版本不一致问题而忧心，同时可以更好的参考网络上的实践。</p>

<h2>安装rrdtool</h2>

<p>推荐按照<a href="http://oss.oetiker.ch/rrdtool/doc/rrdbuild.en.html#IBUILDING_DEPENDENCIES">官网教程</a>步骤操作，很&amp;非常的详细。
教程中环境变量必须得设置！这个很重点！</p>

<p>下面是安装rrdtool过程中用到的软件，列出的顺序即为安装的次序：</p>

<pre><code>[hadoop@umcc97-44 rrdbuild]$ ll -tr | grep -v 'tar'
总计 24132
drwxrwxrwx  6   1000          1000    4096 07-17 12:12 pkg-config-0.23
drwxr-xr-x 11 hadoop            80    4096 07-17 12:28 zlib-1.2.3
drwxr-xr-x  7   1004 avahi-autoipd    4096 07-17 12:29 libpng-1.2.18
drwxr-xr-x  8   1000 users            4096 07-17 12:31 freetype-2.3.5
drwxrwxrwx 15  50138 vcsa            12288 07-17 16:37 libxml2-2.6.32
drwxrwxrwx 15   1488 users            4096 07-17 16:53 fontconfig-2.4.2
drwxrwxrwx  4 sjyw   sjyw             4096 07-17 16:56 pixman-0.10.0
drwxrwsrwx  8   1000 ftp              4096 07-17 16:59 cairo-1.6.4
drwxrwxrwx 12 sjyw   sjyw             4096 07-17 17:01 glib-2.15.4
drwxrwxrwx  9 sjyw   sjyw             4096 07-17 17:16 pango-1.21.1
drwxr-xr-x 11   1003          1001    4096 07-17 17:36 rrdtool-1.4.8
</code></pre>

<p>具体操作的步骤（原来包括操作步骤，发现太累赘了重新调整了一下）：</p>

<pre><code># 下面几个环境变量时基础！
BUILD_DIR=/home/ganglia/rrdbuild
INSTALL_DIR=/opt/rrdtool-1.4.8

export PKG_CONFIG_PATH=${INSTALL_DIR}/lib/pkgconfig
export PATH=$INSTALL_DIR/bin:$PATH

export LDFLAGS="-Wl,--rpath -Wl,${INSTALL_DIR}/lib" 

[root@umcc97-44 rrdbuild]# tar zxvf pkg-config-0.23.tar.gz 
[root@umcc97-44 rrdbuild]# cd pkg-config-0.23
[root@umcc97-44 pkg-config-0.23]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
[root@umcc97-44 pkg-config-0.23]# make &amp;&amp; make install

# 这个环境变量也很重要
[root@umcc97-44 pkg-config-0.23]# export PKG_CONFIG=$INSTALL_DIR/bin/pkg-config
[root@umcc97-44 pkg-config-0.23]# cd ..

[root@umcc97-44 rrdbuild]# tar zxvf zlib-1.2.3.tar.gz 
[root@umcc97-44 rrdbuild]# cd zlib-1.2.3
# 修改了下官网的命令; 64位问题 recompile with -fPIC
[root@umcc97-44 zlib-1.2.3]# CFLAGS="-O3 -fPIC" ./configure
[root@umcc97-44 zlib-1.2.3]# make &amp;&amp; make install

[root@umcc97-44 rrdbuild]# tar zxvf libpng-1.2.18.tar.gz 
[root@umcc97-44 rrdbuild]# cd libpng-1.2.18
[root@umcc97-44 zlib-1.2.3]# cd ../libpng-1.2.18
[root@umcc97-44 libpng-1.2.18]# env CFLAGS="-O3 -fPIC" ./configure --prefix=$INSTALL_DIR
[root@umcc97-44 libpng-1.2.18]# make &amp;&amp; make install

[root@umcc97-44 libpng-1.2.18]# cd ..
[root@umcc97-44 rrdbuild]# tar zxvf freetype-2.3.5.tar.gz 
[root@umcc97-44 rrdbuild]# cd freetype-2.3.5
[root@umcc97-44 freetype-2.3.5]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
[root@umcc97-44 freetype-2.3.5]# make &amp;&amp; make install

[root@umcc97-44 rrdbuild]# tar zxvf libxml2-2.6.32.tar.gz 
[root@umcc97-44 rrdbuild]# cd libxml2-2.6.32
[root@umcc97-44 libxml2-2.6.32]#  ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
[root@umcc97-44 libxml2-2.6.32]# make &amp;&amp; make install

[root@umcc97-44 libxml2-2.6.32]# cd ..
[root@umcc97-44 rrdbuild]# tar zxvf fontconfig-2.4.2.tar.gz 
[root@umcc97-44 rrdbuild]# cd fontconfig-2.4.2
[root@umcc97-44 fontconfig-2.4.2]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC" --with-freetype-config=$INSTALL_DIR/bin/freetype-config
[root@umcc97-44 fontconfig-2.4.2]# make &amp;&amp; make install

[root@umcc97-44 fontconfig-2.4.2]# cd ..
[root@umcc97-44 rrdbuild]# cd pixman-0.10.0
[root@umcc97-44 pixman-0.10.0]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
[root@umcc97-44 pixman-0.10.0]# make &amp;&amp; make install

[root@umcc97-44 pixman-0.10.0]# cd ../cairo-1.6.4
[root@umcc97-44 cairo-1.6.4]# ./configure --prefix=$INSTALL_DIR \
&gt;     --enable-xlib=no \
&gt;     --enable-xlib-render=no \
&gt;     --enable-win32=no \
&gt;     CFLAGS="-O3 -fPIC"
[root@umcc97-44 cairo-1.6.4]# make &amp;&amp; make install

[root@umcc97-44 cairo-1.6.4]# cd ..
[root@umcc97-44 rrdbuild]# tar zxvf glib-2.15.4.tar.gz 
[root@umcc97-44 rrdbuild]# cd glib-2.15.4
[root@umcc97-44 glib-2.15.4]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
[root@umcc97-44 glib-2.15.4]# make &amp;&amp; make install

[root@umcc97-44 rrdbuild]# bunzip2 -c pango-1.21.1.tar.bz2 | tar xf -
[root@umcc97-44 rrdbuild]# ll
[root@umcc97-44 rrdbuild]# cd pango-1.21.1
[root@umcc97-44 pango-1.21.1]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC" --without-x
[root@umcc97-44 pango-1.21.1]# export PATH=$INSTALL_DIR/bin:$PATH
[root@umcc97-44 pango-1.21.1]# make &amp;&amp; make install

[root@umcc97-44 rrdbuild]# cd rrdtool-1.4.8/
[root@umcc97-44 rrdtool-1.4.8]#  ./configure --prefix=$INSTALL_DIR --disable-tcl --disable-python
[root@umcc97-44 rrdtool-1.4.8]# make clean
[root@umcc97-44 rrdtool-1.4.8]# make 
[root@umcc97-44 rrdtool-1.4.8]# make install

## 安装完后，搞个例子玩玩   
[root@umcc97-44 rrdtool-1.4.8]# cd /opt/rrdtool-1.4.8/share/rrdtool/examples/
[root@umcc97-44 examples]# ll
[root@umcc97-44 examples]# ./4charts.pl 
This script has created 4charts.png in the current directory
This demonstrates the use of the TIME and % RPN operators
# 运行完后，会在当前目录生成不同尺寸的png的图片

[hadoop@umcc97-44 ~]$ /opt/rrdtool-1.4.8/bin/rrdtool -v
RRDtool 1.4.8  Copyright 1997-2013 by Tobias Oetiker &lt;tobi@oetiker.ch&gt;
               Compiled Jul 17 2014 17:37:58

Usage: rrdtool [options] command command_options
Valid commands: create, update, updatev, graph, graphv,  dump, restore,
        last, lastupdate, first, info, fetch, tune,
        resize, xport, flushcached

RRDtool is distributed under the Terms of the GNU General
Public License Version 2. (www.gnu.org/copyleft/gpl.html)

For more information read the RRD manpages
</code></pre>

<p>到这里rrd安装好，遇到zlib的CFLAGS变量设置的问题，以及终端断了必须重新设置<strong>环境变量</strong>两个大点的问题！其他如果按照官网的顺序安装基本顺顺利利了。</p>

<p>同时认识到了pkg，其实类似于java的jar嘛，依赖包不一定非要安装在系统的默认位置，自己管理也是一种简单易行的方式。接下来安装gmetad/gmond也使用这样方式，为后面部署gmond带来便利：所有依赖的包都放在一个目录下嘛！
接下来ganglia程序。</p>

<h2>gmetad安装</h2>

<p>需要用到的软件包：</p>

<pre><code>./gangliabuild/ganglia-web-3.5.12
./gangliabuild/apr-1.5.1
./gangliabuild/apr-util-1.5.3
./gangliabuild/confuse-2.7
./gangliabuild/expat-2.0.1
./gangliabuild/ganglia-3.6.0
</code></pre>

<p>整个安装过程，除了make的时刻rrd的库找不到的问题（通过LD_LIBRARY_PATH解决），其他都安装的很顺。</p>

<pre><code># 把下载来的tar全部解压
[root@umcc97-44 gangliabuild]# find . -name "*.tar.gz" -exec tar zxvf {} \;

[root@umcc97-44 gangliabuild]# cd expat-2.0.1
[root@umcc97-44 expat-2.0.1]# INSTALL_DIR=/opt/ganglia
[root@umcc97-44 expat-2.0.1]# ./configure --prefix=$INSTALL_DIR 
[root@umcc97-44 expat-2.0.1]# make &amp;&amp; make install

[root@umcc97-44 expat-2.0.1]# cd ../apr-1.5.1
[root@umcc97-44 apr-1.5.1]# ./configure --prefix=$INSTALL_DIR 
[root@umcc97-44 apr-1.5.1]# make &amp;&amp; make install

[root@umcc97-44 apr-1.5.1]# cd ../apr-util-1.5.3
[root@umcc97-44 apr-util-1.5.3]# ./configure --with-apr=/opt/ganglia --with-expat=/opt/ganglia --prefix=$INSTALL_DIR 
[root@umcc97-44 apr-util-1.5.3]# make &amp;&amp; make install

[root@umcc97-44 apr-util-1.5.3]# cd ../confuse-2.7
[root@umcc97-44 confuse-2.7]# ./configure CFLAGS=-fPIC --disable-nls --prefix=$INSTALL_DIR 
[root@umcc97-44 confuse-2.7]# make &amp;&amp; make install

[root@umcc97-44 confuse-2.7]# cd ../ganglia-3.6.0
[root@umcc97-44 ganglia-3.6.0]# export LDFLAGS="-Wl,--rpath -Wl,${INSTALL_DIR}/lib" 
[root@umcc97-44 ganglia-3.6.0]# export PKG_CONFIG_PATH=${INSTALL_DIR}/lib/pkgconfig
# 注意sysconfdir，运行程序配置所在的目录
[root@umcc97-44 ganglia-3.6.0]# ./configure --prefix=$INSTALL_DIR --with-librrd=/opt/rrdtool-1.4.8 --with-libexpat=/opt/ganglia --with-libconfuse=/opt/ganglia --with-libpcre=no  --with-gmetad --enable-gexec --enable-status -sysconfdir=/etc/ganglia
...
Welcome to..
     ______                  ___
    / ____/___ _____  ____ _/ (_)___ _
   / / __/ __ `/ __ \/ __ `/ / / __ `/
  / /_/ / /_/ / / / / /_/ / / / /_/ /
  \____/\__,_/_/ /_/\__, /_/_/\__,_/
                   /____/

Copyright (c) 2005 University of California, Berkeley

Version: 3.6.0
Library: Release 3.6.0 0:0:0

Type "make" to compile.

[root@umcc97-44 ganglia-3.6.0]# 
# 设置rrd的LIB路径
[root@umcc97-44 ganglia-3.6.0]# export LD_LIBRARY_PATH=/opt/rrdtool-1.4.8/lib
[root@umcc97-44 ganglia-3.6.0]# make
[root@umcc97-44 ganglia-3.6.0]# make install
</code></pre>

<p>接下来是配置gmetad</p>

<pre><code>[root@umcc97-44 ganglia-3.6.0]#  cd gmetad
[root@umcc97-44 gmetad]# cp gmetad.init /etc/init.d/gmetad
[root@umcc97-44 gmetad]# chkconfig gmetad on

[root@umcc97-44 gmetad]# chkconfig --list gmetad
gmetad          0:off   1:off   2:on    3:on    4:on    5:on    6:off

[root@umcc97-44 gmetad]# mkdir -p /var/lib/ganglia/rrds
[root@umcc97-44 gmetad]# chown nobody:nobody /var/lib/ganglia/rrds
[root@umcc97-44 gmetad]# 
# 没有启动起来，程序的路径不对
[root@umcc97-44 gmetad]# service gmetad start
Starting GANGLIA gmetad: 
[root@umcc97-44 gmetad]# 
[root@umcc97-44 gmetad]# ln -s /opt/ganglia/sbin/gmetad /usr/sbin/gmetad
[root@umcc97-44 gmetad]# service gmetad start
Starting GANGLIA gmetad: [  OK  ]

# 配置
[root@umcc97-44 gmetad]# cp gmetad.conf /etc/ganglia/gmetad.conf
[root@umcc97-44 gmetad]# vi /etc/ganglia/gmetad.conf 
 datasource "hadoop" localhost
 rrd_rootdir "/var/lib/ganglia/rrds"

[root@umcc97-44 gmetad]# service gmetad restart
Shutting down GANGLIA gmetad: [  OK  ]
Starting GANGLIA gmetad: [  OK  ]

# 测试下
[root@umcc97-44 gmetad]# telnet localhost 8651
</code></pre>

<h2>gmond安装</h2>

<pre><code>[root@umcc97-44 gmetad]# pwd
/home/ganglia/gangliabuild/ganglia-3.6.0/gmetad
[root@umcc97-44 gmetad]# cd ..
[root@umcc97-44 ganglia-3.6.0]# ./configure --prefix=$INSTALL_DIR  --with-libpcre=no
...
Welcome to..
     ______                  ___
    / ____/___ _____  ____ _/ (_)___ _
   / / __/ __ `/ __ \/ __ `/ / / __ `/
  / /_/ / /_/ / / / / /_/ / / / /_/ /
  \____/\__,_/_/ /_/\__, /_/_/\__,_/
                   /____/

Copyright (c) 2005 University of California, Berkeley

Version: 3.6.0
Library: Release 3.6.0 0:0:0

Type "make" to compile.

# 尽管检查通过了，但是make会报错
# 需要指定lib包位置
[root@umcc97-44 ganglia-3.6.0]# ./configure --prefix=$INSTALL_DIR  --with-libpcre=no  --with-libexpat=/opt/ganglia --with-libconfuse=/opt/ganglia -sysconfdir=/etc/ganglia
[root@umcc97-44 ganglia-3.6.0]# make &amp;&amp; make install

[root@umcc97-44 ganglia-3.6.0]# cd gmond/
[root@umcc97-44 gmond]# ./gmond -t &gt; /etc/ganglia/gmond.conf

# 和gmetad一样，需要把路径把程序做个软连接
[root@umcc97-44 gmond]# cat gmond.init
    #!/bin/sh
    #
    # chkconfig: 2345 70 40
    # description: gmond startup script
    #
    GMOND=/usr/sbin/gmond

...
[root@umcc97-44 gmond]# ln -s /opt/ganglia/sbin/gmond /usr/sbin/gmond

[root@umcc97-44 gmond]# cp gmond.init /etc/init.d/gmond
[root@umcc97-44 gmond]# chkconfig --add gmond
[root@umcc97-44 gmond]# chkconfig --list gmond
gmond           0:off   1:off   2:on    3:on    4:on    5:on    6:off

[root@umcc97-44 ganglia-3.6.0]# vi /etc/ganglia/gmond.conf 
 cluster-name

[root@umcc97-44 ganglia-3.6.0]# service gmond start
Starting GANGLIA gmond: [  OK  ]

# 测试下
[root@umcc97-44 ganglia-3.6.0]# telnet localhost 8649
</code></pre>

<p>查看运行情况：</p>

<pre><code>[root@umcc97-44 ganglia-3.6.0]# ldconfig -v
[root@umcc97-44 ganglia-3.6.0]# /opt/ganglia/bin/gstat -a
</code></pre>

<h2>apache和php环境安装</h2>

<pre><code>[root@umcc97-44 webbuild]# tar zxvf httpd-2.4.9.tar.gz 
[root@umcc97-44 webbuild]# cd httpd-2.4.9
[root@umcc97-44 httpd-2.4.9]# ./configure -with-enable-so -sysconfdir=/etc/httpd
...
checking for APR... no
configure: error: APR not found.  Please read the documentation.

# 前面安装ganglia时也安装过APR但是安装的目录指定的，混用不是很好。查看官方安装2.4的安装文档，可以直接把apr放到srclib下，编译时会同时编译这些依赖
[root@umcc97-44 httpd-2.4.9]# cd srclib/
[root@umcc97-44 srclib]# cp -r /home/ganglia/gangliabuild/apr-1.5.1 ./
[root@umcc97-44 srclib]# cp -r /home/ganglia/gangliabuild/apr-util-1.5.3 ./
[root@umcc97-44 srclib]# mv apr-1.5.1 apr
[root@umcc97-44 srclib]# mv apr-util-1.5.3 apr-util
[root@umcc97-44 srclib]# ll
[root@umcc97-44 srclib]# cd ..
[root@umcc97-44 httpd-2.4.9]#  cd ../
[root@umcc97-44 webbuild]# tar zxvf pcre-8.35.tar.gz 
# 正则表达式的包，这里安装默认位置
[root@umcc97-44 webbuild]# cd pcre-8.35
[root@umcc97-44 pcre-8.35]# ./configure 
[root@umcc97-44 pcre-8.35]# make &amp;&amp; make install

[root@umcc97-44 pcre-8.35]# cd ../httpd-2.4.9
[root@umcc97-44 httpd-2.4.9]# ./configure --with-included-apr -with-enable-so -sysconfdir=/etc/httpd
[root@umcc97-44 httpd-2.4.9]# make &amp;&amp; make install

[root@umcc97-44 httpd-2.4.9]# cd /usr/local/apache2/
[root@umcc97-44 apache2]# cd /etc/httpd

[root@umcc97-44 httpd]# cd /home/ganglia/webbuild/
[root@umcc97-44 webbuild]# tar zxvf php-5.5.14\ \(2\).tar.gz 
[root@umcc97-44 webbuild]# cd php-5.5.14
# 用了安装rrd时的libxml
[root@umcc97-44 php-5.5.14]# ./configure -with-apxs2=/usr/local/apache2/bin/apxs --with-libxml-dir=/opt/rrdtool-1.4.8/ -sysconfdir=/etc -with-config-file-path=/etc -with-config-file-scan-dir=/usr/etc/php.d -with-zlib
[root@umcc97-44 php-5.5.14]# make &amp;&amp; make install

[root@umcc97-44 php-5.5.14]#  
[root@umcc97-44 php-5.5.14]#  vi /etc/httpd/httpd.conf

    LoadModule php5_module        modules/libphp5.so #这个安装php后自动加上了

    DocumentRoot "/var/www/html"
    &lt;Directory "/var/www/html"&gt;

    AddType application/x-httpd-php .php

[root@umcc97-44 php-5.5.14]# /usr/local/apache2/bin/apachectl start
AH00526: Syntax error on line 215 of /etc/httpd/httpd.conf:
DocumentRoot must be a directory

[root@umcc97-44 php-5.5.14]# mkdir -p /var/www/html
[root@umcc97-44 php-5.5.14]# /usr/local/apache2/bin/apachectl start
AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.18.97.44. Set the 'ServerName' directive globally to suppress this message

[root@umcc97-44 php-5.5.14]#  vi /etc/httpd/httpd.conf
    ServerName
[root@umcc97-44 php-5.5.14]#/usr/local/apache2/bin/apachectl start
httpd (pid 31416) already running

[root@umcc97-44 php-5.5.14]# cp /usr/local/apache2/bin/apachectl /etc/init.d/httpd
[root@umcc97-44 php-5.5.14]# chkconfig --add httpd
service httpd does not support chkconfig

[root@umcc97-44 ~]# vi /etc/init.d/httpd 
 #chkconfig: 2345 10 90
 #description: Activates/Deactivates Apache Web Server

[root@umcc97-44 ~]# service httpd start

[root@umcc97-44 ~]# cd /var/www/html/
[root@umcc97-44 ~]# vi index.php
# http://umcc97-44 浏览器查看下结果

# /usr/local/apache2/bin/apachectl -k stop
[root@umcc97-44 ganglia-web]# service httpd -k stop 
# 等apache结束
[root@umcc97-44 ganglia-web]# tail -f /usr/local/apache2/logs/error_log 
</code></pre>

<p>部署ganglia-web：</p>

<pre><code>[root@umcc97-44 ~]# cd /home/ganglia/gangliabuild/ganglia-web-3.5.12
[root@umcc97-44 ganglia-web-3.5.12]# ls
[root@umcc97-44 ganglia-web-3.5.12]# make install
rsync --exclude "rpmbuild" --exclude "*.gz" --exclude "Makefile" --exclude "*debian*" --exclude "ganglia-web-3.5.12" --exclude ".git*" --exclude "*.in" --exclude "*~" --exclude "#*#" --exclude "ganglia-web.spec" --exclude "apache.conf" -a . ganglia-web-3.5.12
mkdir -p //var/lib/ganglia-web/dwoo/compiled &amp;&amp; \
    mkdir -p //var/lib/ganglia-web/dwoo/cache &amp;&amp; \
    mkdir -p //var/lib/ganglia-web &amp;&amp; \
    rsync -a ganglia-web-3.5.12/conf //var/lib/ganglia-web &amp;&amp; \
    mkdir -p //usr/share/ganglia-webfrontend &amp;&amp; \
    rsync --exclude "conf" -a ganglia-web-3.5.12/* //usr/share/ganglia-webfrontend &amp;&amp; \
    chown -R root:root //var/lib/ganglia-web

[root@umcc97-44 ganglia-web-3.5.12]# mv /usr/share/ganglia-webfrontend /var/www/html/ganglia
[root@umcc97-44 ganglia-web-3.5.12]# cd /var/www/html/ganglia/  

# 修改配置，在安装完gmetad后有新建/var/lib/ganglia/rrds其实和conf中的配置是一致的
[root@umcc97-44 ganglia]# cp conf_default.php conf.php  

[root@umcc97-44 ganglia]# cd /var/lib/ganglia-web/
[root@umcc97-44 ganglia-web]# cd dwoo/
[root@umcc97-44 dwoo]# ll
total 8
drwxr-xr-x 2 root root 4096 Jul 17 21:34 cache
drwxr-xr-x 2 root root 4096 Jul 17 21:34 compiled
[root@umcc97-44 dwoo]# chmod 777 *  
# http://umcc97-44/ganglia
</code></pre>

<p>部署gmond到其他集群节点</p>

<pre><code>
[root@umcc97-44 opt]# cat /etc/init.d/gmond 
    #!/bin/sh
    #
    # chkconfig: 2345 70 40
    # description: gmond startup script
    #
    GMOND=/usr/sbin/gmond   

[root@umcc97-44 opt]# vi /etc/ganglia/gmetad.conf   
 data_source
 # 重启gmetad
[root@umcc97-44 opt]# ssh-copy-id -i ~/.ssh/id_rsa.pub umcc97-144
[root@umcc97-44 opt]# scp /etc/init.d/gmond umcc97-144:/etc/init.d/
[root@umcc97-44 opt]# ssh umcc97-144 'mkdir /etc/ganglia' 
[root@umcc97-44 opt]# scp /etc/ganglia/gmond.conf  umcc97-144:/etc/ganglia/
[root@umcc97-44 opt]# rsync -vaz ganglia umcc97-144:/opt/
[root@umcc97-44 opt]# ssh umcc97-144
Last login: Tue Jun 10 12:08:47 2014

[root@umcc97-144 ~]# ln -s /opt/ganglia/sbin/gmond /usr/sbin/gmond
[root@umcc97-144 ~]# chkconfig --add gmond
[root@umcc97-144 ~]# service gmond start
Starting GANGLIA gmond: [  OK  ]

[root@umcc97-144 ~]# 
</code></pre>

<h2>Hadoop/Hbase Metrics配置</h2>

<pre><code>[hadoop@umcc97-44 ~]$ cat hadoop-2.2.0/etc/hadoop/hadoop-metrics*
#
#   Licensed to the Apache Software Foundation (ASF) under one or more
#   contributor license agreements.  See the NOTICE file distributed with
#   this work for additional information regarding copyright ownership.
#   The ASF licenses this file to You under the Apache License, Version 2.0
#   (the "License"); you may not use this file except in compliance with
#   the License.  You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#

# syntax: [prefix].[source|sink].[instance].[options]
# See javadoc of package-info.java for org.apache.hadoop.metrics2 for details

# @changed
#*.sink.file.class=org.apache.hadoop.metrics2.sink.FileSink
# default sampling period, in seconds
#*.period=10

# The namenode-metrics.out will contain metrics from all context
#namenode.sink.file.filename=namenode-metrics.out
# Specifying a special sampling period for namenode:
#namenode.sink.*.period=8

#datanode.sink.file.filename=datanode-metrics.out

# the following example split metrics of different
# context to different sinks (in this case files)
#jobtracker.sink.file_jvm.context=jvm
#jobtracker.sink.file_jvm.filename=jobtracker-jvm-metrics.out
#jobtracker.sink.file_mapred.context=mapred
#jobtracker.sink.file_mapred.filename=jobtracker-mapred-metrics.out

#tasktracker.sink.file.filename=tasktracker-metrics.out

#maptask.sink.file.filename=maptask-metrics.out

#reducetask.sink.file.filename=reducetask-metrics.out



*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31
*.sink.ganglia.period=10

*.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both
*.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40

namenode.sink.ganglia.servers=umcc97-44:8649
resourcemanager.sink.ganglia.servers=umcc97-44:8649

datanode.sink.ganglia.servers=umcc97-44:8649
nodemanager.sink.ganglia.servers=umcc97-44:8649

maptask.sink.ganglia.servers=umcc97-44:8649
reducetask.sink.ganglia.servers=umcc97-44:8649



# Configuration of the "dfs" context for null
dfs.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the "dfs" context for file
#dfs.class=org.apache.hadoop.metrics.file.FileContext
#dfs.period=10
#dfs.fileName=/tmp/dfsmetrics.log

# Configuration of the "dfs" context for ganglia
# Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
# dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# dfs.period=10
# dfs.servers=localhost:8649


# Configuration of the "mapred" context for null
mapred.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the "mapred" context for file
#mapred.class=org.apache.hadoop.metrics.file.FileContext
#mapred.period=10
#mapred.fileName=/tmp/mrmetrics.log

# Configuration of the "mapred" context for ganglia
# Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
# mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# mapred.period=10
# mapred.servers=localhost:8649


# Configuration of the "jvm" context for null
#jvm.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the "jvm" context for file
#jvm.class=org.apache.hadoop.metrics.file.FileContext
#jvm.period=10
#jvm.fileName=/tmp/jvmmetrics.log

# Configuration of the "jvm" context for ganglia
# jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# jvm.period=10
# jvm.servers=localhost:8649

# Configuration of the "rpc" context for null
rpc.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the "rpc" context for file
#rpc.class=org.apache.hadoop.metrics.file.FileContext
#rpc.period=10
#rpc.fileName=/tmp/rpcmetrics.log

# Configuration of the "rpc" context for ganglia
# rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# rpc.period=10
# rpc.servers=localhost:8649


# Configuration of the "ugi" context for null
ugi.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the "ugi" context for file
#ugi.class=org.apache.hadoop.metrics.file.FileContext
#ugi.period=10
#ugi.fileName=/tmp/ugimetrics.log

# Configuration of the "ugi" context for ganglia
# ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# ugi.period=10
# ugi.servers=localhost:8649

[hadoop@umcc97-44 ~]$ cat hbase-0.98.3-hadoop2/conf/hadoop-metrics2-hbase.properties 
# syntax: [prefix].[source|sink].[instance].[options]
# See javadoc of package-info.java for org.apache.hadoop.metrics2 for details

#*.sink.file*.class=org.apache.hadoop.metrics2.sink.FileSink
# default sampling period
#*.period=10

# Below are some examples of sinks that could be used
# to monitor different hbase daemons.

# hbase.sink.file-all.class=org.apache.hadoop.metrics2.sink.FileSink
# hbase.sink.file-all.filename=all.metrics

# hbase.sink.file0.class=org.apache.hadoop.metrics2.sink.FileSink
# hbase.sink.file0.context=hmaster
# hbase.sink.file0.filename=master.metrics

# hbase.sink.file1.class=org.apache.hadoop.metrics2.sink.FileSink
# hbase.sink.file1.context=thrift-one
# hbase.sink.file1.filename=thrift-one.metrics

# hbase.sink.file2.class=org.apache.hadoop.metrics2.sink.FileSink
# hbase.sink.file2.context=thrift-two
# hbase.sink.file2.filename=thrift-one.metrics

# hbase.sink.file3.class=org.apache.hadoop.metrics2.sink.FileSink
# hbase.sink.file3.context=rest
# hbase.sink.file3.filename=rest.metrics


*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31
*.sink.ganglia.period=10

hbase.sink.ganglia.period=10
hbase.sink.ganglia.servers=umcc97-44:8649
</code></pre>

<p>然后properties配置同步到集群的从节点（datanode/regionserver），重启集群。等一会儿就能在ganglia-web界面看到多了很多很多的指标量。</p>

<h2>参考</h2>

<h3>ganglia</h3>

<ul>
<li><a href="http://oss.oetiker.ch/rrdtool/doc/rrdbuild.en.html#IBUILDING_DEPENDENCIES">RRDTool安装</a></li>
<li><a href="http://www.cnblogs.com/qq78292959/archive/2012/05/30/2526761.html">CFLAGS=&ldquo;-O3 -fPIC"为64位编译参数</a></li>
<li><a href="http://www.codesky.net/article/201107/174186.html">pkgconfig作用处理包依赖</a></li>
<li><a href="http://blog.chinaunix.net/uid-23916356-id-3290237.html">gmetad和gmond安装以及配置</a></li>
<li><a href="http://wenku.baidu.com/link?url=RH4EhSP3U_dp4I7goEVA_DFkb0DrgZ3uWw_mSt2hhaRb6mQJLtWxaa75RrwETwtY5e8BvOCI_p9RNrmXn_qbEexTE-PGlgtf6f5T3cGglKq">gmond节点拷贝安装</a></li>
<li><a href="http://blog.chinaunix.net/uid-11121450-id-3147002.html">http://blog.chinaunix.net/uid-11121450-id-3147002.html</a></li>
<li><a href="http://blog.chinaunix.net/uid-23916356-id-3290237.html">http://blog.chinaunix.net/uid-23916356-id-3290237.html</a></li>
<li><a href="http://wenku.baidu.com/link?url=qY7vCTyodgSCsoIg6c2UiHXWv0nEGkS9nd0DbQERxFGEaTvgvi7FMQTKv5Sn1L9H8CX5_gDgAbJJ5jaQh3KhZED7PoB2Bgr2I6mS-vDc1LS">虚拟机操作从零开始弄, 搭了个本地源, 配置</a></li>
<li><a href="http://www.linuxidc.com/Linux/2014-01/95804p2.htm">Hadoop/Hbase metrics2配置</a></li>
<li><a href="https://github.com/cbuchner1/CudaMiner/issues/23">https://github.com/cbuchner1/CudaMiner/issues/23</a></li>
<li><a href="http://bbs.csdn.net/topics/390546319">http://bbs.csdn.net/topics/390546319</a> LIBRARY_PATH是编译时使用的，LD_LIBRARY_PATH是运行时使用的。</li>
</ul>


<h3>apache web</h3>

<ul>
<li><a href="http://blog.sina.com.cn/s/blog_70121e200100lq0h.html">apache程序安装</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_5d15305b0101ceft.html">apache服务安装配置</a></li>
<li><a href="http://www.cnblogs.com/yuboyue/archive/2011/07/18/2109875.html">apache关闭服务</a></li>
<li><a href="http://www.soadmin.com/zonghe/operating-system/1008085.htm">目录权限处理</a></li>
<li><a href="http://blog.163.com/figo_2007@126/blog/static/2318076520111149413935/">http://blog.163.com/figo_2007@126/blog/static/2318076520111149413935/</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_70121e200100lq0h.html">http://blog.sina.com.cn/s/blog_70121e200100lq0h.html</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_815611fb0101cxnl.html">http://blog.sina.com.cn/s/blog_815611fb0101cxnl.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tez编译及使用]]></title>
    <link href="http://winse.github.io/blog/2014/06/18/hadoop-tez-firststep/"/>
    <updated>2014-06-18T04:22:58+08:00</updated>
    <id>http://winse.github.io/blog/2014/06/18/hadoop-tez-firststep</id>
    <content type="html"><![CDATA[<h2>初步了解</h2>

<p>hadoop2自带的mapreduce任务中间只能传递一次，也即一个任务只能聚合一次。而tez项目是对原有yarn架构的一个拓展，使用DAG（无环有向图）实现MRR的任务框架。</p>

<p><img src="http://farm6.staticflickr.com/5571/14256993179_4990fc86d5_o.png" alt="" /></p>

<p>上图中，左边的MR任务完成一个步骤后，需要进行<strong>数据存储</strong>后再执行另一个任务来进行第二个“reduce”； 而tez则可以在reduce后继续执行reduce，减少了中间过程的IO以及mapreduce的启动时间。</p>

<h2>环境整合</h2>

<ul>
<li><a href="http://tez.incubator.apache.org/install.html">Install/Deploy</a></li>
<li>hadoop-2.2.0（umcc97-44：hdfs， umcc97-79：yarn）</li>
<li>windows下使用Cygwin编译</li>
</ul>


<h3>下载编译tez</h3>

<p>首先下载<a href="http://apache.fayea.com/apache-mirror/incubator/tez/tez-0.4.0-incubating/">tez-0.4.0-incubating.tar.gz</a>，同时还需要<a href="http://code.google.com/p/protobuf">protoc</a>的程序支持（编译<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html">hadoop源码</a>也需要这个的）。
解压后，使用mvn编译。</p>

<pre><code>Administrator@winseliu /cygdrive/e/local/libs/big
$ tar zxvf tez-0.4.0-incubating.tar.gz

Administrator@winseliu /cygdrive/e/local/libs/big
$ cd tez-0.4.0-incubating/

Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating
$ mvn install -DskipTests -Dmaven.javadoc.skip
...
[INFO] Reactor Summary:
[INFO]
[INFO] tez ............................................... SUCCESS [1.518s]
[INFO] tez-api ........................................... SUCCESS [8.890s]
[INFO] tez-common ........................................ SUCCESS [0.725s]
[INFO] tez-runtime-internals ............................. SUCCESS [2.529s]
[INFO] tez-runtime-library ............................... SUCCESS [5.100s]
[INFO] tez-mapreduce ..................................... SUCCESS [3.666s]
[INFO] tez-mapreduce-examples ............................ SUCCESS [2.692s]
[INFO] tez-dag ........................................... SUCCESS [13.943s]
[INFO] tez-tests ......................................... SUCCESS [1.691s]
[INFO] tez-dist .......................................... SUCCESS [14.370s]
[INFO] Tez ............................................... SUCCESS [0.245s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 55.791s
[INFO] Finished at: Tue Jun 17 17:33:45 CST 2014
[INFO] Final Memory: 35M/151M
[INFO] ------------------------------------------------------------------------
</code></pre>

<h3>上传tez程序的jars到HDFS</h3>

<p>为了简单我直接把tez放到开发环境的集群上面去测试了。放到本地环境应该也类似。</p>

<pre><code>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating
$ cd tez-dist/

Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist
$ cd target/

Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist/target
$ export HADOOP_USER_NAME=hadoop

Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist/target
$  hadoop dfs -put tez-0.4.0-incubating/tez-0.4.0-incubating/ hdfs://umcc97-44:9000/apps/
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.
</code></pre>

<h3>配置集群环境</h3>

<p>首先看下原来集群的classpath路径，由于已经包括了etc/hadoop目录，所以这里我直接把<code>tez-site.xml</code>放到该目录下。把所有的tez-lib上传到share目录下，并添加到HADOOP_CLASSPATH。</p>

<pre><code>  [hadoop@umcc97-79 hadoop]$ hadoop classpath
  /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.2.0/contrib/capacity-scheduler/*.jar

  # 用于map/reduce
  [hadoop@umcc97-79 hadoop]$ cat tez-site.xml 
  &lt;?xml version="1.0"?&gt;
  &lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;

  &lt;!-- Put site-specific property overrides in this file. --&gt;

  &lt;configuration&gt;
    &lt;property&gt;
      &lt;name&gt;tez.lib.uris&lt;/name&gt;
      &lt;value&gt;${fs.default.name}/apps/tez-0.4.0-incubating,${fs.default.name}/apps/tez-0.4.0-incubating/lib/&lt;/value&gt;
    &lt;/property&gt;
  &lt;/configuration&gt;
  [hadoop@umcc97-79 hadoop]$ 

  [hadoop@umcc97-79 hadoop]$ cd ~/hadoop-2.2.0/share/hadoop/tez/
  [hadoop@umcc97-79 tez]$ ll
  total 9616
  -rw-r--r-- 1 hadoop hadoop  303139 Jun 17 17:33 avro-1.7.4.jar
  -rw-r--r-- 1 hadoop hadoop   41123 Jun 17 17:33 commons-cli-1.2.jar
  -rw-r--r-- 1 hadoop hadoop  610259 Jun 17 17:33 commons-collections4-4.0.jar
  -rw-r--r-- 1 hadoop hadoop 1648200 Jun 17 17:33 guava-11.0.2.jar
  -rw-r--r-- 1 hadoop hadoop  710492 Jun 17 17:33 guice-3.0.jar
  -rw-r--r-- 1 hadoop hadoop  656365 Jun 17 17:33 hadoop-mapreduce-client-common-2.2.0.jar
  -rw-r--r-- 1 hadoop hadoop 1455001 Jun 17 17:33 hadoop-mapreduce-client-core-2.2.0.jar
  -rw-r--r-- 1 hadoop hadoop   21537 Jun 17 17:33 hadoop-mapreduce-client-shuffle-2.2.0.jar
  -rw-r--r-- 1 hadoop hadoop   81743 Jun 17 17:33 jettison-1.3.4.jar
  -rw-r--r-- 1 hadoop hadoop  533455 Jun 17 17:33 protobuf-java-2.5.0.jar
  -rw-r--r-- 1 hadoop hadoop  995968 Jun 17 17:33 snappy-java-1.0.4.1.jar
  -rw-r--r-- 1 hadoop hadoop  749917 Jun 17 17:33 tez-api-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop   34049 Jun 17 17:33 tez-common-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop  970987 Jun 17 17:33 tez-dag-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop  246409 Jun 17 17:33 tez-mapreduce-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop  199934 Jun 17 17:33 tez-mapreduce-examples-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop  114692 Jun 17 17:33 tez-runtime-internals-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop  352177 Jun 17 17:33 tez-runtime-library-0.4.0-incubating.jar
  -rw-r--r-- 1 hadoop hadoop    6845 Jun 17 17:33 tez-tests-0.4.0-incubating.jar
  [hadoop@umcc97-79 tez]$ 

  # 用于client任务提交
  [hadoop@umcc97-79 hadoop]$ grep HADOOP_CLASSPATH hadoop-env.sh
  export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tez/*:${HADOOP_HOME}/share/hadoop/tez/lib/*:$HADOOP_CLASSPATH

  [hadoop@umcc97-79 hadoop]$ sed -n 19,23p mapred-site.xml
  &lt;configuration&gt;
    &lt;property&gt;
      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
      &lt;value&gt;yarn-tez&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<h3>同步，重启yarn</h3>

<pre><code>for h in `cat hadoop-2.2.0/etc/hadoop/slaves ` ; do rsync -vaz --exclude=logs --exclude=pid --exclude=tmp  hadoop-2.2.0 $h:~/ ; done

rsync -vaz --exclude=logs --exclude=pid --exclude=tmp  hadoop-2.2.0 umcc97-44:~/
</code></pre>

<h3>测试效果</h3>

<pre><code>  [hadoop@umcc97-79 ~]$ hadoop classpath
  /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.2.0/share/hadoop/tez/*:/home/hadoop/hadoop-2.2.0/share/hadoop/tez/lib/*:/home/hadoop/hadoop-2.2.0/contrib/capacity-scheduler/*.jar
  [hadoop@umcc97-79 ~]$ cd hadoop-2.2.0/share/hadoop/mapreduce/
  [hadoop@umcc97-79 mapreduce]$ hadoop jar hadoop-mapreduce-client-jobclient-2.2.0-tests.jar sleep -mt 1 -rt 1 -m 1 -r 1

    hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount  /hello/in /hello/out
    hadoop fs -put hadoop-2.2.0/logs/yarn-hadoop-resourcemanager-umcc97-79.* /hello/in
    hadoop fs -rmr /hello/out
    hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount  /hello/in /hello/out
</code></pre>

<h3>回滚，使用时临时修改环境变量即可</h3>

<p>使用了tez后，使用hive-0.12.0不能运行了。由于其他同事需要用hive，得把配置全部修改回去【<a href="/blog/2014/06/21/upgrade-hive/">hive-0.13中使用tez</a>】。</p>

<p>其实在<strong>提交任务</strong>时指定配置参数即可。</p>

<pre><code>[hadoop@umcc97-79 ~]$ export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tez/*:${HADOOP_HOME}/share/hadoop/tez/lib/*:$HADOOP_CLASSPATH
[hadoop@umcc97-79 ~]$ hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount -Dmapreduce.framework.name=yarn-tez  /hello/in /hello/out
</code></pre>

<p>org.apache.tez.mapreduce.examples.OrderedWordCount不仅计算出了结果，同时按个数大小进行了排序。</p>

<p>问题： tez的任务的history还不知道怎么弄的，启动historyserver没作用。</p>
]]></content>
  </entry>
  
</feed>
