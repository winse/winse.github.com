<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Winse Blog]]></title>
  <link href="http://winse.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://winse.github.io/"/>
  <updated>2014-09-05T17:10:30+08:00</updated>
  <id>http://winse.github.io/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[读码] Hadoop2 Balancer磁盘空间平衡（中）]]></title>
    <link href="http://winse.github.io/blog/2014/09/05/read-hadoop-balancer-source-part2/"/>
    <updated>2014-09-05T14:57:25+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/05/read-hadoop-balancer-source-part2</id>
    <content type="html"><![CDATA[<h2>code</h2>

<p>执行<code>hadoop-2.2.0/bin/hadoop balancer -h</code>查看可以设置的参数（和sbin/start-balancer.sh一样）。</p>

<pre><code>Usage: java Balancer
    [-policy &lt;policy&gt;]  the balancing policy: datanode or blockpool
    [-threshold &lt;threshold&gt;]    Percentage of disk capacity
</code></pre>

<p>main方法入口，可以接受threshold（大于等于1小于等于100， 默认值10）和policy（可取datanode[dfsused]/blockpool[
blockpoolused]， 默认值datanode），具体的含义可以查看（上）篇中的javadoc的描述。</p>

<p>然后通过ToolRunner解析参数，并运行Cli工具类来执行HDFS的平衡。</p>

<h3>Cli类获取初始化参数</h3>

<p>1 设置检查</p>

<p><code>WIN_WIDTH</code>(默认1.5h) 已移动的数据会记录movedBlocks（list）变量中，在移动成功的数据<code>CUR_WIN</code>的值经过该<code>WIN_WIDTH</code>时间后会被移动到<code>OLD_WIN</code>&mdash;现在感觉作用不大，为了减少map的大小？</p>

<p><code>checkReplicationPolicyCompatibility()</code>方法检查<code>dfs.block.replicator.classname</code>是否为BlockPlacementPolicyDefault子类：满足3份备份的策略（1st本地，2nd另一个rack，3rd和第二份拷贝不同rack的节点）？</p>

<p>2 获取nameserviceuris</p>

<p>通过<code>DFSUtil#getNsServiceRpcUris()</code>来获取namenodes URI的结果集：</p>

<pre><code>+ nsId &lt;- dfs.nameservices
  ? ha  &lt;- dfs.namenode.rpc-address + [dfs.nameservices] + [dfs.ha.namenodes]
    Y+ =&gt; hdfs://nsId
    N+ =&gt; hdfs://[dfs.namenode.servicerpc-address.[nsId]] 或 hdfs://[dfs.namenode.rpc-address.[nsId]] 第二个满足条件的加入到nonPreferredUris
+ hdfs://[dfs.namenode.servicerpc-address] 或 hdfs://[dfs.namenode.rpc-address]  第二个满足条件的加入到nonPreferredUris
? [fs.defaultFs] 以hfds开头，且不在nonPreferredUris集合中
</code></pre>

<p>HA情况下的配置项可以查看<a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithQJM.html">官网的文档</a></p>

<pre><code>dfs.nameservices
dfs.ha.namenodes.[nameservice ID]
dfs.namenode.rpc-address.[nameservice ID].[name node ID] 
</code></pre>

<p>3 解析threshold和policy参数</p>

<p>默认值: <strong>BalancingPolicy.Node.INSTANCE, 10.0</strong>，打印的日志中也有该参数信息。</p>

<pre><code>2014-09-05 10:55:12,183 INFO Balancer: Using a threshold of 1.0
2014-09-05 10:55:12,186 INFO Balancer: namenodes = [hdfs://umcc97-44:9000]
2014-09-05 10:55:12,186 INFO Balancer: p         = Balancer.Parameters[BalancingPolicy.Node, threshold=1.0]
2014-09-05 10:55:13,744 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2014-09-05 10:55:18,154 INFO net.NetworkTopology: Adding a new node: /default-rack/10.18.97.142:50010
2014-09-05 10:55:18,249 INFO net.NetworkTopology: Adding a new node: /default-rack/10.18.97.144:50010
2014-09-05 10:55:18,311 INFO net.NetworkTopology: Adding a new node: /default-rack/10.18.97.143:50010
2014-09-05 10:55:18,319 INFO Balancer: 2 over-utilized: [Source[10.18.97.144:50010, utilization=8.288283273062705], Source[10.18.97.143:50010, utilization=8.302032354001554]]
2014-09-05 10:55:18,320 INFO Balancer: 1 underutilized: [BalancerDatanode[10.18.97.142:50010, utilization=4.716543864576553]]
</code></pre>

<h3>执行Balancer</h3>

<p>4 调用Balancer#run执行</p>

<pre><code> export HADOOP_OPTS=" -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8087 "
 sbin/start-balancer.sh 
</code></pre>

<p>Balancer的静态方法run，循环处理所有namenodes。实例化namenode的NameNodeConnector对象时，会把当前运行balancer程序的hostname写入到HDFS的<code>/system/balancer.id</code>文件中。</p>

<p><img src="http://file.bmob.cn/M00/0C/96/wKhkA1QJJNqAXxeaAAAho0g2bEU520.png" alt="" /></p>

<p>再就是实例化Balancer，并调用实例方法run来处理每个namenode的平衡。</p>

<p>这里是一个双层循环，要么<strong>出错</strong>要么就是平衡<strong>顺利完成</strong>才算结束。run方法可能的返回状态值机器含义可以查看javadoc（上）篇。</p>

<pre><code>      boolean done = false;
      for(int iteration = 0; !done; iteration++) {
        done = true;
        Collections.shuffle(connectors);
        for(NameNodeConnector nnc : connectors) {
          final Balancer b = new Balancer(nnc, p, conf);
          final ReturnStatus r = b.run(iteration, formatter, conf);
          // clean all lists
          b.resetData(conf);
          if (r == ReturnStatus.IN_PROGRESS) {
            done = false;
          } else if (r != ReturnStatus.SUCCESS) {
            //must be an error statue, return.
            return r.code;
          }
        }

        if (!done) {
          Thread.sleep(sleeptime);
        }
      }
</code></pre>

<p>5 针对每个namenode的平衡处理</p>

<p>针对每个namenode的每次迭代，又可以分出初始化节点、选择移动节点、移动/分发数据三个阶段。</p>

<pre><code>  private ReturnStatus run(int iteration, Formatter formatter, Configuration conf) {
...
      final long bytesLeftToMove = initNodes(nnc.client.getDatanodeReport(DatanodeReportType.LIVE));
      if (bytesLeftToMove == 0) {
        System.out.println("The cluster is balanced. Exiting...");
        return ReturnStatus.SUCCESS;
      }

      final long bytesToMove = chooseNodes();
      if (bytesToMove == 0) {
        System.out.println("No block can be moved. Exiting...");
        return ReturnStatus.NO_MOVE_BLOCK;
      }

      if (!this.nnc.shouldContinue(dispatchBlockMoves())) {
        return ReturnStatus.NO_MOVE_PROGRESS;
      }

      return ReturnStatus.IN_PROGRESS;
...
  }
</code></pre>

<p>5.1 初始化节点</p>

<p>获取集群Live Datanode节点的信息（和通过50070网页呈现的信息差不多），然后调用initNode()方法。</p>

<p><img src="http://file.bmob.cn/M00/0C/CE/wKhkA1QJfcSAIvuAAAAeoThXFMU356.png" alt="" /></p>

<p><code>initNodes()</code>中获取每个Datanode的capacity和dfsUsed数据，计算整个集群dfs的平均使用率avgUtilization。
然后根据每个节点的使用率与集群使用率，以及阀值进行比较划分为4种情况：
<code>overUtilizedDatanodes</code>，<code>aboveAvgUtilizedDatanodes</code>，<code>belowAvgUtilizedDatanodes</code>，<code>underUtilizedDatanodes</code>。</p>

<p><img src="http://file.bmob.cn/M00/0C/95/wKhkA1QJH2uAa8UEAABq7RCSLQ0452.png" alt="" /></p>

<p>同时取<strong>超出平均+阀值</strong>和<strong>低于平均-阀值</strong>的字节数最大值，即集群达到平衡需要移动的字节数。</p>

<hr />

<p>为了测试，如果集群已经平衡，可以搞点数据让集群不平衡，方便查看调试。</p>

<pre><code>bin/hadoop fs -D dfs.replication=1 -put XXXXX /abc

sbin/start-balancer.sh -threshold 1
</code></pre>

<p>5.2 选择节点</p>

<p>初始化节点后，同时计算出了需要移动的数据量。接下来就是选择移动数据的节点<code>chooseNodes</code>，以及接收对应数据的节点。</p>

<pre><code>  private long chooseNodes() {
    // First, match nodes on the same node group if cluster is node group aware
    if (cluster.isNodeGroupAware()) {
      chooseNodes(SAME_NODE_GROUP);
    }

    chooseNodes(SAME_RACK);
    chooseNodes(ANY_OTHER);

    long bytesToMove = 0L;
    for (Source src : sources) {
      bytesToMove += src.scheduledSize;
    }
    return bytesToMove;
  }
  private void chooseNodes(final Matcher matcher) {
    chooseDatanodes(overUtilizedDatanodes, underUtilizedDatanodes, matcher);
    chooseDatanodes(overUtilizedDatanodes, belowAvgUtilizedDatanodes, matcher);
    chooseDatanodes(underUtilizedDatanodes, aboveAvgUtilizedDatanodes, matcher);
  }

  private &lt;D extends BalancerDatanode, C extends BalancerDatanode&gt; void 
      chooseDatanodes(Collection&lt;D&gt; datanodes, Collection&lt;C&gt; candidates,
          Matcher matcher) {
    for (Iterator&lt;D&gt; i = datanodes.iterator(); i.hasNext();) {
      final D datanode = i.next();
      for(; chooseForOneDatanode(datanode, candidates, matcher); );
      if (!datanode.hasSpaceForScheduling()) {
        i.remove(); // “超出”部分全部有去处了
      }
    }
  }

  private &lt;C extends BalancerDatanode&gt; boolean chooseForOneDatanode(
      BalancerDatanode dn, Collection&lt;C&gt; candidates, Matcher matcher) {
    final Iterator&lt;C&gt; i = candidates.iterator();
    final C chosen = chooseCandidate(dn, i, matcher);

    if (chosen == null) {
      return false;
    }
    if (dn instanceof Source) {
      matchSourceWithTargetToMove((Source)dn, chosen);
    } else {
      matchSourceWithTargetToMove((Source)chosen, dn);
    }
    if (!chosen.hasSpaceForScheduling()) {
      i.remove(); // 可用的空间已经全部分配出去了
    }
    return true;
  }

  private &lt;D extends BalancerDatanode, C extends BalancerDatanode&gt;
      C chooseCandidate(D dn, Iterator&lt;C&gt; candidates, Matcher matcher) {
    if (dn.hasSpaceForScheduling()) {
      for(; candidates.hasNext(); ) {
        final C c = candidates.next();
        if (!c.hasSpaceForScheduling()) {
          candidates.remove();
        } else if (matcher.match(cluster, dn.getDatanode(), c.getDatanode())) {
          return c;
        }
      }
    }
    return null;
  }  
</code></pre>

<p>选择到<strong>接收节点</strong>后，接下来计算可以移动的数据量（取双方的available的最小值），然后把<strong>接收节点</strong>和<strong>数据量</strong>的信息NodeTask存储到需移动数据节点Source的NodeTasks对象中。</p>

<pre><code>  private void matchSourceWithTargetToMove(
      Source source, BalancerDatanode target) {
    long size = Math.min(source.availableSizeToMove(), target.availableSizeToMove());
    NodeTask nodeTask = new NodeTask(target, size);
    source.addNodeTask(nodeTask);
    target.incScheduledSize(nodeTask.getSize());
    sources.add(source);
    targets.add(target);
  }
</code></pre>

<p>5.3 移动数据</p>

<p>（待）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Mapreduce输入输出压缩]]></title>
    <link href="http://winse.github.io/blog/2014/09/01/hadoop2-mapreduce-compress/"/>
    <updated>2014-09-01T16:05:13+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/01/hadoop2-mapreduce-compress</id>
    <content type="html"><![CDATA[<p>当数据达到一定量时，自然就想到了对数据进行压缩来降低存储压力。在Hadoop的任务中提供了5个参数来控制输入输出的数据的压缩格式。添加map输出数据压缩可以降低集群间的网络传输，最终reduce输出压缩可以减低hdfs的集群存储空间。</p>

<p>如果是使用hive等工具的话，效果会更加明显。因为hive的查询结果是临时存储在hdfs中，然后再通过一个<strong>Fetch Operator</strong>来获取数据，最后清理掉，压缩存储临时的数据可以减少磁盘的读写。</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;Should the job outputs be compressed?
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.output.fileoutputformat.compress.type&lt;/name&gt;
  &lt;value&gt;RECORD&lt;/value&gt;
  &lt;description&gt;If the job outputs are to compressed as SequenceFiles, how should
               they be compressed? Should be one of NONE, RECORD or BLOCK.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;
  &lt;description&gt;If the job outputs are compressed, how should they be compressed?
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;Should the outputs of the maps be compressed before being
               sent across the network. Uses SequenceFile compression.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;
  &lt;description&gt;If the map outputs are compressed, how should they be 
               compressed?
  &lt;/description&gt;
&lt;/property&gt;
</code></pre>

<p>上面5个属性弄好，在core-sitem.xml加下<code>io.compression.codecs</code>基本就完成配置了。</p>

<p>这里主要探究下mapreduce（下面全部简称MR）过程中自动解压缩。刚刚接触Hadoop一般都不会去了解什么压缩不压缩的，先把hdfs-api，MR-api弄一遭。配置的TextInputFormat竟然能正确的读取tar.gz文件的内容，觉得不可思议，TextInputFormat不是直接读取txt行记录的输入嘛？难道还能读取压缩文件，先解压再&hellip;？？</p>

<p>先说下OutputFormat，在MR中调用context.write写入数据的方法时，最终使用OutputFormat创建的RecordWriter进行持久化。在TextOutputFormat创建RecordWriter时，如果使用压缩会在结果文件名上<strong>加对应压缩库的后缀</strong>，如gzip压缩对应的后缀gz、snappy压缩对应后缀snappy等。对应下面代码的<code>getDefaultWorkFile</code>。</p>

<p><img src="http://file.bmob.cn/M00/0B/03/wKhkA1QEjD2ASHiZAAExjXuQ25Y062.png" alt="" /></p>

<p>同样对应的TextInputFormat的RecordReader也进行类似的处理：根据<strong>文件的后缀</strong>来判定该文件是否使用压缩，并使用对应的输入流InputStream来解码。</p>

<p><img src="http://file.bmob.cn/M00/0B/03/wKhkA1QEjZaAdBeJAAEvRVMKVWY059.png" alt="" /></p>

<p>此处的关键代码为<code>CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);</code>，根据分块（split）的文件名来判断使用的压缩算法。
初始化Codec实现、以及根据文件名来获取压缩算法的实现还是挺有意思的：通过反转字符串然后最近匹配（headMap）来获取对应的结果。</p>

<pre><code>  private void addCodec(CompressionCodec codec) {
    String suffix = codec.getDefaultExtension();
    codecs.put(new StringBuilder(suffix).reverse().toString(), codec);
    codecsByClassName.put(codec.getClass().getCanonicalName(), codec);

    String codecName = codec.getClass().getSimpleName();
    codecsByName.put(codecName.toLowerCase(), codec);
    if (codecName.endsWith("Codec")) {
      codecName = codecName.substring(0, codecName.length() - "Codec".length());
      codecsByName.put(codecName.toLowerCase(), codec);
    }
  }

  public CompressionCodec getCodec(Path file) {
    CompressionCodec result = null;
    if (codecs != null) {
      String filename = file.getName();
      String reversedFilename = new StringBuilder(filename).reverse().toString();
      SortedMap&lt;String, CompressionCodec&gt; subMap = 
        codecs.headMap(reversedFilename);
      if (!subMap.isEmpty()) {
        String potentialSuffix = subMap.lastKey();
        if (reversedFilename.startsWith(potentialSuffix)) {
          result = codecs.get(potentialSuffix);
        }
      }
    }
    return result;
  }
</code></pre>

<p>了解了这些，MR（TextInputFormat）的输入文件可以比较随意些：各种压缩文件、原始文件都可以，只要文件有对应压缩算法的后缀即可。hive的解压缩功能也很容易了，如果使用hive存储text形式的文件，进行压缩无需进行额外的程序代码修改，仅仅修改MR的配置即可，注意下<strong>文件后缀</strong>！！</p>

<p>如，在MR中生成了snappy压缩的文件，此时<strong>不能</strong>在文件的后面添加东西。否则在hive查询时，根据<strong>后缀</strong>进行解压会导致结果乱码/不正确。</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;hive.exec.compress.output&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt; This controls whether the final outputs of a query (to a local/hdfs file or a hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.exec.compress.intermediate&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt; This controls whether intermediate files produced by hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* &lt;/description&gt;
&lt;/property&gt;
</code></pre>

<p>hive也弄了两个参数来控制它自己的MR的输出输入压缩控制属性。其他的配置使用mapred-site.xml的配置即可。</p>

<p><img src="http://file.bmob.cn/M00/0B/1D/wKhkA1QFQLmAfyZSAAIOx4UEIbY016.png" alt="" /></p>

<p>网上一些资料有<code>hive.intermediate.compression.codec</code>和<code>hive.intermediate.compression.type</code>两个参数能调整中间过程的压缩算法。其实和mapreduce的参数功能是一样的。</p>

<p><img src="http://file.bmob.cn/M00/0B/1F/wKhkA1QFQWyAUDMLAAGyNqR_X-c417.png" alt="" /></p>

<p>附上解压缩的全部配置：</p>

<pre><code>$#core-site.xml
    &lt;property&gt;
        &lt;name&gt;io.compression.codecs&lt;/name&gt;
        &lt;value&gt;
    org.apache.hadoop.io.compress.GzipCodec,
    org.apache.hadoop.io.compress.DefaultCodec,
    org.apache.hadoop.io.compress.BZip2Codec,
    org.apache.hadoop.io.compress.SnappyCodec
        &lt;/value&gt;
    &lt;/property&gt;

$#mapred-site.xml
    &lt;property&gt;
        &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; 
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapred.output.compression.codec&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
    &lt;/property&gt;

$#hive-site.xml
    &lt;property&gt;
        &lt;name&gt;hive.exec.compress.output&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<p>运行hive后，临时存储在HDFS的结果数据，注意文件的后缀。</p>

<p><img src="http://file.bmob.cn/M00/0B/20/wKhkA1QFRjSACnLfAABVdoK0f1c803.png" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://www.geek521.com/?p=4814">深入学习《Programing Hive》：数据压缩</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读码] Hadoop2 Balancer磁盘空间平衡（上）]]></title>
    <link href="http://winse.github.io/blog/2014/08/06/read-hadoop-balancer-source-part1/"/>
    <updated>2014-08-06T22:14:29+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/06/read-hadoop-balancer-source-part1</id>
    <content type="html"><![CDATA[<h2>javadoc</h2>

<p>在一些节点满了或者加入了新的节点情况下，使用balancer工具可以平衡HDFS集群磁盘空间使用率。该功能作为一个单独的程序，能同时与集群的其他文件操作一起进行。</p>

<p>threshold（阀值）参数是个介于1%~100%的值，默认情况下是10%。指定了集群达到平衡的指标值。当节点的利用率（所在节点的磁盘使用率，只HDFS可利用的部分）与集群利用率（集群HDFS的使用率）之间的差不大于阀值threshold就表示集群已经处于平衡状态。所以，阀值threshold越小，集群各节点数据分布越平衡（集群越平衡），当然这也会耗费更多的时间来达到小的平衡阀值。同时，当数据同时又在进行读写操作时，可能平衡并不能达到非常小的阀值。</p>

<p><img src="http://file.bmob.cn/M00/0C/95/wKhkA1QJH2uAa8UEAABq7RCSLQ0452.png" alt="" /></p>

<p>这个工具依次地把磁盘使用率高的机器的块移动到使用率低的数据节点上。每次迭代移动/接受的数据小于10G或者节点容量的阀值百分比（In each iteration a datanode moves or receives no more than the lesser of 10G bytes or the threshold fraction of its capacity. ）。每次迭代不会大于20分钟。每次迭代完成后，balancer把数据节点信息更新到namenode，重新计算利用率后，再进行下一次迭代直到集群利用率阀值。</p>

<p>配置<code>dfs.balance.bandwidthPerSec</code>控制balancer操作传输的带宽，默认配置是1048576（1M/s）这个属性决定了一个块从一个数据节点移动到另一个节点的最大速率。默认是1M/s。bandwidth越高集群达到平衡越快，但是程序之间的竞争会更激烈。如果通过配置文件来修改这个属性，需要在下次启动HDFS才能生效。可以通过<code>hdfs dfsadmin -setBalancerBandwidth 10485760</code>来动态的设置。</p>

<p>每次迭代会输出开始时间，迭代的次数，上一次迭代移动的数据量，集群达到平衡还需要移动的数据量，该次迭代将移动的数据量。一般情况下，“Bytes Already Moved”将会增加同时“Bytes Left To Move”将会减少（但是如果此时有大数据量写入，那么Bytes Left To Move可能不减反增）。</p>

<pre><code>Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved
</code></pre>

<p>多个balancer程序不能同时运行。</p>

<p>balancer程序会自动退出当存在以下情况：</p>

<ul>
<li>集群已经平衡</li>
<li>没有块将会被移动</li>
<li>连续5次的处理没有块移动</li>
<li>连接namenode时出现IOException</li>
<li>另一个balancer程序在跑</li>
</ul>


<p>根据上面的5中情况，balancer程序退出，同时会打印如下的信息：</p>

<ul>
<li>The cluster is balanced. Exiting</li>
<li>No block can be moved. Exiting&hellip;</li>
<li>No block has been moved for 5 iterations. Exiting&hellip;</li>
<li>Received an IO exception: failure reason. Exiting&hellip;</li>
<li>Another balancer is running. Exiting&hellip;</li>
</ul>


<p>当balancer运行时，管理员可以随时运行stop-balancer.sh来中断balancer程序。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop的datanode数据节点软/硬件配置应该一致]]></title>
    <link href="http://winse.github.io/blog/2014/08/02/hadoop-datanode-config-should-equals/"/>
    <updated>2014-08-02T22:21:12+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/02/hadoop-datanode-config-should-equals</id>
    <content type="html"><![CDATA[<p>最好的就是集群的所有的datanode的节点的<strong>硬件配置一样</strong>！当然系统时间也的一致，hosts等等这些。机器配置一样时可以使用脚本进行批量处理，给维护带来很大的便利性。</p>

<p>今天收到运维的信息，说集群的一台机器硬盘爆了！上到环境查看<code>df -h</code>发现硬盘配置和其他datanode的不同！但是hadoop hdfs-site.xml的<code>dfs.datanode.data.dir</code>却是一样的！</p>

<p>经验： dir的配置应该是一个系统设备对应一个路径，而不是一个系统目录对应dir的一个路径！</p>

<h2>问题现象以及根源</h2>

<p>问题机器A的磁盘情况：</p>

<pre><code>[hadoop@hadoop-slaver8 ~]$ df -h
文件系统              容量  已用  可用 已用%% 挂载点
/dev/sda3             2.7T  2.5T   53G  98% /
tmpfs                  32G  260K   32G   1% /dev/shm
/dev/sda1              97M   32M   61M  35% /boot

[hadoop@hadoop-slaver8 /]$ ll
总用量 170
dr-xr-xr-x.   2 root   root    4096 2月  12 19:39 bin
dr-xr-xr-x.   5 root   root    1024 2月  13 02:40 boot
drwxr-xr-x.   2 root   root    4096 2月  23 2012 cgroup
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data1
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data10
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data11
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data12
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data13
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data14
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data15
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data2
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data3
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data4
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data5
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data6
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data7
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data8
drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data9
</code></pre>

<p>再看集群其他机器：</p>

<pre><code>[hadoop@hadoop-slaver1 ~]$ df -h
文件系统              容量  已用  可用 已用%% 挂载点
/dev/sda3             2.7T   32G  2.5T   2% /
tmpfs                  32G   88K   32G   1% /dev/shm
/dev/sda1              97M   32M   61M  35% /boot
/dev/sdb1             1.8T  495G  1.3T  29% /data1
/dev/sdb2             1.8T  485G  1.3T  28% /data2
/dev/sdb3             1.8T  492G  1.3T  29% /data3
/dev/sdb4             1.8T  488G  1.3T  28% /data4
/dev/sdb5             1.8T  486G  1.3T  28% /data5
/dev/sdb6             1.8T  480G  1.3T  28% /data6
/dev/sdb7             1.8T  479G  1.3T  28% /data7
/dev/sdb8             1.8T  474G  1.3T  28% /data8
/dev/sdb9             1.8T  480G  1.3T  28% /data9
/dev/sdb10            1.8T  478G  1.3T  28% /data10
/dev/sdb11            1.8T  475G  1.3T  28% /data11
/dev/sdb12            1.8T  489G  1.3T  29% /data12
/dev/sdb13            1.8T  475G  1.3T  28% /data13
/dev/sdb14            1.8T  476G  1.3T  28% /data14
/dev/sdb15            1.8T  469G  1.3T  27% /data15
</code></pre>

<p>出问题机器没有挂存储，仅仅是建立了对应的目录结构，并不是把目录挂载到单独的存储设备上。</p>

<p>同时查看50070的前面的信息，hadoop把每个逗号分隔后的路径默认都做一个磁盘设备来计算！</p>

<pre><code>Node              Address             ..Admin State CC    Used  NU    RU(%) R(%)      Blocks Block  Pool Used Block Pool Used (%)
hadoop-slaver1  192.168.32.21:50010 2   In Service  26.86   7.05    1.37    18.44   26.25       68.66   264844  7.05    26.25   
hadoop-slaver8  192.168.32.28:50010 1   In Service  37.94   2.46    34.71   0.77    6.48        2.03    29637   2.46    6.48    
</code></pre>

<p>配置容量是所有配置的路径所在盘容量的<strong>累加</strong>。总的剩余空间（余量）也是各个dir配置路径的剩余空间<strong>累加</strong>的！这样很容易出现问题！
最好的就是集群的所有的datanode的节点的<strong>硬件配置一样</strong>！当然系统时间也的一致，hosts等等这些。</p>

<h2>问题处理</h2>

<p>首先得把问题解决啊：</p>

<ul>
<li>把<code>dfs.datanode.data.dir</code>路径个数调整为磁盘个数！</li>
<li>修改该datanode的hdfs-site的配置，添加<code>dfs.datanode.du.reserved</code>，留给系统的空间设置为400多G。</li>
<li>冗余份数也没有必要3份，浪费空间。如果两台机器同时出现问题，还是同一份数据，那只能说是天意！你可以去趟澳门赌一圈了！</li>
</ul>


<pre><code>&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&lt;value&gt;/data1/hadoop/data&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;
&lt;value&gt;437438953472&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;2&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>设置了reserved保留空间后，再看LIVE页面slaver8的容量变少了且正好等于(盘的容量2.7T-430G~=2.26T 计算容量的hdfs源码在<code>FsVolumeImpl.getCapacity()</code>)。</p>

<pre><code>hadoop-slaver8  192.168.32.28:50010 1   In Service  2.26    2.23    0.00    0.03    98.66
</code></pre>

<p>datanode和blockpool的平衡处理，可以参考<a href="http://hadoop-master1:50070/dfsnodelist.jsp?whatNodes=LIVE">Live Datanodes</a>的容量和进行！</p>

<pre><code>[hadoop@hadoop-master1 ~]$ hdfs balancer -help
Usage: java Balancer
        [-policy &lt;policy&gt;]      the balancing policy: datanode or blockpool
        [-threshold &lt;threshold&gt;]        Percentage of disk capacity

[hadoop@hadoop-slaver8 ~]$ hadoop-2.2.0/bin/hdfs getconf -confkey dfs.datanode.du.reserved
137438953472
</code></pre>

<p>删除一些没用的备份数据。配置好以后，重启当前slaver8节点，并进行数据平衡（如果觉得麻烦，直接丢掉原来的一个目录下的数据也行，可能更快！均衡器运行的太慢！！）</p>

<pre><code>[hadoop@hadoop-slaver8 ~]$  ~/hadoop-2.2.0/sbin/hadoop-daemon.sh stop datanode

[hadoop@hadoop-slaver8 ~]$  for i in 6 7 8 9 10 11 12 13 14 15; do  cd /data$i/hadoop/data/current/BP-1414312971-192.168.32.11-1392479369615/current/finalized;  find . -type f -exec mv {} /data1/hadoop/data/current/BP-1414312971-192.168.32.11-1392479369615/current/finalized/{} \;; done

[hadoop@hadoop-slaver8 ~]$  ~/hadoop-2.2.0/sbin/hadoop-daemon.sh start datanode


[hadoop@hadoop-master1 ~]$ hdfs dfsadmin -setBalancerBandwidth 10485760
[hadoop@hadoop-master1 ~]$ hdfs balancer -threshold 60
</code></pre>

<p>查看datanode的日志，由于移动数据，有些blk的id一样，会清理一些数据。对于均衡器程序的阀值越小集群越平衡！默认是10（%），会移动很多的数据（准备看下均衡器的源码，了解各个参数以及运行的逻辑）！</p>

<h2>参考</h2>

<ul>
<li><a href="http://blog.csdn.net/lingzihan1215/article/details/8700532">hadoop的datanode多磁盘空间处理</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Snappy Compress]]></title>
    <link href="http://winse.github.io/blog/2014/07/30/hadoop2-snappy-compress/"/>
    <updated>2014-07-30T00:25:39+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/30/hadoop2-snappy-compress</id>
    <content type="html"><![CDATA[<p>网上查了很多资料说的很复杂，要叉叉叉叉叉！其实hadoop2已经集成了hadoop-snappy，只要安装snappy即可。但是也没有一些文章说的只要编译snappy然后放到lib/native路径下即可，还需要重新编译libhadoop的library包。</p>

<p>查找hadoop-snappy的源码的时刻，在C代码里面定义了<code>HADOOP_SNAPPY_LIBRARY</code>，然后理着这个思路去查找，发现在CMakeFile文件中也定义了对应的变量，然后再查找pom.xml的native profile中定义了snappy.prefix的属性。最后就有了下面的步骤。</p>

<p>1) build snappy</p>

<p>编译Snappy，并把lib拷贝/同步到hadoop的native目录下。</p>

<pre><code>tar zxf snappy-1.1.1.tar.gz 
cd snappy-1.1.1
./configure --prefix=/home/hadoop/snappy
make
make install

cd snappy
cd lib/
rysnc -vaz * ~/hadoop-2.2.0/lib/native/
</code></pre>

<p>2) rebuild hadoop common project</p>

<p>重新编译hadoop的lib，覆盖原来的文件。</p>

<pre><code>[hadoop@master1 hadoop-common]$ mvn package -Dmaven.javadoc.skip=true -DskipTests -Dsnappy.prefix=/home/hadoop/snappy -Drequire.snappy=true -Pnative 

[hadoop@master1 hadoop-common]$ cd ~/hadoop-2.2.0-src/hadoop-common-project/hadoop-common/
[hadoop@master1 hadoop-common]$ cd target/native/target/usr/local/lib/
[hadoop@master1 lib]$ ll
total 1252
-rw-rw-r--. 1 hadoop hadoop 820824 Jul 30 00:18 libhadoop.a
lrwxrwxrwx. 1 hadoop hadoop     18 Jul 30 00:18 libhadoop.so -&gt; libhadoop.so.1.0.0
-rwxrwxr-x. 1 hadoop hadoop 455542 Jul 30 00:18 libhadoop.so.1.0.0
[hadoop@master1 lib]$ rsync -vaz * ~/hadoop-2.2.0/lib/native/
sending incremental file list
libhadoop.a
libhadoop.so.1.0.0

sent 409348 bytes  received 53 bytes  818802.00 bytes/sec
total size is 1276384  speedup is 3.12
[hadoop@master1 lib]$ 
</code></pre>

<p>3) check</p>

<p>检查程序snappy是否已经配置成功</p>

<pre><code>[hadoop@master1 ~]$ hadoop checknative -a
14/07/30 00:22:14 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
14/07/30 00:22:14 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library
Native library checking:
hadoop: true /home/hadoop/hadoop-2.2.0/lib/native/libhadoop.so.1.0.0
zlib:   true /lib64/libz.so.1
snappy: true /home/hadoop/hadoop-2.2.0/lib/native/libsnappy.so.1
lz4:    true revision:43
bzip2:  false 
14/07/30 00:22:14 INFO util.ExitUtil: Exiting with status 1
[hadoop@master1 ~]$ 
</code></pre>

<p>4) 跑一个压缩程序</p>

<p>先参考网上的，直接用hbase的带的测试类运行（前提：需要在hbase-env.sh中配置HADOOP_HOME，这样hbase才能找到hadoop下的lib本地库）</p>

<pre><code>[hadoop@master1 ~]$ hbase-0.98.3-hadoop2/bin/hbase org.apache.hadoop.hbase.util.CompressionTest file:///tmp/abc.snappy snappy
2014-07-30 08:50:42,617 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/hadoop/hbase-0.98.3-hadoop2/lib/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2014-07-30 08:50:44,515 INFO  [main] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-30 08:50:44,522 INFO  [main] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
2014-07-30 08:50:45,388 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
2014-07-30 08:50:45,408 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
2014-07-30 08:50:45,430 ERROR [main] hbase.KeyValue: Unexpected getShortMidpointKey result, fakeKey:testkey, firstKeyInBlock:testkey
2014-07-30 08:50:47,088 INFO  [main] compress.CodecPool: Got brand-new decompressor [.snappy]
SUCCESS
[hadoop@master1 ~]$ 
</code></pre>

<p>看到最后的<strong>SUCCESS</strong>就说明安装配置成功了！</p>

<p>接下来自己写程序测试压缩/解压缩，首先编写java类：</p>

<pre><code>import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionInputStream;
import org.apache.hadoop.io.compress.CompressionOutputStream;
import org.apache.hadoop.io.compress.SnappyCodec;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.zookeeper.common.IOUtils;

public class SnappyCompressTest {

        public static void main(String[] args) throws FileNotFoundException, IOException {
                try {
                        execute(args);
                } catch (Exception e) {
                        System.out.println("Usage: $0 read|write file[.snappy]");
                }
        }

        private static void execute(String[] args) throws FileNotFoundException, IOException {
                String op = args[0];
                String file = args[1];
                String snappyFile = file + ".snappy";

                Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
                CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());

                if (StringUtils.equalsIgnoreCase(op, "read")) {
                        FileInputStream fin = new FileInputStream(snappyFile);
                        CompressionInputStream in = codec.createInputStream(fin);
                        FileOutputStream fout = new FileOutputStream(file);
                        IOUtils.copyBytes(in, fout, 4096, true);
                } else {
                        FileInputStream fin = new FileInputStream(file);
                        CompressionOutputStream out = codec.createOutputStream(new FileOutputStream(snappyFile));
                        IOUtils.copyBytes(fin, out, 4096, true);
                }
        }

}
</code></pre>

<p>编译运行，测试读写功能。使用hadoop命令可以简化很多工作，把当前路径加入到<code>HADOOP_CLASSPATH</code>。</p>

<pre><code>[hadoop@master1 test]$ javac -cp `hadoop classpath` SnappyCompressTest.java 
[hadoop@master1 test]$ export HADOOP_CLASSPATH=$PWD
[hadoop@master1 test]$ hadoop SnappyCompressTest 
Usage: $0 read|write file[.snappy]
[hadoop@master1 test]$ hadoop SnappyCompressTest write test.txt 
[hadoop@master1 test]$ ll
total 16
-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:23 test.txt
-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
[hadoop@master1 test]$ rm test.txt
[hadoop@master1 test]$ hadoop SnappyCompressTest read test.txt 
[hadoop@master1 test]$ ll
total 16
-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:28 test.txt
-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
[hadoop@master1 test]$ cat test.txt
abc
abc
abc
</code></pre>

<p>5) hbase中添加压缩</p>

<p>把所有library，以及hbase的配置同步其他所有从节点。对hbase的表使用Snappy压缩。</p>

<pre><code>hbase(main):001:0&gt; create 'st1', 'f1'
hbase(main):005:0&gt; alter 'st1', {NAME=&gt;'f1', COMPRESSION=&gt;'snappy'}
Updating all regions with the new schema...
0/1 regions updated.
1/1 regions updated.
Done.
0 row(s) in 2.7880 seconds

hbase(main):010:0&gt; create 'sst1','f1'
0 row(s) in 0.5730 seconds

=&gt; Hbase::Table - sst1
hbase(main):011:0&gt; flush 'sst1'
0 row(s) in 2.5380 seconds

hbase(main):012:0&gt; flush 'st1'
0 row(s) in 7.5470 seconds
</code></pre>

<p>对于hbase来说，使用压缩消耗还是挺大的。插入10w数据中间进行compaction时停顿比较久。最后flush写数据的时间也长了很多！
下面是文件写入后的文件大小对比（由于是进行简单的测试，写入的数据重复比较多。具体比例没有参考价值）：</p>

<p><img src="http://file.bmob.cn/M00/05/5A/wKhkA1PYz9CAB-TdAAEWX8LGpUo149.png" alt="" /></p>

<p>6) 正式环境下解压snappy文件</p>

<pre><code>
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionInputStream;
import org.apache.hadoop.io.compress.SnappyCodec;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.zookeeper.common.IOUtils;

public class DecompressTest {
    public static void main(String[] args) throws IOException {

        Configuration conf = new Configuration();
        Path path = new Path(args[0]);
        FileSystem fs = path.getFileSystem(conf);

        Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
        CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());

        InputStream fin = fs.open(path);
        CompressionInputStream in = codec.createInputStream(fin);

        IOUtils.copyBytes(in, System.out, 4096, true);

        fin.close();

        System.out.println("SUCCESS");

    }
}

// build &amp; run

&gt;DecompressTest.java 
vi DecompressTest.java 
javac -cp `hadoop classpath`  DecompressTest.java 
export HADOOP_CLASSPATH=.
# snappyfile on hdfs
hadoop DecompressTest /user/hive/t_ods_access_log2/month=201408/day=20140828/hour=2014082808/t_ods_access_log2-2014082808.our.snappy.1409187524328
</code></pre>
]]></content>
  </entry>
  
</feed>
