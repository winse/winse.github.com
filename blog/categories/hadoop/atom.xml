<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2015-03-09T14:10:12+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Windows Build hadoop-2.6]]></title>
    <link href="http://winseliu.com/blog/2015/03/09/windows-build-hadoop-2-dot-6/"/>
    <updated>2015-03-09T12:01:55+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/09/windows-build-hadoop-2-dot-6</id>
    <content type="html"><![CDATA[<h2>环境</h2>

<pre><code>C:\Users\winse&gt;java -version
java version "1.7.0_02"
Java(TM) SE Runtime Environment (build 1.7.0_02-b13)
Java HotSpot(TM) Client VM (build 22.0-b10, mixed mode, sharing)

C:\Users\winse&gt;protoc --version
libprotoc 2.5.0

winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0/bin
$ uname -a
CYGWIN_NT-6.3-WOW64 Lenovo-PC 1.7.33-2(0.280/5/3) 2014-11-13 15:45 i686 Cygwin

winse@Lenovo-PC ~
$ cygcheck -c cygwin
Cygwin Package Information
Package              Version        Status
cygwin               1.7.33-1       OK
</code></pre>

<h2>编译具体步骤</h2>

<p>hadoop-2.6还不能直接编译java-x86的dll的。需要自己处理/打patch<a href="https://issues.apache.org/jira/browse/HADOOP-9922">HADOOP-9922</a>，但是官网jira-patch给出来的和2.6.0-src对不上。自己动手丰衣足食，把x64的全部改成Win32即可，附编译成功的patch<a href="http://yunpan.cn/cJaZzSu6DIibg">下载hadoop-2.6.0-common-native-win32-diff.patch（提取码：08fd）</a>。</p>

<ul>
<li>用visual studio2010的x86命令行进入：</li>
</ul>


<pre><code>Visual Studio 命令提示(2010)

Setting environment for using Microsoft Visual Studio 2010 x86 tools.
</code></pre>

<ul>
<li>切换到hadoop源码目录，打补丁和编译。protobuf目录和cygwin-bin目录加入PATH：</li>
</ul>


<pre><code>cd hadoop-2.6.0-src
cd hadoop-common-project\hadoop-common
patch -p0 &lt; hadoop-2.6.0-common-native-win32-diff.patch

set PATH=%PATH%;E:\local\home\Administrator\bin;c:\cygwin\bin

mvn package -Pdist,native-win -DskipTests -Dtar -Dmaven.javadoc.skip=true
</code></pre>

<ul>
<li>编译完成后，直接把<code>hadoop-common\target\bin</code>目录下的内容拷贝到程序的bin目录下。</li>
</ul>


<p>在windows下，执行java程序java.library.path默认到PATH路径找。这也是需要定义环境变量HADOOP_HOME，以及把bin加入到PATH的原因吧！</p>

<pre><code>HADOOP_HOME=E:\local\opt\bigdata\hadoop-2.6.0
PATH=%HADOOP_HOME%\bin;%PATH%
</code></pre>

<p>bin目录结构:</p>

<pre><code>winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0/bin
$ ls -l
total 3372
----------+ 1 winse None  159183 Nov 14 05:20 container-executor
----------+ 1 winse None    5479 Nov 14 05:20 hadoop
----------+ 1 winse None    8298 Nov 14 05:20 hadoop.cmd
----------+ 1 winse None   71680 Mar  9 10:33 hadoop.dll
----------+ 1 winse None   17033 Mar  9 10:33 hadoop.exp
----------+ 1 winse None   28666 Mar  9 10:33 hadoop.lib
----------+ 1 winse None  502784 Mar  9 10:33 hadoop.pdb
----------+ 1 winse None   11142 Nov 14 05:20 hdfs
----------+ 1 winse None    6923 Nov 14 05:20 hdfs.cmd
----------+ 1 winse None 1213602 Mar  9 10:33 libwinutils.lib
----------+ 1 winse None    5205 Nov 14 05:20 mapred
----------+ 1 winse None    5949 Nov 14 05:20 mapred.cmd
----------+ 1 winse None    1776 Nov 14 05:20 rcc
----------+ 1 winse None  201659 Nov 14 05:20 test-container-executor
----------+ 1 winse None   97792 Mar  9 10:33 winutils.exe
----------+ 1 winse None 1059840 Mar  9 10:33 winutils.pdb
----------+ 1 winse None   11380 Nov 14 05:20 yarn
----------+ 1 winse None   10895 Nov 14 05:20 yarn.cmd

winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0/bin
$ file winutils.exe hadoop.dll
winutils.exe: PE32 executable (console) Intel 80386, for MS Windows
hadoop.dll:   PE32 executable (DLL) (GUI) Intel 80386, for MS Windows
</code></pre>

<h2>配置</h2>

<ul>
<li>配置坑：</li>
</ul>


<pre><code>winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0
$ find . -name "*-default.xml" | xargs -I{} grep "hadoop.tmp.dir" {}
  &lt;value&gt;${hadoop.tmp.dir}/mapred/local&lt;/value&gt;
  &lt;value&gt;${hadoop.tmp.dir}/mapred/system&lt;/value&gt;
  &lt;value&gt;${hadoop.tmp.dir}/mapred/staging&lt;/value&gt;
  &lt;value&gt;${hadoop.tmp.dir}/mapred/temp&lt;/value&gt;
  &lt;value&gt;${hadoop.tmp.dir}/mapred/history/recoverystore&lt;/value&gt;
  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
  &lt;value&gt;${hadoop.tmp.dir}/io/local&lt;/value&gt;
  &lt;value&gt;${hadoop.tmp.dir}/s3&lt;/value&gt;
  &lt;value&gt;${hadoop.tmp.dir}/s3a&lt;/value&gt;
  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/name&lt;/value&gt;
  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/data&lt;/value&gt;
  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/namesecondary&lt;/value&gt;
    &lt;value&gt;${hadoop.tmp.dir}/yarn/system/rmstore&lt;/value&gt;
    &lt;value&gt;${hadoop.tmp.dir}/nm-local-dir&lt;/value&gt;
    &lt;value&gt;${hadoop.tmp.dir}/yarn-nm-recovery&lt;/value&gt;
    &lt;value&gt;${hadoop.tmp.dir}/yarn/timeline&lt;/value&gt;
</code></pre>

<p>就dfs的配置项前面加了<code>file://</code>前缀！</p>

<p>所以，在windows下如果只配置hadoop.tmp.dir（<code>file:///e:/tmp/hadoop</code>）的话得同时配置：</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
  &lt;value&gt;${hadoop.tmp.dir}/dfs/name&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
  &lt;value&gt;${hadoop.tmp.dir}/dfs/data&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
  &lt;value&gt;${hadoop.tmp.dir}/dfs/namesecondary&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>接下来格式化，启动都和linux操作一样。不多说。</p>

<h2>其他</h2>

<p>调试，下载maven源码等</p>

<pre><code>set HADOOP_NAMENODE_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"

mvn dependency:resolve -Dclassifier=sources

mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs 

mvn dependency:sources 
mvn dependency:resolve -Dclassifier=javadoc

/* 操作HDFS */
set HADOOP_ROOT_LOGGER=DEBUG,console
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VMware-Centos6 Build hadoop-2.6]]></title>
    <link href="http://winseliu.com/blog/2015/03/08/vmware-build-hadoop2-dot-6/"/>
    <updated>2015-03-08T08:22:14+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/08/vmware-build-hadoop2-dot-6</id>
    <content type="html"><![CDATA[<p>每次编译hadoop（-common）都是惊心动魄，没一次顺顺当当的！由于作者的偷懒，引发的有一起血案~~~</p>

<h2>环境说明</h2>

<ul>
<li>操作系统</li>
</ul>


<pre><code>[root@localhost ~]# uname -a
Linux localhost.localdomain 2.6.32-431.el6.x86_64 #1 SMP Fri Nov 22 03:15:09 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
[root@localhost ~]# cat /etc/redhat-release 
CentOS release 6.5 (Final)
</code></pre>

<ul>
<li>使用VMware的<strong>Shared Folders</strong>建立了maven和hadoop-2.6.0-src到宿主机器的映射：</li>
</ul>


<pre><code>[root@localhost ~]# ll -a hadoop-2.6.0-src maven
lrwxrwxrwx. 1 root root 26 Mar  7 22:47 hadoop-2.6.0-src -&gt; /mnt/hgfs/hadoop-2.6.0-src
lrwxrwxrwx. 1 root root 15 Mar  7 22:47 maven -&gt; /mnt/hgfs/maven
</code></pre>

<h2>具体操作</h2>

<pre><code># 安装maven，jdk
cat apache-maven-3.2.3-bin.tar.gz | ssh root@192.168.154.130 "cat - | tar zxv "

tar zxvf jdk-7u60-linux-x64.gz -C ~/
vi .bash_profile 

# 开发环境
yum install gcc glibc-headers gcc-c++ zlib-devel
yum install openssl-devel

# 安装protobuf
tar zxvf protobuf-2.5.0.tar.gz 
cd protobuf-2.5.0
./configure 
make &amp;&amp; make install

# 从映射文件中拷贝hadoop-common到linux文件系统，然后在编译hadoop-common
cd hadoop-2.6.0-src/hadoop-common-project/hadoop-common/
cd ..
scp -r  hadoop-common ~/
cd ~/hadoop-common
mvn install
mvn -X package -Pdist,native -Dmaven.test.skip=true -Dmaven.javadoc.skip=true

## 编译全部，中午吃饭的时刻编译比较好~~~
scp -r /mnt/hgfs/hadoop-2.6.0-src ~/
mvn package -Pdist,native -DskipTests -Dmaven.javadoc.skip=true
</code></pre>

<h2>遇到的问题</h2>

<ul>
<li><p>第一个文件肯定是没有<strong>c</strong>的编译环境，安装gcc即可</p></li>
<li><p><code>configure: error: C++ preprocessor "/lib/cpp" fails sanity check</code>，安装c++</p></li>
</ul>


<p>-> <a href="http://www.cnblogs.com/niocai/archive/2011/11/04/2236458.html">configure: error: C++ preprocessor &ldquo;/lib/cpp&rdquo; fails sanity check</a></p>

<ul>
<li><code>Unknown lifecycle phase "c"</code>， 安装操作提示最后的链接操<code>mvn install</code></li>
</ul>


<p>-> <a href="http://blog.csdn.net/kamemo/article/details/6523992">执行第一maven用例出错：Unknown lifecycle phase &ldquo;complile&rdquo;.</a>
-> <a href="https://cwiki.apache.org/confluence/display/MAVEN/LifecyclePhaseNotFoundException">LifecyclePhaseNotFoundException</a></p>

<ul>
<li><code>CMake Error at /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:108 (message): Could NOT find ZLIB (missing: ZLIB_INCLUDE_DIR)</code>， 缺少zlib-devel</li>
</ul>


<p>-> <a href="http://ask.csdn.net/questions/62307">Cmake时报错：Could NOT find ImageMagick</a></p>

<ul>
<li><code>cmake_symlink_library: System Error: Operation not supported</code>， 共享的windows目录下不能创建linux的软链接</li>
</ul>


<p>-> <a href="http://bbs.chinaunix.net/forum.php?mod=viewthread&amp;tid=3595245&amp;fromuid=26971268">参见9楼回复</a></p>

<blockquote><p>创建链接不成功，要确认当前帐户下是否有权限在编译的目录中有创建链接的权限</p>

<p>比如，你如果是在一个WINDOWS机器上的共享目录中编译，就没法创建链接，就会失败。把源码复制到本地的目录中再编译就不会有这问题。</p></blockquote>

<ul>
<li>全部编译时使用skipTests</li>
</ul>


<pre><code>main:
     [echo] Running test_libhdfs_threaded
     [exec] nmdCreate: NativeMiniDfsCluster#Builder#Builder error:
     [exec] java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/MiniDFSCluster$Builder
     [exec] Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdfs.MiniDFSCluster$Builder
     [exec]     at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
     [exec]     at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
     [exec]     at java.security.AccessController.doPrivileged(Native Method)
     [exec]     at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
     [exec]     at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
     [exec]     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
     [exec]     at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
     [exec] TEST_ERROR: failed on /root/hadoop-2.6.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c:326 (errno: 2): got NULL from tlhCluster
</code></pre>

<ul>
<li><code>Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the</code>，安装openssl-devel</li>
</ul>


<pre><code>main:
    [mkdir] Created dir: /root/hadoop-2.6.0-src/hadoop-tools/hadoop-pipes/target/native
     [exec] -- The C compiler identification is GNU 4.4.7
     [exec] -- The CXX compiler identification is GNU 4.4.7
     [exec] -- Check for working C compiler: /usr/bin/cc
     [exec] -- Check for working C compiler: /usr/bin/cc -- works
     [exec] -- Detecting C compiler ABI info
     [exec] -- Detecting C compiler ABI info - done
     [exec] -- Check for working CXX compiler: /usr/bin/c++
     [exec] -- Check for working CXX compiler: /usr/bin/c++ -- works
     [exec] -- Detecting CXX compiler ABI info
     [exec] -- Detecting CXX compiler ABI info - done
     [exec] -- Configuring incomplete, errors occurred!
     [exec] See also "/root/hadoop-2.6.0-src/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeOutput.log".
     [exec] CMake Error at /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:108 (message):
     [exec]   Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the
     [exec]   system variable OPENSSL_ROOT_DIR (missing: OPENSSL_LIBRARIES
     [exec]   OPENSSL_INCLUDE_DIR)
     [exec] Call Stack (most recent call first):
     [exec]   /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:315 (_FPHSA_FAILURE_MESSAGE)
     [exec]   /usr/share/cmake/Modules/FindOpenSSL.cmake:313 (find_package_handle_standard_args)
     [exec]   CMakeLists.txt:20 (find_package)
     [exec] 
     [exec] 
</code></pre>

<h2>成功</h2>

<pre><code>[INFO] Executed tasks
[INFO] 
[INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---
[INFO] Skipping javadoc generation
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache Hadoop Main ................................. SUCCESS [ 43.005 s]
[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 25.511 s]
[INFO] Apache Hadoop Annotations .......................... SUCCESS [ 21.177 s]
[INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 11.728 s]
[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 51.274 s]
[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 35.625 s]
[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 21.936 s]
[INFO] Apache Hadoop Auth ................................. SUCCESS [ 24.665 s]
[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 17.058 s]
[INFO] Apache Hadoop Common ............................... SUCCESS [06:07 min]
[INFO] Apache Hadoop NFS .................................. SUCCESS [ 41.279 s]
[INFO] Apache Hadoop KMS .................................. SUCCESS [ 59.186 s]
[INFO] Apache Hadoop Common Project ....................... SUCCESS [  7.216 s]
[INFO] Apache Hadoop HDFS ................................. SUCCESS [04:29 min]
[INFO] Apache Hadoop HttpFS ............................... SUCCESS [ 52.883 s]
[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.972 s]
[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 24.901 s]
[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  7.486 s]
[INFO] hadoop-yarn ........................................ SUCCESS [  7.466 s]
[INFO] hadoop-yarn-api .................................... SUCCESS [ 32.970 s]
[INFO] hadoop-yarn-common ................................. SUCCESS [ 25.549 s]
[INFO] hadoop-yarn-server ................................. SUCCESS [  6.709 s]
[INFO] hadoop-yarn-server-common .......................... SUCCESS [ 25.292 s]
[INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 29.555 s]
[INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [ 12.800 s]
[INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 14.025 s]
[INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 21.121 s]
[INFO] hadoop-yarn-server-tests ........................... SUCCESS [ 24.019 s]
[INFO] hadoop-yarn-client ................................. SUCCESS [ 18.949 s]
[INFO] hadoop-yarn-applications ........................... SUCCESS [  7.586 s]
[INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  8.428 s]
[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [ 12.671 s]
[INFO] hadoop-yarn-site ................................... SUCCESS [  7.518 s]
[INFO] hadoop-yarn-registry ............................... SUCCESS [ 18.518 s]
[INFO] hadoop-yarn-project ................................ SUCCESS [ 38.781 s]
[INFO] hadoop-mapreduce-client ............................ SUCCESS [ 13.133 s]
[INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 23.772 s]
[INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 22.815 s]
[INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [ 16.810 s]
[INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.404 s]
[INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [ 18.157 s]
[INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 14.637 s]
[INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  9.190 s]
[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  9.037 s]
[INFO] hadoop-mapreduce ................................... SUCCESS [ 59.280 s]
[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 26.724 s]
[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 31.503 s]
[INFO] Apache Hadoop Archives ............................. SUCCESS [ 19.867 s]
[INFO] Apache Hadoop Rumen ................................ SUCCESS [ 27.401 s]
[INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 20.102 s]
[INFO] Apache Hadoop Data Join ............................ SUCCESS [ 20.382 s]
[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [ 12.207 s]
[INFO] Apache Hadoop Extras ............................... SUCCESS [ 24.069 s]
[INFO] Apache Hadoop Pipes ................................ SUCCESS [ 31.975 s]
[INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 32.225 s]
[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [02:45 min]
[INFO] Apache Hadoop Client ............................... SUCCESS [01:38 min]
[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 15.450 s]
[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 46.489 s]
[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [01:31 min]
[INFO] Apache Hadoop Tools ................................ SUCCESS [  7.603 s]
[INFO] Apache Hadoop Distribution ......................... SUCCESS [ 32.967 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 39:30 min
[INFO] Finished at: 2015-03-08T10:55:47+08:00
[INFO] Final Memory: 102M/340M
[INFO] ------------------------------------------------------------------------
</code></pre>

<p>把src编译出来的native下面的文件拷贝到hadoop集群程序目录下：</p>

<pre><code>[hadoop@hadoop-master1 lib]$ scp -r root@172.17.42.1:~/hadoop-2.6.0-src/hadoop-dist/target/hadoop-2.6.0/lib/native ./
[hadoop@hadoop-master1 lib]$ cd native/
[hadoop@hadoop-master1 native]$ ll
total 4356
-rw-r--r--. 1 hadoop hadoop 1119518 Mar  8 03:11 libhadoop.a
-rw-r--r--. 1 hadoop hadoop 1486964 Mar  8 03:11 libhadooppipes.a
lrwxrwxrwx. 1 hadoop hadoop      18 Mar  3 21:08 libhadoop.so -&gt; libhadoop.so.1.0.0
-rwxr-xr-x. 1 hadoop hadoop  671237 Mar  8 03:11 libhadoop.so.1.0.0
-rw-r--r--. 1 hadoop hadoop  581944 Mar  8 03:11 libhadooputils.a
-rw-r--r--. 1 hadoop hadoop  359490 Mar  8 03:11 libhdfs.a
lrwxrwxrwx. 1 hadoop hadoop      16 Mar  3 21:08 libhdfs.so -&gt; libhdfs.so.0.0.0
-rwxr-xr-x. 1 hadoop hadoop  228451 Mar  8 03:11 libhdfs.so.0.0.0
</code></pre>

<p>前后对比：</p>

<pre><code>[hadoop@hadoop-master1 hadoop-2.6.0]$ hadoop fs -ls /
15/03/08 03:09:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 3 items
-rw-r--r--   1 hadoop supergroup       1366 2015-03-06 16:49 /README.txt
drwx------   - hadoop supergroup          0 2015-03-06 16:54 /tmp
drwxr-xr-x   - hadoop supergroup          0 2015-03-06 16:54 /user
[hadoop@hadoop-master1 hadoop-2.6.0]$ hadoop fs -ls /
Found 3 items
-rw-r--r--   1 hadoop supergroup       1366 2015-03-06 16:49 /README.txt
drwx------   - hadoop supergroup          0 2015-03-06 16:54 /tmp
drwxr-xr-x   - hadoop supergroup          0 2015-03-06 16:54 /user
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop查看作业状态Rest接口]]></title>
    <link href="http://winseliu.com/blog/2014/12/07/hadoop-mr-rest-api/"/>
    <updated>2014-12-07T10:09:49+08:00</updated>
    <id>http://winseliu.com/blog/2014/12/07/hadoop-mr-rest-api</id>
    <content type="html"><![CDATA[<p>hadoop yarn提供了web端查看任务状态，同时可以通过rest的方式获取任务的相关信息。rest接口和网页端的每个界面一一对应。</p>

<p><img src="http://file.bmob.cn/M00/D7/C0/oYYBAFSDxeaARA6AAAenwsLMShM027.png" alt="" /></p>

<p>上面的5个图的链接为：</p>

<pre><code>http://hadoop-master1:8088/cluster/apps/RUNNING
http://hadoop-master1:8088/cluster/app/application_1417676507722_1846
http://hadoop-master1:8088/proxy/application_1417676507722_1846/
http://hadoop-master1:8088/proxy/application_1417676507722_1846/mapreduce/job/job_1417676507722_1846
http://hadoop-master1:19888/jobhistory/job/job_1417676507722_1846/mapreduce/job/job_1417676507722_1846
</code></pre>

<h2>查看正在运行的任务</h2>

<pre><code>curl http://hadoop-master1:8088/ws/v1/cluster/apps?states=RUNNING
...

curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/info
...
curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs
...

curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867
...
curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/counters
...
curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/conf
</code></pre>

<p>如果上面的任务是已经完成的，获取对应的信息时返回的值是空的。</p>

<pre><code>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/counters
</code></pre>

<h2>查看执行完成的任务</h2>

<pre><code>curl http://hadoop-master1:19888/ws/v1/history
curl http://hadoop-master1:19888/ws/v1/history/info
...
curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs?startedTimeBegin=$(date +%s -d '-1 hour')000
...
curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867
curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867/counters
curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867/conf

curl -H "Accept: application/xml" "http://hadoop-master1:8088/ws/v1/cluster/apps?states=FINISHED&amp;limit=1" | xmllint --format - 
</code></pre>

<p>后面的参数和运行任务一致，只是提供服务不同。</p>

<h2>xml转csv</h2>

<p><img src="http://file.bmob.cn/M00/D7/D5/oYYBAFSEHuKAaAgHAACb9_KkMEU331.png" alt="" /></p>

<pre><code>$ curl -H "Accept: application/xml" "http://hadoop-master1:8088/ws/v1/cluster/apps?startedTimeBegin=$(date +%s -d '-1 hour')000" 2&gt;/dev/null | xsltproc yarn.xslt -  | sort -r

application_1417676507722_1973,AccessLogOnlyHiveJob,RUNNING,UNDEFINED,1417942144941,0,19416
application_1417676507722_1972,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417942084278,1417942098184,13906
application_1417676507722_1971,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417941603456,1417941617773,14317
application_1417676507722_1970,AccessLogOnlyHiveJob,FINISHED,SUCCEEDED,1417941581080,1417942142287,561207
application_1417676507722_1969,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417941422664,1417941436456,13792
</code></pre>

<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Hadoop YARN - Introduction to the web services REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">ResourceManager REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/MapredAppMasterRest.html">MapReduce Application Master REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html">History Server REST API&rsquo;s.</a></li>
<li><a href="http://blog.csdn.net/wypblog/article/details/21159795">Hadoop YARN中web服务的REST API介绍</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在windows开发测试mapreduce几种方式]]></title>
    <link href="http://winseliu.com/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/"/>
    <updated>2014-09-17T12:55:38+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature</id>
    <content type="html"><![CDATA[<blockquote><p>备注： 文后面的maven打包、以及执行的shell脚本还是极好的&hellip;</p></blockquote>

<p>hadoop提供的两大组件HDFS、MapReduce。其中HDFS提供了丰富的API，最重要的有类似shell的脚本进行操作。而编写程序，要很方便的调试测试，其实是一件比较麻烦和繁琐的事情。</p>

<p>首先可能针对拆分的功能进行<strong>单独的方法</strong>级别的单元测试，然后到map/reduce的一个<strong>完整的处理过程</strong>的测试，再就是针对<strong>整个MR</strong>的测试，前面说的都是在IDE中完成后，最后需要到<strong>测试环境</strong>对其进行验证。</p>

<ul>
<li>单独的方法这里就不必多讲，直接使用eclipse自带的junit即可完成。</li>
<li>mrunit，针对map/reduce的测试，以至于整个MR流程的测试，但是mrunit的输入是针对小数据量的。</li>
<li>本地模式运行程序，模拟正式的环境来进行测试，数据直接从hdfs获取。</li>
<li>测试环境远程调试，尽管经过前面的步骤可能还会遇到各种问题，此时可结合<code>remote debug</code>来定位问题。</li>
</ul>


<h3>mrunit测试map/reduce</h3>

<p>首先去到<a href="http://mrunit.apache.org/">官网下载</a>，把对应的jar加入到你项目的依赖。懒得去手工下载的话直接使用maven。</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.mrunit&lt;/groupId&gt;
        &lt;artifactId&gt;mrunit&lt;/artifactId&gt;
        &lt;version&gt;1.1.0&lt;/version&gt;
        &lt;classifier&gt;hadoop2&lt;/classifier&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
</code></pre>

<p>可以对mapreduce的各种情况（map/reduce/map-reduce/map-combine-reduce）进行简单的测试，验证逻辑上是否存在问题。<a href="https://cwiki.apache.org/confluence/display/MRUNIT/MRUnit+Tutorial">官方文档的例子</a>已经很具体详细了。</p>

<p>先新建初始化driver（MapDriver/ReduceDriver/MapReduceDriver)，然后添加配置配置信息（configuration），再指定withInput来进行输入数据，和withOutput对应的输出数据。运行调用runTest方法就会模拟mr的整个运行机制来对单条的记录进行处理。因为都是在一个jvm中执行，调试是很方便的。</p>

<pre><code>    private MapReduceDriver&lt;LongWritable, Text, KeyWrapper, ValueWrapper, Text, Text&gt; mrDriver;

    @Before
    public void setUp() {
        AccessLogMapper mapper = new AccessLogMapper();
        AccessLogReducer reducer = new AccessLogReducer();
        // AccessLogCombiner combiner = new AccessLogCombiner();

        mrDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer);

        // mDriver = MapDriver.newMapDriver(mapper);
        // mcrDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer, combiner);
    }

    private String[] datas;

    @After
    public void run() throws IOException {
        if (datas != null) {
            // 配置
            ...
            mrDriver.setConfiguration(config);
            // mrDriver.getConfiguration().addResource("job_1399189058775_0627_conf.xml");

          // 输入输出
            Text input = new Text();
            int i = 0;
            for (String data : datas) {
                input.set(data);
                mrDriver.withInput(new LongWritable(++i), new Text(data));
            }
            mrDriver.withOutputFormat(MultipleFileOutputFormat.class, TextInputFormat.class);
            mrDriver.runTest();
        }
    }

    // / datas

    private String[] datas() {
        return ...;
    }

    @Test
    public void testOne() throws IOException {
        datas = new String[] { datas()[0] };
    }
</code></pre>

<h2>local方式进行本地测试</h2>

<p>mapreduce默认提供了两种任务框架： local和yarn。YARN环境需要把程序发布到nodemanager上去运行，对于开发测试来讲，还是太繁琐了。</p>

<p>使用local的方式，既不用打包同时拥有IDE本地调试的便利，同时数据直接从HDFS中获取，也就是说，除了任务框架不同，其他都一样，程序的输入输出，任务代码的业务逻辑。为全面开发调试/测试提供了极其重要的方式。</p>

<p>只需要指定服务为local的服务框架，再加上输入输出即可。如果本地用户和hdfs的用户不同，设置下环境变量<code>HADOOP_USER_NAME</code>。同样map、reduce通过线程来模拟，都运行的同一个JVM中，断点调试也很方便。</p>

<pre><code>public class WordCountTest {

    static {
        System.setProperty("HADOOP_USER_NAME", "hadoop");
    }

    private static final String HDFS_SERVER = "hdfs://umcc97-44:9000";

    @Test
    public void test() throws Exception {
        WordCount.main(new String[]{
                "-Dmapreduce.framework.name=local", 
                "-Dfs.defaultFS=" + HDFS_SERVER, 
                HDFS_SERVER + "/user/hadoop/dta/001.tar.gz", 
                HDFS_SERVER + "/user/hadoop/output/"});
    }

}
</code></pre>

<h3>测试环境打包测试</h3>

<p>放到测试环境后，appmanager、map、reduce都是运行在不同的jvm；还有就是需要对程序进行打包，挺啰嗦而且麻烦的事情，依赖包多的话，包还挺大，每次job都需要传递这么大一个文件，也挺浪费的。</p>

<p>提供两种打包方式，一种是直接jar运行的，一种是所有的jar压缩包tar.gz方式。可以结合distributecache减少每次执行程序需要传递给nodemanager的数据量，以及结合mapreduce运行时配置参数可以进行远程调试。</p>

<pre><code>调试appmanager
-Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090" 
调试map
-Dmapreduce.map.java.opts
调试reduce
-Dmapreduce.reduce.java.opts
</code></pre>

<h3>小结</h3>

<p>通过以上3中方式基本上能处理工作终于到的大部分问题了。大部分的功能使用mrunit测试就可以了，还可以单独的测试map，或者reduce挺不错的。</p>

<h3>附录：maven打包</h3>

<pre><code>    &lt;profile&gt;
        &lt;id&gt;jar&lt;/id&gt;
        &lt;build&gt;
            &lt;plugins&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                    &lt;executions&gt;
                        &lt;execution&gt;
                            &lt;id&gt;make-assembly&lt;/id&gt;
                            &lt;phase&gt;package&lt;/phase&gt;
                            &lt;goals&gt;
                                &lt;goal&gt;single&lt;/goal&gt;
                            &lt;/goals&gt;
                        &lt;/execution&gt;
                    &lt;/executions&gt;
                    &lt;configuration&gt;
                        &lt;descriptorRefs&gt;
                            &lt;descriptorRef&gt;
                                jar-with-dependencies
                            &lt;/descriptorRef&gt;
                        &lt;/descriptorRefs&gt;
                    &lt;/configuration&gt;
                &lt;/plugin&gt;

            &lt;/plugins&gt;
        &lt;/build&gt;
    &lt;/profile&gt;

    &lt;profile&gt;
        &lt;id&gt;tar&lt;/id&gt;
        &lt;build&gt;
            &lt;plugins&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                    &lt;executions&gt;
                        &lt;execution&gt;
                            &lt;id&gt;make-assembly&lt;/id&gt;
                            &lt;phase&gt;package&lt;/phase&gt;
                            &lt;goals&gt;
                                &lt;goal&gt;single&lt;/goal&gt;
                            &lt;/goals&gt;
                        &lt;/execution&gt;
                    &lt;/executions&gt;
                    &lt;configuration&gt;
                        &lt;appendAssemblyId&gt;true&lt;/appendAssemblyId&gt;
                        &lt;descriptors&gt;
                            &lt;descriptor&gt;${basedir}/../assemblies/application.xml&lt;/descriptor&gt;
                        &lt;/descriptors&gt;
                    &lt;/configuration&gt;
                &lt;/plugin&gt;
            &lt;/plugins&gt;
        &lt;/build&gt;
    &lt;/profile&gt;
</code></pre>

<p>打包成tar.gz的描述文件：</p>

<pre><code>    &lt;assembly&gt;
        &lt;id&gt;dist-${env}&lt;/id&gt;
        &lt;formats&gt;
            &lt;format&gt;tar.gz&lt;/format&gt;
        &lt;/formats&gt;
        &lt;includeBaseDirectory&gt;true&lt;/includeBaseDirectory&gt;
        &lt;fileSets&gt;
            &lt;fileSet&gt;
                &lt;directory&gt;${basedir}/src/main/scripts&lt;/directory&gt;
                &lt;outputDirectory&gt;/bin&lt;/outputDirectory&gt;
                &lt;includes&gt;
                    &lt;include&gt;*.sh&lt;/include&gt;
                &lt;/includes&gt;
                &lt;fileMode&gt;0755&lt;/fileMode&gt;
                &lt;lineEnding&gt;unix&lt;/lineEnding&gt;
            &lt;/fileSet&gt;
            &lt;fileSet&gt;
                &lt;directory&gt;${basedir}/target/classes&lt;/directory&gt;
                &lt;outputDirectory&gt;/conf&lt;/outputDirectory&gt;
                &lt;includes&gt;
                    &lt;include&gt;*.xml&lt;/include&gt;
                    &lt;include&gt;*.properties&lt;/include&gt;
                &lt;/includes&gt;
            &lt;/fileSet&gt;
            &lt;fileSet&gt;
                &lt;directory&gt;${basedir}/target&lt;/directory&gt;
                &lt;outputDirectory&gt;/lib/core&lt;/outputDirectory&gt;
                &lt;includes&gt;
                    &lt;include&gt;${project.artifactId}-${project.version}.jar
                    &lt;/include&gt;
                &lt;/includes&gt;
            &lt;/fileSet&gt;
        &lt;/fileSets&gt;
        &lt;dependencySets&gt;
            &lt;dependencySet&gt;
                &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt;
                &lt;outputDirectory&gt;/lib/common&lt;/outputDirectory&gt;
                &lt;scope&gt;runtime&lt;/scope&gt;
            &lt;/dependencySet&gt;
        &lt;/dependencySets&gt;
    &lt;/assembly&gt;
</code></pre>

<p>运行整个程序的shell脚本</p>

<pre><code>#!/bin/sh

bin=`which $0`
bin=`dirname ${bin}`
bin=`cd "$bin"; pwd`

export ANAYSER_HOME=`dirname "$bin"`

export ANAYSER_LOG_DIR=$ANAYSER_HOME/logs

export ANAYSER_OPTS="-Dproc_dta_analyser -server -Xms1024M -Xmx2048M -Danalyser.log.dir=${ANAYSER_LOG_DIR}"

export HADOOP_HOME=${HADOOP_HOME:-/home/hadoop/hadoop-2.2.0}
export ANAYSER_CLASSPATH=$ANAYSER_HOME/conf
export ANAYSER_CLASSPATH=$ANAYSER_CLASSPATH:$HADOOP_HOME/etc/hadoop

for f in $ANAYSER_HOME/lib/core/*.jar ; do
  export ANAYSER_CLASSPATH+=:$f
done

for f in $ANAYSER_HOME/lib/common/*.jar ; do
  export ANAYSER_CLASSPATH+=:$f
done

if [ ! -d $ANAYSER_LOG_DIR ] ; then
  mkdir -p $ANAYSER_LOG_DIR
fi

[ -w "$ANAYSER_PID_DIR" ] ||  mkdir -p "$ANAYSER_PID_DIR"

nohup ${JAVA_HOME}/bin/java $ANAYSER_OPTS -cp $ANAYSER_CLASSPATH com.analyser.AnalyserStarter &gt;$ANAYSER_LOG_DIR/stdout 2&gt;$ANAYSER_LOG_DIR/stderr &amp;
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala Wordcount on Hadoop2]]></title>
    <link href="http://winseliu.com/blog/2014/09/12/scala-wordcount-on-hadoop/"/>
    <updated>2014-09-12T07:52:01+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/12/scala-wordcount-on-hadoop</id>
    <content type="html"><![CDATA[<p>从了解scala，到spark再次遇见scala，准备好好学学这门语言。函数式编程大势所趋，简洁的语法，更抽象好用的集合操作。土生土长的JVM的语言，以及凭借其与java的互操作性，发展前景一片光明。在云计算以及手机（android）开发都有其大展拳脚的地方。</p>

<p>工作中大部分时间写mapreduce，项目空白期实践了一下把scala搬上hadoop。整体来说用scala写个helloworld是比较简单的，就一些细节的东西比较繁琐。尽管用了几年的eclipse了，但是<a href="http://scala-ide.org/">scala-ide</a>还是需要再适应适应！scala-idea也没有大家说的那么好，和webstorm比差远了。</p>

<p><div><script src='https://gist.github.com/5df39f77e8bd59348a7a.js'></script>
<noscript><pre><code>package com.github.winse.hadoop

import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.Reducer
import org.apache.hadoop.io.Text
import org.apache.hadoop.io.IntWritable
import org.apache.hadoop.io.LongWritable
import org.apache.hadoop.mapreduce.Mapper
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
import org.apache.hadoop.fs.Path
import scala.Array.canBuildFrom
import org.apache.hadoop.conf.Configured
import org.apache.hadoop.util.Tool
import org.apache.hadoop.util.ToolRunner

class ScalaMapper extends Mapper[LongWritable, Text, Text, IntWritable] {

  val one = new IntWritable(1);

  override def map(key: LongWritable, value: Text, context: Mapper[LongWritable, Text, Text, IntWritable]#Context) {
    value.toString().split(&quot;\\s+&quot;).map(word =&gt; context.write(new Text(word), one))
  }

}

class ScalaReducer extends Reducer[Text, IntWritable, Text, IntWritable] {

  override def reduce(key: Text, values: java.lang.Iterable[IntWritable], context: Reducer[Text, IntWritable, Text, IntWritable]#Context) {
    var sum: Int = 0

    val itr = values.iterator()
    while (itr.hasNext()) {
      sum += itr.next().get()
    }
    context.write(key, new IntWritable(sum))
  }

}

object HelloScalaMapRed extends Configured with Tool {

  override def run(args: Array[String]): Int = {

    val job = Job.getInstance(getConf(), &quot;WordCount Scala.&quot;)
    job.setJarByClass(getClass())

    job.setOutputKeyClass(classOf[Text])
    job.setOutputValueClass(classOf[IntWritable])

    job.setMapperClass(classOf[ScalaMapper])
    job.setCombinerClass(classOf[ScalaReducer])
    job.setReducerClass(classOf[ScalaReducer])

    FileInputFormat.addInputPath(job, new Path(&quot;/scala/in/&quot;));
    FileOutputFormat.setOutputPath(job, new Path(&quot;/scala/out/&quot;));

    job.waitForCompletion(true) match {
      case true =&gt; 0
      case false =&gt; 1
    }

  }

  def main(args: Array[String]) {
    val res: Int = ToolRunner.run(new Configuration(), this, args)
    System.exit(res);
  }

}</code></pre></noscript></div>
</p>

<p>使用scala主要原因：</p>

<ul>
<li>写JavaBean更简单方便</li>
<li>多返回值无需定义Result实体类</li>
<li>集合更抽象的方法真的很好用</li>
<li>trait可以更便捷的进行操作层面的聚合，也就是可以把操作分离出来，进行组合就可以实现新的功能。这不就是decorate模式嘛！java的decorate多麻烦的！加点东西太麻烦了！！！</li>
</ul>


<p>上面的scala代码和java的比较类似，主要在集合操作上不同而已，变量定义简单化。</p>

<p>编写好代码后就是运行调试。</p>

<p>前面其他的文章已经说过了，默认<code>mapreduce.framework.name</code>的配置是本地<code>local</code>，所以直接运行就像运行一个普通的本地java程序。这就不多将了。
这里主要讲讲怎么把代码打包放到真实的集群环境运行，相比java的版本要添加那些步骤。</p>

<p>从项目的maven pom中可以发现，其实就是多了scala-lang的新依赖而已，其他都是hadoop自带的公共包。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHV6AAJoCAABANktCWmk664.png" alt="" /></p>

<p>所以运行程序只需要指定把scala-lang.jar添加到运行环境的classpath中即可。使用maven打包后的项目结构如下：</p>

<pre><code>[hadoop@master1 scalamapred-1.0.5]$ cd lib/
[hadoop@master1 lib]$ ls -l
total 8
drwxrwxr-x. 2 hadoop hadoop 4096 Sep 11 23:10 common
drwxrwxr-x. 2 hadoop hadoop 4096 Sep 11 23:56 core
[hadoop@master1 lib]$ ll core/
total 12
-rw-r--r--. 1 hadoop hadoop 11903 Sep 11 23:55 scalamapred-1.0.5.jar
[hadoop@master1 lib]$ ls common/
activation-1.1.jar                commons-lang-2.6.jar            hadoop-hdfs-2.2.0.jar                     jaxb-api-2.2.2.jar                      log4j-1.2.17.jar
aopalliance-1.0.jar               commons-logging-1.1.1.jar       hadoop-mapreduce-client-common-2.2.0.jar  jaxb-impl-2.2.3-1.jar                   management-api-3.0.0-b012.jar
asm-3.1.jar                       commons-math-2.1.jar            hadoop-mapreduce-client-core-2.2.0.jar    jersey-client-1.9.jar                   netty-3.6.2.Final.jar
avro-1.7.4.jar                    commons-net-3.1.jar             hadoop-yarn-api-2.2.0.jar                 jersey-core-1.9.jar                     paranamer-2.3.jar
commons-beanutils-1.7.0.jar       gmbal-api-only-3.0.0-b023.jar   hadoop-yarn-client-2.2.0.jar              jersey-grizzly2-1.9.jar                 protobuf-java-2.5.0.jar
commons-beanutils-core-1.8.0.jar  grizzly-framework-2.1.2.jar     hadoop-yarn-common-2.2.0.jar              jersey-guice-1.9.jar                    scala-library-2.10.4.jar
commons-cli-1.2.jar               grizzly-http-2.1.2.jar          hadoop-yarn-server-common-2.2.0.jar       jersey-json-1.9.jar                     servlet-api-2.5.jar
commons-codec-1.4.jar             grizzly-http-server-2.1.2.jar   jackson-core-asl-1.8.8.jar                jersey-server-1.9.jar                   slf4j-api-1.7.1.jar
commons-collections-3.2.1.jar     grizzly-http-servlet-2.1.2.jar  jackson-jaxrs-1.8.3.jar                   jersey-test-framework-core-1.9.jar      slf4j-log4j12-1.7.1.jar
commons-compress-1.4.1.jar        grizzly-rcm-2.1.2.jar           jackson-mapper-asl-1.8.8.jar              jersey-test-framework-grizzly2-1.9.jar  snappy-java-1.0.4.1.jar
commons-configuration-1.6.jar     guava-17.0.jar                  jackson-xc-1.8.3.jar                      jets3t-0.6.1.jar                        stax-api-1.0.1.jar
commons-daemon-1.0.13.jar         guice-3.0.jar                   jasper-compiler-5.5.23.jar                jettison-1.1.jar                        xmlenc-0.52.jar
commons-digester-1.8.jar          guice-servlet-3.0.jar           jasper-runtime-5.5.23.jar                 jetty-6.1.26.jar                        xz-1.0.jar
commons-el-1.0.jar                hadoop-annotations-2.2.0.jar    javax.inject-1.jar                        jetty-util-6.1.26.jar                   zookeeper-3.4.5.jar
commons-httpclient-3.1.jar        hadoop-auth-2.2.0.jar           javax.servlet-3.1.jar                     jsch-0.1.42.jar
commons-io-2.1.jar                hadoop-common-2.2.0.jar         javax.servlet-api-3.0.1.jar               jsp-api-2.1.jar
[hadoop@master1 lib]$ 
</code></pre>

<p>在lib文件夹下面包括common和core两放置jar的文件夹，common是项目的依赖包，core下面的是项目的源码jar。</p>

<p>接下来运行程序，通过libjar把<strong>scala-library的包加入到mapreduce的运行时classpath</strong>。当然也可以把scala-library加入到<code>mapreduce.application.classpath</code>（默认值为<code>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</code>）。</p>

<pre><code>[hadoop@master1 scalamapred-1.0.5]$ for j in `find . -name "*.jar"` ; do export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$j ; done
[hadoop@master1 scalamapred-1.0.5]$ 
[hadoop@master1 scalamapred-1.0.5]$ export HADOOP_CLASSPATH=
[hadoop@master1 scalamapred-1.0.5]$ export HADOOP_CLASSPATH=/home/hadoop/scalamapred-1.0.5/lib/core/*:/home/hadoop/scalamapred-1.0.5/lib/common/*
[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed -libjars lib/common/scala-library-2.10.4.jar 
</code></pre>

<h2>问题攻略</h2>

<p>上面如果不加libjar的话，会在nodemanager的代码中抛出异常。本来认为不加依赖包也就不能执行mapreduce里面的代码而已。问题的根源在哪里呢？</p>

<p>给代码添加远程调试的配置，然后运行一步步的查找问题（一次找不到就多运行调试几次）。</p>

<pre><code>[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed  -Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090"

// 我这里slaver就一台，取到机器上查看运行的程序

[hadoop@slaver1 nmPrivate]$ ps axu|grep java
hadoop    1427  0.6 10.5 1562760 106344 ?      Sl   Sep11   0:45 /opt/jdk1.7.0_60//bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=hadoop-hadoop-datanode-slaver1.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode
hadoop    2874  2.5 11.7 1599312 118980 ?      Sl   00:08   0:57 /opt/jdk1.7.0_60//bin/java -Dproc_nodemanager -Xmx1000m -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.home.dir= -Dyarn.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dyarn.policy.file=hadoop-policy.xml -server -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -classpath /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/etc/hadoop/nm-config/log4j.properties org.apache.hadoop.yarn.server.nodemanager.NodeManager
hadoop    3750  0.0  0.1 106104  1200 ?        Ss   00:43   0:00 /bin/bash -c /opt/jdk1.7.0_60//bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stderr 
hadoop    3759  0.1  1.8 737648 18232 ?        Sl   00:43   0:00 /opt/jdk1.7.0_60//bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster
hadoop    3778  0.0  0.0 103256   832 pts/0    S+   00:45   0:00 grep java

// 取到对应的目录下查看launcher.sh的脚本
// appmaster launcher

[hadoop@slaver1 nm-local-dir]$ cd nmPrivate/application_1410453720744_0007/
[hadoop@slaver1 application_1410453720744_0007]$ ll
total 4
drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:43 container_1410453720744_0007_01_000001
[hadoop@slaver1 application_1410453720744_0007]$ less container_1410453720744_0007_01_000001/
container_1410453720744_0007_01_000001.tokens       launch_container.sh                                 
.container_1410453720744_0007_01_000001.tokens.crc  .launch_container.sh.crc                            
[hadoop@slaver1 application_1410453720744_0007]$ less container_1410453720744_0007_01_000001/launch_container.sh 
#!/bin/bash

export NM_HTTP_PORT="8042"
export LOCAL_DIRS="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007"
export HADOOP_COMMON_HOME="/home/hadoop/hadoop-2.2.0"
export JAVA_HOME="/opt/jdk1.7.0_60/"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export HADOOP_YARN_HOME="/home/hadoop/hadoop-2.2.0"
export CLASSPATH="$PWD:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/share/hadoop/common/*:$HADOOP_COMMON_HOME/share/hadoop/common/lib/*:$HADOOP_HDFS_HOME/share/hadoop/hdfs/*:$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*:$HADOOP_YARN_HOME/share/hadoop/yarn/*:$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*:job.jar/job.jar:job.jar/classes/:job.jar/lib/*:$PWD/*"
export HADOOP_TOKEN_FILE_LOCATION="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001/container_tokens"
export NM_HOST="slaver1"
export APPLICATION_WEB_PROXY_BASE="/proxy/application_1410453720744_0007"
export JVM_PID="$$"
export USER="hadoop"
export HADOOP_HDFS_HOME="/home/hadoop/hadoop-2.2.0"
export PWD="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001"
export CONTAINER_ID="container_1410453720744_0007_01_000001"
export HOME="/home/"
export NM_PORT="40888"
export LOGNAME="hadoop"
export APP_SUBMIT_TIME_ENV="1410455811401"
export MAX_APP_ATTEMPTS="2"
export HADOOP_CONF_DIR="/home/hadoop/hadoop-2.2.0/etc/hadoop"
export MALLOC_ARENA_MAX="4"
export LOG_DIRS="/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001"
ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/10/job.jar" "job.jar"
ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/13/job.xml" "job.xml"
mkdir -p jobSubmitDir
ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/11/job.splitmetainfo" "jobSubmitDir/job.splitmetainfo"
mkdir -p jobSubmitDir
ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/12/job.split" "jobSubmitDir/job.split"
exec /bin/bash -c "$JAVA_HOME/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stderr "

// 去到TMP对应的目录下，查看整个运行的根目录

[hadoop@slaver1 ~]$ cd /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001
[hadoop@slaver1 container_1410453720744_0007_01_000001]$ ll
total 28
-rw-r--r--. 1 hadoop hadoop   95 Sep 12 00:43 container_tokens
-rwx------. 1 hadoop hadoop  468 Sep 12 00:43 default_container_executor.sh
lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:43 job.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/10/job.jar
drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:43 jobSubmitDir
lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:43 job.xml -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/13/job.xml
-rwx------. 1 hadoop hadoop 3005 Sep 12 00:43 launch_container.sh
drwx--x---. 2 hadoop hadoop 4096 Sep 12 00:43 tmp
[hadoop@slaver1 container_1410453720744_0007_01_000001]$ 
</code></pre>

<p>为了对应，我这里列出来在添加了libjar的TMP目录的列表：</p>

<pre><code>[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed  -Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090" -libjars lib/common/scala-library-2.10.4.jar 

[hadoop@slaver1 container_1410453720744_0007_01_000001]$ cd /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/container_1410453720744_0008_01_000001
[hadoop@slaver1 container_1410453720744_0008_01_000001]$ ll
total 32
-rw-r--r--. 1 hadoop hadoop   95 Sep 12 00:49 container_tokens
-rwx------. 1 hadoop hadoop  468 Sep 12 00:49 default_container_executor.sh
lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:49 job.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/filecache/10/job.jar
drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:49 jobSubmitDir
lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:49 job.xml -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/filecache/13/job.xml
-rwx------. 1 hadoop hadoop 3127 Sep 12 00:49 launch_container.sh
lrwxrwxrwx. 1 hadoop hadoop   85 Sep 12 00:49 scala-library-2.10.4.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/filecache/10/scala-library-2.10.4.jar
drwx--x---. 2 hadoop hadoop 4096 Sep 12 00:49 tmp
[hadoop@slaver1 container_1410453720744_0008_01_000001]$ 
</code></pre>

<p>windows本地使用eclipse和进行跟踪调试代码。</p>

<p><img src="http://file.bmob.cn/M00/0E/A1/wKhkA1QUG0aARyPVAAMnUXGDgbY378.png" alt="" /></p>

<p>此时可以通过8088的网页查看状态，当前有一个mrappmaster在执行，如果第一个失败，会尝试执行第二次。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHDGAe0anAAEfiNTmB1k734.png" alt="" /></p>

<p>运行调试多次后，<strong>最终确定问题</strong>所在。在master中会检查是否为链式mr，而加载该class的时刻，同时要加载父类的class，即scala的类，所以在这里会抛出异常。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHFOAWJulAAPOawkAbgo349.png" alt="" /></p>

<p>去到查看程序运行的日志，可以看到程序抛出的异常<strong>NoClassDefFoundError</strong>。</p>

<pre><code>[hadoop@slaver1 ~]$ less /home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410448728371_0003/*/syslog
2014-09-11 22:55:12,616 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1410448728371_0003_000001
...
2014-09-11 22:55:18,677 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1410448728371_0003 to jobTokenSecretManager
2014-09-11 22:55:19,119 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster
java.lang.NoClassDefFoundError: scala/Function1
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:190)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1277)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1217)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:135)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1420)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1358)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:972)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:134)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1227)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1035)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1445)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1441)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1374)
Caused by: java.lang.ClassNotFoundException: scala.Function1
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 22 more
2014-09-11 22:55:19,130 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster received a signal. Signaling RMCommunicator and JobHistoryEventHandler.
</code></pre>

<h2>意外收获</h2>

<ul>
<li>推测执行初始化代码</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHHCATFHtAAMeDcCHWzU166.png" alt="" /></p>

<ul>
<li>OutputFormat的获取Committer代码</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHImAJAq1AALGEfA-F9k811.png" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://digifesto.com/2013/04/15/hadoop-with-scala-hacking-notes/">Hadoop with Scala: hacking notes</a></li>
<li><a href="https://github.com/derrickcheng/ScalaOnHadoop/blob/master/src/main/scala/WordCount.scala">ScalaOnHadoop WordCount.scala</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
