<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Winse Blog]]></title>
  <link href="http://winseliu.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2016-08-19T17:31:14+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hdfs异构存储实操]]></title>
    <link href="http://winseliu.com/blog/2016/05/05/hdfs-heterogeneous-storage/"/>
    <updated>2016-05-05T21:41:39+08:00</updated>
    <id>http://winseliu.com/blog/2016/05/05/hdfs-heterogeneous-storage</id>
    <content type="html"><![CDATA[<p>[注意] 查看官方文档一定要和自己使用的环境对应！操作 storagepolicies 不同版本对应的命令不同（2.6.3<->2.7.2）！</p>

<p>我这里测试环境使用的是 2.6.3 <a href="https://hadoop.apache.org/docs/r2.6.3/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Heterogeneous Storage: Archival Storage, SSD &amp; Memory</a></p>

<h2>配置</h2>

<p>直接把内存盘放到 /dev/shm 下，单独挂载一个 tmpfs 的效果也差不多。<a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">r2.7.2 Memory Storage Support in HDFS</a> 2.6.3没有这个文档 概念都适应的。</p>

<p>1 调节系统参数</p>

<pre><code>vi /etc/security/limits.conf

    hadoop           -       nofile          65535
    hadoop           -       nproc           65535
    hadoop           -       memlock         268435456
</code></pre>

<p>需要调节memlock的大小，否则启动datanode报错。</p>

<pre><code>2016-05-05 19:22:22,674 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.lang.RuntimeException: Cannot start datanode because the configured max locked memory size (dfs.datanode.max.locked.memory) of 134217728 bytes is more than the datanode's available RLIMIT_MEMLOCK ulimit of 65536 bytes.
        at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1067)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.&lt;init&gt;(DataNode.java:417)
</code></pre>

<p>2 添加RAM_DISK</p>

<pre><code>vi hdfs-site.xml

    &lt;property&gt;
    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
    &lt;value&gt;/data/bigdata/hadoop/dfs/data,[RAM_DISK]/dev/shm/dfs/data&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
    &lt;name&gt;dfs.datanode.max.locked.memory&lt;/name&gt;
    &lt;value&gt;134217728&lt;/value&gt;
    &lt;/property&gt;
</code></pre>

<p>注意内存盘的写法，<code>[RAM_DISK]</code> 必须这些写，不然datanode不知道指定路径的storage的类型(默认是 DISK )。<a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html#Storage_Types_and_Storage_Policies">Storage_Types_and_Storage_Policies</a></p>

<blockquote><p>The default storage type of a datanode storage location will be DISK if it does not have a storage type tagged explicitly.</p></blockquote>

<p>3 同步配置并重启dfs</p>

<pre><code>[root@cu2 ~]# scp /etc/security/limits.conf cu3:/etc/security/
[hadoop@cu2 hadoop-2.6.3]$ rsync -vaz etc cu3:~/hadoop-2.6.3/ 

[hadoop@cu2 hadoop-2.6.3] sbin/stop-dfs.sh
[hadoop@cu2 hadoop-2.6.3] sbin/start-dfs.sh
</code></pre>

<p>可以去到datanode查看日志，可以看到 /dev/shm/dfs/data 路径 <strong>StorageType</strong> 为 <strong>RAM_DISK</strong> ：</p>

<pre><code>2016-05-05 19:33:39,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /data/bigdata/hadoop/dfs/data/current
2016-05-05 19:33:39,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /data/bigdata/hadoop/dfs/data/current, StorageType: DISK
2016-05-05 19:33:39,863 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /dev/shm/dfs/data/current
2016-05-05 19:33:39,863 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /dev/shm/dfs/data/current, StorageType: RAM_DISK
</code></pre>

<p>同时查看 内存盘 的路径内容：</p>

<pre><code>[hadoop@cu2 hadoop-2.6.3]$ ssh cu3 tree /dev/shm/dfs
/dev/shm/dfs
└── data
    ├── current
    │   ├── BP-1108852639-192.168.0.148-1452322889531
    │   │   ├── current
    │   │   │   ├── finalized
    │   │   │   ├── rbw
    │   │   │   └── VERSION
    │   │   └── tmp
    │   └── VERSION
    └── in_use.lock

7 directories, 3 files
</code></pre>

<h2>测试使用</h2>

<p>通过三个例子对比，简单描述下使用。首先，使用默认的方式(主要用于对比)，第二个例子写文件是添加参数，第三个设置目录的存储类型（目录/文件会继承父目录的存储类型）</p>

<p>1 测试1</p>

<pre><code>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -put README.txt /tmp/

[hadoop@cu2 hadoop-2.6.3]$ hdfs fsck /tmp/README.txt -files -blocks -locations
...
/tmp/README.txt 1366 bytes, 1 block(s):  OK
0. BP-1108852639-192.168.0.148-1452322889531:blk_1073752574_11776 len=1366 repl=1 [192.168.0.148:50010]

[hadoop@cu3 hadoop-2.6.3]$ find /data/bigdata/hadoop/dfs/data/ /dev/shm/dfs/data/ -name "*1073752574*"
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir41/blk_1073752574_11776.meta
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir41/blk_1073752574
</code></pre>

<p>2 写文件时添加 lazy_persist 标识</p>

<pre><code># 添加 -l 参数，后台代码会加上 LAZY_PERSIST 标识。
[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -help put 
-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt; :
  Copy files from the local file system into fs. Copying fails if the file already
  exists, unless the -f flag is given.
  Flags:

  -p  Preserves access and modification times, ownership and the mode. 
  -f  Overwrites the destination if it already exists.                 
  -l  Allow DataNode to lazily persist the file to disk. Forces        
         replication factor of 1. This flag will result in reduced
         durability. Use with care.
</code></pre>

<p><img src="/images/blogs/storage-lazy.png" alt="" /></p>

<pre><code># -l 参数会把 replication 强制设置成数字1 ！
[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -put -l README.txt /tmp/readme.txt2

# 查看namenode的日志，可以看到文件写入到 RAM_DISK 类型的存储
[hadoop@cu2 hadoop-2.6.3]$ less logs/hadoop-hadoop-namenode-cu2.log 

    2016-05-05 20:38:36,465 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/readme.txt2._COPYING_. BP-1108852639-192.168.0.148-1452322889531 blk_1073752578_11780{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-dcb2673f-3297-4bd7-af1c-ac0ee3eebaf9:NORMAL:192.168.0.30:50010|RBW]]}
    2016-05-05 20:38:36,592 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 192.168.0.30:50010 is added to blk_1073752578_11780{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[RAM_DISK]DS-bf1ab64f-7eb3-41e0-8466-43287de9893d:NORMAL:192.168.0.30:50010|FINALIZED]]} size 0
    2016-05-05 20:38:36,594 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/readme.txt2._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1388277364_1

# 具体的内容所在位置
[hadoop@cu4 ~]$ tree /dev/shm/dfs/data/
/dev/shm/dfs/data/
├── current
│   ├── BP-1108852639-192.168.0.148-1452322889531
│   │   ├── current
│   │   │   ├── finalized
│   │   │   │   └── subdir0
│   │   │   │       └── subdir42
│   │   │   │           ├── blk_1073752578
│   │   │   │           └── blk_1073752578_11780.meta
│   │   │   ├── rbw
│   │   │   └── VERSION
│   │   └── tmp
│   └── VERSION
└── in_use.lock
</code></pre>

<p>3 设置目录的存储类型</p>

<pre><code>[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -mkdir /ramdisk
[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -setStoragePolicy /ramdisk LAZY_PERSIST 
Set storage policy LAZY_PERSIST on /ramdisk

[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -put README.txt /ramdisk

[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk
The storage policy of /ramdisk:
BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}

# 不支持通配符
[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk/*
getStoragePolicy: File/Directory does not exist: /ramdisk/*

[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk/README.txt
The storage policy of /ramdisk/README.txt:
BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}


# 添加replication参数，再测试多个备份只有一个写ram_disk
[hadoop@cu2 hadoop-2.6.3]$ hdfs dfs -Ddfs.replication=3 -put README.txt /ramdisk/readme.txt2

[hadoop@cu2 hadoop-2.6.3]$ hdfs dfsadmin -getStoragePolicy /ramdisk/readme.txt2
The storage policy of /ramdisk/readme.txt2:
BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}

[hadoop@cu2 hadoop-2.6.3]$ hdfs fsck /ramdisk/readme.txt2 -files -blocks -locations

    /ramdisk/readme.txt2 1366 bytes, 1 block(s):  OK
    0. BP-1108852639-192.168.0.148-1452322889531:blk_1073752580_11782 len=1366 repl=3 [192.168.0.30:50010, 192.168.0.174:50010, 192.168.0.148:50010]

[hadoop@cu3 ~]$ find /data/bigdata/hadoop/dfs/data /dev/shm/dfs/data -name "*1073752580*"
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580_11782.meta

# 已经把ram_disk的内容持久化到磁盘了("Lazy_Persist")
[hadoop@cu4 ~]$ find /data/bigdata/hadoop/dfs/data /dev/shm/dfs/data -name "*1073752580*"
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/lazypersist/subdir0/subdir42/blk_1073752580
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/lazypersist/subdir0/subdir42/blk_1073752580_11782.meta
/dev/shm/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580
/dev/shm/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580_11782.meta

[hadoop@cu5 ~]$ find /data/bigdata/hadoop/dfs/data /dev/shm/dfs/data -name "*1073752580*"
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580_11782.meta
/data/bigdata/hadoop/dfs/data/current/BP-1108852639-192.168.0.148-1452322889531/current/finalized/subdir0/subdir42/blk_1073752580
</code></pre>

<p>[设想] 对于那些处理完就删除的临时文件，可以把持久化的时间设置的久一点 <code>dfs.datanode.lazywriter.interval.sec</code>。这样就不需要写磁盘了。</p>

<p>不要妄想了，反正都会持久化！就是缓冲的效果，其他没有了！！一次性存储并且不需要持久化的还是用alluxio吧。</p>

<pre><code>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.LazyWriter#saveNextReplica
    org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService#submitLazyPersistTask
</code></pre>

<h2>参考</h2>

<ul>
<li>挺详细的<a href="http://blog.csdn.net/androidlushangderen/article/details/51105876">HDFS异构存储</a></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[整理] Hadoop入门]]></title>
    <link href="http://winseliu.com/blog/2016/04/23/hadoop-guide-catalog/"/>
    <updated>2016-04-23T15:45:34+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/23/hadoop-guide-catalog</id>
    <content type="html"><![CDATA[<h2>1. 环境准备</h2>

<p>工欲善事其必先利其器。不要吝啬硬件上投入，找一个适合自己的环境！</p>

<ul>
<li>Windows

<ul>
<li><a href="/blog/2014/02/23/quickly-open-program-in-windows/">快速打开程序</a></li>
<li>Cygwin：Windows本地编译需要，执行命令比 cmd 更方便</li>
</ul>
</li>
<li><a href="/blog/2011/02/28/win7-install-fedora-linux/">Windows + Linux双系统</a></li>
<li>Linux

<ul>
<li><a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a></li>
<li><a href="/blog/2015/09/13/review-linux-101-hacks/">【linux 101 Hacks】读后感</a>

<ul>
<li><a href="/images/blogs/linux-101-hacks-review-securecrt-config.png">Socket5代理</a></li>
</ul>
</li>
<li><a href="/blog/2016/03/11/install-and-config-openvpn/">OpenVPN</a></li>
<li>docker

<ul>
<li><a href="/blog/2014/09/27/docker-start-guide-on-centos/">Docker入门</a></li>
<li><a href="/blog/2014/09/30/docker-ssh-on-centos/">配置ssh</a></li>
<li><a href="/blog/2014/10/18/docker-dnsmasq-handler-hosts-build-hadoop-cluster/">Dnsmasq</a></li>
</ul>
</li>
</ul>
</li>
</ul>


<h2>2. 安装部署hadoop/spark</h2>

<h4>编译安装</h4>

<ul>
<li>Hadoop安装与升级:

<ul>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-1-install-in-docker/">Docker中安装</a></li>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-2-hadoop-upgrade/">2.2升级到2.6</a></li>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-3-ha/">HA配置</a></li>
<li><a href="/blog/2016/01/07/hadoop-install-and-upgrade-4-ha-upgrade/">HA升级</a></li>
</ul>
</li>
<li><a href="/blog/2015/03/08/vmware-build-hadoop2-dot-6/">Centos6 Build hadoop2.6</a></li>
<li><a href="/blog/2015/03/09/windows-build-hadoop-2-dot-6/">Windows Build hadoop2.6</a></li>
<li><a href="/blog/2014/10/16/spark-build-and-configuration/">各版本Spark编译/搭建环境</a></li>
</ul>


<h4>功能优化</h4>

<ul>
<li><a href="/blog/2014/09/01/hadoop2-mapreduce-compress/">Hadoop2 Mapreduce输入输出压缩</a></li>
<li><a href="/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading/">Hadoop2 ShortCircuit Local Reading</a></li>
<li><a href="/blog/2014/07/30/hadoop2-snappy-compress/">Hadoop2 Snappy Compress</a>

<ul>
<li><a href="/blog/2016/04/08/snappy-centos5-on-hive-on-spark/">Hive-on-spark Snappy on Centos5</a></li>
</ul>
</li>
<li><a href="/blog/2016/05/05/hdfs-heterogeneous-storage.markdown">HDFS RamDisk内存缓冲</a></li>
</ul>


<h4>维护</h4>

<ul>
<li><a href="/blog/2013/02/22/hadoop-cluster-increases-nodes/">Hadoop集群增加节点</a></li>
<li><a href="/blog/2014/07/29/safely-remove-datanode/">安全的关闭datanode</a></li>
<li><a href="/blog/2015/03/25/deploy-separated-yarn-on-exists-hdfs-cluster/">已有HDFS上部署yarn</a></li>
<li><a href="/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs/">Hadoop不同版本yarn和hdfs混搭，spark-yarn环境配置</a></li>
</ul>


<h4>旧版本安装</h4>

<ul>
<li><a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a></li>
<li><a href="/blog/2013/03/24/pseudo-distributed-hadoop-in-windows/"><del>Windows配置hadoop伪分布式环境(续)</del></a> 不再推荐cygwin下部署Hadoop。</li>
<li><a href="/blog/2013/03/02/quickly-build-a-second-hadoop-cluster/">快速搭建第二个hadoop分布式集群环境</a></li>
<li><a href="/blog/2013/03/27/run-on-hadoop-on-ant/"><del>Ant实现hadoop插件Run-on-Hadoop</del></a></li>
</ul>


<h2>3. 进阶</h2>

<h4>配置深入理解</h4>

<ul>
<li><a href="/blog/2014/08/02/hadoop-datanode-config-should-equals/">Hadoop的datanode数据节点机器配置</a></li>
<li><a href="/blog/2016/03/17/hadoop-memory-opts-and-args/">Hadoop内存环境变量和参数</a></li>
<li><a href="/blog/2016/04/11/spark-on-yarn-memory-allocate/">Spark-on-yarn内存分配</a></li>
<li><a href="/blog/2016/03/25/spark-sql-executors-dynamic-on-yarn/">SparkSQL-on-YARN的Executors池(动态)配置</a></li>
</ul>


<h4>问题定位</h4>

<ul>
<li><a href="/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/">在windows开发测试mapreduce几种方式</a></li>
<li><a href="/blog/2014/04/22/remote-debug-hadoop2/">远程调试hadoop2以及错误处理方法</a></li>
<li><a href="/blog/2014/08/25/step-by-step-found-java-oom-error/">逐步定位Java程序OOM的异常</a></li>
</ul>


<h4>读码</h4>

<ul>
<li>Hadoop2 Balancer磁盘空间平衡

<ul>
<li><a href="/blog/2014/08/06/read-hadoop-balancer-source-part1/">上</a></li>
<li><a href="/blog/2014/09/05/read-hadoop-balancer-source-part2/">中</a></li>
<li><a href="/blog/2014/09/05/read-hadoop-balancer-source-part3/">下</a></li>
</ul>
</li>
<li><a href="/blog/2015/03/13/hadoop-distcp/">Hadoop Distcp</a></li>
</ul>


<h4>其他</h4>

<ul>
<li><a href="/blog/2014/09/12/scala-wordcount-on-hadoop/">Scala Wordcount on Hadoop2</a></li>
<li><a href="/blog/2014/12/07/hadoop-mr-rest-api/">MR Rest接口</a></li>
</ul>


<h2>4. Hadoop平台</h2>

<ul>
<li>zookeeper</li>
<li>hive

<ul>
<li><a href="/blog/2014/06/21/upgrade-hive/">Upgrade Hive: 0.12.0 to 0.13.1</a></li>
<li>tez:

<ul>
<li><a href="/blog/2014/06/18/hadoop-tez-firststep/">Tez编译及使用</a></li>
<li><a href="/blog/2016/01/12/tez-ui-config-and-run/">配置TEZ-UI</a></li>
</ul>
</li>
<li>hive on spark

<ul>
<li><a href="/blog/2016/03/28/hive-on-spark/">Hive on Spark</a></li>
<li><a href="/blog/2016/04/08/snappy-centos5-on-hive-on-spark/">Hive-on-spark Snappy on Centos5</a></li>
<li><a href="/blog/2016/03/29/limit-on-sparksql-and-hive/">Limit on Sparksql and Hive</a></li>
</ul>
</li>
<li><a href="/blog/2016/04/08/dbcp-parameters/">DBCP参数在Hive JDBC上的实践</a></li>
<li><a href="/blog/2016/04/13/hiveserver2-ui-and-upgrade-hive2-dot-0-0/">Hiveserver2 Ui and Upgrade hive2.0.0</a></li>
</ul>
</li>
<li>kafka

<ul>
<li><a href="/blog/2015/01/08/kafka-guide/">Kafka快速入门</a></li>
</ul>
</li>
<li>alluxio(tachyon)

<ul>
<li><a href="/blog/2015/04/15/tachyon-quickstart/">Tachyon入门指南</a></li>
<li><a href="/blog/2015/04/18/tachyon-deep-source/">Tachyon剖析</a></li>
<li><a href="/blog/2016/04/15/alluxio-quickstart2/">Alluxio入门大全2</a></li>
</ul>
</li>
</ul>


<h2>5. 监控与自动化部署</h2>

<h4>监控</h4>

<ul>
<li><a href="/blog/2013/02/26/linux-top-command-mannual/">top</a></li>
<li>nagios

<ul>
<li><a href="/blog/2015/09/25/nagios-start-guide/">Nagios监控主机</a></li>
</ul>
</li>
<li><del>cacti</del>    Ganglia更简单

<ul>
<li><a href="/blog/2015/09/22/cacti-start-guide/">Cacti监控主机</a></li>
<li><a href="/blog/2015/10/13/cacti-batch-adding-configurations/">Cacti批量添加配置</a></li>
</ul>
</li>
<li>ganglia

<ul>
<li><a href="/blog/2014/07/18/install-ganglia-on-redhat/"><del>Install Ganglia on Redhat5+</del></a> 手动安装依赖太麻烦了！</li>
<li><a href="/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/">安装配置Ganglia(2)</a></li>
<li><a href="/blog/2016/02/01/ganglia-python-extension/">Ganglia扩展-Python</a></li>
<li><a href="/blog/2016/02/25/ganglia-web-ui-views/">Ganglia页自定义视图</a></li>
</ul>
</li>
</ul>


<h4>自动化</h4>

<ul>
<li>git:

<ul>
<li><a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a></li>
<li><a href="/blog/2014/02/19/maven-package-dependent-git-projects/">打包依赖的git项目</a></li>
<li><a href="/blog/2013/05/27/handle-git-conflict/">处理git冲突</a></li>
</ul>
</li>
<li><a href="/blog/2014/09/07/expect-automate-and-batch-config-ssh/">expect-批量实现SSH无密钥登录</a></li>
<li>puppet

<ul>
<li><a href="/blog/2016/04/08/puppet-install/">puppet4.4.1入门安装</a></li>
<li><a href="/blog/2016/04/21/puppet-domain-fdqn/">puppet入门之域名证书</a></li>
<li><a href="/blog/2016/04/21/puppetdb-install-and-config/">puppetdb安装配置</a>

<ul>
<li><a href="/blog/2015/12/13/postgresql-start-guide/">postgresql入门</a></li>
</ul>
</li>
<li>puppet-ui

<ul>
<li><a href="/blog/2016/05/05/puppetboard-install/">puppetboard安装</a></li>
<li><a href="/blog/2016/04/21/puppetexplorer-setting/">puppetexplorer设置</a></li>
<li>foreman</li>
</ul>
</li>
<li><a href="/blog/2016/04/04/rpm-build-your-package/">RPM打包</a></li>
<li>puppet基本使用以及配置集群</li>
<li>mcollective

<ul>
<li><a href="/blog/2016/04/28/mcollective-quick-start/">安装配置</a></li>
<li><a href="/blog/2016/04/28/mcollective-plugins/">插件安装</a></li>
</ul>
</li>
<li><a href="/blog/2016/05/03/hiera-and-facts/">Hiera</a></li>
</ul>
</li>
</ul>


<p>&hellip;</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark-on-yarn内存分配]]></title>
    <link href="http://winseliu.com/blog/2016/04/11/spark-on-yarn-memory-allocate/"/>
    <updated>2016-04-11T19:44:51+08:00</updated>
    <id>http://winseliu.com/blog/2016/04/11/spark-on-yarn-memory-allocate</id>
    <content type="html"><![CDATA[<p>上次写了一篇关于配置参数是如何影响mapreduce的实际调度的<a href="/blog/2016/03/17/hadoop-memory-opts-and-args/">参考</a>：</p>

<ul>
<li>opts（yarn.app.mapreduce.am.command-opts、mapreduce.map.java.opts、mapreduce.reduce.java.opts）是实际运行程序是内存参数。</li>
<li>memory（yarn.app.mapreduce.am.resource.mb、mapreduce.map.memory.mb、mapreduce.reduce.memory.mb）是用于ResourceManager计算集群资源使用和调度。</li>
</ul>


<p>了解参数区别，就没有再深究task内存的问题了。</p>

<h2>新问题-内存分配</h2>

<p>这次又遇到内存问题：spark使用yarn-client的方式运行时，spark有memoryOverhead的设置，但是加了额外的内存后，再经过集群调度内存浪费严重，对于本来就小内存的集群来说完全无法接受。</p>

<ul>
<li>am默认是512加上384 overhead，也就是896m。但是调度后am分配内存资源为1024。</li>
<li>executor默认是1024加上384，等于1408M。单调度后executor分配内存资源为2048。</li>
</ul>


<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-0.png" alt="" /></p>

<p>从appmaster的日志可以看出来请求的内存大小是1408：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-1.png" alt="" /></p>

<p><strong>一个executor就浪费了500M，本来可以跑4个executor的但现在只能执行3个！</strong></p>

<p>关于内存参数的具体含义查看官网： <a href="http://spark.apache.org/docs/latest/running-on-yarn.html">spark-on-yarn</a> 和 <a href="http://hadoop.apache.org/docs/r2.6.4/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a></p>

<table>
<thead>
<tr>
<th></th>
<th style="text-align:center;"> <em>参数</em>                                  </th>
<th></th>
<th style="text-align:left;"> <em>值</em></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.am.memory                    </td>
<td></td>
<td style="text-align:left;"> 512m</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.driver.memory                     </td>
<td></td>
<td style="text-align:left;"> 1g</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.executor.memoryOverhead      </td>
<td></td>
<td style="text-align:left;"> executorMemory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.driver.memoryOverhead        </td>
<td></td>
<td style="text-align:left;"> driverMemory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> spark.yarn.am.memoryOverhead            </td>
<td></td>
<td style="text-align:left;"> AM memory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.nodemanager.resource.memory-mb     </td>
<td></td>
<td style="text-align:left;"> 8192</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.scheduler.minimum-allocation-mb    </td>
<td></td>
<td style="text-align:left;"> 1024</td>
</tr>
<tr>
<td></td>
<td style="text-align:center;"> yarn.scheduler.maximum-allocation-mb    </td>
<td></td>
<td style="text-align:left;"> 8192</td>
</tr>
</tbody>
</table>


<p>分配的内存看着像是 <strong>最小分配内存</strong> 的整数倍。把 <code>yarn.scheduler.minimum-allocation-mb</code> 修改为512，重启yarn再运行，executor的分配的内存果真减少到1536(<strong>512*3</strong>)。</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-3.png" alt="" /></p>

<p>同时 <a href="http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html">http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html</a> 这篇文章也讲 <strong>在YARN中，Container申请的内存大小必须为yarn.scheduler.minimum-allocation-mb的整数倍</strong> 。我们不去猜，调试下调度代码，看看究竟是什么情况。</p>

<pre><code>[hadoop@cu2 hadoop-2.6.3]$ sbin/yarn-daemon.sh stop resourcemanager 

[hadoop@cu2 hadoop]$ grep "minimum-allocation-mb" -1 yarn-site.xml 
&lt;property&gt;
&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;&lt;value&gt;512&lt;/value&gt;
&lt;/property&gt;

[hadoop@cu2 hadoop-2.6.3]$ export YARN_RESOURCEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8000"
[hadoop@cu2 hadoop-2.6.3]$ sbin/yarn-daemon.sh start resourcemanager 
</code></pre>

<p>本地eclipse在 <code>CapacityScheduler#allocate</code> 打断点，然后跑任务：</p>

<pre><code>hive&gt; set hive.execution.engine=spark;
hive&gt; select count(*) from t_ods_access_log2 where month=201512;
</code></pre>

<p>AppMaster内存分配：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-appmaster.png" alt="" /></p>

<p>Executor内存分配：</p>

<p><img src="/images/blogs/hive-on-spark-memory/hive-on-spark-memory-allocate-executor.png" alt="" /></p>

<p>request进到allocate后，最终调用 <code>DefaultResourceCalculator.normalize</code> 重新计算了一遍请求需要的资源，把内存调整了。默认的DefaultResourceCalculator可以通过 capacity-scheduler.xml 的 <code>yarn.scheduler.capacity.resource-calculator</code> 来修改。</p>

<p>具体代码调度过程如下：</p>

<pre><code>  public Allocation allocate(ApplicationAttemptId applicationAttemptId,
      List&lt;ResourceRequest&gt; ask, List&lt;ContainerId&gt; release, 
      List&lt;String&gt; blacklistAdditions, List&lt;String&gt; blacklistRemovals) {
    ...
    // Sanity check
    SchedulerUtils.normalizeRequests(
        ask, getResourceCalculator(), getClusterResource(),
        getMinimumResourceCapability(), maximumAllocation);
...

  public static void normalizeRequest(
      ResourceRequest ask, 
      ResourceCalculator resourceCalculator, 
      Resource clusterResource,
      Resource minimumResource,
      Resource maximumResource,
      Resource incrementResource) {
    Resource normalized = 
        Resources.normalize(
            resourceCalculator, ask.getCapability(), minimumResource,
            maximumResource, incrementResource);
    ask.setCapability(normalized);
  } 
...

  public static Resource normalize(
      ResourceCalculator calculator, Resource lhs, Resource min,
      Resource max, Resource increment) {
    return calculator.normalize(lhs, min, max, increment);
  }
...

  public Resource normalize(Resource r, Resource minimumResource,
      Resource maximumResource, Resource stepFactor) {
    int normalizedMemory = Math.min(
        roundUp(
            Math.max(r.getMemory(), minimumResource.getMemory()),
            stepFactor.getMemory()),
            maximumResource.getMemory());
    return Resources.createResource(normalizedMemory);
  }
...

  public static int roundUp(int a, int b) {
    return divideAndCeil(a, b) * b;
  }
</code></pre>

<h2>小结</h2>

<p>今天又重新认识一个yarn参数 <code>yarn.scheduler.minimum-allocation-mb</code> ，不仅仅是最小分配的内存，同时分配的资源也是minimum-allocation-mb的整数倍，还告诉我们 <code>yarn.nodemanager.resource.memory-mb</code> 也最好是minimum-allocation-mb的整数倍。</p>

<p>间接的学习了新的参数，可以通过 <code>yarn.scheduler.capacity.resource-calculator</code> 参数 来修改 CapacityScheduler 调度器的资源计算类。</p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop内存环境变量和参数]]></title>
    <link href="http://winseliu.com/blog/2016/03/17/hadoop-memory-opts-and-args/"/>
    <updated>2016-03-17T14:09:26+08:00</updated>
    <id>http://winseliu.com/blog/2016/03/17/hadoop-memory-opts-and-args</id>
    <content type="html"><![CDATA[<h2>问题：</h2>

<p><a href="https://www.zhihu.com/question/25498407">https://www.zhihu.com/question/25498407</a></p>

<p>问题是hadoop内存的配置，涉及两个方面：</p>

<ul>
<li>namenode/datanode/resourcemanager/nodemanager的HEAPSIZE环境变量</li>
<li>在配置文件/Configuration中影响MR运行的变量</li>
</ul>


<p>尽管搞hadoop有好一阵子了，对这些变量有个大概的了解，但没有真正的去弄懂他们的区别。乘着这个机会好好的整整（其实就是下载源码然后全文查找<sup>V</sup>^）。</p>

<h2>HEAPSIZE环境变量</h2>

<p>hadoop-env.sh配置文件hdfs和yarn脚本都会加载。hdfs是一脉相承使用 <strong>HADOOP_HEAPSIZE</strong> ，而yarn使用新的环境变量 <strong>YARN_HEAPSIZE</strong> 。</p>

<p>hadoop/hdfs/yarn命令最终会把HEAPSIZE的参数转换了 <strong>JAVA_HEAP_MAX</strong>，把它作为启动参数传递给Java。</p>

<ul>
<li>hadoop</li>
</ul>


<p>hadoop命令是把 <code>HADOOP_HEAPSIZE</code> 转换为 <code>JAVA_HEAP_MAX</code> ，调用路径：</p>

<p><code>hadoop -&gt; hadoop-config.sh -&gt; hadoop-env.sh</code></p>

<pre><code>JAVA_HEAP_MAX=-Xmx1000m 

# check envvars which might override default args
if [ "$HADOOP_HEAPSIZE" != "" ]; then
  #echo "run with heapsize $HADOOP_HEAPSIZE"
  JAVA_HEAP_MAX="-Xmx""$HADOOP_HEAPSIZE""m"
  #echo $JAVA_HEAP_MAX
fi
</code></pre>

<ul>
<li>hdfs</li>
</ul>


<p>hdfs其实就是从hadoop脚本里面分离出来的。调用路径：</p>

<p><code>hdfs -&gt; hdfs-config.sh -&gt; hadoop-config.sh -&gt; hadoop-env.sh</code></p>

<ul>
<li>yarn</li>
</ul>


<p>yarn也调用了hadoop-env.sh，但是设置内存的参数变成了 <strong>YARN_HEAPSIZE</strong> 。调用路径：</p>

<p><code>yarn -&gt; yarn-config.sh -&gt; hadoop-config.sh -&gt; hadoop-env.sh</code></p>

<pre><code>JAVA_HEAP_MAX=-Xmx1000m 

# For setting YARN specific HEAP sizes please use this
# Parameter and set appropriately
# YARN_HEAPSIZE=1000

# check envvars which might override default args
if [ "$YARN_HEAPSIZE" != "" ]; then
  JAVA_HEAP_MAX="-Xmx""$YARN_HEAPSIZE""m"
fi
</code></pre>

<ul>
<li>实例：</li>
</ul>


<p>配置hadoop参数的时刻，一般都是配置 <strong>hadoop-env.sh</strong> 如：<code>export HADOOP_HEAPSIZE=16000</code> 。查看相关进程命令有：</p>

<pre><code>/usr/local/jdk1.7.0_17/bin/java -Dproc_resourcemanager -Xmx1000m
/usr/local/jdk1.7.0_17/bin/java -Dproc_timelineserver -Xmx1000m
/usr/local/jdk1.7.0_17/bin/java -Dproc_nodemanager -Xmx1000m 
/usr/local/jdk1.7.0_17/bin/java -Dproc_journalnode -Xmx16000m
/usr/local/jdk1.7.0_17/bin/java -Dproc_namenode -Xmx16000m
/usr/local/jdk1.7.0_17/bin/java -Dproc_journalnode -Xmx16000m
/usr/local/jdk1.7.0_17/bin/java -Dproc_datanode -Xmx16000m
</code></pre>

<p>与hdfs有关的内存都修改成功了。而与yarn的还是默认的1g(堆)内存。</p>

<h2>MR配置文件参数</h2>

<p>分成两组，一种是直接设置数字(mb结束的属性)，一种是配置java虚拟机变量的-Xmx。</p>

<pre><code>* yarn.app.mapreduce.am.resource.mb、mapreduce.map.memory.mb、mapreduce.reduce.memory.mb
    用于调度计算内存，是不是还能分配任务（计算额度）
* yarn.app.mapreduce.am.command-opts、mapreduce.map.java.opts、mapreduce.reduce.java.opts
    程序实际启动使用的参数
</code></pre>

<p>一个是控制中枢，一个是实实在在的限制。</p>

<ul>
<li>官网文档的介绍：</li>
</ul>


<blockquote><ul>
<li>mapreduce.map.memory.mb 1024    The amount of memory to request from the scheduler for each map task.</li>
<li>mapreduce.reduce.memory.mb  1024    The amount of memory to request from the scheduler for each reduce task.</li>
<li>mapred.child.java.opts  -Xmx200m    Java opts for the task processes.</li>
</ul>
</blockquote>

<ul>
<li><p>下面用实践来验证效果：</p>

<ul>
<li>先搞一个很大大只有一个block的文件，把程序运行时间拖长一点</li>
<li>修改opts参数，查看效果</li>
<li>修改mb参数，查看效果</li>
</ul>
</li>
<li><p>实践一</p></li>
</ul>


<p>mapreduce.map.memory.mb设置为1000，而mapreduce.map.java.opts设置为1200m。程序照样跑的很欢！！</p>

<p>同时从map的 YarnChild 进程看出起实际作用的是 mapreduce.map.java.opts 参数。memory.mb用来计算节点是否有足够的内存来跑任务，以及用来计算整个集群的可用内存等。而java.opts则是用来限制真正任务的堆内存用量。</p>

<p><strong>注意</strong> ： 这里仅仅是用来测试，正式环境java.opts的内存应该小于memory.mb！！具体配置参考：<a href="http://blog.javachen.com/2015/06/05/yarn-memory-and-cpu-configuration.html">yarn-memory-and-cpu-configuration</a></p>

<p><img src="/images/blogs/hadoop-opts/yarn-opts-mb.jpg" alt="" /></p>

<ul>
<li>实践二</li>
</ul>


<p>map.memory.mb设置太大，导致调度失败！</p>

<p><img src="/images/blogs/hadoop-opts/yarn-mb-1.jpg" alt="" /></p>

<ul>
<li>实践三</li>
</ul>


<p>尽管实际才用不大于1.2G的内存，但是由于mapreduce.map.memory.mb设置为8G，整个集群显示已用内存18G（2 * 8g + 1 * 2g）。登录实际运行任务的机器，实际内存其实不多。</p>

<p><img src="/images/blogs/hadoop-opts/yarn-mb-2.jpg" alt="" />
<img src="/images/blogs/hadoop-opts/yarn-mb-3.jpg" alt="" /></p>

<p>reduce和am（appmaster）的参数类似。</p>

<p><img src="/images/blogs/hadoop-opts/yarn-appmaster-mb-1.jpg" alt="" />
<img src="/images/blogs/hadoop-opts/yarn-appmaster-mb-2.jpg" alt="" /></p>

<h2>mapred.child.java.opts参数</h2>

<p>这是一个过时的属性，当然你设置也能起效果(没有设置mapreduce.map.java.opts/mapreduce.reduce.java.opts)。相当于把MR的java.opts都设置了。</p>

<p><img src="/images/blogs/hadoop-opts/mapred-opts.jpg" alt="" /></p>

<p>获取map/reduce的opts中间会取 <strong>mapred.child.java.opts</strong> 的值。</p>

<p><img src="/images/blogs/hadoop-opts/mapred-opts-2.jpg" alt="" /></p>

<h2>admin-opts</h2>

<p>查找源码后，其实opts被分成两部分：admin和user。admin的写在前面，user在后面。user设置的opts可以覆盖admin设置的。应该是方便用于设置默认值吧。</p>

<h2>实例</h2>

<p>同时在一台很牛掰的机器上跑程序（分了yarn.nodemanager.resource.memory-mb 26G内存），但是总是只能一次跑一个任务，但还剩很多内存(20G)没有用啊！！初步怀疑是调度算法的问题。</p>

<p>查看了调度的日志，初始化的时刻会输出 <strong>scheduler.capacity.LeafQueue</strong> 的日志，打印了集群控制的一些参数。然后 同时找到一篇<a href="http://stackoverflow.com/questions/33465300/why-does-yarn-job-not-transition-to-running-state">http://stackoverflow.com/questions/33465300/why-does-yarn-job-not-transition-to-running-state</a> 说是调整 <strong>yarn.scheduler.capacity.maximum-am-resource-percent</strong> ，是用于控制appmaster最多可用的资源。</p>

<p>appmaster的默认内存是： <strong>yarn.app.mapreduce.am.resource.mb  1536</strong>（client设置有效）， <strong>yarn.scheduler.capacity.maximum-am-resource-percent 0.1</strong>。</p>

<p>跑第二job的时刻，第二个appmaster调度的时刻没有足够的内存（26G * 0.1 - 1.536 > 1.536），所以就跑不了两个job。</p>

<h2>CLIENT_OPTS</h2>

<p>一般 HADOOP 集群都会配套 HIVE，hive直接用 sql 来查询数据比mapreduce简单很多。启动hive是直接用 hadoop jar 来启动的。相对于一个客户端程序。控制hive内存的就是 HADOOP_CLIENT_OPTS 环境变量中的 -Xmx 。</p>

<p>所以要调整 hive 内存的使用，可以通过调整 HADOOP_CLIENT_OPTS 来控制。（当然理解这些环境变量，你就可以随心随欲的改）</p>

<pre><code>[hadoop@cu2 hive]$ sh -x bin/hiveserver2 
...
++ exec /home/hadoop/hadoop/bin/hadoop jar /home/hadoop/hive/lib/hive-service-1.2.1.jar org.apache.hive.service.server.HiveServer2

[hadoop@cu2 hive]$ grep -3  "HADOOP_CLIENT_OPTS" ~/hadoop/etc/hadoop/hadoop-env.sh
export HADOOP_PORTMAP_OPTS="-Xmx512m $HADOOP_PORTMAP_OPTS"

# The following applies to multiple commands (fs, dfs, fsck, distcp etc)
export HADOOP_CLIENT_OPTS="-Xmx128m $HADOOP_CLIENT_OPTS"
#HADOOP_JAVA_PLATFORM_OPTS="-XX:-UsePerfData $HADOOP_JAVA_PLATFORM_OPTS"

# On secure datanodes, user to run the datanode as after dropping privileges.

[hadoop@cu2 hive]$ jinfo 10249
...

VM Flags:

-Xmx256m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/hadoop-2.6.3/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.6.3 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/hadoop/hadoop-2.6.3/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx128m -Dhadoop.security.logger=INFO,NullAppender

[hadoop@cu2 hive]$ jmap -heap 10249
...
Heap Configuration:
   MinHeapFreeRatio = 40
   MaxHeapFreeRatio = 70
   MaxHeapSize      = 134217728 (128.0MB)
...
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装配置Ganglia(2)]]></title>
    <link href="http://winseliu.com/blog/2016/01/23/install-and-config-ganglia-on-redhat-2/"/>
    <updated>2016-01-23T17:47:28+08:00</updated>
    <id>http://winseliu.com/blog/2016/01/23/install-and-config-ganglia-on-redhat-2</id>
    <content type="html"><![CDATA[<p>前一篇介绍了全部手工安装Ganglia的文章，当时安装测试的环境比较简单。按照网上的步骤安装好，看到图了以为就懂了。Ganglia的基本多播/单播的概念都没弄懂。</p>

<p>这次有机会把Ganglia安装到正式环境，由于网络复杂一些，遇到新的问题。也更进一步的了解了Ganglia。</p>

<p>后端Gmetad(ganglia meta daemon)和Gmond(ganglia monitoring daemon)是Ganglia的两个组件。</p>

<p>Gmetad负责收集各个cluster的数据，并更新到rrd数据库中；Gmond把本机的数据UDP广播（或者单播给某台机），同时收集集群节点的数据供Gmetad读取。Gmetad并不用于监控数据的汇总，是对已经采集好的全部数据处理并存储到rrdtool数据库。</p>

<h2>搭建yum环境</h2>

<p>由于正式环境没有提供外网环境，所以需要把安装光盘拷贝到机器，作为yum的本地源。</p>

<pre><code>mount -t iso9660 -o loop rhel-server-6.4-x86_64-dvd\[ED2000.COM\].iso iso/
ln -s iso rhel6.4

vi /etc/yum.repos.d/rhel.repo 
[os]
name = Linux OS Packages
baseurl = file:///opt/rhel6.4
enabled=1
gpgcheck = 0
</code></pre>

<p>再极端点，yum程序都没有安装。到 Packages 目录用 rpm 安装 <code>yum*</code> 。</p>

<p>安装httpd后，把 rhel6.4 源建一个软链接到 <code>/var/www/html/rhel6.4</code> ，其他机器就可以使用该源来进行安装软件了。</p>

<pre><code>cat /etc/yum.repos.d/rhel.repo
[http]
name=LOCAL YUM server
baseurl = http://cu-omc1/rhel6.4
enabled=1
gpgcheck=0
</code></pre>

<p>注意：如果用CentOS的ISO会有两个光盘，两个地址用逗号分隔全部加到baseurl（http方式也一样）：</p>

<pre><code>[centos-local]
name=Centos Local
baseurl=file:///mnt/cdrom,file:///mnt/cdrom2 
failovermethod=priority
enabled=1
gpgcheck=0
</code></pre>

<h2>使用yum安装依赖</h2>

<pre><code>yum install -y gcc gd httpd php php-devel php-mysql php-pear php-common php-gd php-mbstring php-cli 

yum install -y rrdtool 

yum install -y apr*

# 编译Ganglia时加 --with-libpcre=no 可以不安装pcre
yum install -y pcre*

# yum install -y zlib-devel
</code></pre>

<h2>(仅)编译安装Ganglia</h2>

<p>下载下面的软件(yum没有这些软件)：</p>

<ul>
<li><a href="http://rpm.pbone.net/index.php3/stat/4/idpl/15992683/dir/scientific_linux_6/com/rrdtool-devel-1.3.8-6.el6.x86_64.rpm.html">rrdtool-devel-1.3.8-6.el6.x86_64.rpm</a></li>
<li><a href="http://download.savannah.gnu.org/releases/confuse/">confuse-2.7.tar.gz</a></li>
<li><a href="http://sourceforge.net/projects/ganglia/files/ganglia%20monitoring%20core/">ganglia</a></li>
<li><a href="http://sourceforge.net/projects/ganglia/files/ganglia-web/">ganglia-web</a></li>
</ul>


<p>安装：</p>

<pre><code>umask 0022 # 临时修改下，不然后面会遇到权限问题

rpm -ivh rrdtool-devel-1.3.8-6.el6.x86_64.rpm 

# 如果yum可以安装的话：yum install -y libconfuse*
tar zxf confuse-2.7.tar.gz
cd confuse-2.7
./configure CFLAGS=-fPIC --disable-nls
make &amp;&amp; make install

tar zxf ganglia-3.7.2.tar.gz 
cd ganglia-3.7.2
./configure --with-gmetad --enable-gexec --enable-status --prefix=/usr/local/ganglia
# 可选项，用于指定默认配置位置 `-sysconfdir=/etc/ganglia`

make &amp;&amp; make install

cp gmetad/gmetad.init /etc/init.d/gmetad
chkconfig gmetad on
# 查看gmetad的情况
chkconfig --list | grep gm

df -h # 把rrds目录放到最大的分区，再做个链接到data目录下
mkdir -p /data/ganglia/rrds
chown nobody:nobody /data/ganglia/rrds
ln -s /usr/local/ganglia/sbin/gmetad /usr/sbin/gmetad

gmetad -h # 查看默认的config位置。下面步骤AB 二选一 根据是否配置 sysconfdir 选项
# 步骤A
# cp gmetad/gmetad.conf /etc/ganglia/
# 步骤B
vi /etc/init.d/gmetad 
  /usr/local/ganglia/etc/gmetad.conf #修改原来的默认配置路径

cd ganglia-3.7.2/gmond/
ln -s /usr/local/ganglia/sbin/gmond /usr/sbin/gmond
cp gmond.init /etc/init.d/gmond
chkconfig gmond on
chkconfig --list gmond

gmond -h # 查看默认的config位置。
./gmond -t &gt;/usr/local/ganglia/etc/gmond.conf
vi /etc/init.d/gmond 
  /usr/local/ganglia/etc/gmond.conf #修改原来的默认配置路径
</code></pre>

<h2>配置</h2>

<ul>
<li>Ganglia配置</li>
</ul>


<pre><code>vi /usr/local/ganglia/etc/gmetad.conf
  datasource "HADOOP" hadoop-master1
  datasource "CU" cu-ud1
  rrd_rootdir "/data/ganglia/rrds"
  gridname "bigdata"

vi /usr/local/ganglia/etc/gmond.conf
  cluster {
   name = "CU"

  udp_send_channel {
   bind_hostname = yes
</code></pre>

<p><a href="http://ixdba.blog.51cto.com/2895551/1149003">http://ixdba.blog.51cto.com/2895551/1149003</a></p>

<p>Ganglia的收集数据工作可以工作在单播（unicast)或多播(multicast)模式下，默认为多播模式。</p>

<ul>
<li>单播：发送自己 <strong>收集</strong> 到的监控数据到特定的一台或几台机器上，可以跨网段</li>
<li>多播：发送自己收集到的监控数据到同一网段内所有的机器上，同时收集同一网段内的所有机器发送过来的监控数据。因为是以广播包的形式发送，因此需要同一网段内。但同一网段内，又可以定义不同的发送通道。</li>
</ul>


<p>主机多网卡(多IP)情况下需要绑定到特定的IP，设置bind_hostname来设置要绑定的IP地址。单IP情况下可以不需要考虑。</p>

<p>多播情况下只能在单一网段进行，如果集群存在多个网段，可以分拆成多个子集群（data_source)，或者使用单播来进行配置。期望配置简单点的话，配置多个 data_source 。</p>

<ul>
<li><code>data_source "cluster-db" node1 node2</code>  定义集群名称，以及获取集群监控数据的节点。由于采用multicast模式，每台gmond节点都有本集群内节点服务器的所有监控数据，因此不必把所有节点都列出来。node1 node2是or的关系，如果node1无法下载，则才会尝试去node2下载，所以它们应该都是同一个集群的节点，保存着同样的数据。</li>
<li><code>cluster.name</code> 本节点属于哪个cluster，需要与data_source对应。</li>
<li><code>host.location</code> 类似于hostname的作用。</li>
<li><code>udp_send_channel.mcast_join/host</code> 多播地址，工作在239.2.11.71通道下。如果使用单播模式，则要写host=node1，单播模式下可以配置多个upd_send_channel</li>
<li><code>udp_recv_channel.mcast_join</code></li>
</ul>


<p><strong>参考思路</strong> (未具体实践)：多网段情况可以用单播解决，要是单网段要配置多个data_source(集群)那就换个多播的端口吧！</p>

<h2>启动以及测试</h2>

<pre><code>service httpd restart
service gmetad start
service gmond start

[root@cu-omc1 ganglia]# netstat -anp | grep gm
tcp        0      0 0.0.0.0:8649                0.0.0.0:*                   LISTEN      916/gmond           
tcp        0      0 0.0.0.0:8651                0.0.0.0:*                   LISTEN      12776/gmetad        
tcp        0      0 0.0.0.0:8652                0.0.0.0:*                   LISTEN      12776/gmetad        
udp        0      0 239.2.11.71:8649            0.0.0.0:*                               916/gmond           
udp        0      0 192.168.31.11:60126         239.2.11.71:8649            ESTABLISHED 916/gmond           
unix  2      [ ]         DGRAM                    1331526917 12776/gmetad        
[root@cu-omc1 ganglia]# bin/gstat -a
CLUSTER INFORMATION
       Name: CU
      Hosts: 0
Gexec Hosts: 0
 Dead Hosts: 0
  Localtime: Wed Jun 15 20:17:36 2016

There are no hosts up at this time



netstat -anp | grep -E "gmond|gmetad"

# 启动如果有问题，使用调试模式启动查找问题
/usr/sbin/gmetad -d 10

/usr/local/ganglia/bin/gstat -a
/usr/local/ganglia/bin/gstat -a -i hadoop-master1

telnet localhost 8649
telnet localhost 8651
</code></pre>

<p>问题：多播地址绑定失败</p>

<p>如果telnet8649没有数据，查看下route是否有 [hostname对应的IP] 到 [239.2.11.71] 的路由！(多网卡多IP的时刻，可能default的路由并非主机名对应IP的地址)</p>

<blockquote><p><a href="http://llydmissile.blog.51cto.com/7784666/1411239">http://llydmissile.blog.51cto.com/7784666/1411239</a>
<a href="http://www.cnblogs.com/Cherise/p/4350581.html">http://www.cnblogs.com/Cherise/p/4350581.html</a></p>

<p>测试过程中可能会出现以下错误：Error creating multicast server mcast_join=239.2.11.71 port=8649 mcast_if=NULL family=&lsquo;inet4&rsquo;. Will try again&hellip;，系统不支持多播，需要将多播ip地址加入路由表，使用route add -host 239.2.11.71 dev eth0命令即可，将该命令加入/etc/rc.d/rc.local文件中，一劳永逸</p></blockquote>

<pre><code>[root@hadoop-master4 ~]# gmond -d 10
loaded module: core_metrics
loaded module: cpu_module
loaded module: disk_module
loaded module: load_module
loaded module: mem_module
loaded module: net_module
loaded module: proc_module
loaded module: sys_module
udp_recv_channel mcast_join=239.2.11.71 mcast_if=NULL port=8649 bind=239.2.11.71 buffer=0
Error creating multicast server mcast_join=239.2.11.71 port=8649 mcast_if=NULL family='inet4'.  Will try again...
</code></pre>

<p>环境的default route被清理掉了(或者是由于网关和本机不在同一网段)。需要手动添加一条到网卡的route。</p>

<pre><code>[root@hadoop-master4 ~]# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.32.0    *               255.255.255.0   U     0      0        0 bond0
192.168.31.0    192.168.32.254  255.255.255.0   UG    0      0        0 bond0
link-local      *               255.255.0.0     U     1006   0        0 bond0
[root@hadoop-master4 ~]# route add -host 239.2.11.71 dev bond0
[root@hadoop-master4 ~]# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
239.2.11.71     *               255.255.255.255 UH    0      0        0 bond0
192.168.32.0    *               255.255.255.0   U     0      0        0 bond0
192.168.31.0    192.168.32.254  255.255.255.0   UG    0      0        0 bond0
link-local      *               255.255.0.0     U     1006   0        0 bond0
</code></pre>

<h2>安装GWeb</h2>

<pre><code>cd ~/ganglia-web-3.7.1
vi Makefile # 一次性配置好，不再需要去修改conf_default.php
    GDESTDIR = /var/www/html/ganglia
    GCONFDIR = /usr/local/ganglia/etc/
    GWEB_STATEDIR = /var/www/html/ganglia
    # Gmetad rootdir (parent location of rrd folder)
    GMETAD_ROOTDIR = /data/ganglia
    APACHE_USER = apache
make install

# 注意：内网还是需要改下 conf_default.php 一堆jquery的js。
# 如果Web不能访问，查看下防火墙以及SELinux
</code></pre>

<ul>
<li>httpd登录密码配置</li>
</ul>


<pre><code>htpasswd -c /var/www/html/ganglia/etc/htpasswd.users gangliaadmin 

vi /etc/httpd/conf/httpd.conf 

    &lt;Directory "/var/www/html/ganglia"&gt;
    #  SSLRequireSSL
       Options None
       AllowOverride None
       &lt;IfVersion &gt;= 2.3&gt;
          &lt;RequireAll&gt;
             Require all granted
    #        Require host 127.0.0.1

             AuthName "Ganglia Access"
             AuthType Basic
             AuthUserFile /var/www/html/ganglia/etc/htpasswd.users
             Require valid-user
          &lt;/RequireAll&gt;
       &lt;/IfVersion&gt;
       &lt;IfVersion &lt; 2.3&gt;
          Order allow,deny
          Allow from all
    #     Order deny,allow
    #     Deny from all
    #     Allow from 127.0.0.1

          AuthName "Ganglia Access"
          AuthType Basic
          AuthUserFile /var/www/html/ganglia/etc/htpasswd.users
          Require valid-user
       &lt;/IfVersion&gt;
    &lt;/Directory&gt;

service httpd restart
</code></pre>

<p>如果在nginx做权限控制，一样很简单：</p>

<pre><code>location /ganglia {
        proxy_pass http://localhost/ganglia;
        auth_basic "Ganglia Access";
        auth_basic_user_file "/var/www/html/ganglia/etc/htpasswd.users";
}
</code></pre>

<h2>集群配置</h2>

<pre><code>cd /usr/local 
# for h in cu-ud{1,2} hadoop-master{1,2} ; do echo $h ; done
for h in cu-ud1 cu-ud2 hadoop-master1 hadoop-master2 ; do 
    cd /usr/local;
    rsync -vaz  ganglia $h:/usr/local/ ;
    ssh $h ln -s /usr/local/ganglia/sbin/gmond /usr/sbin/gmond ;
    scp /etc/init.d/gmond $h:/etc/init.d/ ;
    ssh $h "chkconfig gmond on" ;
    ssh $h "yum install apr* -y" ; 
    ssh $h "service gmond start" ; 
done

# 不同的集群，gmond.conf的cluster.name需要修改

telnet hadoop-master1 8649
netstat -anp | grep gm
</code></pre>

<p>要是集群有变动，添加还好，删除的话，会存在原来的旧数据，页面会提示机器down掉了。可以删除rrds目录下对应集群中节点的数据，然后重庆gmetad/httpd即可。</p>

<h2>参考</h2>

<h3>内容</h3>

<pre><code>防火墙规则设置
iptables -I INPUT 3 -p tcp -m tcp --dport 80 -j ACCEPT
iptables -I INPUT 3 -p udp -m udp --dport 8649 -j ACCEPT

service iptables save
service iptables restart

关闭selinux
vi /etc/selinux/config
SELINUX=disabled
setenforce 0
</code></pre>

<p>实际应用中，需要监控的机器往往在不同的网段内，这个时候，就不能用gmond默认的多播方式（用于同一个网段内）来传送数据，必须使用单播的方法。</p>

<p>gmond可以配置成为一个cluster，这些gmond节点之间相互发送各自的监控数据。所以每个gmond节点上实际上都会有 cluster内的所有节点的监控数据。gmetad只需要去某一个节点获取数据就可以了。</p>

<p>web front-end 一个基于web的监控界面，通常和Gmetad安装在同一个节点上(还需确认是否可以不在一个节点上，因为php的配置文件中ms可配置gmetad的地址及端口)，它从Gmetad取数据，并且读取rrd数据库，生成图片，显示出来。</p>

<p>gmetad周期性的去gmond节点或者gmetad节点poll数据。一个gmetad可以设置多个datasource，每个datasource可以有多个备份，一个失败还可以去其他host取数据。Gmetad只有tcp通道，一方面他向datasource发送请求，另一方面会使用一个tcp端口，发 布自身收集的xml文件，默认使用8651端口。所以gmetad即可以从gmond也可以从其他的gmetad得到xml数据。</p>

<p>对于IO来说，Gmetad默认15秒向gmond取一次xml数据，如果gmond和gmetad都是在同一个节点，这样就相当于本地io请求。同时gmetad请求完xml文件后，还需要对其解析，也就是说按默认设置每15秒需要解析一个10m级别的xml文件，这样cpu的压力就会很大。同时它还有写入RRD数据库，还要处理来自web客户端的解析请求，也会读RRD数据库。这样本身的IO CPU 网络压力就很大，因此这个节点至少应该是个空闲的而且能力比较强的节点。</p>

<ul>
<li>多播模式配置
这个是默认的方式，基本上不需要修改配置文件，且所有节点的配置是一样的。这种模式的好处是所有的节点上的 gmond 都有完备的数据，gmetad 连接其中任意一个就可以获取整个集群的所有监控数据，很方便。
其中可能要修改的是 mcast_if 这个参数，用于指定多播的网络接口。如果有多个网卡，要填写对应的内网接口。</li>
<li>单播模式配置
监控机上的接收 Channel 配置。我们使用 UDP 单播模式，非常简单。我们的集群有部分机器在另一个机房，所以监听了 0.0.0.0，如果整个集群都在一个内网中，建议只 bind 内网地址。如果有防火墙，要打开相关的端口。</li>
<li>最重要的配置项是 data_source: <code>data_source "my-cluster" localhost:8648</code> 如果使用的是默认的 8649 端口，则端口部分可以省略。如果有多个集群，则可以指定多个 data_source，每行一个。</li>
<li>最后是 gridname 配置，用于给整个 Grid 命名</li>
<li><a href="https://github.com/ganglia/gmond_python_modules">https://github.com/ganglia/gmond_python_modules</a></li>
</ul>


<h3>网址</h3>

<ul>
<li><a href="http://yhz.me/blog/Install-Ganglia-On-CentOS.html">在 CentOS 6.5 上安装 Ganglia 3.6.0</a></li>
<li>*<a href="http://ixdba.blog.51cto.com/2895551/1149003">分布式监控系统ganglia配置文档</a></li>
<li><p>*<a href="http://www.3mu.me/%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BC%80%E6%BA%90%E7%9B%91%E6%8E%A7%E8%BD%AF%E4%BB%B6ganglia-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/">企业级开源监控软件Ganglia 安装与配置</a></p></li>
<li><p>*<a href="http://jerrypeng.me/2014/07/04/server-side-java-monitoring-ganglia/">Java 服务端监控方案（二. Ganglia 篇）</a></p></li>
<li><a href="http://jerrypeng.me/2014/07/22/server-side-java-monitoring-nagios/">Java 服务端监控方案（三. Nagios 篇）</a></li>
<li><p><a href="https://github.com/ganglia/ganglia-web/wiki/Nagios-Integration">https://github.com/ganglia/ganglia-web/wiki/Nagios-Integration</a></p></li>
<li><p><a href="https://ganglia.wikimedia.org/latest/">维基百科Ganglia</a></p></li>
</ul>


<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
