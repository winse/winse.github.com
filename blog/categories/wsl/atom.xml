<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: #wsl | Winse Blog]]></title>
  <link href="http://winse.github.io/blog/categories/wsl/atom.xml" rel="self"/>
  <link href="http://winse.github.io/"/>
  <updated>2024-01-16T13:54:48+08:00</updated>
  <id>http://winse.github.io/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AIGC Setup on Win11 WSL2]]></title>
    <link href="http://winse.github.io/blog/2024/01/15/aigc-setup-on-windows-wsl-2/"/>
    <updated>2024-01-15T01:25:22+08:00</updated>
    <id>http://winse.github.io/blog/2024/01/15/aigc-setup-on-windows-wsl-2</id>
    <content type="html"><![CDATA[<p>看 <a href="https://github.com/01-ai/Yi">Yi</a>官方文档，一开始摸不着头脑，不知道从哪里入手。 网上找了一些资料，查到了苏洋的博客，先把环境搭建起来。</p>

<ul>
<li><a href="https://soulteary.com/2023/07/29/docker-based-deep-learning-environment-under-windows.html">基于 Docker 的深度学习环境：Windows</a></li>
<li><a href="https://soulteary.com/2023/07/29/get-started-with-stability-ai-sdxl-1-0-release-using-docker.html">使用 Docker 快速上手 Stability AI 的 SDXL 1.0 正式版-Linux</a></li>
</ul>


<p>为了在 Windows11 机器方便使用GPU，以及开源很多工程都提供docker入门，但WSL2慢，考虑本地已经搞了一个WSL1了会不会冲突，同时虚拟机里面也安装不了WSL2，VMWare桌面虚拟机的话直接使用GPU没有很好的方式等等，纠结了一天，最终还是选了安装 WSL2+Docker Desktop。</p>

<p>跟着文章，你将会了解Windows+WLS2+Docker怎么跑GPU模型，以及在国内怎么下载模型文件。</p>

<h2>使用WSL2</h2>

<p>在 启用或关闭Windows功能 中选择 虚拟机平台。</p>

<p></p>

<pre><code>wsl --update
wsl --set-default-version 2
</code></pre>

<p>然后在微软商店Microsoft Store里面安装 <strong>Ubuntu-20.04</strong> （版本选20或者22）的系统（通过应用商店的话就规避了可能安装同一个的Linux的问题：已经安装 在应用商店的按钮不是[获取]是[打开]）。</p>

<pre><code>winse@DESKTOP-BR4MG38:~$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

winse@DESKTOP-BR4MG38:~$ uname -a
Linux DESKTOP-BR4MG38 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
</code></pre>

<p>对比WSL1，WSL2的 <code>ip a</code> ，WSL2还是干净很多，把宿主机的一些信息合并到linux里面了（如：hosts）。</p>

<h2>Docker Desktop + WSL2</h2>

<ul>
<li><p><a href="https://docs.docker.com/desktop/install/windows-install/">https://docs.docker.com/desktop/install/windows-install/</a></p></li>
<li><p>微软的安装内容讲的差不多，增加了使用vscode的内容
<a href="https://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers#develop-in-remote-containers-using-vs-code">https://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers#develop-in-remote-containers-using-vs-code</a></p></li>
</ul>


<p>通过exe安装，安装过程中选择使用WSL2，装好后wsl显示多出了两个linux。</p>

<pre><code>C:\Users\P15&gt;wsl -l -v
  NAME                   STATE           VERSION
* Ubuntu                 Stopped         1
  Ubuntu-20.04           Running         2
  docker-desktop-data    Running         2
  docker-desktop         Running         2
</code></pre>

<pre><code>winse@DESKTOP-BR4MG38:~$ su -
root@DESKTOP-BR4MG38:~# echo "winse ALL=(ALL:ALL) NOPASSWD: ALL" &gt;&gt;/etc/sudoers

root@DESKTOP-BR4MG38:~# sed -i.bak -e 's|archive.ubuntu.com/ubuntu/|mirrors.aliyun.com/ubuntu/|' -e 's|security.ubuntu.com/ubuntu/|mirrors.aliyun.com/ubuntu/|' /etc/apt/sources.list
</code></pre>

<p>在 WSL-Ubuntu 里面可以直接用 Win11 的程序，直接查看docker的信息：</p>

<pre><code>winse@DESKTOP-BR4MG38:~$ docker version
Client: Docker Engine - Community
 Cloud integration: v1.0.35+desktop.5
 Version:           24.0.7
 API version:       1.43
 Go version:        go1.20.10
 Git commit:        afdd53b
 Built:             Thu Oct 26 09:08:17 2023
 OS/Arch:           linux/amd64
 Context:           default

Server: Docker Desktop
 Engine:
  Version:          24.0.7
  API version:      1.43 (minimum version 1.12)
  Go version:       go1.20.10
  Git commit:       311b9ff
  Built:            Thu Oct 26 09:08:02 2023
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.25
  GitCommit:        d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f
 runc:
  Version:          1.1.10
  GitCommit:        v1.1.10-0-g18a0cb0
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0

winse@DESKTOP-BR4MG38:~$ which docker
/usr/bin/docker
winse@DESKTOP-BR4MG38:~$ ll  /usr/bin/docker
lrwxrwxrwx 1 root root 48 Jan 13 11:03 /usr/bin/docker -&gt; /mnt/wsl/docker-desktop/cli-tools/usr/bin/docker*
</code></pre>

<p>其实用的就是windows的docker</p>

<p><img src="/images/blogs/ai/wsl2-docker-cli.png" alt="" /></p>

<p>镜像加速</p>

<p><img src="/images/blogs/ai/docker-mirror.png" alt="" /></p>

<p></p>

<p>保存会重启docker，再查看docker的信息，确认Registry Mirrors：</p>

<pre><code>winse@DESKTOP-BR4MG38:~$ docker info
Client: Docker Engine - Community
 Version:    24.0.7
 Context:    default
 Debug Mode: false
 Plugins:
  buildx: Docker Buildx (Docker Inc.)
    Version:  v0.12.0-desktop.2
    Path:     /usr/local/lib/docker/cli-plugins/docker-buildx
  compose: Docker Compose (Docker Inc.)
    Version:  v2.23.3-desktop.2
    Path:     /usr/local/lib/docker/cli-plugins/docker-compose
  dev: Docker Dev Environments (Docker Inc.)
    Version:  v0.1.0
    Path:     /usr/local/lib/docker/cli-plugins/docker-dev
  extension: Manages Docker extensions (Docker Inc.)
    Version:  v0.2.21
    Path:     /usr/local/lib/docker/cli-plugins/docker-extension
  feedback: Provide feedback, right in your terminal! (Docker Inc.)
    Version:  0.1
    Path:     /usr/local/lib/docker/cli-plugins/docker-feedback
  init: Creates Docker-related starter files for your project (Docker Inc.)
    Version:  v0.1.0-beta.10
    Path:     /usr/local/lib/docker/cli-plugins/docker-init
  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)
    Version:  0.6.0
    Path:     /usr/local/lib/docker/cli-plugins/docker-sbom
  scan: Docker Scan (Docker Inc.)
    Version:  v0.26.0
    Path:     /usr/local/lib/docker/cli-plugins/docker-scan
  scout: Docker Scout (Docker Inc.)
    Version:  v1.2.0
    Path:     /usr/local/lib/docker/cli-plugins/docker-scout

Server:
 Containers: 1
  Running: 1
  Paused: 0
  Stopped: 0
 Images: 5
 Server Version: 24.0.7
 Storage Driver: overlay2
  Backing Filesystem: extfs
  Supports d_type: true
  Using metacopy: false
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: cgroupfs
 Cgroup Version: 1
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f
 runc version: v1.1.10-0-g18a0cb0
 init version: de40ad0
 Security Options:
  seccomp
   Profile: unconfined
 Kernel Version: 5.15.133.1-microsoft-standard-WSL2
 Operating System: Docker Desktop
 OSType: linux
 Architecture: x86_64
 CPUs: 16
 Total Memory: 31.26GiB
 Name: docker-desktop
 ID: 340fee1c-e22a-485c-a973-f0e26d7535c9
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 HTTP Proxy: http.docker.internal:3128
 HTTPS Proxy: http.docker.internal:3128
 No Proxy: hubproxy.docker.internal
 Experimental: false
 Insecure Registries:
  hubproxy.docker.internal:5555
  127.0.0.0/8
 Registry Mirrors:
  https://us69kjun.mirror.aliyuncs.com/
  https://docker.mirrors.ustc.edu.cn/
  https://hub-mirror.c.163.com/
  https://mirror.baidubce.com/
 Live Restore Enabled: false

WARNING: No blkio throttle.read_bps_device support
WARNING: No blkio throttle.write_bps_device support
WARNING: No blkio throttle.read_iops_device support
WARNING: No blkio throttle.write_iops_device support
WARNING: daemon is not using the default seccomp profile
</code></pre>

<p></p>

<h2>GPU</h2>

<h3>Driver</h3>

<p>根据Win11机器的显卡安装最新版本驱动（不要在WSL中安装任何Linux版的Nvidia驱动！）</p>

<p><a href="https://www.nvidia.com/Download/index.aspx">https://www.nvidia.com/Download/index.aspx</a></p>

<p>输入nvidia-smi，查验是否安装成功。WSL2里面啥都不用做，在WSL2命令行直接就能查看nvidia-smi。</p>

<p>启动docker也能一样查看</p>

<pre><code>winse@DESKTOP-BR4MG38:stable-diffusion-taiyi$ docker run -it --rm --gpus all ubuntu nvidia-smi
</code></pre>

<p>其实这个启动的container也是一个WSL2。注意：WSL中不需要安装任何Linux版的Nvidia驱动！</p>

<p>验证 WLS2中Docker跑起来的容器 是否能够正常调用GPU：</p>

<ul>
<li><a href="https://soulteary.com/2023/07/29/docker-based-deep-learning-environment-under-windows.html">https://soulteary.com/2023/07/29/docker-based-deep-learning-environment-under-windows.html</a></li>
</ul>


<pre><code>winse@DESKTOP-BR4MG38:~$ docker pull nvcr.io/nvidia/pytorch:23.07-py3
23.07-py3: Pulling from nvidia/pytorch
3153aa388d02: Pulling fs layer
...
ee3f0ae6e80f: Pull complete
d4528227b5b8: Pull complete
Digest: sha256:c53e8702a4ccb3f55235226dab29ef5d931a2a6d4d003ab47ca2e7e670f7922b
Status: Downloaded newer image for nvcr.io/nvidia/pytorch:23.07-py3
nvcr.io/nvidia/pytorch:23.07-py3

What's Next?
  1. Sign in to your Docker account → docker login
  2. View a summary of image vulnerabilities and recommendations → docker scout quickview nvcr.io/nvidia/pytorch:23.07-py3


winse@DESKTOP-BR4MG38:~$ docker run -it --gpus=all --rm nvcr.io/nvidia/pytorch:23.07-py3 nvidia-smi

=============
== PyTorch ==
=============

NVIDIA Release 23.07 (build 63867923)
PyTorch Version 2.1.0a0+b5021ba

Container image Copyright (c) 2023, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.

Copyright (c) 2014-2023 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION &amp; AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be
   insufficient for PyTorch.  NVIDIA recommends the use of the following flags:
   docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 ...

Sat Jan 13 14:01:37 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.146.01             Driver Version: 537.99       CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro T2000                   On  | 00000000:01:00.0  On |                  N/A |
| N/A   43C    P8               6W /  60W |    856MiB /  4096MiB |      9%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A        27      G   /Xwayland                                 N/A      |
|    0   N/A  N/A        41      G   /Xwayland                                 N/A      |
|    0   N/A  N/A        42      G   /Xwayland                                 N/A      |
+---------------------------------------------------------------------------------------+


winse@DESKTOP-BR4MG38:~$ docker run --rm --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark
Unable to find image 'nvcr.io/nvidia/k8s/cuda-sample:nbody' locally
nbody: Pulling from nvidia/k8s/cuda-sample
22c5ef60a68e: Pull complete
1939e4248814: Pull complete
548afb82c856: Pull complete
a424d45fd86f: Pull complete
207b64ab7ce6: Pull complete
f65423f1b49b: Pull complete
2b60900a3ea5: Pull complete
e9bff09d04df: Pull complete
edc14edf1b04: Pull complete
1f37f461c076: Pull complete
9026fb14bf88: Pull complete
Digest: sha256:59261e419d6d48a772aad5bb213f9f1588fcdb042b115ceb7166c89a51f03363
Status: Downloaded newer image for nvcr.io/nvidia/k8s/cuda-sample:nbody
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
        -fullscreen       (run n-body simulation in fullscreen mode)
        -fp64             (use double precision floating point values for simulation)
        -hostmem          (stores simulation data in host memory)
        -benchmark        (run benchmark to measure performance)
        -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
        -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
        -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
        -compare          (compares simulation results running once on the default GPU and once on the CPU)
        -cpu              (run n-body simulation on the CPU)
        -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Quadro T2000]
16384 bodies, total time for 10 iterations: 64.071 ms
= 41.897 billion interactions per second
= 837.937 single-precision GFLOP/s at 20 flops per interaction


#再跑一遍
winse@DESKTOP-BR4MG38:~$ docker run --rm --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
        -fullscreen       (run n-body simulation in fullscreen mode)
        -fp64             (use double precision floating point values for simulation)
        -hostmem          (stores simulation data in host memory)
        -benchmark        (run benchmark to measure performance)
        -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
        -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
        -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
        -compare          (compares simulation results running once on the default GPU and once on the CPU)
        -cpu              (run n-body simulation on the CPU)
        -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Quadro T2000]
16384 bodies, total time for 10 iterations: 23.398 ms
= 114.724 billion interactions per second
= 2294.490 single-precision GFLOP/s at 20 flops per interaction
</code></pre>

<h3>WSL2 cuda-toolkit</h3>

<p>开发环境/运行环境
* <a href="https://zhuanlan.zhihu.com/p/555151725">https://zhuanlan.zhihu.com/p/555151725</a>
* <a href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html#cuda-support-for-WSL2">https://docs.nvidia.com/cuda/wsl-user-guide/index.html#cuda-support-for-WSL2</a>
* <a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=WSL-Ubuntu&amp;target_version=2.0&amp;target_type=deb_network">https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=WSL-Ubuntu&amp;target_version=2.0&amp;target_type=deb_network</a></p>

<p><img src="/images/blogs/ai/wsl2-cuda.png" alt="" /></p>

<pre><code>wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-3
</code></pre>

<p>运行安装：</p>

<pre><code>
(demo_env) winse@DESKTOP-BR4MG38:ai$ wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
--2024-01-14 23:53:22--  https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144, 72.21.80.5, 72.21.80.6, ...
Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: https://developer.download.nvidia.cn/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb [following]
--2024-01-14 23:53:23--  https://developer.download.nvidia.cn/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
Resolving developer.download.nvidia.cn (developer.download.nvidia.cn)... 59.36.216.26, 59.36.216.27, 175.4.58.180, ...
Connecting to developer.download.nvidia.cn (developer.download.nvidia.cn)|59.36.216.26|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4328 (4.2K) [application/x-deb]
Saving to: ‘cuda-keyring_1.1-1_all.deb’

cuda-keyring_1.1-1_all.deb      100%[====================================================&gt;]   4.23K  --.-KB/s    in 0s

2024-01-14 23:53:23 (1.61 GB/s) - ‘cuda-keyring_1.1-1_all.deb’ saved [4328/4328]

(demo_env) winse@DESKTOP-BR4MG38:ai$
(demo_env) winse@DESKTOP-BR4MG38:ai$ sudo dpkg -i cuda-keyring_1.1-1_all.deb
(demo_env) winse@DESKTOP-BR4MG38:ai$ sudo apt-get update
(demo_env) winse@DESKTOP-BR4MG38:ai$ sudo apt-get -y install cuda-toolkit-12-3
</code></pre>

<pre><code>(base) winse@DESKTOP-BR4MG38:~$ vi .bashrc

export PATH=/usr/local/cuda/bin:$PATH
</code></pre>

<p>新打开一个shell：</p>

<pre><code>(base) winse@DESKTOP-BR4MG38:~$ nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Wed_Nov_22_10:17:15_PST_2023
Cuda compilation tools, release 12.3, V12.3.107
Build cuda_12.3.r12.3/compiler.33567101_0
</code></pre>

<h3>cuDNN</h3>

<p><a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a></p>

<p>NVIDIA CUDA® Deep Neural Network library 支持神经网络的推理。</p>

<p>注册下载对应CUDA的版本 <a href="https://developer.nvidia.com/rdp/cudnn-download">https://developer.nvidia.com/rdp/cudnn-download</a></p>

<p>注意：如果不在WSL2-Ubuntu中直接使用cuDNN，后续通过容器直接拉取包含cuDNN的容器，就可以省略这一部分。</p>

<p><a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-deb">https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-deb</a></p>

<pre><code>#sudo dpkg -i cudnn-local-repo-ubuntu2004-8.9.6.50_1.0-1_amd64.deb
#sudo dpkg -r cudnn-local-repo-ubuntu2004-8.9.6.50
#sudo rm /etc/apt/sources.list.d/cudnn-local-ubuntu2004-8.9.6.50.list

(base) winse@DESKTOP-BR4MG38:i$ sudo dpkg -i cudnn-local-repo-ubuntu2004-8.9.7.29_1.0-1_amd64.deb

(base) winse@DESKTOP-BR4MG38:i$ sudo cp /var/cudnn-local-repo-ubuntu2004-8.9.7.29/cudnn-local-30472A84-keyring.gpg /usr/share/keyrings/


(base) winse@DESKTOP-BR4MG38:i$ sudo apt install zlib1g

(base) winse@DESKTOP-BR4MG38:i$ sudo apt update

(base) winse@DESKTOP-BR4MG38:i$ apt search libcudnn8
Sorting... Done
Full Text Search... Done
libcudnn8/unknown 8.9.7.29-1+cuda12.2 amd64
  cuDNN runtime libraries

libcudnn8-dev/unknown 8.9.7.29-1+cuda12.2 amd64
  cuDNN development libraries and headers

libcudnn8-samples/unknown 8.9.7.29-1+cuda12.2 amd64
  cuDNN samples

(base) winse@DESKTOP-BR4MG38:i$ sudo apt install libcudnn8 libcudnn8-dev libcudnn8-samples
</code></pre>

<p>校验是否安装成功</p>

<p><a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#verify">https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#verify</a></p>

<p>运行报错参考 <a href="https://forums.developer.nvidia.com/t/freeimage-is-not-set-up-correctly-please-ensure-freeimae-is-set-up-correctly/66950">https://forums.developer.nvidia.com/t/freeimage-is-not-set-up-correctly-please-ensure-freeimae-is-set-up-correctly/66950</a></p>

<pre><code>(base) winse@DESKTOP-BR4MG38:i$ cp -r /usr/src/cudnn_samples_v8 ./
(base) winse@DESKTOP-BR4MG38:i$ cd cudnn_samples_v8/mnistCUDNN/
(base) winse@DESKTOP-BR4MG38:mnistCUDNN$


(base) winse@DESKTOP-BR4MG38:mnistCUDNN$ sudo apt-get install libfreeimage3 libfreeimage-dev


(base) winse@DESKTOP-BR4MG38:mnistCUDNN$ make clean &amp;&amp; make

(base) winse@DESKTOP-BR4MG38:mnistCUDNN$ ./mnistCUDNN
Executing: mnistCUDNN
cudnnGetVersion() : 8907 , CUDNN_VERSION from cudnn.h : 8907 (8.9.7)
Host compiler version : GCC 9.4.0

There are 1 CUDA capable devices on your machine :
device 0 : sms 16  Capabilities 7.5, SmClock 1785.0 Mhz, MemSize (Mb) 4095, MemClock 6001.0 Mhz, Ecc=0, boardGroupID=0
Using device 0

Testing single precision
...

Result of classification: 1 3 5

Test passed!

Testing half precision (math in single precision)
...

Result of classification: 1 3 5

Test passed!
</code></pre>

<h3>nvidia-container-toolkit???</h3>

<p>Docker Desktop + WSL2不用安装 nvidia-container-toolkit ???</p>

<p></p>

<h2>WSL2 Python - conda</h2>

<p>下载安装conda</p>

<ul>
<li><a href="https://blog.csdn.net/weixin_44029053/article/details/119480776">https://blog.csdn.net/weixin_44029053/article/details/119480776</a></li>
</ul>


<h3>下载miniconda</h3>

<p><a href="https://docs.conda.io/projects/miniconda/en/latest/">https://docs.conda.io/projects/miniconda/en/latest/</a></p>

<pre><code>mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh

~/miniconda3/bin/conda init bash
</code></pre>

<p>运行脚本安装：</p>

<pre><code>winse@DESKTOP-BR4MG38:ai$ mkdir miniconda3
winse@DESKTOP-BR4MG38:ai$ cd miniconda3/
winse@DESKTOP-BR4MG38:miniconda3$

winse@DESKTOP-BR4MG38:miniconda3$ bash miniconda.sh -b -u -p ~/miniconda3
PREFIX=/home/winse/miniconda3
Unpacking payload ...

Installing base environment...


Downloading and Extracting Packages:


Downloading and Extracting Packages:

Preparing transaction: done
Executing transaction: done
installation finished.

winse@DESKTOP-BR4MG38:miniconda3$ ~/miniconda3/bin/conda init bash
no change     /home/winse/miniconda3/condabin/conda
no change     /home/winse/miniconda3/bin/conda
no change     /home/winse/miniconda3/bin/conda-env
no change     /home/winse/miniconda3/bin/activate
no change     /home/winse/miniconda3/bin/deactivate
no change     /home/winse/miniconda3/etc/profile.d/conda.sh
no change     /home/winse/miniconda3/etc/fish/conf.d/conda.fish
no change     /home/winse/miniconda3/shell/condabin/Conda.psm1
no change     /home/winse/miniconda3/shell/condabin/conda-hook.ps1
no change     /home/winse/miniconda3/lib/python3.11/site-packages/xontrib/conda.xsh
no change     /home/winse/miniconda3/etc/profile.d/conda.csh
modified      /home/winse/.bashrc

==&gt; For changes to take effect, close and re-open your current shell. &lt;==
</code></pre>

<h3>添加源：</h3>

<pre><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
conda config --set show_channel_urls yes 

conda config --show channels
#conda config --remove-key channels
</code></pre>

<h3>运行测试GPU</h3>

<pre><code>conda create -n demo_env python=3.8
conda activate demo_env
conda install pytorch==1.6.0 cudatoolkit=10.1 torchaudio=0.6.0 -c pytorch

#conda list
#conda deactivate

#conda env list
#conda remove -n demo_env --all
#conda env remove --name old_name
</code></pre>

<p>验证是否安装成功
在我们到demo_env环境下，打开Python，输入以下语句：</p>

<pre><code>(demo_env) winse@DESKTOP-BR4MG38:ai$ python
Python 3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:21:28)
[GCC 12.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; x = torch.rand(5,3)
&gt;&gt;&gt; print(x)
tensor([[0.4343, 0.3966, 0.1862],
        [0.1502, 0.0788, 0.7713],
        [0.3505, 0.7065, 0.9952],
        [0.6420, 0.2574, 0.7550],
        [0.8292, 0.7714, 0.9014]])
&gt;&gt;&gt; print(torch.cuda.is_available())
True
</code></pre>

<h2>模型下载</h2>

<p><strong>非常重要，不然时间都浪费等待下载上了。模型动辄几G，稍微大一点的就几十G，是需要慎重和反复探索。</strong></p>

<p>一开始用代理和GIT下载的，又慢又浪费时间又浪费空间。放着下了一晚，早起起来磁盘空间不够 o(╥﹏╥)o 。</p>

<p><img src="/images/blogs/ai/git-down-ai-models.png" alt="" /></p>

<p></p>

<p>参考 <a href="https://soulteary.com/2024/01/09/summary-of-reliable-download-solutions-for-ai-models.html">https://soulteary.com/2024/01/09/summary-of-reliable-download-solutions-for-ai-models.html</a></p>

<h3>国内的modelscope下载</h3>

<p>modelscope它还结合了aliyun提供了一定时长的免费环境，在本地折腾折腾后再上去跑跑。这里只通过它去下载模型（下载的方式是没有.git的文件，少占一半多的磁盘空间）。</p>

<ul>
<li><a href="https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD">https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD</a></li>
</ul>


<p></p>

<pre><code>(base) winse@DESKTOP-BR4MG38:ai$ conda activate demo

(demo) winse@DESKTOP-BR4MG38:ai$ pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

(demo) winse@DESKTOP-BR4MG38:ai$ pip install modelscope


(demo) winse@DESKTOP-BR4MG38:ai$ python -c "from modelscope.hub.snapshot_download import snapshot_download;snapshot_download('damo/nlp_xlmr_named-entity-recognition_viet-ecommerce-title', cache_dir='./')"
2024-01-14 16:52:36,017 - modelscope - INFO - PyTorch version 1.11.0+cu113 Found.
2024-01-14 16:52:36,018 - modelscope - INFO - Loading ast index from /home/winse/.cache/modelscope/ast_indexer
2024-01-14 16:52:36,050 - modelscope - INFO - Loading done! Current index file version is 1.11.0, with md5 85336421feb1dc1ec9dde85ceee20f42 and a total number of 953 components indexed
2024-01-14 16:52:36,757 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.19k/1.19k [00:00&lt;00:00, 12.3MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:00&lt;00:00, 1.97MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:00&lt;00:00, 1.81MB/s]
Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1.04G/1.04G [00:36&lt;00:00, 30.1MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.68k/2.68k [00:00&lt;00:00, 18.2MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.68k/2.68k [00:00&lt;00:00, 27.7MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.83M/4.83M [00:00&lt;00:00, 7.97MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:00&lt;00:00, 1.58MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8.68M/8.68M [00:00&lt;00:00, 11.3MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 470/470 [00:00&lt;00:00, 4.53MB/s]


(demo) winse@DESKTOP-BR4MG38:ai$ python -c "from modelscope.hub.snapshot_download import snapshot_download;snapshot_download('Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1', cache_dir='./')"
2024-01-15 08:19:52,588 - modelscope - INFO - PyTorch version 1.11.0+cu113 Found.
2024-01-15 08:19:52,589 - modelscope - INFO - Loading ast index from /home/winse/.cache/modelscope/ast_indexer
2024-01-15 08:19:52,741 - modelscope - INFO - Loading done! Current index file version is 1.11.0, with md5 85336421feb1dc1ec9dde85ceee20f42 and a total number of 953 components indexed
2024-01-15 08:19:53,874 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0
Downloading: 100%|████████████████████████████████████████████████████████████████| 257k/257k [00:00&lt;00:00, 1.48MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████| 600/600 [00:00&lt;00:00, 488kB/s]
Downloading: 100%|██████████████████████████████████████████████████████████████████| 793/793 [00:00&lt;00:00, 1.35MB/s]
Downloading: 100%|██████████████████████████████████████████████████████████████████| 884/884 [00:00&lt;00:00, 1.47MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████| 4.56k/4.56k [00:00&lt;00:00, 220kB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████| 146/146 [00:00&lt;00:00, 251kB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████▉| 3.20G/3.20G [01:29&lt;00:00, 38.5MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████| 319M/319M [00:39&lt;00:00, 8.39MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████| 571k/571k [00:00&lt;00:00, 2.35MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████| 583k/583k [00:00&lt;00:00, 2.16MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████| 571k/571k [00:00&lt;00:00, 2.10MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████| 539/539 [00:00&lt;00:00, 924kB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████████| 196k/196k [00:00&lt;00:00, 998kB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████| 226k/226k [00:00&lt;00:00, 1.24MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████| 342/342 [00:00&lt;00:00, 617kB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████| 390M/390M [00:30&lt;00:00, 13.5MB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████▉| 1.13G/1.13G [00:41&lt;00:00, 29.3MB/s]
Downloading: 100%|██████████████████████████████████████████████████████████████| 8.90k/8.90k [00:00&lt;00:00, 3.09MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████| 298/298 [00:00&lt;00:00, 482kB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████| 186/186 [00:00&lt;00:00, 304kB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████▉| 3.89G/3.89G [02:12&lt;00:00, 31.6MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████| 477k/477k [00:00&lt;00:00, 2.02MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████| 477k/477k [00:00&lt;00:00, 1.72MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████| 198k/198k [00:00&lt;00:00, 1.01MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████| 212k/212k [00:00&lt;00:00, 1.10MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████| 555/555 [00:00&lt;00:00, 933kB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████████| 107k/107k [00:00&lt;00:00, 782kB/s]
</code></pre>

<p>对比一下git和直接下载空间的占用，时间就更加不用说了。</p>

<p><img src="/images/blogs/ai/download-vs.png" alt="" /></p>

<h3>modelscope</h3>

<p>把整个流程跑一下，跑个简单的例子：</p>

<p>环境安装
<a href="https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85">https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85</a></p>

<p>运行时依赖</p>

<pre><code>(demo) winse@DESKTOP-BR4MG38:ai$ pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
#pip config set global.index-url https://mirrors.cloud.aliyuncs.com/pypi/simple 
#pip config set install.trusted-host mirrors.cloud.aliyuncs.com

(demo) winse@DESKTOP-BR4MG38:ai$ pip3 install torch==1.11.0 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113

$ pip install transformers sentencepiece pyvi
</code></pre>

<p>测试模型：</p>

<pre><code>
https://modelscope.cn/models/damo/nlp_xlmr_named-entity-recognition_viet-ecommerce-title/summary

(demo) winse@DESKTOP-BR4MG38:ai$ python
Python 3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:21:28)
[GCC 12.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from modelscope.pipelines import pipeline
2024-01-15 01:22:20,476 - modelscope - INFO - PyTorch version 1.11.0+cu113 Found.
2024-01-15 01:22:20,478 - modelscope - INFO - Loading ast index from /home/winse/.cache/modelscope/ast_indexer
2024-01-15 01:22:20,498 - modelscope - INFO - Loading done! Current index file version is 1.11.0, with md5 85336421feb1dc1ec9dde85ceee20f42 and a total number of 953 components indexed
&gt;&gt;&gt; from modelscope.utils.constant import Tasks
&gt;&gt;&gt; ner_pipeline = pipeline(Tasks.named_entity_recognition, 'damo/nlp_xlmr_named-entity-recognition_viet-ecommerce-title', model_revision='v1.0.1')
2024-01-15 01:22:28,618 - modelscope - INFO - initiate model from damo/nlp_xlmr_named-entity-recognition_viet-ecommerce-title
2024-01-15 01:22:28,620 - modelscope - INFO - initiate model from location damo/nlp_xlmr_named-entity-recognition_viet-ecommerce-title.
2024-01-15 01:22:28,630 - modelscope - INFO - initialize model from damo/nlp_xlmr_named-entity-recognition_viet-ecommerce-title
2024-01-15 01:22:30,945 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing
2024-01-15 01:22:34,599 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTokenClassificationWithCRF.

2024-01-15 01:22:34,599 - modelscope - INFO - All the weights of ModelForTokenClassificationWithCRF were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTokenClassificationWithCRF for predictions without further training.
&gt;&gt;&gt; result = ner_pipeline('Nón vành dễ thương cho bé gái')
&gt;&gt;&gt; print(result)
{'output': [{'type': 'product', 'start': 0, 'end': 8, 'prob': 0.98140895, 'span': 'Nón vành'}, {'type': 'style', 'start': 9, 'end': 18, 'prob': 0.99752563, 'span': 'dễ thương'}, {'type': 'consumer_group', 'start': 23, 'end': 29, 'prob': 0.99895895, 'span': 'bé gái'}]}
&gt;&gt;&gt;
</code></pre>

<h3>TODO</h3>

<ul>
<li>huggingface</li>
</ul>


<p>国外的，后续用到了再补</p>

<p></p>

<ul>
<li>downloader（跳过）</li>
</ul>


<p>当然，如果进场要用到各种工具的特定版本来下载依赖，用一个docker镜像来作为下载器，也是不错的方法</p>

<pre><code>#Python轻量环境
docker pull python:3.10-slim

@1
#将本地目录挂载到容器里，一会作为模型下载目录使用
docker run --rm -it -v `pwd`:/models python:3.10-slim bash

sed -i 's/snapshot.debian.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apt/sources.list.d/debian.sources
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

cd /models


@2
#创建一个持续运行的 Python 容器
docker run -d --name=downloader -v `pwd`:/models python:3.10-slim tail -f /etc/hosts
#使用命令进入容器进行配置和下载模型
docker exec -it downloader bash
</code></pre>

<h2>太乙模型</h2>

<h3>[尝试/试错]</h3>

<p>开始是参照 <a href="https://soulteary.com/2022/12/09/use-docker-to-quickly-get-started-with-the-chinese-stable-diffusion-model-taiyi.html">使用 Docker 来快速上手中文 Stable Diffusion 模型：太乙</a> 文章里面说的蛮简单的，想着我这个WSL2+GPU应该也是可以的。</p>

<p>开始的时刻还是git clone的，等到怕了，几个小时还不一定成功，后面才改成下载的方式！</p>

<pre><code>#git clone https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1
winse@DESKTOP-BR4MG38:/mnt/i/ai/stable-diffusion-taiyi$ git clone https://www.modelscope.cn/Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1.git
Cloning into 'Taiyi-Stable-Diffusion-1B-Chinese-v0.1'...
remote: Enumerating objects: 85, done.
remote: Counting objects: 100% (6/6), done.
remote: Compressing objects: 100% (6/6), done.
remote: Total 85 (delta 2), reused 0 (delta 0), pack-reused 79
Unpacking objects: 100% (85/85), 3.61 GiB | 1.98 MiB/s, done.
Filtering content: 100% (5/5), 8.92 GiB | 1.71 MiB/s, done.


#镜像也下载了好几次 才pull下来
winse@DESKTOP-BR4MG38:/mnt/i/ai$ docker pull soulteary/stable-diffusion:taiyi-0.1
taiyi-0.1: Pulling from soulteary/stable-diffusion
a404e5416296: Pull complete
af6d12d8d61a: Pull complete
bc57d500b85c: Pull complete
fcd60060414d: Pull complete
65b27d733eb0: Pull complete
266c4315d44f: Pull complete
7ed4190451a3: Pull complete
975671c72e25: Pull complete
213ba1e17e15: Pull complete
37bbbc68318a: Pull complete
80438d07027f: Pull complete
74c79bc62d3a: Pull complete
f8054e9907fb: Pull complete
dc8d44bb4941: Pull complete
625444b7a83c: Pull complete
0b90667ff465: Pull complete
67d73c5193e1: Pull complete
Digest: sha256:69cc4b5fc890dd7ccffff9dbfc2eb2262a0a727574b8beeeafe621f9ef135d16
Status: Downloaded newer image for soulteary/stable-diffusion:taiyi-0.1
docker.io/soulteary/stable-diffusion:taiyi-0.1

What's Next?
  View a summary of image vulnerabilities and recommendations → docker scout quickview soulteary/stable-diffusion:taiyi-0.1


#这也是得在纯Linux机器上的Docker才行的
#wget https://github.com/soulteary/docker-stable-diffusion-taiyi/blob/main/docker-compose.yml
#我这就直接运行
winse@DESKTOP-BR4MG38:stable-diffusion-taiyi$ docker run --gpus all --rm -it -v $(pwd)/Taiyi-Stable-Diffusion-1B-Chinese-v0.1:/stable-diffusion-webui/models/Taiyi-Stable-Diffusion-1B-Chinese-v0.1 -p 7860:7860 soulteary/stable-diffusion:taiyi-0.1

#Windows cmd
#docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --rm -it -v C:/docker-sdxl/stabilityai/:/app/stabilityai -p 7860:7860 soulteary/sdxl:runtime
</code></pre>

<p>运行起来后访问 <a href="http://localhost:7860/">http://localhost:7860/</a> ，输入 小船，河流，星空，星星，山峦，油画 查看Win11的任务管理器，GPU是打满了的，但生成的图片是全黑，啥都没有！</p>

<h3>WSL-Ubuntu部署</h3>

<p>试了很多次都不行，最后还是回到原点，不能偷懒，先把效果跑出来：</p>

<ul>
<li><a href="https://modelscope.cn/models/Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1/summary#%E4%BD%BF%E7%94%A8-usage">https://modelscope.cn/models/Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1/summary#%E4%BD%BF%E7%94%A8-usage</a></li>
</ul>


<h4>安装依赖（通过代理）</h4>

<pre><code>#https://stackoverflow.com/questions/37776228/pycharm-python-opencv-and-cv2-install-error
(demo) winse@DESKTOP-BR4MG38:~$ pip3 install opencv-python

$ pip install diffusers

##pip install accelerate
(demo) winse@DESKTOP-BR4MG38:~$ unset all_proxy &amp;&amp; unset ALL_PROXY
(demo) winse@DESKTOP-BR4MG38:~$ pip install pysocks

(demo) winse@DESKTOP-BR4MG38:~$ export ALL_PROXY=socks5://172.22.240.1:23333 HTTPS_PROXY=socks5://172.22.240.1:23333 HTTP_PROXY=socks5://172.22.240.1:23333
(demo) winse@DESKTOP-BR4MG38:~$ pip install git+https://github.com/huggingface/accelerate  
</code></pre>

<h4>测试（跑了一个小时，-_-||）（此时还没安装cuDNN）：</h4>

<pre><code>(demo) winse@DESKTOP-BR4MG38:ai$ python
Python 3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:21:28)
[GCC 12.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from modelscope.utils.constant import Tasks
2024-01-15 10:07:41,619 - modelscope - INFO - PyTorch version 1.11.0+cu113 Found.
2024-01-15 10:07:41,630 - modelscope - INFO - Loading ast index from /home/winse/.cache/modelscope/ast_indexer
2024-01-15 10:07:41,751 - modelscope - INFO - Loading done! Current index file version is 1.11.0, with md5 85336421feb1dc1ec9dde85ceee20f42 and a total number of 953 components indexed
&gt;&gt;&gt; from modelscope.pipelines import pipeline
&gt;&gt;&gt; import cv2
&gt;&gt;&gt; pipe = pipeline(task=Tasks.text_to_image_synthesis, model='Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1', model_revision='v1.0.0')
Loading pipeline components...:   0%|                                                          | 0/7 [00:00&lt;?, ?it/s]
/home/winse/miniconda3/envs/demo/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:28: 
FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.
  warnings.warn(
Loading pipeline components...:  57%|████████████████████████████▌                     | 4/7 [00:32&lt;00:23,  7.83s/it]
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
Loading pipeline components...: 100%|██████████████████████████████████████████████████| 7/7 [03:23&lt;00:00, 29.02s/it]
&gt;&gt;&gt; prompt = '飞流直下三千尺，油画'
&gt;&gt;&gt; output = pipe({'text': prompt})
/home/winse/miniconda3/envs/demo/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:889: FutureWarning: `callback_steps` is deprecated and will be removed in version 1.0.0. Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`
  deprecate(
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
You may ignore this warning if your `pad_token_id` (0) is identical to the `bos_token_id` (0), `eos_token_id` (2), or the `sep_token_id` (None), and your input is not padded.
100%|██████████████████████████████████████████████████████████████████████████████| 50/50 [1:03:11&lt;00:00, 75.83s/it]
&gt;&gt;&gt; cv2.imwrite('result.png', output['output_imgs'][0])
True
&gt;&gt;&gt;
</code></pre>

<p><img src="/images/blogs/ai/taiyi-result.png" alt="" /></p>

<h4>再测个快的</h4>

<p>有 cuDNN 加持确实快，10分钟就跑出来了！没安装之前时间估计是三个小时的！！！</p>

<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; from diffusers import StableDiffusionPipeline
&gt;&gt;&gt; torch.backends.cudnn.benchmark = True

&gt;&gt;&gt; pipe = StableDiffusionPipeline.from_pretrained("Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1", torch_dtype=torch.float16)
Loading pipeline components...:  57%|████████████████████████████▌                     | 4/7 [00:05&lt;00:03,  1.32s/it]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
Loading pipeline components...: 100%|██████████████████████████████████████████████████| 7/7 [00:32&lt;00:00,  4.58s/it]
&gt;&gt;&gt; pipe.to('cuda')
StableDiffusionPipeline {
  "_class_name": "StableDiffusionPipeline",
  "_diffusers_version": "0.25.0",
  "_name_or_path": "Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1",
  "feature_extractor": [
    "transformers",
    "CLIPFeatureExtractor"
  ],
  "image_encoder": [
    null,
    null
  ],
  "requires_safety_checker": true,
  "safety_checker": [
    "stable_diffusion",
    "StableDiffusionSafetyChecker"
  ],
  "scheduler": [
    "diffusers",
    "PNDMScheduler"
  ],
  "text_encoder": [
    "transformers",
    "BertModel"
  ],
  "tokenizer": [
    "transformers",
    "BertTokenizer"
  ],
  "unet": [
    "diffusers",
    "UNet2DConditionModel"
  ],
  "vae": [
    "diffusers",
    "AutoencoderKL"
  ]
}

&gt;&gt;&gt;
&gt;&gt;&gt; prompt = '飞流直下三千尺，油画'
&gt;&gt;&gt; image = pipe(prompt, guidance_scale=7.5).images[0]
100%|████████████████████████████████████████████████████████████████████████████████| 50/50 [09:32&lt;00:00, 11.45s/it]
&gt;&gt;&gt; image.save("飞流.png")
&gt;&gt;&gt;
</code></pre>

<p><img src="/images/blogs/ai/taiyi-cudnn-result.png" alt="" /></p>

<h2>改一下conda的名字</h2>

<pre><code>(demo) winse@DESKTOP-BR4MG38:ai$ conda deactivate
(base) winse@DESKTOP-BR4MG38:ai$ conda rename -n demo modelscope
Source:      /home/winse/miniconda3/envs/demo
Destination: /home/winse/miniconda3/envs/modelscope
Packages: 22
Files: 33924

Downloading and Extracting Packages:


Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done

(base) winse@DESKTOP-BR4MG38:ai$ conda env list
# conda environments:
#
base                  *  /home/winse/miniconda3
modelscope               /home/winse/miniconda3/envs/modelscope
</code></pre>

<h2>模型下载脚本</h2>

<p>比如下载：<a href="https://modelscope.cn/models/Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-EN-v0.1/summary">https://modelscope.cn/models/Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-EN-v0.1/summary</a></p>

<pre><code>$ tail -16 ~/.bashrc

function modelscope_download() {
model=$1

conda activate modelscope

#pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
#pip install modelscope

python -c "from modelscope.hub.snapshot_download import snapshot_download;snapshot_download('$model', cache_dir='./')"

conda deactivate

}

$ source ~/.bashrc


(base) winse@DESKTOP-BR4MG38:ai$ modelscope_download "Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-EN-v0.1"
2024-01-15 15:17:13,613 - modelscope - INFO - PyTorch version 1.11.0+cu113 Found.
2024-01-15 15:17:13,614 - modelscope - INFO - Loading ast index from /home/winse/.cache/modelscope/ast_indexer
2024-01-15 15:17:13,637 - modelscope - INFO - Loading done! Current index file version is 1.11.0, with md5 85336421feb1dc1ec9dde85ceee20f42 and a total number of 953 components indexed
2024-01-15 15:17:16,879 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2.31M/2.31M [00:00&lt;00:00, 6.01MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 612/612 [00:00&lt;00:00, 436kB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 547/547 [00:00&lt;00:00, 804kB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 743/743 [00:00&lt;00:00, 1.25MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 4.46k/4.46k [00:00&lt;00:00, 244kB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 139/139 [00:00&lt;00:00, 231kB/s]
Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████▉| 3.20G/3.20G [02:36&lt;00:00, 22.0MB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 319M/319M [00:39&lt;00:00, 8.40MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2.11M/2.11M [00:00&lt;00:00, 4.05MB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 512k/512k [00:00&lt;00:00, 1.28MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 543/543 [00:00&lt;00:00, 845kB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 342/342 [00:00&lt;00:00, 521kB/s]
Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████▉| 1.13G/1.13G [01:22&lt;00:00, 14.8MB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 469M/469M [00:12&lt;00:00, 39.0MB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5.00/5.00 [00:00&lt;00:00, 6.02kB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 7.97k/7.97k [00:00&lt;00:00, 7.06MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 284/284 [00:00&lt;00:00, 441kB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 389/389 [00:00&lt;00:00, 348kB/s]
Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████▉| 11.3G/11.3G [04:07&lt;00:00, 49.0MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 697/697 [00:00&lt;00:00, 950kB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 939k/939k [00:00&lt;00:00, 2.50MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2.87M/2.87M [00:00&lt;00:00, 5.79MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 3.44M/3.44M [00:00&lt;00:00, 6.17MB/s]


#tree -L 2
(base) winse@DESKTOP-BR4MG38:ai$ tree -L 1 Fengshenbang/
Fengshenbang/
├── Taiyi-Stable-Diffusion-1B-Chinese-EN-v0.1
└── Taiyi-Stable-Diffusion-1B-Chinese-v0.1

2 directories, 0 files
</code></pre>

<p>再跑一个中英文的模型试试</p>

<pre><code>(base) winse@DESKTOP-BR4MG38:ai$ conda activate modelscope
(modelscope) winse@DESKTOP-BR4MG38:ai$
(modelscope) winse@DESKTOP-BR4MG38:ai$ python
Python 3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:21:28)
[GCC 12.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from diffusers import StableDiffusionPipeline
&gt;&gt;&gt; pipe = StableDiffusionPipeline.from_pretrained("Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-EN-v0.1").to("cuda")
Loading pipeline components...:  43%|█████████████████████████████████▊                                             | 3/7 [00:17&lt;00:21,  5.46s/it]/home/winse/miniconda3/envs/modelscope/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.
  warnings.warn(
Loading pipeline components...:  86%|███████████████████████████████████████████████████████████████████▋           | 6/7 [00:17&lt;00:01,  1.90s/it]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
Loading pipeline components...: 100%|███████████████████████████████████████████████████████████████████████████████| 7/7 [00:27&lt;00:00,  3.87s/it]
&gt;&gt;&gt;
&gt;&gt;&gt; prompt = '小桥流水人家，Van Gogh style'
&gt;&gt;&gt; image = pipe(prompt, guidance_scale=10).images[0]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [14:19&lt;00:00, 17.19s/it]
&gt;&gt;&gt; image.save("小桥.png")
&gt;&gt;&gt;
</code></pre>

<p><img src="/images/blogs/ai/taiyi-zh-en-result.png" alt="" /></p>

<h2>太乙webui - 亦步亦趋</h2>

<p>这里记录了安装的详细过程，比较繁琐，如果直接安装可以跳到[太乙webui - 纯净版]。</p>

<ul>
<li><a href="https://github.com/IDEA-CCNL/stable-diffusion-webui/blob/master/README.md">https://github.com/IDEA-CCNL/stable-diffusion-webui/blob/master/README.md</a></li>
<li><a href="https://github.com/soulteary/docker-stable-diffusion-taiyi/blob/main/docker/Dockerfile">https://github.com/soulteary/docker-stable-diffusion-taiyi/blob/main/docker/Dockerfile</a></li>
<li><a href="https://github.com/IDEA-CCNL/stable-diffusion-webui/blob/master/webui.sh">https://github.com/IDEA-CCNL/stable-diffusion-webui/blob/master/webui.sh</a></li>
</ul>


<p>选一个跟我现在用的环境一样的版本和系统：<a href="https://hub.docker.com/r/nvidia/cuda/tags">https://hub.docker.com/r/nvidia/cuda/tags</a></p>

<p>镜像的描述：CUDA and cuDNN images from gitlab.com/nvidia/cuda</p>

<h3>试错</h3>

<pre><code>(modelscope) winse@DESKTOP-BR4MG38:ai$ docker pull nvidia/cuda:12.3.1-devel-ubuntu20.04
12.3.1-devel-ubuntu20.04: Pulling from nvidia/cuda
12.3.1-devel-ubuntu20.04: Pulling from nvidia/cuda
25ad149ed3cf: Pull complete
ba7b66a9df40: Pull complete
520797292d92: Pull complete
c5f2ffd06d8b: Pull complete
1698c67699a3: Pull complete
16dd7c0d35aa: Pull complete
568cac1e538c: Pull complete
6252d19a7f1d: Pull complete
f573e2686be4: Pull complete
0074e75104ac: Pull complete
df35fae9e247: Pull complete
Digest: sha256:befbdfddbb52727f9ce8d0c574cac0f631c606b1e6f0e523f3a0777fe2720c99
Status: Downloaded newer image for nvidia/cuda:12.3.1-devel-ubuntu20.04
docker.io/nvidia/cuda:12.3.1-devel-ubuntu20.04

What's Next?
  1. Sign in to your Docker account → docker login
  2. View a summary of image vulnerabilities and recommendations → docker scout quickview nvidia/cuda:12.3.1-devel-ubuntu20.04


(modelscope) winse@DESKTOP-BR4MG38:ai$ docker run --rm --gpus all --ipc host --ulimit memlock=-1 --ulimit stack=67108864 -it -v /mnt/i/ai:/app/stabilityai -p 7860:7860 docker.io/nvidia/cuda:12.3.1-devel-ubuntu20.04
docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy'
nvidia-container-cli: requirement error: unsatisfied condition: cuda&gt;=12.3, please update your driver to a newer version, or use an earlier cuda container: unknown.
</code></pre>

<h3>重新下载镜像并配置</h3>

<pre><code>(modelscope) winse@DESKTOP-BR4MG38:ai$ nvidia-smi
Mon Jan 15 17:04:18 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.146.01             Driver Version: 537.99       CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro T2000                   On  | 00000000:01:00.0  On |                  N/A |
| N/A   46C    P8               6W /  60W |    589MiB /  4096MiB |      7%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A        39      G   /Xwayland                                 N/A      |
|    0   N/A  N/A        42      G   /Xwayland                                 N/A      |
|    0   N/A  N/A        44      G   /Xwayland                                 N/A      |
+---------------------------------------------------------------------------------------+
</code></pre>

<p>版本不能高于本地CUDA，重新下载镜像：</p>

<pre><code>(modelscope) winse@DESKTOP-BR4MG38:ai$ docker pull nvidia/cuda:12.2.2-devel-ubuntu20.04
12.2.2-devel-ubuntu20.04: Pulling from nvidia/cuda
12.2.2-devel-ubuntu20.04: Pulling from nvidia/cuda
96d54c3075c9: Pull complete
db26cf78ae4f: Pull complete
5adc7ab504d3: Pull complete
e4f230263527: Pull complete
95e3f492d47e: Pull complete
35dd1979297e: Pull complete
39a2c88664b3: Pull complete
d8f6b6cd09da: Pull complete
fe19bbed4a4a: Pull complete
469ef7e9efe0: Pull complete
e30c6425f419: Pull complete
Digest: sha256:b7074ef6f9aa30c27fe747f3a7e10402ec442f001290718c73e0972d1ee61342
Status: Downloaded newer image for nvidia/cuda:12.2.2-devel-ubuntu20.04
docker.io/nvidia/cuda:12.2.2-devel-ubuntu20.04

What's Next?
  1. Sign in to your Docker account → docker login
  2. View a summary of image vulnerabilities and recommendations → docker scout quickview nvidia/cuda:12.2.2-devel-ubuntu20.04
</code></pre>

<h3>运行容器实例</h3>

<pre><code>(modelscope) winse@DESKTOP-BR4MG38:P15$ docker run --rm --gpus all --ipc host --ulimit memlock=-1 --ulimit stack=67108864 -it -v /mnt/i/ai:/app/stabilityai -p 7860:7860 docker.io/nvidia/cuda:12.2.2-devel-ubuntu20.04


==========
== CUDA ==
==========

CUDA Version 12.2.2

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

root@41af85cb0007:/# nvidia-smi 
Mon Jan 15 13:25:43 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.146.01             Driver Version: 537.99       CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro T2000                   On  | 00000000:01:00.0  On |                  N/A |
| N/A   43C    P8               3W /  60W |    620MiB /  4096MiB |      2%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
root@41af85cb0007:/# docker ps -a 
bash: docker: command not found
root@41af85cb0007:/# python -V 
bash: python: command not found
</code></pre>

<p>尽管用的是WSL 2 based engine，但是不是Windows管理的。</p>

<h3>配值webui</h3>

<p>[解决被官方忽视的 AI 容器应用问题] <a href="https://soulteary.com/2022/12/09/use-docker-to-quickly-get-started-with-the-chinese-stable-diffusion-model-taiyi.html">https://soulteary.com/2022/12/09/use-docker-to-quickly-get-started-with-the-chinese-stable-diffusion-model-taiyi.html</a>
<a href="https://github.com/soulteary/docker-stable-diffusion-taiyi/blob/main/docker/Dockerfile">https://github.com/soulteary/docker-stable-diffusion-taiyi/blob/main/docker/Dockerfile</a>
是对官方的依赖安装的拆解。可以参考，还是不建议这么搞，如果能促成源头修正那就是另一种说法了。</p>

<p>由于他的镜像我也跑不起来，所以直接按照官方来安装，参考借鉴他遇到解决过的问题。</p>

<pre><code>root@41af85cb0007:/# sed -i.bak -e 's|archive.ubuntu.com/ubuntu/|mirrors.tuna.tsinghua.edu.cn/ubuntu/|' -e 's|security.ubuntu.com/ubuntu/|mirrors.tuna.tsinghua.edu.cn/ubuntu/|' /etc/apt/sources.list

root@41af85cb0007:/# apt update 

root@41af85cb0007:/# apt install -y git wget curl iputils-ping iproute2 traceroute

root@41af85cb0007:/# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

root@41af85cb0007:/# bash Miniconda3-latest-Linux-x86_64.sh -b
PREFIX=/root/miniconda3
Unpacking payload ...

Installing base environment...


Downloading and Extracting Packages:


Downloading and Extracting Packages:

Preparing transaction: done
Executing transaction: done
installation finished.

root@41af85cb0007:/# /root/miniconda3/bin/conda init bash 
no change     /root/miniconda3/condabin/conda
no change     /root/miniconda3/bin/conda
no change     /root/miniconda3/bin/conda-env
no change     /root/miniconda3/bin/activate
no change     /root/miniconda3/bin/deactivate
no change     /root/miniconda3/etc/profile.d/conda.sh
no change     /root/miniconda3/etc/fish/conf.d/conda.fish
no change     /root/miniconda3/shell/condabin/Conda.psm1
no change     /root/miniconda3/shell/condabin/conda-hook.ps1
no change     /root/miniconda3/lib/python3.11/site-packages/xontrib/conda.xsh
no change     /root/miniconda3/etc/profile.d/conda.csh
modified      /root/.bashrc

==&gt; For changes to take effect, close and re-open your current shell. &lt;==

root@41af85cb0007:/# source ~/.bashrc
(base) root@41af85cb0007:/# 



#https://github.com/IDEA-CCNL/stable-diffusion-webui/zipball/master/
#https://github.com/IDEA-CCNL/stable-diffusion-webui/tarball/master/
#https://docs.github.com/en/repositories/working-with-files/using-files/downloading-source-code-archives#source-code-archive-urls
(base) root@41af85cb0007:/opt# wget -c https://github.com/IDEA-CCNL/stable-diffusion-webui/archive/refs/heads/master.tar.gz -O stable-diffusion-webui.tgz

(base) root@41af85cb0007:/opt# tar zxf stable-diffusion-webui.tgz 
(base) root@41af85cb0007:/opt# mv stable-diffusion-webui-master stable-diffusion-webui
(base) root@41af85cb0007:/opt# cd stable-diffusion-webui


#@@ 反正走代理，没必要
#(webui) root@41af85cb0007:/opt/stable-diffusion-webui# pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
#Writing to /root/.config/pip/pip.conf


#https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/4345
#https://stackoverflow.com/questions/75099182/stable-diffusion-error-couldnt-install-torch-no-matching-distribution-found
#ERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python &gt;=3.7,&lt;3.10; 1.6.3 Requires-Python &gt;=3.7,&lt;3.10; 1.7.0 Requires-Python &gt;=3.7,&lt;3.10; 1.7.1 Requires-Python &gt;=3.7,&lt;3.10
(base) root@41af85cb0007:/opt/stable-diffusion-webui# conda create -n py39 python=3.9
(base) root@41af85cb0007:/opt/stable-diffusion-webui# conda activate py39
(py39) root@41af85cb0007:/opt/stable-diffusion-webui# 


#@@ 用到github，得socks代理一下，@@先去掉代理不然又解析不了@@
(py39) root@41af85cb0007:/opt/stable-diffusion-webui# unset all_proxy &amp;&amp; unset ALL_PROXY &amp;&amp; unset https_proxy &amp;&amp; unset HTTPS_PROXY
(py39) root@41af85cb0007:/opt/stable-diffusion-webui# pip install pysocks
Collecting pysocks
  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fca218ae370&gt;, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/8d/59/b4572118e098ac8e46e399a1dd0f2d85403ce8bbaad9ec79373ed6badaf9/PySocks-1.7.1-py3-none-any.whl
  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)
Installing collected packages: pysocks
Successfully installed pysocks-1.7.1
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv


#?? ImportError: libGL.so.1: cannot open shared object file: No such file or directory

(py39) root@41af85cb0007:/opt/stable-diffusion-webui# apt-get install ffmpeg libsm6 libxext6  -y
...
Setting up tzdata (2023c-0ubuntu0.20.04.2) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration questions will narrow this down by presenting a list of cities, representing the time zones in which they are located.

  1. Africa  2. America  3. Antarctica  4. Australia  5. Arctic  6. Asia  7. Atlantic  8. Europe  9. Indian  10. Pacific  11. SystemV  12. US  13. Etc
Geographic area: 6

Please select the city or region corresponding to your time zone.

  1. Aden      9. Baghdad   17. Chita       25. Dushanbe     33. Irkutsk    41. Kashgar       49. Macau         57. Omsk        65. Rangoon        73. Taipei    81. Ujung_Pandang  89. Yekaterinburg
  2. Almaty    10. Bahrain  18. Choibalsan  26. Famagusta    34. Istanbul   42. Kathmandu     50. Magadan       58. Oral        66. Riyadh         74. Tashkent  82. Ulaanbaatar    90. Yerevan
  3. Amman     11. Baku     19. Chongqing   27. Gaza         35. Jakarta    43. Khandyga      51. Makassar      59. Phnom_Penh  67. Sakhalin       75. Tbilisi   83. Urumqi
  4. Anadyr    12. Bangkok  20. Colombo     28. Harbin       36. Jayapura   44. Kolkata       52. Manila        60. Pontianak   68. Samarkand      76. Tehran    84. Ust-Nera
  5. Aqtau     13. Barnaul  21. Damascus    29. Hebron       37. Jerusalem  45. Krasnoyarsk   53. Muscat        61. Pyongyang   69. Seoul          77. Tel_Aviv  85. Vientiane
  6. Aqtobe    14. Beirut   22. Dhaka       30. Ho_Chi_Minh  38. Kabul      46. Kuala_Lumpur  54. Nicosia       62. Qatar       70. Shanghai       78. Thimphu   86. Vladivostok
  7. Ashgabat  15. Bishkek  23. Dili        31. Hong_Kong    39. Kamchatka  47. Kuching       55. Novokuznetsk  63. Qostanay    71. Singapore      79. Tokyo     87. Yakutsk
  8. Atyrau    16. Brunei   24. Dubai       32. Hovd         40. Karachi    48. Kuwait        56. Novosibirsk   64. Qyzylorda   72. Srednekolymsk  80. Tomsk     88. Yangon
Time zone: 70


Current default time zone: 'Asia/Shanghai'
Local time is now:      Tue Jan 16 02:00:49 CST 2024.
Universal Time is now:  Mon Jan 15 18:00:49 UTC 2024.
Run 'dpkg-reconfigure tzdata' if you wish to change it.

Setting up libxcb-present0:amd64 (1.14-2) ...
Setting up libglib2.0-data (2.64.6-1~ubuntu20.04.6) ...
Setting up libslang2:amd64 (2.3.2-4) ...
....
Setting up libavdevice58:amd64 (7:4.2.7-0ubuntu0.1) ...
Setting up ffmpeg (7:4.2.7-0ubuntu0.1) ...
Processing triggers for libc-bin (2.31-0ubuntu9.12) ...
/sbin/ldconfig.real: /lib/x86_64-linux-gnu/libcudadebugger.so.1 is not a symbolic link

/sbin/ldconfig.real: /lib/x86_64-linux-gnu/libcuda.so.1 is not a symbolic link

Processing triggers for libgdk-pixbuf2.0-0:amd64 (2.40.0+dfsg-3ubuntu0.4) ...



#@@ 改改改：1 可以root跑，2 直接用当前项目，下载没带.git， 3 使用conda管理环境
(py39) root@41af85cb0007:/opt/stable-diffusion-webui# diff -u webui.sh.back  webui.sh
--- webui.sh.back       2024-01-16 02:10:15.775277218 +0800
+++ webui.sh    2024-01-16 02:41:22.748823781 +0800
@@ -64,23 +64,23 @@
 if [[ $(id -u) -eq 0 ]]
 then
     printf "\n%s\n" "${delimiter}"
-    printf "\e[1m\e[31mERROR: This script must not be launched as root, aborting...\e[0m"
-    printf "\n%s\n" "${delimiter}"
-    exit 1
+#    printf "\e[1m\e[31mERROR: This script must not be launched as root, aborting...\e[0m"
+#    printf "\n%s\n" "${delimiter}"
+#    exit 1
 else
     printf "\n%s\n" "${delimiter}"
     printf "Running on \e[1m\e[32m%s\e[0m user" "$(whoami)"
     printf "\n%s\n" "${delimiter}"
 fi

-if [[ -d .git ]]
-then
+#if [[ -d .git ]]
+#then
     printf "\n%s\n" "${delimiter}"
     printf "Repo already cloned, using it as install directory"
     printf "\n%s\n" "${delimiter}"
     install_dir="${PWD}/../"
     clone_dir="${PWD##*/}"
-fi
+#fi

 # Check prerequisites
 for preq in "${GIT}" "${python_cmd}"
@@ -120,19 +120,20 @@
 cd "${install_dir}"/"${clone_dir}"/ || { printf "\e[1m\e[31mERROR: Can't cd to %s/%s/, aborting...\e[0m" "${install_dir}" "${clone_dir}"; exit 1; }
 if [[ ! -d "${venv_dir}" ]]
 then
-    "${python_cmd}" -m venv "${venv_dir}"
+#    "${python_cmd}" -m venv "${venv_dir}"
+    mkdir -p "${venv_dir}"
     first_launch=1
 fi
 # shellcheck source=/dev/null
-if [[ -f "${venv_dir}"/bin/activate ]]
-then
-    source "${venv_dir}"/bin/activate
-else
-    printf "\n%s\n" "${delimiter}"
-    printf "\e[1m\e[31mERROR: Cannot activate python venv, aborting...\e[0m"
-    printf "\n%s\n" "${delimiter}"
-    exit 1
-fi
+#if [[ -f "${venv_dir}"/bin/activate ]]
+#then
+#    source "${venv_dir}"/bin/activate
+#else
+#    printf "\n%s\n" "${delimiter}"
+#    printf "\e[1m\e[31mERROR: Cannot activate python venv, aborting...\e[0m"
+#    printf "\n%s\n" "${delimiter}"
+#    exit 1
+#fi

 printf "\n%s\n" "${delimiter}"
 printf "Launching launch.py..."



(py39) root@41af85cb0007:/opt/stable-diffusion-webui# export HTTPS_PROXY=socks5://172.22.240.1:23333
(py39) root@41af85cb0007:/opt/stable-diffusion-webui# bash webui.sh

################################################################
Install script for stable-diffusion + Web UI
Tested on Debian 11 (Bullseye)

################################################################

################################################################

################################################################
Repo already cloned, using it as install directory

################################################################

################################################################
Launching launch.py...

################################################################
Python 3.9.18 (main, Sep 11 2023, 13:41:44) 
[GCC 11.2.0]
Commit hash: &lt;none&gt;
Installing torch and torchvision
Installing gfpgan
Installing clip
Cloning Stable Diffusion into repositories/stable-diffusion...
Cloning Taming Transformers into repositories/taming-transformers...
Cloning K-diffusion into repositories/k-diffusion...
Cloning CodeFormer into repositories/CodeFormer...
Cloning BLIP into repositories/BLIP...
Installing requirements for CodeFormer
Installing requirements for Web UI
repositories/Taiyi-Stable-Diffusion-1B-Chinese-v0.1/feature_extractor/preprocessor_config.json | File missing.
repositories/Taiyi-Stable-Diffusion-1B-Chinese-v0.1 does not exist or file is missing. (1)Do you want to redownload the Taiyi model? Or (2)move your downloaded Taiyi model path? 1/2: 2

Detection failed, please reconfirm that the model has been moved to: repositories/Taiyi-Stable-Diffusion-1B-Chinese-v0.1
Please move the Taiyi model to: repositories/Taiyi-Stable-Diffusion-1B-Chinese-v0.1. Completed? y: y


???
  File "/root/miniconda3/envs/py39/lib/python3.9/site-packages/httpx/_transports/default.py", line 275, in __init__
    self._pool = httpcore.AsyncConnectionPool(
TypeError: __init__() got an unexpected keyword argument 'socket_options'


#https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/13236

#pip install -U httpcore
(py39) root@41af85cb0007:/opt/stable-diffusion-webui# pip3 install httpx==0.24.1

(py39) root@41af85cb0007:/opt/stable-diffusion-webui# unset all_proxy &amp;&amp; unset ALL_PROXY &amp;&amp; unset https_proxy &amp;&amp; unset HTTPS_PROXY &amp;&amp; unset http_proxy &amp;&amp; unset HTTP_PROXY
(py39) root@41af85cb0007:/opt/stable-diffusion-webui# bash webui.sh


???
ImportError: cannot import name '_compare_version' from 'torchmetrics.utilities.imports' (/root/miniconda3/envs/py39/lib/python3.9/site-packages/torchmetrics/utilities/imports.py)    

#https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/11648
#conda list torchmetrics
(py39) root@41af85cb0007:/opt/stable-diffusion-webui# conda install --force-reinstall torchmetrics==0.11.4

pip install torchmetrics==0.11.4 torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117


???

export COMMANDLINE_ARGS="--lowvram --precision full --no-half --skip-torch-cuda-test"


???
RuntimeError: Cannot add middleware after an application has started

(py39) root@41af85cb0007:/opt/stable-diffusion-webui# pip install fastapi==0.90.1


@@终于启动了
(py39) root@41af85cb0007:/opt/stable-diffusion-webui# bash webui.sh

################################################################
Install script for stable-diffusion + Web UI
Tested on Debian 11 (Bullseye)
################################################################

################################################################

################################################################
Repo already cloned, using it as install directory
################################################################

################################################################
Create and activate python venv
################################################################

################################################################
Launching launch.py...
################################################################
Python 3.9.18 (main, Sep 11 2023, 13:41:44)
[GCC 11.2.0]
Commit hash: &lt;none&gt;
Installing requirements for Web UI
Obtaining file:///opt/stable-diffusion-webui
ERROR: file:///opt/stable-diffusion-webui does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.
Launching Web UI with arguments: --lowvram --precision full --no-half --ckpt /opt/stable-diffusion-webui/repositories/Taiyi-Stable-Diffusion-1B-Chinese-v0.1/Taiyi-Stable-Diffusion-1B-Chinese-v0.1.ckpt --listen --port 12345
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 859.52 M params.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
making attention of type 'vanilla' with 512 in_channels
Loading weights [e2e75020] from /opt/stable-diffusion-webui/repositories/Taiyi-Stable-Diffusion-1B-Chinese-v0.1/Taiyi-Stable-Diffusion-1B-Chinese-v0.1.ckpt
Applying cross attention optimization (Doggettx).
Model loaded.
Loaded a total of 0 textual inversion embeddings.
Embeddings:
Running on local URL:  http://0.0.0.0:12345

To create a public link, set `share=True` in `launch()`.
</code></pre>

<p>启动镜像的时刻忘了挂数据U盘了，直接全部拷贝到容器里面：</p>

<pre><code>(base) root@41af85cb0007:/opt/stable-diffusion-webui/repositories# mkdir Taiyi-Stable-Diffusion-1B-Chinese-v0.1 

#(base) winse@DESKTOP-BR4MG38:Taiyi-Stable-Diffusion-1B-Chinese-v0.1$ tar tf 1.tar * 
#(base) winse@DESKTOP-BR4MG38:Taiyi-Stable-Diffusion-1B-Chinese-v0.1$ docker cp 1.tar 41af85cb0007:/opt/stable-diffusion-webui/repositories/Taiyi-Stable-Diffusion-1B-Chinese-v0.1/

(base) root@41af85cb0007:/opt/stable-diffusion-webui/repositories/Taiyi-Stable-Diffusion-1B-Chinese-v0.1# tar xf 1.tar 
(base) root@41af85cb0007:/opt/stable-diffusion-webui/repositories/Taiyi-Stable-Diffusion-1B-Chinese-v0.1# rm -rf 1.tar 
(base) root@41af85cb0007:/opt/stable-diffusion-webui/repositories/Taiyi-Stable-Diffusion-1B-Chinese-v0.1# ll
total 4084196
drwxr-xr-x 9 root  root        4096 Jan 15 17:57 ./
drwxrwxr-x 9 root  root        4096 Jan 15 17:42 ../
-rwxrwxrwx 1 webui webui 4182159787 Jan 15 00:26 Taiyi-Stable-Diffusion-1B-Chinese-v0.1.ckpt*
-rwxrwxrwx 1 webui webui        146 Jan 15 00:19 configuration.json*
drwxrwxrwx 2 webui webui       4096 Jan 15 17:50 feature_extractor/
-rwxrwxrwx 1 webui webui        539 Jan 15 00:22 model_index.json*
drwxrwxrwx 2 webui webui       4096 Jan 15 17:50 safety_checker/
drwxrwxrwx 2 webui webui       4096 Jan 15 17:50 scheduler/
drwxrwxrwx 2 webui webui       4096 Jan 15 17:50 text_encoder/
drwxrwxrwx 2 webui webui       4096 Jan 15 17:50 tokenizer/
drwxrwxrwx 2 webui webui       4096 Jan 15 17:50 unet/
drwxrwxrwx 2 webui webui       4096 Jan 15 17:50 vae/
</code></pre>

<p></p>

<h2>太乙webui - 纯净版</h2>

<pre><code>(base) winse@DESKTOP-BR4MG38:P15$ cat /etc/os-release 
(base) winse@DESKTOP-BR4MG38:P15$ cat /etc/issue
Ubuntu 20.04.6 LTS \n \l

(base) winse@DESKTOP-BR4MG38:P15$ nvidia-smi
Tue Jan 16 07:09:11 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.146.01             Driver Version: 537.99       CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro T2000                   On  | 00000000:01:00.0  On |                  N/A |
| N/A   50C    P8               4W /  60W |    537MiB /  4096MiB |      1%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A        32      G   /Xwayland                                 N/A      |
|    0   N/A  N/A        39      G   /Xwayland                                 N/A      |
|    0   N/A  N/A        41      G   /Xwayland                                 N/A      |
+---------------------------------------------------------------------------------------+
</code></pre>

<p>选一个跟我现在用的环境一样的版本和系统：<a href="https://hub.docker.com/r/nvidia/cuda/tags">https://hub.docker.com/r/nvidia/cuda/tags</a> CUDA and cuDNN images from gitlab.com/nvidia/cuda</p>

<pre><code>(base) winse@DESKTOP-BR4MG38:P15$ docker pull nvidia/cuda:12.2.2-devel-ubuntu20.04

(base) winse@DESKTOP-BR4MG38:P15$ docker run --gpus all --ipc host --ulimit memlock=-1 --ulimit stack=67108864 -it -v /mnt/i/ai:/app/stabilityai -p 7860:7860 docker.io/nvidia/cuda:12.2.2-devel-ubuntu20.04

root@c65a73d918b1:/# sed -i.bak -e 's|archive.ubuntu.com/ubuntu/|mirrors.tuna.tsinghua.edu.cn/ubuntu/|' -e 's|security.ubuntu.com/ubuntu/|mirrors.tuna.tsinghua.edu.cn/ubuntu/|' /etc/apt/sources.list
root@c65a73d918b1:/# apt update 

root@c65a73d918b1:/# apt install -y git wget vim

root@c65a73d918b1:/# cd 
root@c65a73d918b1:~# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
root@c65a73d918b1:~# bash Miniconda3-latest-Linux-x86_64.sh -b -u                
root@c65a73d918b1:~# ~/miniconda3/bin/conda init bash
root@c65a73d918b1:~# source ~/.bashrc
(base) root@c65a73d918b1:~# 

(base) root@c65a73d918b1:~# wget -c https://github.com/IDEA-CCNL/stable-diffusion-webui/archive/refs/heads/master.tar.gz -O - | tar zxf - 
(base) root@c65a73d918b1:~# mv stable-diffusion-webui-master stable-diffusion-webui        

(base) root@c65a73d918b1:~# cd stable-diffusion-webui
(base) root@c65a73d918b1:~/stable-diffusion-webui# conda create -n py3.10 python=3.10
(base) root@c65a73d918b1:~/stable-diffusion-webui# conda activate py3.10


(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# mkdir .git 
(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# diff -u webui.sh.back  webui.sh 
--- webui.sh.back       2024-01-15 23:39:24.271256229 +0000
+++ webui.sh    2024-01-15 23:42:17.070404354 +0000
@@ -64,9 +64,6 @@
 if [[ $(id -u) -eq 0 ]]
 then
     printf "\n%s\n" "${delimiter}"
-    printf "\e[1m\e[31mERROR: This script must not be launched as root, aborting...\e[0m"
-    printf "\n%s\n" "${delimiter}"
-    exit 1
 else
     printf "\n%s\n" "${delimiter}"
     printf "Running on \e[1m\e[32m%s\e[0m user" "$(whoami)"
@@ -120,19 +117,11 @@
 cd "${install_dir}"/"${clone_dir}"/ || { printf "\e[1m\e[31mERROR: Can't cd to %s/%s/, aborting...\e[0m" "${install_dir}" "${clone_dir}"; exit 1; }
 if [[ ! -d "${venv_dir}" ]]
 then
-    "${python_cmd}" -m venv "${venv_dir}"
+#    "${python_cmd}" -m venv "${venv_dir}"
+    mkdir -p "${venv_dir}"
     first_launch=1
 fi
 # shellcheck source=/dev/null
-if [[ -f "${venv_dir}"/bin/activate ]]
-then
-    source "${venv_dir}"/bin/activate
-else
-    printf "\n%s\n" "${delimiter}"
-    printf "\e[1m\e[31mERROR: Cannot activate python venv, aborting...\e[0m"
-    printf "\n%s\n" "${delimiter}"
-    exit 1
-fi

 printf "\n%s\n" "${delimiter}"
 printf "Launching launch.py..."


(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# pip install pysocks

#https://github.com/invoke-ai/InvokeAI/issues/3560#issuecomment-1689474997
#https://blog.csdn.net/shark1357/article/details/131238924
(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple
(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# pip install gfpgan==1.3.8

(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# apt-get install ffmpeg libsm6 libxext6  -y

(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# pip install httpcore httpx==0.24.1 torchmetrics==0.11.4 fastapi==0.90.1

(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# vi webui-user.sh

export COMMANDLINE_ARGS="--lowvram --precision full --no-half --skip-torch-cuda-test"


(py3.10) root@c65a73d918b1:~# cd stable-diffusion-webui/repositories/
(py3.10) root@c65a73d918b1:~/stable-diffusion-webui/repositories# ln -s /app/stabilityai/Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1/

(py3.10) root@c65a73d918b1:~/stable-diffusion-webui/repositories#  ll Taiyi-Stable-Diffusion-1B-Chinese-v0.1
lrwxrwxrwx 1 root root 69 Jan 16 07:58 Taiyi-Stable-Diffusion-1B-Chinese-v0.1 -&gt; /app/stabilityai/Fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1//


(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# pip install socksio httpx[socks]


#@@ 加代理下载部署时需要的github代码
(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# export HTTPS_PROXY=socks5://172.22.240.1:23333
(py3.10) root@c65a73d918b1:~/stable-diffusion-webui# bash webui.sh --port 7860
</code></pre>

<p>然后在Windows浏览器访问： <a href="http://localhost:7860/">http://localhost:7860/</a></p>

<p><img src="/images/blogs/ai/webui.png" alt="" /></p>

<p><img src="/images/blogs/ai/webui2.png" alt="" /></p>

<p></p>

<p></p>

<h2>TODO</h2>

<p>汉化： <a href="https://github.com/VinsonLaro/stable-diffusion-webui-chinese">https://github.com/VinsonLaro/stable-diffusion-webui-chinese</a></p>

<p>&ndash;END</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Windows Run Ubuntu]]></title>
    <link href="http://winse.github.io/blog/2017/10/30/windows-run-ubuntu/"/>
    <updated>2017-10-30T07:17:53+08:00</updated>
    <id>http://winse.github.io/blog/2017/10/30/windows-run-ubuntu</id>
    <content type="html"><![CDATA[<p>@@更新 2021-05-25 [wsl-Ubuntu 20.04.2 LTS] 配置octopress
@@更新 2023-11-28 [wsl-Ubuntu 22.04.2 LTS (Jammy Jellyfish)] 配置octopress</p>

<p>最新版的Windows已经可以安装ubuntu了，算是微软开源后的一个阶段性的成果了。</p>

<p>功能和windows兼容性很强（不像cygwin），软链接、文件权限都是和系统一致的。并且基本所有ubuntu的功能都可以使用，安装jekyll、docker都很顺利。同时打开系统的程序也很方便。非常赞和值得程序员去尝试！！</p>

<p>本文后面会逐渐增加使用过程中的一些操作，今天先更新系统安装、octopress安装、docker安装。</p>

<h2>系统安装</h2>

<p>直接去微软的官网下载最新版系统ISO，然后安装系统 <strong> 专业版 </strong>（教育版比较干净一些，但是网上没有破解方式啊）。</p>

<p></p>

<h2>Ubuntu</h2>

<ul>
<li><a href="https://docs.microsoft.com/en-us/windows/wsl/about">Windows Subsystem for Linux Documentation</a></li>
</ul>


<blockquote><p>The Windows Subsystem for Linux lets developers run Linux environments &ndash; including most command-line tools, utilities, and applications &ndash; directly on Windows, unmodified, without the overhead of a virtual machine.</p></blockquote>

<h4>安装Ubuntu</h4>

<p>最新版的Ubuntu已经进入到稳定版。直接打开商店，搜索Ubuntu，然后安装即可。大概200M的样子，很快就安装了。然后Launch会初始化创建用户。</p>

<p>相关的一些有用的文档：</p>

<ul>
<li><a href="https://msdn.microsoft.com/zh-cn/commandline/wsl/user-support">https://msdn.microsoft.com/zh-cn/commandline/wsl/user-support</a></li>
<li>权限相关 <a href="https://github.com/Microsoft/WSL/issues/81">https://github.com/Microsoft/WSL/issues/81</a></li>
</ul>


<h4>安装mintty</h4>

<ul>
<li><a href="https://superuser.com/questions/1110045/windows-10-bash-and-mintty">https://superuser.com/questions/1110045/windows-10-bash-and-mintty</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22033219">https://zhuanlan.zhihu.com/p/22033219</a></li>
</ul>


<p>CMD方式操作Ubuntu太难受了，mintty操作就像SecurtCRT一样，简单方便。安装 <a href="https://github.com/mintty/wsltty">wsltty-1.8.0-install</a> ，使用mintty来运行ubuntu。</p>

<p>默认mintty的配置放在 <code>%APPDATA%\wsltty</code>。在子目录theme下可以 <a href="http://ciembor.github.io/4bit/">http://ciembor.github.io/4bit/</a> 下载一个主题放到该目录下，然后在mintty配置页面选择该主题。</p>

<h4>系统文件</h4>

<ul>
<li><a href="https://github.com/Microsoft/WSL/issues/402">https://github.com/Microsoft/WSL/issues/402</a></li>
</ul>


<p>root挂载点：<code>C:\Users\&lt;user&gt;\AppData\Local\lxss</code> or <code>C:\Users\&lt;username&gt;\AppData\Local\Packages\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\LocalState</code></p>

<h2>安装Jekyll</h2>

<pre><code># https://superuser.com/questions/116625/recursively-change-owner-windows-7
#takeown /f "C:\path\to\folder" /r
#icacls "C:\path\to\folder" /reset /T
用powershell管理员权限 "提"权
PS E:\winsegit\octopress&gt; takeown /F . /R
修改就旧系统的文件属性：
PS E:\winsegit\octopress&gt; cacls *.* /T /G Everyone:F

sudo apt-get update
sudo apt-get install ruby-bundler
sudo apt-get install ruby-dev
sudo apt-get install make
sudo apt-get install gcc
bundle install

sudo gem update
sudo gem uninstall rake
sudo gem install rake -v 10.5.0
# sudo apt-get install nodejs

rake preview

winse@DESKTOP-ADH7K1Q:~/winsegit/octopress$ rake preview
/usr/lib/ruby/vendor_ruby/bundler/shared_helpers.rb:78: warning: Insecure world writable dir /mnt/c/Windows/System32 in PATH, mode 040777
Starting to watch source with Jekyll and Compass. Starting Rack on port 4000
/usr/lib/ruby/vendor_ruby/bundler/shared_helpers.rb:78: warning: Insecure world writable dir /mnt/c/Windows/System32 in PATH, mode 040777
/usr/lib/ruby/vendor_ruby/bundler/shared_helpers.rb:78: warning: Insecure world writable dir /mnt/c/Windows/System32 in PATH, mode 040777
/var/lib/gems/2.3.0/gems/liquid-2.6.1/lib/liquid/htmltags.rb:43: warning: key "index0" is duplicated and overwritten on line 46
[2017-10-29 22:48:09] INFO  WEBrick 1.3.1
[2017-10-29 22:48:09] INFO  ruby 2.3.1 (2016-04-26) [x86_64-linux-gnu]
[2017-10-29 22:48:09] INFO  WEBrick::HTTPServer#start: pid=39 port=4000
Configuration file: /mnt/e/winsegit/octopress/_config.yml
            Source: source
       Destination: public
      Generating...
                    done.
</code></pre>

<p>上面warning提示也有对应的Issue，但也没所谓暂时不理： <a href="https://github.com/Microsoft/WSL/issues/1426">https://github.com/Microsoft/WSL/issues/1426</a></p>

<p>UPDATE 2023年11月18日09:22:51 (WSL1 + rvm)</p>

<pre><code>winse@DESKTOP-BR4MG38:octopress$ sudo apt install make gcc -y
[sudo] password for winse:
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
gcc is already the newest version (4:11.2.0-1ubuntu1).
gcc set to manually installed.
make is already the newest version (4.3-4.1build1).
make set to manually installed.
0 upgraded, 0 newly installed, 0 to remove and 101 not upgraded.

winse@DESKTOP-BR4MG38:octopress$ gpg --keyserver hkp://keyserver.ubuntu.com --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB
gpg: key 105BD0E739499BDB: 2 duplicate signatures removed
gpg: /home/winse/.gnupg/trustdb.gpg: trustdb created
gpg: key 105BD0E739499BDB: public key "Piotr Kuczynski &lt;piotr.kuczynski@gmail.com&gt;" imported
gpg: key 3804BB82D39DC0E3: public key "Michal Papis (RVM signing) &lt;mpapis@gmail.com&gt;" imported
gpg: Total number processed: 2
gpg:               imported: 2
winse@DESKTOP-BR4MG38:octopress$ curl -sSL https://get.rvm.io | bash -s stable
Downloading https://github.com/rvm/rvm/archive/1.29.12.tar.gz
Downloading https://github.com/rvm/rvm/releases/download/1.29.12/1.29.12.tar.gz.asc
gpg: Signature made Sat 16 Jan 2021 02:46:22 AM CST
gpg:                using RSA key 7D2BAF1CF37B13E2069D6956105BD0E739499BDB
gpg: Good signature from "Piotr Kuczynski &lt;piotr.kuczynski@gmail.com&gt;" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: 7D2B AF1C F37B 13E2 069D  6956 105B D0E7 3949 9BDB
GPG verified '/home/winse/.rvm/archives/rvm-1.29.12.tgz'
Installing RVM to /home/winse/.rvm/
    RVM PATH line found in /home/winse/.bashrc.
    RVM PATH line not found for Zsh, rerun this command with '--auto-dotfiles' flag to fix it.
    RVM sourcing line found in /home/winse/.bash_profile.
    RVM sourcing line not found for Zsh, rerun this command with '--auto-dotfiles' flag to fix it.
Installation of RVM in /home/winse/.rvm/ is almost complete:

  * To start using RVM you need to run `source /home/winse/.rvm/scripts/rvm`
    in all your open shell windows, in rare cases you need to reopen all shell windows.
Thanks for installing RVM 🙏
Please consider donating to our open collective to help us maintain RVM.

👉  Donate: https://opencollective.com/rvm/donate


winse@DESKTOP-BR4MG38:octopress$

##-&gt; @see https://github.com/rvm/rvm/issues/4764#issuecomment-624796884
    Open your terminal
    run /etc/apt/sources.list using vi /etc/apt/sources.list (It is possible that you can't save this file, and might get "permission denied" error. SO, in that case, open this file with sudo vi /etc/apt/sources.list)
    after opening it, go to the end of file and paste this line:
    deb http://security.ubuntu.com/ubuntu bionic-security main
    save this file by typing :wq
    now, paste this command in terminal:
    sudo apt update &amp;&amp; apt-cache policy libssl1.0-dev
    You will see an update like libssl1.0-dev: Installed: (none) Candidate: 1.0.2n-1ubuntu5.3 Version table: 1.0.2n-1ubuntu5.3 500 500 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages
    then, run this command:
    sudo apt-get install libssl1.0-dev

winse@DESKTOP-BR4MG38:~$ sudo apt update &amp;&amp; apt-cache policy libssl1.0-dev
Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease
Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease
Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]
Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
Err:4 http://security.ubuntu.com/ubuntu bionic-security InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3B4FE6ACC0B21F32
Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,162 kB]
Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [998 kB]
Reading package lists... Done
W: GPG error: http://security.ubuntu.com/ubuntu bionic-security InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3B4FE6ACC0B21F32
E: The repository 'http://security.ubuntu.com/ubuntu bionic-security InRelease' is not signed.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.

##--&gt; @see https://askubuntu.com/questions/943539/the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-avai

winse@DESKTOP-BR4MG38:~$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3B4FE6ACC0B21F32
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).
Executing: /tmp/apt-key-gpghome.Ak5EisV4UJ/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3B4FE6ACC0B21F32
gpg: key 3B4FE6ACC0B21F32: public key "Ubuntu Archive Automatic Signing Key (2012) &lt;ftpmaster@ubuntu.com&gt;" imported
gpg: Total number processed: 1
gpg:               imported: 1

winse@DESKTOP-BR4MG38:~$ sudo apt update &amp;&amp; apt-cache policy libssl1.0-dev
Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease
Hit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
Hit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease
Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
103 packages can be upgraded. Run 'apt list --upgradable' to see them.
W: http://security.ubuntu.com/ubuntu/dists/bionic-security/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.
libssl1.0-dev:
  Installed: (none)
  Candidate: 1.0.2n-1ubuntu5.13
  Version table:
     1.0.2n-1ubuntu5.13 500
        500 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages

winse@DESKTOP-BR4MG38:~$ sudo apt-get install libssl1.0-dev
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following additional packages will be installed:
  libssl1.0.0
The following NEW packages will be installed:
  libssl1.0-dev libssl1.0.0
0 upgraded, 2 newly installed, 0 to remove and 103 not upgraded.
Need to get 2,455 kB of archives.
After this operation, 10.8 MB of additional disk space will be used.
Do you want to continue? [Y/n] y
Get:1 http://security.ubuntu.com/ubuntu bionic-security/main amd64 libssl1.0.0 amd64 1.0.2n-1ubuntu5.13 [1,089 kB]
Get:2 http://security.ubuntu.com/ubuntu bionic-security/main amd64 libssl1.0-dev amd64 1.0.2n-1ubuntu5.13 [1,366 kB]
Fetched 2,455 kB in 4s (665 kB/s)
Preconfiguring packages ...
Selecting previously unselected package libssl1.0.0:amd64.
(Reading database ... 29956 files and directories currently installed.)
Preparing to unpack .../libssl1.0.0_1.0.2n-1ubuntu5.13_amd64.deb ...
Unpacking libssl1.0.0:amd64 (1.0.2n-1ubuntu5.13) ...
Selecting previously unselected package libssl1.0-dev:amd64.
Preparing to unpack .../libssl1.0-dev_1.0.2n-1ubuntu5.13_amd64.deb ...
Unpacking libssl1.0-dev:amd64 (1.0.2n-1ubuntu5.13) ...
Setting up libssl1.0.0:amd64 (1.0.2n-1ubuntu5.13) ...
Setting up libssl1.0-dev:amd64 (1.0.2n-1ubuntu5.13) ...
Processing triggers for libc-bin (2.35-0ubuntu3.1) ...
winse@DESKTOP-BR4MG38:~$

winse@DESKTOP-BR4MG38:octopress$ rvm install 2.3
Searching for binary rubies, this might take some time.
No binary rubies available for: ubuntu/22.04/x86_64/ruby-2.3.8.
Continuing with compilation. Please read 'rvm help mount' to get more information on binary rubies.
Checking requirements for ubuntu.
Requirements installation successful.
Installing Ruby from source to: /home/winse/.rvm/rubies/ruby-2.3.8, this may take a while depending on your cpu(s)...
ruby-2.3.8 - #downloading ruby-2.3.8, this may take a while depending on your connection...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 13.7M  100 13.7M    0     0  3149k      0  0:00:04  0:00:04 --:--:-- 3149k
ruby-2.3.8 - #extracting ruby-2.3.8 to /home/winse/.rvm/src/ruby-2.3.8.....
ruby-2.3.8 - #configuring..........................................................
ruby-2.3.8 - #post-configuration..
ruby-2.3.8 - #compiling.............................................................................................................................................................................................................................................
ruby-2.3.8 - #installing..............
ruby-2.3.8 - #making binaries executable..
ruby-2.3.8 - #downloading rubygems-3.0.9
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  865k  100  865k    0     0   701k      0  0:00:01  0:00:01 --:--:--  701k
No checksum for downloaded archive, recording checksum in user configuration.
ruby-2.3.8 - #extracting rubygems-3.0.9.....
ruby-2.3.8 - #removing old rubygems........
ruby-2.3.8 - #installing rubygems-3.0.9................................................................
ruby-2.3.8 - #gemset created /home/winse/.rvm/gems/ruby-2.3.8@global
ruby-2.3.8 - #importing gemset /home/winse/.rvm/gemsets/global.gems..............................there was an error installing gem rubygems-bundler
.............................
ruby-2.3.8 - #generating global wrappers........
ruby-2.3.8 - #gemset created /home/winse/.rvm/gems/ruby-2.3.8
ruby-2.3.8 - #importing gemsetfile /home/winse/.rvm/gemsets/default.gems evaluated to empty gem list
ruby-2.3.8 - #generating default wrappers........
ruby-2.3.8 - #adjusting #shebangs for (gem irb erb ri rdoc testrb rake).
Install of ruby-2.3.8 - #complete
Ruby was built without documentation, to build it run: rvm docs generate-ri
winse@DESKTOP-BR4MG38:octopress$

winse@DESKTOP-BR4MG38:octopress$ rvm list
   ruby-2.3.8 [ x86_64 ]

# Default ruby not set. Try 'rvm alias create default &lt;ruby&gt;'.

# =&gt; - current
# =* - current &amp;&amp; default
#  * - default

winse@DESKTOP-BR4MG38:octopress$

winse@DESKTOP-BR4MG38:octopress$ bash --login
winse@DESKTOP-BR4MG38:octopress$ rvm use 2.3.8 --default
Using /home/winse/.rvm/gems/ruby-2.3.8

##--&gt; @see https://mirrors.tuna.tsinghua.edu.cn/help/rubygems/

winse@DESKTOP-BR4MG38:octopress$ unset HTTPS_PROXY
winse@DESKTOP-BR4MG38:octopress$ bundle config mirror.https://rubygems.org https://mirrors.tuna.tsinghua.edu.cn/rubygems

winse@DESKTOP-BR4MG38:octopress$ bundle install
Fetching source index from https://mirrors.tuna.tsinghua.edu.cn/rubygems/
Resolving dependencies...
Fetching rake 10.5.0
Installing rake 10.5.0
Fetching RedCloth 4.2.9
Installing RedCloth 4.2.9 with native extensions
Fetching bigdecimal 1.3.2
Installing bigdecimal 1.3.2 with native extensions
Fetching blankslate 2.1.2.4
Installing blankslate 2.1.2.4
Using bundler 1.17.3
Fetching timers 1.1.0
Installing timers 1.1.0
Fetching celluloid 0.15.2
Installing celluloid 0.15.2
Fetching chunky_png 1.3.1
Installing chunky_png 1.3.1
Fetching fast-stemmer 1.0.2
Installing fast-stemmer 1.0.2 with native extensions
Fetching classifier 1.3.4
Installing classifier 1.3.4
Fetching coffee-script-source 1.7.1
Installing coffee-script-source 1.7.1
Fetching execjs 2.2.1
Installing execjs 2.2.1
Fetching coffee-script 2.3.0
Installing coffee-script 2.3.0
Fetching colorator 0.1
Installing colorator 0.1
Fetching fssm 0.2.10
Installing fssm 0.2.10
Fetching sass 3.2.19
Installing sass 3.2.19
Fetching compass 0.12.6
Installing compass 0.12.6
Fetching ffi 1.9.3
Installing ffi 1.9.3 with native extensions
Fetching tilt 1.4.1
Installing tilt 1.4.1
Fetching haml 4.0.5
Installing haml 4.0.5
Fetching jekyll-coffeescript 1.0.0
Installing jekyll-coffeescript 1.0.0
Fetching jekyll-gist 1.1.0
Installing jekyll-gist 1.1.0
Fetching jekyll-paginate 1.0.0
Installing jekyll-paginate 1.0.0
Fetching jekyll-sass-converter 1.0.0
Installing jekyll-sass-converter 1.0.0
Fetching rb-fsevent 0.9.4
Installing rb-fsevent 0.9.4
Fetching rb-inotify 0.9.5
Installing rb-inotify 0.9.5
Fetching listen 2.7.9
Installing listen 2.7.9
Fetching jekyll-watch 1.0.0
Installing jekyll-watch 1.0.0
Fetching kramdown 1.4.0
Installing kramdown 1.4.0
Fetching liquid 2.6.1
Installing liquid 2.6.1
Fetching mercenary 0.3.4
Installing mercenary 0.3.4
Fetching posix-spawn 0.3.8
Installing posix-spawn 0.3.8 with native extensions
Fetching yajl-ruby 1.1.0
Installing yajl-ruby 1.1.0 with native extensions
Fetching pygments.rb 0.6.0
Installing pygments.rb 0.6.0
Fetching redcarpet 3.1.2
Installing redcarpet 3.1.2 with native extensions
Fetching safe_yaml 1.0.4
Installing safe_yaml 1.0.4
Fetching parslet 1.5.0
Installing parslet 1.5.0
Fetching toml 0.1.1
Installing toml 0.1.1
Fetching jekyll 2.1.1
Installing jekyll 2.1.1
Fetching jekyll-date-format 1.0.0
Installing jekyll-date-format 1.0.0
Fetching jekyll-page-hooks 1.3.0
Installing jekyll-page-hooks 1.3.0
Fetching jekyll-sitemap 0.5.0
Installing jekyll-sitemap 0.5.0
Fetching json 1.8.2
Installing json 1.8.2 with native extensions
Fetching rack 1.5.2
Installing rack 1.5.2
Fetching rack-protection 1.5.3
Installing rack-protection 1.5.3
Fetching rdiscount 2.1.7.1
Installing rdiscount 2.1.7.1 with native extensions
Fetching rubypants 0.2.0
Installing rubypants 0.2.0
Fetching sass-globbing 1.0.0
Installing sass-globbing 1.0.0
Fetching sinatra 1.4.5
Installing sinatra 1.4.5
Fetching stringex 1.4.0
Installing stringex 1.4.0
Bundle complete! 16 Gemfile dependencies, 50 gems now installed.
Use `bundle info [gemname]` to see where a bundled gem is installed.
Post-install message from haml:

HEADS UP! Haml 4.0 has many improvements, but also has changes that may break
your application:

* Support for Ruby 1.8.6 dropped
* Support for Rails 2 dropped
* Sass filter now always outputs &lt;style&gt; tags
* Data attributes are now hyphenated, not underscored
* html2haml utility moved to the html2haml gem
* Textile and Maruku filters moved to the haml-contrib gem

For more info see:

http://rubydoc.info/github/haml/haml/file/CHANGELOG.md

winse@DESKTOP-BR4MG38:octopress$

winse@DESKTOP-BR4MG38:octopress$ rake preview
Starting to watch source with Jekyll and Compass. Starting Rack on port 14000
/home/winse/.rvm/gems/ruby-2.3.8/gems/liquid-2.6.1/lib/liquid/htmltags.rb:43: warning: key "index0" is duplicated and overwritten on line 46
[2023-11-18 09:42:44] INFO  WEBrick 1.3.1
[2023-11-18 09:42:44] INFO  ruby 2.3.8 (2018-10-18) [x86_64-linux]
[2023-11-18 09:42:44] INFO  WEBrick::HTTPServer#start: pid=24150 port=14000
Configuration file: /mnt/e/winsegit/octopress/_config.yml
            Source: source
       Destination: public
      Generating...
                    done.
</code></pre>

<p>UPDATE 2021-05-25 24:09 (wsl-linux + rvm)</p>

<pre><code>sudo apt-get update
sudo apt-get install make
sudo apt-get install gcc

## https://ruby-china.org/wiki/rvm-guide

winse@LAPTOP-I9ECVAQ4:e$ gpg --keyserver hkp://pool.sks-keyservers.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB

# curl -sSL https://get.rvm.io | bash -s stable
# https://raw.githubusercontent.com/rvm/rvm/master/binscripts/rvm-installer
winse@LAPTOP-I9ECVAQ4:e$ cat rvm-installer.txt | bash -s stable 

winse@LAPTOP-I9ECVAQ4:e$ echo "ruby_url=https://cache.ruby-china.com/pub/ruby" &gt; ~/.rvm/user/db


# 依赖安装
## https://www.garron.me/en/linux/install-ruby-2-3-3-ubuntu.html
Edit this file /etc/apt/sources.list and add this line to the end of it.

deb http://security.ubuntu.com/ubuntu bionic-security main

After that run:

sudo apt update &amp;&amp; apt-cache policy libssl1.0-dev

Finally,

sudo apt-get install libssl1.0-dev

##END

winse@LAPTOP-I9ECVAQ4:~$ rvm install 2.3

winse@LAPTOP-I9ECVAQ4:~$ rvm list

winse@LAPTOP-I9ECVAQ4:~$ bash --login
winse@LAPTOP-I9ECVAQ4:~$ rvm use 2.3.8 --default
Using /home/winse/.rvm/gems/ruby-2.3.8

## 
winse@LAPTOP-I9ECVAQ4:octopress$ bundle install

winse@LAPTOP-I9ECVAQ4:octopress$ rake preview

winse@DESKTOP-BR4MG38:~$ ln -s /mnt/e/local/npp/notepad++.exe text
</code></pre>

<h2>安装docker</h2>

<ul>
<li><a href="https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/#install-docker-ce-1">https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/#install-docker-ce-1</a></li>
</ul>


<pre><code>winse@DESKTOP-ADH7K1Q:~$ uname -r
4.4.0-43-Microsoft
winse@DESKTOP-ADH7K1Q:~$ uname -a
Linux DESKTOP-ADH7K1Q 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux

winse@DESKTOP-ADH7K1Q:~$  sudo apt-get install \
     apt-transport-https \
     ca-certificates \
     curl \
     software-properties-common

winse@DESKTOP-ADH7K1Q:~$ lsb_release -cs
xenial
winse@DESKTOP-ADH7K1Q:~$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
OK
winse@DESKTOP-ADH7K1Q:~$ sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"

winse@DESKTOP-ADH7K1Q:~$ sudo apt-get update

winse@DESKTOP-ADH7K1Q:~$ sudo apt-get install docker-ce

winse@DESKTOP-ADH7K1Q:~$ sudo service docker start
 * Starting Docker: docker                                                                                                                     [ OK ]
winse@DESKTOP-ADH7K1Q:~$ sudo docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
winse@DESKTOP-ADH7K1Q:~$ sudo docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
winse@DESKTOP-ADH7K1Q:~$
</code></pre>

<h2>运行windows系统应用</h2>

<ul>
<li><a href="https://www.howtogeek.com/285082/how-to-run-windows-programs-from-windows-10s-bash-shell/">https://www.howtogeek.com/285082/how-to-run-windows-programs-from-windows-10s-bash-shell/</a></li>
<li><a href="https://github.com/Microsoft/WSL/issues/333">https://github.com/Microsoft/WSL/issues/333</a></li>
<li><a href="https://msdn.microsoft.com/en-us/commandline/wsl/interop">https://msdn.microsoft.com/en-us/commandline/wsl/interop</a></li>
</ul>


<pre><code>winse@DESKTOP-ADH7K1Q:~$ ln -s /mnt/e/local/usr/share/npp/notepad++.exe text

winse@DESKTOP-ADH7K1Q:~$ cp -r ../winse-cygwin/new_post.sh ./
winse@DESKTOP-ADH7K1Q:~$ cat new_post.sh
cd ~/winsegit/octopress/
rake new_post["$1"] | tail -1 | awk -F: '{print $2}' | while read line
do
name=${line#source/_posts/}
newpath=source/_stash/$name
mv $line $newpath

echo -e "\n\n--END" &gt;&gt;$newpath

~/text $newpath &amp;
done

winse@DESKTOP-ADH7K1Q:~$ ./new_post.sh "windows run ubuntu"
/usr/lib/ruby/vendor_ruby/bundler/shared_helpers.rb:78: warning: Insecure world writable dir /mnt/c in PATH, mode 040777
mkdir -p source/_posts
</code></pre>

<h2>重新配置git</h2>

<p>用powershell修改原来的权限后，然后用shell来进行设置。</p>

<pre><code>#powershell
takeown /F . /R
cacls *.* /T /G Everyone:F

#ubuntu shell
winse@DESKTOP-ADH7K1Q:~$ cp -r ../winse-cygwin/.ssh ./

winse@DESKTOP-ADH7K1Q:~$ cd .ssh
winse@DESKTOP-ADH7K1Q:~/.ssh$ chmod 600 id_rsa authorized_keys
winse@DESKTOP-ADH7K1Q:~/.ssh$ chmod 644 id_rsa.pub config

winse@DESKTOP-ADH7K1Q:~/winsegit/octopress/_deploy$ rm -rf *
winse@DESKTOP-ADH7K1Q:~/winsegit/octopress/_deploy$ git clone git@github.com:winse/winse.github.com.git ./

winse@DESKTOP-ADH7K1Q:~/winsegit/octopress$ git config --global user.email winseliu@qq.com
winse@DESKTOP-ADH7K1Q:~/winsegit/octopress$ git config --global user.name winse

winse@DESKTOP-ADH7K1Q:~/winsegit/octopress$ rake preview
winse@DESKTOP-ADH7K1Q:~/winsegit/octopress$ sh public.git.sh
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
