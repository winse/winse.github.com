
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Winse Blog</title>
  <meta name="author" content="Winse Liu">

  
  <meta name="description" content="注册个域名（net.cn）
添加CNAME文件（github.com）
添加解析记录（net.cn） 如果是使用子域名的话非常简单。在（pages）CNAME文件中填写www.winseliu.com，然后在（net.cn）解析页添加CNAME指向winse.github.io即可。 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://winseliu.com">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Winse Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
-->

  <!--
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-43198550-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


  -->
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Winse Blog</a></h1>
  
    <h2>走走停停, 熙熙攘攘, 忙忙碌碌, 不知何畏.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:winseliu.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/24/github-custom-domain/">为github Pages页面设置自定义域名</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-24T00:17:19+08:00" pubdate data-updated="true">Fri 2014-10-24 00:17</time>
        
      </p>
    
  </header>


  <div class="entry-content"><ol>
<li>注册个域名（net.cn）</li>
<li>添加CNAME文件（github.com）</li>
<li>添加解析记录（net.cn）</li>
</ol>


<p><img src="http://file.bmob.cn/M00/20/C5/wKhkA1RJKuyAf6lWAACIJ28IFe8161.png" alt="" /></p>

<p>如果是使用子域名的话非常简单。在（pages）CNAME文件中填写www.winseliu.com，然后在（net.cn）解析页添加CNAME指向winse.github.io即可。</p>

<p>如果想默认顶级域名也能访问，需要添加的两个ip指向，参见上图。通知（pages）CNAME中使用winseliu.com。</p>

<h2>参考</h2>

<ul>
<li><a href="https://help.github.com/articles/my-custom-domain-isn-t-working/">My custom domain isn&rsquo;t working</a></li>
<li><a href="https://help.github.com/articles/tips-for-configuring-an-a-record-with-your-dns-provider/">Tips for configuring an A record with your DNS provider</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/18/modify-hosts-build-hadoop-cluster-on-docker/">Dnsmasq解决docker集群节点互通问题</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-18T04:19:21+08:00" pubdate data-updated="true">Sat 2014-10-18 04:19</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>上个星期学习了一下docker，写了一个<a href="https://github.com/winse/docker-hadoop/tree/Pseudo-Distributed">伪分布式的Dockerfile</a>。</p>

<p>通过<code>--link</code>的方式master能访问slaver，毕竟slaver的相关信息已经被写入到master的hosts文件里面去了嘛！理所当然认为，直接把master的hosts文件全部复制一份到所有slaver节点问题就解决了。</p>

<p>等真正操作的时刻，发现不是那么回事，docker容器不给修改hosts文件！！</p>

<h2>错误实现</h2>

<p>首先，看下不当的操作：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -d --name slaver1 -h slaver1 hadoop
</span><span class='line'>[root@docker ~]# docker run -d --name slaver2 -h slaver2 hadoop
</span><span class='line'>[root@docker ~]# docker run -d --name master -h master --link slaver1:slaver1 --link slaver2:slaver2 hadoop
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
</span><span class='line'>dafc82678811        hadoop:latest       /bin/sh -c '/usr/sbi   40 seconds ago      Up 40 seconds       22/tcp              master
</span><span class='line'>86d2da5209c5        hadoop:latest       /bin/sh -c '/usr/sbi   49 seconds ago      Up 48 seconds       22/tcp              master/slaver2,slaver2
</span><span class='line'>7b9761fb05a8        hadoop:latest       /bin/sh -c '/usr/sbi   56 seconds ago      Up 55 seconds       22/tcp              master/slaver1,slaver1</span></code></pre></td></tr></table></div></figure>


<p>此时，通过<code>--link</code>连接方式，master的hosts中已经包括了slaver1和slaver2，按照正常的路子，登录master拷贝其hosts到slaver节点，一切就妥妥的了。现实是残酷的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-bash-4.1# scp /etc/hosts slaver1:/etc/
</span><span class='line'>scp: /etc//hosts: Read-only file system</span></code></pre></td></tr></table></div></figure>


<h2>DNS完美解决问题</h2>

<p>首先需要在宿主机器上安装dns服务器，bind不多说比较麻烦。这里参考网上人家解决方式，使用dnsmasq来搭建DNS服务器。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# yum install dnsmasq -y
</span><span class='line'>
</span><span class='line'>[root@docker ~]# cp /etc/resolv.conf /etc/resolv.dnsmasq.conf 
</span><span class='line'>[root@docker ~]# touch /etc/dnsmasq.hosts
</span><span class='line'>
</span><span class='line'>[root@docker ~]# vi /etc/resolv.conf
</span><span class='line'>[root@docker ~]# cat /etc/resolv.conf
</span><span class='line'>; generated by /sbin/dhclient-script
</span><span class='line'>nameserver 127.0.0.1 
</span><span class='line'>
</span><span class='line'>[root@docker ~]# vi /etc/dnsmasq.conf
</span><span class='line'>[root@docker ~]# cat /etc/dnsmasq.conf
</span><span class='line'>...
</span><span class='line'>resolv-file=/etc/resolv.dnsmasq.conf
</span><span class='line'>...
</span><span class='line'>addn-hosts=/etc/dnsmasq.hosts
</span><span class='line'>
</span><span class='line'>[root@docker ~]# service dnsmasq restart
</span><span class='line'>
</span><span class='line'>[root@docker ~]# dig www.baidu.com
</span><span class='line'>...
</span><span class='line'>;; SERVER: 127.0.0.1#53(127.0.0.1)
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>通过dig可以查看当前的DNS服务器你已经修改为localhost了。然后启动docker容器来搭建环境。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d  --dns 172.17.42.1 --name slaver1 -h slaver1 hadoop
</span><span class='line'>[root@docker ~]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver2 hadoop
</span><span class='line'>[root@docker ~]# docker run -d  --dns 172.17.42.1 --name master -h master hadoop
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
</span><span class='line'>f6e63b311e60        hadoop:latest       /bin/sh -c '/usr/sbi   6 seconds ago       Up 5 seconds        22/tcp              master
</span><span class='line'>454ae2c3e435        hadoop:latest       /bin/sh -c '/usr/sbi   13 seconds ago      Up 12 seconds       22/tcp              slaver2
</span><span class='line'>7698230a03fb        hadoop:latest       /bin/sh -c '/usr/sbi   21 seconds ago      Up 20 seconds       22/tcp              slaver1
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps | grep hadoop | awk '{print $1}' | xargs -I{} docker inspect -f '{{.NetworkSettings.IPAddress}} {{.Config.Hostname}}' {} &gt; /etc/dnsmasq.hosts
</span><span class='line'>[root@docker ~]# service dnsmasq restart
</span><span class='line'>
</span><span class='line'>[root@docker ~]# ssh hadoop@master
</span><span class='line'>hadoop@master's password: 
</span><span class='line'>[hadoop@master ~]$ ping slaver1
</span><span class='line'>PING slaver1 (172.17.0.9) 56(84) bytes of data.
</span><span class='line'>64 bytes from slaver1 (172.17.0.9): icmp_seq=1 ttl=64 time=1.79 ms
</span><span class='line'>...
</span><span class='line'>[hadoop@master ~]$ ping slaver2
</span><span class='line'>PING slaver2 (172.17.0.10) 56(84) bytes of data.
</span><span class='line'>64 bytes from slaver2 (172.17.0.10): icmp_seq=1 ttl=64 time=1.96 ms
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'></span></code></pre></td></tr></table></div></figure>


<p>节点互通后，后面的步骤都类似了，ssh无密钥通信，格式化namenode，启动等等。</p>

<h2>遇到的问题</h2>

<ul>
<li>一开始我把配置文件放在/root目录下，dnsmasq总是不起作用。最后放到/etc目录就可以，不知道啥子问题。</li>
<li>配置dns启动docker容器后，如果不起作用看下<code>/etc/resolv.conf</code>确保仅有nameserver一行数据。</li>
</ul>


<p>DNS可以正常工作的配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-bash-4.1# ping slaver
</span><span class='line'>PING slaver (172.17.0.7) 56(84) bytes of data.
</span><span class='line'>64 bytes from slaver (172.17.0.7): icmp_seq=1 ttl=64 time=0.095 ms
</span><span class='line'>
</span><span class='line'>-bash-4.1# cat /etc/resolv.conf 
</span><span class='line'>nameserver 172.17.42.1</span></code></pre></td></tr></table></div></figure>


<p>DNS配置存在问题的情况：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-bash-4.1# cat /etc/resolv.conf 
</span><span class='line'>nameserver 172.17.42.1
</span><span class='line'>search localdomain</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://top.jobbole.com/7904/">DNS和Docker的小技巧</a></li>
<li><a href="http://www.07net01.com/linux/zuixindnsmasqanzhuangbushuxiangjie_centos6__653221_1381214991.html">dnsmasq安装部署详解-centos6</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/16/build-and-configuration-spark/">编译配置Spark</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-16T16:55:39+08:00" pubdate data-updated="true">Thu 2014-10-16 16:55</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>官网提供的hadoop版本没有2.5的。这里我自己下载源码再进行编译。先下载spark-1.1.0.tgz，解压然后执行命令编译：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive -X -DskipTests clean package</span></code></pre></td></tr></table></div></figure>


<p>建议加上maven参数，不然很可能出现OOM。编译的时间也挺长的，可以先去吃个饭。或者取消一些功能的编译（如hive）。</p>

<p>编译完后，在assembly功能下会生成包括所有spark及其依赖的jar文件。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker scala-2.10]# cd spark-1.1.0/assembly/target/scala-2.10/
</span><span class='line'>[root@docker scala-2.10]# ll -h
</span><span class='line'>total 135M
</span><span class='line'>-rw-r--r--. 1 root root 135M Oct 15 21:18 spark-assembly-1.1.0-hadoop2.5.1.jar</span></code></pre></td></tr></table></div></figure>


<h2>打包</h2>

<p>上面我们已经编译好了spark程序，这里对其进行打包集成到一个压缩包。使用程序自带的make-distribution.sh即可。</p>

<p>为了减少重新编译的巨长的等待时间，修改下脚本<code>make-distribution.sh</code>的maven编译参数，去掉maven的clean阶段操作，修改最终结果如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>
</span><span class='line'>#BUILD_COMMAND="mvn clean package -DskipTests $@"
</span><span class='line'>BUILD_COMMAND="mvn package -DskipTests $@"</span></code></pre></td></tr></table></div></figure>


<p>然后执行命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0]# sh -x make-distribution.sh --tgz  --skip-java-test -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive 
</span><span class='line'>[root@docker spark-1.1.0]# ll -h
</span><span class='line'>total 185M
</span><span class='line'>...
</span><span class='line'>-rw-r--r--. 1 root root 185M Oct 16 00:09 spark-1.1.0-bin-2.5.1.tgz</span></code></pre></td></tr></table></div></figure>


<p>最终会在目录行打包生成tgz的文件。</p>

<h2>本地运行local</h2>

<p>把本机ip主机名写入到hosts，方便以后windows本机查看日志</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# echo 192.168.154.128 docker &gt;&gt; /etc/hosts
</span><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# cat /etc/hosts
</span><span class='line'>...
</span><span class='line'>192.168.154.128 docker</span></code></pre></td></tr></table></div></figure>


<h3>运行helloworld：</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# bin/run-example SparkPi 10
</span><span class='line'>Spark assembly has been built with Hive, including Datanucleus jars on classpath
</span><span class='line'>...
</span><span class='line'>14/10/16 00:22:36 INFO SparkContext: Job finished: reduce at SparkPi.scala:35, took 2.848632007 s
</span><span class='line'>Pi is roughly 3.139344
</span><span class='line'>14/10/16 00:22:36 INFO SparkUI: Stopped Spark web UI at http://docker:4040
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<h3>交互式操作：</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# bin/spark-shell --master local[2]
</span><span class='line'>Welcome to
</span><span class='line'>      ____              __
</span><span class='line'>     / __/__  ___ _____/ /__
</span><span class='line'>    _\ \/ _ \/ _ `/ __/  '_/
</span><span class='line'>   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0
</span><span class='line'>      /_/
</span><span class='line'>
</span><span class='line'>Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_60)
</span><span class='line'>...
</span><span class='line'>14/10/16 00:25:57 INFO SparkUI: Started SparkUI at http://docker:4040
</span><span class='line'>14/10/16 00:25:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>14/10/16 00:25:58 INFO Executor: Using REPL class URI: http://192.168.154.128:39385
</span><span class='line'>14/10/16 00:25:58 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@docker:57417/user/HeartbeatReceiver
</span><span class='line'>14/10/16 00:25:58 INFO SparkILoop: Created spark context..
</span><span class='line'>Spark context available as sc.
</span><span class='line'>
</span><span class='line'>scala&gt; 
</span></code></pre></td></tr></table></div></figure>


<p>说明下环境，我使用windows作为开发环境，使用虚拟机中的linux作为测试环境。同时通过ssh连接的隧道来实现windows无缝的访问虚拟机linux操作系统。</p>

<p>启动交互式访问后，就可以通过浏览器访问4040查看spark程序的状态。</p>

<p><img src="http://file.bmob.cn/M00/1E/4B/wKhkA1Q_3NOALefuAAEimqVy6-s418.png" alt="" /></p>

<p>任务已经启动，接下来就可以进行操作：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val textFile=sc.textFile("README.md")
</span><span class='line'>textFile: org.apache.spark.rdd.RDD[String] = README.md MappedRDD[1] at textFile at &lt;console&gt;:12
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.count()
</span><span class='line'>res0: Long = 141
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.first()
</span><span class='line'>res1: String = # Apache Spark
</span><span class='line'>
</span><span class='line'>scala&gt; val linesWithSpark = textFile.filter(line=&gt;line.contains("Spark"))
</span><span class='line'>linesWithSpark: org.apache.spark.rdd.RDD[String] = FilteredRDD[2] at filter at &lt;console&gt;:14
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.filter(line=&gt;line.contains("Spark")).count()
</span><span class='line'>res2: Long = 21
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)
</span><span class='line'>res3: Int = 15
</span><span class='line'>
</span><span class='line'>scala&gt; import java.lang.Math
</span><span class='line'>import java.lang.Math
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b)=&gt;Math.max(a,b))
</span><span class='line'>res4: Int = 15
</span><span class='line'>
</span><span class='line'>scala&gt; val wordCounts = textFile.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_)
</span><span class='line'>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at &lt;console&gt;:15
</span><span class='line'>
</span><span class='line'>scala&gt; wordCounts.collect()
</span><span class='line'>res5: Array[(String, Int)] = Array((means,1), (under,2), (this,4), (Because,1), (Python,2), (agree,1), (cluster.,1), (its,1), (follows.,1), (general,2), (have,2), (YARN,,3), (pre-built,1), (locally.,1), (locally,2), (changed,1), (MRv1,,1), (several,1), (only,1), (sc.parallelize(1,1), (This,2), (learning,,1), (basic,1), (requests,1), (first,1), (Configuration,1), (MapReduce,2), (CLI,1), (graph,1), (without,1), (documentation,1), ("yarn-client",1), ([params]`.,1), (any,2), (setting,2), (application,1), (prefer,1), (SparkPi,2), (engine,1), (version,3), (file,1), (documentation,,1), (&lt;http://spark.apache.org/&gt;,1), (MASTER,1), (entry,1), (example,3), (are,2), (systems.,1), (params,1), (scala&gt;,1), (provides,1), (refer,1), (MLLib,1), (Interactive,2), (artifact,1), (configure,1), (can,8), (&lt;art...
</span></code></pre></td></tr></table></div></figure>


<p>执行了上面一些操作后，通过网页查看状态变化：</p>

<p><img src="http://file.bmob.cn/M00/1E/4C/wKhkA1Q_3w6AM6njAAF-MCCYh2s170.png" alt="" /></p>

<h2>Spark-standalone集群</h2>

<p>部署集群需要用到多个服务器，这里我使用docker来进行部署。</p>

<p>本来应该早早完成本文的实践，但是在搭建docker-hadoop集群时花费了很多的时间。关于搭建集群dnsmasq处理域名问题参见下一篇文章。
最终实现可以参考：<a href="https://github.com/winse/docker-hadoop/tree/spark-yarn">docker-hadoop</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver1 spark-yarn
</span><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver2 spark-yarn
</span><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name master -h master spark-yarn
</span><span class='line'>
</span><span class='line'>[root@docker docker-hadoop]# docker ps | grep spark | awk '{print $1}' | xargs -I{} docker inspect -f ' ' {} &gt; /etc/dnsmasq.hosts
</span><span class='line'>[root@docker docker-hadoop]# cat /etc/dnsmasq.hosts 
</span><span class='line'>172.17.0.29 master
</span><span class='line'>172.17.0.28 slaver2
</span><span class='line'>172.17.0.27 slaver1
</span><span class='line'>[root@docker docker-hadoop]# service dnsmasq restart
</span><span class='line'>[root@docker docker-hadoop]# ssh hadoop@master
</span><span class='line'>
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id master
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id localhost
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id slaver1
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id slaver2
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ sbin/start-all.sh 
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ /opt/jdk1.7.0_67/bin/jps  -m
</span><span class='line'>266 Jps -m
</span><span class='line'>132 Master --ip master --port 7077 --webui-port 8080
</span></code></pre></td></tr></table></div></figure>


<p>通过网页可以查看集群的状态：</p>

<p><img src="http://file.bmob.cn/M00/1E/F8/wKhkA1RClV2AE0biAAEmpXJlzTc914.png" alt="" /></p>

<p>运行任务连接到master：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-shell --master spark://master:7077
</span><span class='line'>...
</span><span class='line'>14/10/17 11:31:08 INFO BlockManagerMasterActor: Registering block manager slaver2:55473 with 265.4 MB RAM
</span><span class='line'>14/10/17 11:31:09 INFO BlockManagerMasterActor: Registering block manager slaver1:33441 with 265.4 MB RAM
</span><span class='line'>
</span><span class='line'>scala&gt; </span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmG-AO--XAAD84ATrCew955.png" alt="" /></p>

<p>从上图可以看到，程序已经正确连接到spark集群，master为driver，任务节点为slaver1和slaver2。下面运行下程序，然后通过网页查看运行的状态。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val textFile=sc.textFile("README.md")
</span><span class='line'>scala&gt; textFile.count()
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmdmAB3M9AAFIzMb4yk0370.png" alt="" /></p>

<p>系统安装好了，启动spark-standalone集群和hadoop-yarn一样。配置ssh、java，然后启动，配合网页8080/4040可以实时的了解任务的指标。</p>

<h2>yarn集群</h2>

<p>如果你是按照前面的步骤来操作的，需要先把spark-standalone的集群停掉。端口8080和yarn web使用端口冲突，会导致yarn启动失败。</p>

<p>修改spark-env.sh，添加HADOOP_CONF_DIR参数。然后提交任务到yarn上执行就行了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ cat conf/spark-env.sh
</span><span class='line'>#!/usr/bin/env bash
</span><span class='line'>
</span><span class='line'>JAVA_HOME=/opt/jdk1.7.0_67 
</span><span class='line'>
</span><span class='line'>HADOOP_CONF_DIR=/opt/hadoop-2.5.1/etc/hadoop
</span><span class='line'>
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.1.0-hadoop2.5.1.jar  10</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/FD/wKhkA1RCszeAALCPAAK1Nzk6faQ330.png" alt="" /></p>

<p>运行的结果输出在driver的slaver2节点，对应输出型来说不是很直观。spark-yarn提供了另一种方式，driver直接本地运行<em>yarn-client</em>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client lib/spark-examples-1.1.0-hadoop2.5.1.jar  10
</span><span class='line'>...
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8248 ms on slaver1 (1/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 231 ms on slaver1 (2/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 158 ms on slaver1 (3/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 284 ms on slaver1 (4/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 175 ms on slaver1 (5/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 301 ms on slaver1 (6/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 175 ms on slaver1 (7/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 143 ms on slaver1 (8/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 164 ms on slaver1 (9/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO cluster.YarnClientSchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@slaver2:51923/user/Executor#1132577949] with ID 1
</span><span class='line'>14/10/17 13:31:04 INFO util.RackResolver: Resolved slaver2 to /default-rack
</span><span class='line'>14/10/17 13:31:04 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 397 ms on slaver1 (10/10)
</span><span class='line'>14/10/17 13:31:04 INFO cluster.YarnClientClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
</span><span class='line'>14/10/17 13:31:04 INFO scheduler.DAGScheduler: Stage 0 (reduce at SparkPi.scala:35) finished in 26.084 s
</span><span class='line'>14/10/17 13:31:04 INFO spark.SparkContext: Job finished: reduce at SparkPi.scala:35, took 28.31400558 s
</span><span class='line'>Pi is roughly 3.140248</span></code></pre></td></tr></table></div></figure>


<h2>总结</h2>

<p>本文主要是搭建spark的环境搭建，本地运行、以及在docker中搭建spark集群、yarn集群三种方式。本地运行最简单方便，但是没有模拟到集群环境；spark提供了yarn框架上的实现，直接提交任务到yarn即可；spark集群相对比较简单和方便，接下来的远程调试主要通过spark伪分布式集群方式来进行。</p>

<h2>参考</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/building-with-maven.html">Building Spark with Maven</a></li>
<li><a href="http://spark.apache.org/docs/latest/quick-start.html">Quick Start</a></li>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html">Spark Configuration</a></li>
<li><a href="http://www.07net01.com/linux/zuixindnsmasqanzhuangbushuxiangjie_centos6__653221_1381214991.html">DNS</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/12/read-spark1-source-starter/">[读码] Spark1.1.0前篇</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-12T13:12:57+08:00" pubdate data-updated="true">Sun 2014-10-12 13:12</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>看过亚太研究院的spark在线教学视频，说spark1.0的源码仅有3w+的代码，蠢蠢欲动。先具体看下源码的量，估算估算；然后搭建eclipse读码环境。</p>

<h2>计算源码行数</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ git branch -v
</span><span class='line'>* (detached from v1.1.0) 2f9b2bd [maven-release-plugin] prepare release v1.1.0-rc4
</span><span class='line'>  master                 4d8ae70 [behind 1246] Cleanup on Connection and ConnectionManager
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ find . -name "*.scala" | grep 'src/main' | xargs sed  -e 's:\/\*.*\*\/::' -e  '/\/\*/, /\*\//{
</span><span class='line'>/\/\*/{
</span><span class='line'> s:\/\*.*::p
</span><span class='line'>}
</span><span class='line'>/\*\//{
</span><span class='line'> s:.*\*\/::p
</span><span class='line'>}
</span><span class='line'>d
</span><span class='line'>}' | sed -e '/^\s*$/d' -e '/^\s*\/\//d' | grep -v '^import' | grep -v '^package' | wc -l
</span><span class='line'>72967
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^scala^java
</span><span class='line'>1749
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^src/main^core/src/main
</span><span class='line'>877
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^java^scala
</span><span class='line'>38526
</span></code></pre></td></tr></table></div></figure>


<p>全部源码的数量（去掉测试）大概在7W左右，仅计算核心代码core下面的代码量在4W。从量上面来说还是比较乐观的，学习scala然后读spark的源码。</p>

<p>spark1.0.0的核心代码量在3w左右。1.1多了大概1w行！！</p>

<h2>Docker</h2>

<p>查看目录结构的时刻，看到spark1下面竟然有docker，不过看Dockerfile的内容只是简单的安装了scala、把本机的spark映射到docker容器、然后运行spark主从集群。</p>

<h2>导入eclipse</h2>

<p>spark使用主要使用scala编写，首先需要下载<a href="http://scala-ide.org/download/sdk.html">scala-ide</a>直接下载2.10的版本（基于eclipse，很多操作都类似）；然后下载<a href="https://github.com/apache/spark.git">spark的源码</a>检出v1.1.0的；然后使用maven生成eclipse工程文件。</p>

<p>(不推荐)使用<a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-Eclipse">sbt生成工程文件</a>。这种方式会缺少一些依赖的jar，处理比较麻烦，还不清楚到底是少了啥！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd sbt/
</span><span class='line'>$ sed -i 's/^M//g' *
</span><span class='line'>$ cd ..
</span><span class='line'>$ sbt/sbt eclipse -mem 512</span></code></pre></td></tr></table></div></figure>


<p>(推荐)使用MVN编译生成，<a href="http://spark.apache.org/docs/latest/building-with-maven.html">使用Maven生成官网文章</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ git clean -x -fd #清理非仓库代码
</span><span class='line'>
</span><span class='line'>$ echo $SCALA_HOME #指定scala-home
</span><span class='line'>/cygdrive/d/scala
</span><span class='line'>
</span><span class='line'># 这里我直接修改默认值，理论上加 -Phadoop-2.2 选项应该也是可以的
</span><span class='line'>$ vi pom.xml # hadoop.version 2.2.0
</span><span class='line'>$ mvn eclipse:eclipse
</span><span class='line'>
</span><span class='line'>$ find . -name ".classpath" | xargs sed -i -e 's/including="\*\*\/\*.java"//' -e 's/excluding="\*\*\/\*.java"//'
</span><span class='line'>
</span><span class='line'>#也可以把添加特性的操作/添加scala源码包操作批量处理掉</span></code></pre></td></tr></table></div></figure>


<p>然后导入到eclipse，然后再针对性的处理报错：</p>

<ul>
<li>先把每个工程都<strong>添加scala特性</strong></li>
<li>把含有python源码包的去掉（手动删除.classpath中classpathentry即可）</li>
<li>确认下并加上<code>src/test/scala</code>的源码包。</li>
</ul>


<p>注意，进行上面的步骤之前，由于scala源文件比较多，编译的时间会比较长，先把Project->Build Automatically去掉，然后一次性把问题处理掉后再手动build！</p>

<ul>
<li>手动使用<code>existing maven projects</code>导入yarn/stable，然后把<strong>yarn/common以链接的形式引入</strong>，并添加到源码包。</li>
</ul>


<p><img src="http://file.bmob.cn/M00/1C/E7/wKhkA1Q7jQ2AMhweAAOC-l-jcz4872.png" alt="" /></p>

<p>还有一个<strong> value q is not a member of StringContext </strong><a href="http://docs.scala-lang.org/overviews/quasiquotes/intro.html">quasiquotes</a>的错误，有些类需要在2.10添加编译组件才能正常编译，修改scala编译首选项。</p>

<p><img src="http://file.bmob.cn/M00/1D/07/wKhkA1Q76GyAFNYPAAEYJfk_ZGw816.png" alt="" /></p>

<p>添加依赖的编译组件后，整个功能就能正常编译通过了。接下来就能调试看源码了。</p>

<h2>Maven编译spark</h2>

<p>如果使用的hadoop版本在官网没有集成assembly版本，可以使用maven手动构建。至于打包可以查看下一篇文章。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>$ mvn -Pyarn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean package</span></code></pre></td></tr></table></div></figure>


<p><code>yarn</code>的profile能够可执行的jar文件（包括所有依赖的spark）。</p>

<h2>小结</h2>

<p>断断续续的写了两天，字数统计弄了大半天，主要在于多行注释的处理。时间最主要都消耗在sbt、maven构建eclipse项目文件（生成、fixed）上。编译scala量上去后确实非常非常的慢，不管是maven还是eclipse都慢！</p>

<p>下一篇将使用docker搭建spark环境，并使用远程调试连接到helloworld程序。</p>

<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/24800129/scala-maven-builder-doesnt-understand-quasiquotes">Scala maven builder doesn&rsquo;t understand quasiquotes</a></li>
<li><a href="http://docs.scala-lang.org/overviews/macros/paradise.html">Macro Paradise</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/07/thinking/">思考</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-07T19:07:26+08:00" pubdate data-updated="true">Tue 2014-10-07 19:07</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>随着年龄的增大，很多原来不曾想的问题慢慢的都开始环绕在自己四周。开始让自己不得不反思，不得不去改变。</p>

<p>本人是一个性格比较极端，又很内向，所以对自己不关心、无自己原来没有直接联系的东西，很少体现积极主动的一面。时时刻刻展现着保守派的作风。自己又在学习能力方面自我感觉良好，对现状总是很不满，对一样事物的持续坚持的耐久力不足（倒不是不能吃苦、吃不了苦的问题）！</p>

<p>从出生到毕业，一直以来都有亲人朋友让我依靠，有很明确值得挑战和超越的目标（总体水平一般，在我前面的人乌压压一片）。出来工作后一直都很迷失，不知道自己能干啥，可以干啥，师范类专业连教师资格证都没有拿到（不是后悔，自己觉得不应该）！！现在想来其实自己太执拗，像极了不撞南墙死不改的蛮牛！！</p>

<p>年龄增加体力不及，开始思考着应该去锻炼锻炼了，但是一直各种借口无疾而终！觉得身体还行，以后再说。。。
工作资历增加直接辅导指导的大哥不再，开始各种瞎折腾，东一锤西一棒，终究是拣了芝麻丢了西瓜！觉得学习能力强以后都敢都来得及，以后再学呗。。。   <br/>
但是在运动场上，一直坚持运动的同学，打个3、4个小时的羽毛球气不喘一下，这时开始懊悔。
当原来一起协作的同事，开始在领域有所斩获，各种嫉妒羡慕的心里开始作祟。</p>

<p>星期一个个的开始了结束，自己却没有得到该有的锤炼和进度，在大势所趋下，自己却总是那么的慢慢吞吞！阅读一个类的千行源码，竟然断断续续花费了仅2个月！本来年前看tomcat原来的计划最终石沉大海！</p>

<p>总是对自己不够狠；狠下来一次后，总是各种理由，最终不能坚持！</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/2">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>佛爷</h1>
  <p>来之不易, 且等且珍惜. <br>得之我幸; 不得<span style="display:none">-争-复争-且不得</span>, 命也, 乐享天命, 福也. </p>
  <p><a href="https://github.com/winse"><i class="fa fa-github-alt">winse</i></a>&nbsp;&nbsp;<a href="http://weibo.com/winseliu"><i class="fa fa-weibo">winseliu</i></a></p>
</section>
<section>
  <h1><a class='category' href='/blog/categories/recommend/'>Recommend</a></h1>
	<ul role="list">
		
			<li class="post">
				<a href="/blog/2014/09/30/docker-ssh-on-centos/">配置ssh登录docker-centos</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/">在windows开发测试mapreduce几种方式</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/07/27/start-redis/">[读读书]Redis入门指南</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/04/21/hadoop2-windows-startguide/">Windows下部署/配置/调试hadoop2</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/30/git-cheatsheet/">GIT操作记录手册</a>
			</li>
		
			<li class="post">
				<a href="/blog/2014/03/18/jekyll-edit-link-in-web-page/">Jekyll页面添加编辑按钮</a>
			</li>
		
			<li class="post">
				<a href="/blog/2013/09/19/let-shell-command-efficient/">让敲Shell命令高效起来</a>
			</li>
		
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/10/24/github-custom-domain/">为github Pages页面设置自定义域名</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/18/modify-hosts-build-hadoop-cluster-on-docker/">Dnsmasq解决docker集群节点互通问题</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/16/build-and-configuration-spark/">编译配置Spark</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/12/read-spark1-source-starter/">[读码] Spark1.1.0前篇</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/07/thinking/">思考</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/30/docker-ssh-on-centos/">配置ssh登录docker-centos</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/27/docker-start-guide-on-centos/">Docker入门</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/">在windows开发测试mapreduce几种方式</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>

	 
	<ul role="list">
		
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/android/'>android</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/blabla/'>blabla</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/books/'>books</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/debug/'>debug</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/docker/'>docker</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/ganglia/'>ganglia</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/git/'>git</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hadoop/'>hadoop</a> (25) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hbase/'>hbase</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/hive/'>hive</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/java/'>java</a> (9) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/jekyll/'>jekyll</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/recommend/'>recommend</a> (7) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/redis/'>redis</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/scala/'>scala</a> (3) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/shell/'>shell</a> (4) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/spark/'>spark</a> (2) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tez/'>tez</a> (1) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/tools/'>tools</a> (11) 
		</li>
		 
		<li style="float:left; width:120px"> 
			<a class='category' href='/blog/categories/topics/'>topics</a> (2) 
		</li>
		
		
		<li style="clear:both; width: 1px; margin: 0; padding: 0;"></li>
		<li class="category"><a href="/blog/archives">All categories</a> (67)</li>
	</ul>
	
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/winse">@winse</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'winse',
            count: 4,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>Softs, I&#8217;m using</h1>
  <ul>
    <li class="post">
		<a href="http://hadoop.apache.org/releases.html">hadoop-2.2.0</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/HBASE/?selectedTab=com.atlassian.jira.jira-projects-plugin:changelog-panel">hbase-0.98.3</a>
	</li>
	<li class="post">
		<a href="https://hive.apache.org/downloads.html">hive-0.13.1</a>
	</li>
	<li class="post">
		<a href="https://issues.apache.org/jira/browse/TEZ/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel">tez-0.4.0</a>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Winse Liu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253461959'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253461959%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'winseliu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
