<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Winse Blog]]></title>
  <link href="http://winseliu.com/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2016-01-09T12:08:51+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop安装与升级-(4)HA升级]]></title>
    <link href="http://winseliu.com/blog/2016/01/07/hadoop-install-and-upgrade-4-ha-upgrade/"/>
    <updated>2016-01-07T23:04:27+08:00</updated>
    <id>http://winseliu.com/blog/2016/01/07/hadoop-install-and-upgrade-4-ha-upgrade</id>
    <content type="html"><![CDATA[<p>官网的文档[HDFSHighAvailabilityWithQJM.html]和[HdfsRollingUpgrade.html]（Note that rolling upgrade is supported only from Hadoop-2.4.0 onwards.）很详细，但是没有一个整体的案例。这里整理下操作记录下来。</p>

<ol>
<li>关闭所有的namenode，部署新版本的hadoop</li>
<li>启动所有的journalnode，是所有！！升级namenode的同时，也会升级所有journalnode！！</li>
<li>使用-upgrade选项启动一台namenode。启动的这台namenode会直接进入active状态，升级本地的元数据，同时会升级shared edit log（也就是journalnode的数据）</li>
<li>使用-bootstrapStandby启动其他namenode，同步更新。不能使用-upgrade选项！（我也没试，不知道试了是啥效果）</li>
</ol>


<h2>关闭集群，部署新版本的hadoop</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/stop-dfs.sh
</span><span class='line'>16/01/08 09:10:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Stopping namenodes on [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master2: stopping namenode
</span><span class='line'>hadoop-master1: stopping namenode
</span><span class='line'>hadoop-slaver1: stopping datanode
</span><span class='line'>hadoop-slaver2: stopping datanode
</span><span class='line'>hadoop-slaver3: stopping datanode
</span><span class='line'>Stopping journal nodes [hadoop-master1]
</span><span class='line'>hadoop-master1: stopping journalnode
</span><span class='line'>16/01/08 09:10:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Stopping ZK Failover Controllers on NN hosts [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master1: stopping zkfc
</span><span class='line'>hadoop-master2: stopping zkfc
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ 
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ cd ~/hadoop-2.6.3
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ ll
</span><span class='line'>total 52
</span><span class='line'>drwxr-xr-x 2 hadoop hadoop  4096 Dec 18 01:52 bin
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop    32 Jan  8 06:05 etc -&gt; /home/hadoop/hadoop-2.2.0/ha-etc
</span><span class='line'>drwxr-xr-x 2 hadoop hadoop  4096 Dec 18 01:52 include
</span><span class='line'>drwxr-xr-x 3 hadoop hadoop  4096 Dec 18 01:52 lib
</span><span class='line'>drwxr-xr-x 2 hadoop hadoop  4096 Dec 18 01:52 libexec
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop 15429 Dec 18 01:52 LICENSE.txt
</span><span class='line'>drwxrwxr-x 2 hadoop hadoop  4096 Jan  8 03:37 logs
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop   101 Dec 18 01:52 NOTICE.txt
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop  1366 Dec 18 01:52 README.txt
</span><span class='line'>drwxr-xr-x 2 hadoop hadoop  4096 Dec 18 01:52 sbin
</span><span class='line'>drwxr-xr-x 3 hadoop hadoop  4096 Jan  7 08:00 share
</span><span class='line'>
</span><span class='line'>#// 同步
</span><span class='line'>[hadoop@hadoop-master1 ~]$ for h in hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do rsync -vaz --delete --exclude=logs ~/hadoop-2.6.3 $h:~/ ; done</span></code></pre></td></tr></table></div></figure>


<h2>启动所有Journalnode</h2>

<p>2.6和2.2用的是一份配置！etc通过软链接到2.2的ha-etc配置。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ sbin/hadoop-daemons.sh --hostnames "hadoop-master1" --script /home/hadoop/hadoop-2.2.0/bin/hdfs start journalnode
</span><span class='line'>hadoop-master1: starting journalnode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-journalnode-hadoop-master1.out
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ jps
</span><span class='line'>31047 JournalNode
</span><span class='line'>244 QuorumPeerMain
</span><span class='line'>31097 Jps</span></code></pre></td></tr></table></div></figure>


<h2>升级一台namenode</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ bin/hdfs namenode -upgrade
</span><span class='line'>...
</span><span class='line'>16/01/08 09:13:54 INFO namenode.NameNode: createNameNode [-upgrade]
</span><span class='line'>...
</span><span class='line'>16/01/08 09:13:57 INFO namenode.FSImage: Starting upgrade of local storage directories.
</span><span class='line'>   old LV = -47; old CTime = 0.
</span><span class='line'>   new LV = -60; new CTime = 1452244437060
</span><span class='line'>16/01/08 09:13:57 INFO namenode.NNUpgradeUtil: Starting upgrade of storage directory /data/tmp/dfs/name
</span><span class='line'>16/01/08 09:13:57 INFO namenode.FSImageTransactionalStorageInspector: No version file in /data/tmp/dfs/name
</span><span class='line'>16/01/08 09:13:57 INFO namenode.NNUpgradeUtil: Performing upgrade of storage directory /data/tmp/dfs/name
</span><span class='line'>16/01/08 09:13:57 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>官网文档上说，除了升级了namenode的本地元数据外，sharededitlog也被升级了的。</p>

<p>查看journalnode的日志，确实journalnode也升级了：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ less logs/hadoop-hadoop-journalnode-hadoop-master1.log 
</span><span class='line'>...
</span><span class='line'>2016-01-08 09:13:57,070 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Starting upgrade of edits directory /data/journal/zfcluster
</span><span class='line'>2016-01-08 09:13:57,072 INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Starting upgrade of storage directory /data/journal/zfcluster
</span><span class='line'>2016-01-08 09:13:57,185 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Starting upgrade of edits directory: .
</span><span class='line'>   old LV = -47; old CTime = 0.
</span><span class='line'>   new LV = -60; new CTime = 1452244437060
</span><span class='line'>2016-01-08 09:13:57,185 INFO org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil: Performing upgrade of storage directory /data/journal/zfcluster
</span><span class='line'>2016-01-08 09:13:57,222 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastWriterEpoch from 2 to 3 for client /172.17.0.1
</span><span class='line'>2016-01-08 09:16:57,731 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from 3 to 4 for client /172.17.0.1
</span><span class='line'>2016-01-08 09:16:57,735 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage FileJournalManager(root=/data/journal/zfcluster)
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>升级的namenode是前台运行的，不要关闭这个进程。接下来把另一台namenode同步一下。</p>

<h2>同步另一台namenode</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.3]$ bin/hdfs namenode -bootstrapStandby
</span><span class='line'>...
</span><span class='line'>=====================================================
</span><span class='line'>About to bootstrap Standby ID nn2 from:
</span><span class='line'>           Nameservice ID: zfcluster
</span><span class='line'>        Other Namenode ID: nn1
</span><span class='line'>  Other NN's HTTP address: http://hadoop-master1:50070
</span><span class='line'>  Other NN's IPC  address: hadoop-master1/172.17.0.1:8020
</span><span class='line'>             Namespace ID: 639021326
</span><span class='line'>            Block pool ID: BP-1695500896-172.17.0.1-1452152050513
</span><span class='line'>               Cluster ID: CID-7d5c31d8-5cd4-46c8-8e04-49151578e5bb
</span><span class='line'>           Layout version: -60
</span><span class='line'>       isUpgradeFinalized: false
</span><span class='line'>=====================================================
</span><span class='line'>16/01/08 09:15:19 INFO ha.BootstrapStandby: The active NameNode is in Upgrade. Prepare the upgrade for the standby NameNode as well.
</span><span class='line'>16/01/08 09:15:19 INFO common.Storage: Lock on /data/tmp/dfs/name/in_use.lock acquired by nodename 5008@hadoop-master2
</span><span class='line'>16/01/08 09:15:21 INFO namenode.TransferFsImage: Opening connection to http://hadoop-master1:50070/imagetransfer?getimage=1&txid=1126&storageInfo=-60:639021326:1452244437060:CID-7d5c31d8-5cd4-46c8-8e04-49151578e5bb
</span><span class='line'>16/01/08 09:15:21 INFO namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
</span><span class='line'>16/01/08 09:15:21 INFO namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
</span><span class='line'>16/01/08 09:15:21 INFO namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001126 size 977 bytes.
</span><span class='line'>16/01/08 09:15:21 INFO namenode.NNUpgradeUtil: Performing upgrade of storage directory /data/tmp/dfs/name
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<h2>重新启动集群</h2>

<p>ctrl+c关闭hadoop-master1 upgrade的namenode。启动整个集群。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ sbin/start-dfs.sh
</span><span class='line'>16/01/08 09:16:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Starting namenodes on [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master1: starting namenode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-namenode-hadoop-master1.out
</span><span class='line'>hadoop-master2: starting namenode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-namenode-hadoop-master2.out
</span><span class='line'>hadoop-slaver3: starting datanode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-datanode-hadoop-slaver3.out
</span><span class='line'>hadoop-slaver2: starting datanode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-datanode-hadoop-slaver2.out
</span><span class='line'>hadoop-slaver1: starting datanode, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-datanode-hadoop-slaver1.out
</span><span class='line'>Starting journal nodes [hadoop-master1]
</span><span class='line'>hadoop-master1: journalnode running as process 31047. Stop it first.
</span><span class='line'>16/01/08 09:16:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Starting ZK Failover Controllers on NN hosts [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master2: starting zkfc, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-zkfc-hadoop-master2.out
</span><span class='line'>hadoop-master1: starting zkfc, logging to /home/hadoop/hadoop-2.6.3/logs/hadoop-hadoop-zkfc-hadoop-master1.out
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ jps
</span><span class='line'>31047 JournalNode
</span><span class='line'>244 QuorumPeerMain
</span><span class='line'>31596 DFSZKFailoverController
</span><span class='line'>31655 Jps
</span><span class='line'>31294 NameNode
</span></code></pre></td></tr></table></div></figure>


<h2>后记：Journalnode重置</h2>

<p>在HA和non-HA环境来回的切换，最后启动HA时master起不来，执行bootstrapStandby也不行。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2016-01-08 06:15:36,746 WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: Unable to determine input streams from QJM to [172.17.0.1:8485]. Skipping.
</span><span class='line'>org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 1/1. 1 exceptions thrown:
</span><span class='line'>172.17.0.1:8485: Asked for firstTxId 1022 which is in the middle of file /data/journal/zfcluster/current/edits_0000000000000001021-0000000000000001022
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FileJournalManager.getRemoteEditLogs(FileJournalManager.java:198)
</span><span class='line'>        at org.apache.hadoop.hdfs.qjournal.server.Journal.getEditLogManifest(Journal.java:640)
</span><span class='line'>        at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.getEditLogManifest(JournalNodeRpcServer.java:181)
</span><span class='line'>        at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.getEditLogManifest(QJournalProtocolServerSideTranslatorPB.java:203)
</span><span class='line'>        at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:17453)
</span><span class='line'>        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>关闭集群，启动journalnode，跳转到没有问题的namenode机器，执行initializeSharedEdits命令。然后在有问题的namenode上重新初始化！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start journalnode
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.2.0]$ bin/hdfs namenode -initializeSharedEdits
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start namenode
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ bin/hdfs namenode -bootstrapStandby
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/start-dfs.sh</span></code></pre></td></tr></table></div></figure>


<p>后话（谨慎，没有试验过，猜想而已）： 其实上面HA升级的步骤，如果upgrade时没用启动journalnode，导致了问题的话，把journalnode重置应该也是可以的。</p>

<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html#HDFS_UpgradeFinalizationRollback_with_HA_Enabled">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html#HDFS_UpgradeFinalizationRollback_with_HA_Enabled</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Upgrade_and_Rollback">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Upgrade_and_Rollback</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop安装与升级-(3)HA配置]]></title>
    <link href="http://winseliu.com/blog/2016/01/07/hadoop-install-and-upgrade-3-ha/"/>
    <updated>2016-01-07T23:04:27+08:00</updated>
    <id>http://winseliu.com/blog/2016/01/07/hadoop-install-and-upgrade-3-ha</id>
    <content type="html"><![CDATA[<p>官网的文档[HDFSHighAvailabilityWithQJM.html]很详细，但是没有一个整体的案例。这里整理下操作记录下来。</p>

<h2>配置</h2>

<p>hadoop-master1和hadoop-master2之间无密钥登录（failover要用到）：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 hadoop-2.2.0]$ ssh-keygen
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.2.0]$ ssh-copy-id hadoop-master2
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.2.0]$ ssh-copy-id hadoop-master1</span></code></pre></td></tr></table></div></figure>


<p>配置文件修改：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ vi etc/hadoop/core-site.xml 
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;fs.defaultFS&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hdfs://zfcluster&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master1&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span><span class='line'>&lt;value&gt;/data/tmp&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ vi etc/hadoop/hdfs-site.xml 
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.replication&lt;/name&gt;
</span><span class='line'>&lt;value&gt;1&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
</span><span class='line'>&lt;value&gt; &lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.nameservices&lt;/name&gt;
</span><span class='line'>&lt;value&gt;zfcluster&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.ha.namenodes.zfcluster&lt;/name&gt;
</span><span class='line'>&lt;value&gt;nn1,nn2&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.namenode.rpc-address.zfcluster.nn1&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master1:8020&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.namenode.rpc-address.zfcluster.nn2&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master2:8020&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.namenode.http-address.zfcluster.nn1&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master1:50070&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.namenode.http-address.zfcluster.nn2&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master2:50070&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;
</span><span class='line'>&lt;value&gt;qjournal://hadoop-master1:8485/zfcluster&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.client.failover.proxy.provider.zfcluster&lt;/name&gt;
</span><span class='line'>&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;
</span><span class='line'>&lt;value&gt;/data/journal&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;
</span><span class='line'>&lt;value&gt;sshfence&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;
</span><span class='line'>&lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;
</span><span class='line'>&lt;value&gt;true&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span></code></pre></td></tr></table></div></figure>


<h2>启动</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ cd ..
</span><span class='line'>[hadoop@hadoop-master1 ~]$ for h in hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do rsync -vaz --delete --exclude=logs hadoop-2.2.0 $h:~/ ; done
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ cd hadoop-2.2.0/
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start journalnode
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start namenode
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.2.0]$ bin/hdfs namenode -bootstrapStandby
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ bin/hdfs namenode -initializeSharedEdits
</span><span class='line'>
</span><span class='line'>#// 此时可以启动datanode，通过50070端口看namenode的状态
</span><span class='line'>
</span><span class='line'>#// Automatic failover，zkfc和namenode没有启动顺序的问题！
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ bin/hdfs zkfc -formatZK
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start zkfc
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start zkfc
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ bin/hdfs haadmin -failover nn1 nn2
</span><span class='line'>
</span><span class='line'>#// 测试failover，把一个active的namenode直接kill掉，看看另一个是否变成active！
</span><span class='line'>
</span><span class='line'># 重启
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/stop-dfs.sh
</span><span class='line'>16/01/07 10:57:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Stopping namenodes on [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master1: stopping namenode
</span><span class='line'>hadoop-master2: stopping namenode
</span><span class='line'>hadoop-slaver1: stopping datanode
</span><span class='line'>hadoop-slaver2: stopping datanode
</span><span class='line'>hadoop-slaver3: stopping datanode
</span><span class='line'>Stopping journal nodes [hadoop-master1]
</span><span class='line'>hadoop-master1: stopping journalnode
</span><span class='line'>16/01/07 10:58:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Stopping ZK Failover Controllers on NN hosts [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master2: no zkfc to stop
</span><span class='line'>hadoop-master1: no zkfc to stop
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/start-dfs.sh
</span><span class='line'>16/01/07 10:59:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Starting namenodes on [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master2: starting namenode, logging to /home/hadoop/hadoop-2.2.0/logs/hadoop-hadoop-namenode-hadoop-master2.out
</span><span class='line'>hadoop-master1: starting namenode, logging to /home/hadoop/hadoop-2.2.0/logs/hadoop-hadoop-namenode-hadoop-master1.out
</span><span class='line'>hadoop-slaver1: starting datanode, logging to /home/hadoop/hadoop-2.2.0/logs/hadoop-hadoop-datanode-hadoop-slaver1.out
</span><span class='line'>hadoop-slaver3: starting datanode, logging to /home/hadoop/hadoop-2.2.0/logs/hadoop-hadoop-datanode-hadoop-slaver3.out
</span><span class='line'>hadoop-slaver2: starting datanode, logging to /home/hadoop/hadoop-2.2.0/logs/hadoop-hadoop-datanode-hadoop-slaver2.out
</span><span class='line'>Starting journal nodes [hadoop-master1]
</span><span class='line'>hadoop-master1: starting journalnode, logging to /home/hadoop/hadoop-2.2.0/logs/hadoop-hadoop-journalnode-hadoop-master1.out
</span><span class='line'>16/01/07 10:59:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Starting ZK Failover Controllers on NN hosts [hadoop-master1 hadoop-master2]
</span><span class='line'>hadoop-master2: starting zkfc, logging to /home/hadoop/hadoop-2.2.0/logs/hadoop-hadoop-zkfc-hadoop-master2.out
</span><span class='line'>hadoop-master1: starting zkfc, logging to /home/hadoop/hadoop-2.2.0/logs/hadoop-hadoop-zkfc-hadoop-master1.out
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ jps
</span><span class='line'>15241 DFSZKFailoverController
</span><span class='line'>14882 NameNode
</span><span class='line'>244 QuorumPeerMain
</span><span class='line'>18715 Jps
</span><span class='line'>15076 JournalNode</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html</a></li>
<li><a href="http://www.xlgps.com/article/40993.html">http://www.xlgps.com/article/40993.html</a></li>
<li><a href="http://hbase.apache.org/book.html#basic.prerequisites">http://hbase.apache.org/book.html#basic.prerequisites</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop安装与升级-(2)2.2升级到2.6]]></title>
    <link href="http://winseliu.com/blog/2016/01/07/hadoop-install-and-upgrade-2-hadoop-upgrade/"/>
    <updated>2016-01-07T22:04:27+08:00</updated>
    <id>http://winseliu.com/blog/2016/01/07/hadoop-install-and-upgrade-2-hadoop-upgrade</id>
    <content type="html"><![CDATA[<p>升级的命令很简单，但是不要瞎整！升级就一个命令就搞定了！</p>

<h2>部署2.6.3</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 ~]$ tar zxvf hadoop-2.6.3.tar.gz 
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ cd hadoop-2.6.3/share/
</span><span class='line'>[hadoop@hadoop-master1 share]$ rm -rf doc/
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ rm -rf lib/native/*
</span><span class='line'>
</span><span class='line'>#// 拷贝四个配置文件到hadoop-2.6.3
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ cd etc/hadoop/
</span><span class='line'>[hadoop@hadoop-master1 hadoop]$ cp -f ~/hadoop-2.2.0/etc/hadoop/*-site.xml ./
</span><span class='line'>[hadoop@hadoop-master1 hadoop]$ cp -f ~/hadoop-2.2.0/etc/hadoop/slaves ./
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop]$ cd 
</span><span class='line'>[hadoop@hadoop-master1 ~]$ for h in hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do rsync -vaz --delete --exclude=logs hadoop-2.6.3 $h:~/ ; done
</span></code></pre></td></tr></table></div></figure>


<h2>升级（最佳方式）</h2>

<p>直接使用upgrade选项启动dfs即可。（secondarynamenode不要单独操作来升级，反正就是执行upgrade启动dfs就好了）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ sbin/start-dfs.sh -upgrade
</span><span class='line'>
</span><span class='line'>// 2.2和2.6都没有这个命令
</span><span class='line'>// hadoop dfsadmin -upgradeProgress status
</span><span class='line'>hadoop dfsadmin -finalizeUpgrade
</span></code></pre></td></tr></table></div></figure>


<p>参考[Hadoop: The Definitive Guide/Chapter 10. Administering Hadoop/Maintenance/Upgrades]</p>

<h2>瞎整1</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 先停集群
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/stop-dfs.sh
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ sbin/start-dfs.sh
</span></code></pre></td></tr></table></div></figure>


<p>直接在原来的2.2基础上启动，datanode启动没问题，但是namenode报错：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ less logs/hadoop-hadoop-namenode-hadoop-master1.log 
</span><span class='line'>...
</span><span class='line'>2016-01-07 08:05:23,582 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
</span><span class='line'>java.io.IOException: 
</span><span class='line'>File system image contains an old layout version -47.
</span><span class='line'>An upgrade to version -60 is required.
</span><span class='line'>Please restart NameNode with the "-rollingUpgrade started" option if a rolling upgrade is already started; or restart NameNode with the "-upgrade" option to start a new upgrade.
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:232)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1022)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:741)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:538)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:597)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:764)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:748)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1441)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1507)
</span><span class='line'>2016-01-07 08:05:23,583 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
</span><span class='line'>2016-01-07 08:05:23,585 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
</span><span class='line'>/************************************************************
</span><span class='line'>SHUTDOWN_MSG: Shutting down NameNode at hadoop-master1/172.17.0.1
</span><span class='line'>************************************************************/</span></code></pre></td></tr></table></div></figure>


<p>重新启动，使用upgrade选项启动：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ sbin/stop-dfs.sh
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.3]$ sbin/start-dfs.sh -upgrade</span></code></pre></td></tr></table></div></figure>


<p>或者还原到2.2：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># **所有**slaver节点的VERSION改回47
</span><span class='line'>[hadoop@hadoop-slaver3 ~]$ vi /data/tmp/dfs/data/current/VERSION 
</span><span class='line'>...
</span><span class='line'>layoutVersion=-47
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/stop-dfs.sh
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/start-dfs.sh -rollback
</span></code></pre></td></tr></table></div></figure>


<h2>原理</h2>

<p>升级的时刻，首先备份原来的数据到previous目录下，升级后的放置到current目录下。namenode这样没啥大问题，但是datanode也是这样结构current和previous，那相当有问题，那数据量不是翻倍了？</p>

<p>查看数据后，发现一个名字的文件current和previous里面使用的是一个inode。也就是说用的是硬链接，数据只有一份！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ bin/hadoop fs -put *.txt /
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver3 ~]$ cd /data/tmp/dfs/data/
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver3 BP-1695500896-172.17.0.1-1452152050513]$ test current/finalized/subdir0/subdir0/blk_1073741825 -ef previous/finalized/blk_1073741825
</span><span class='line'>[hadoop@hadoop-slaver3 BP-1695500896-172.17.0.1-1452152050513]$ echo $?
</span><span class='line'>0
</span><span class='line'>[hadoop@hadoop-slaver3 BP-1695500896-172.17.0.1-1452152050513]$ ls -i current/finalized/subdir0/subdir0/blk_1073741825 
</span><span class='line'>142510 current/finalized/subdir0/subdir0/blk_1073741825
</span><span class='line'>[hadoop@hadoop-slaver3 BP-1695500896-172.17.0.1-1452152050513]$ ls -i previous/finalized/blk_1073741825
</span><span class='line'>142510 previous/finalized/blk_1073741825
</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop安装与升级-Docker中安装(1)]]></title>
    <link href="http://winseliu.com/blog/2016/01/07/hadoop-install-and-upgrade-1-install-in-docker/"/>
    <updated>2016-01-07T21:04:27+08:00</updated>
    <id>http://winseliu.com/blog/2016/01/07/hadoop-install-and-upgrade-1-install-in-docker</id>
    <content type="html"><![CDATA[<h2>集群机器准备</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# docker -v
</span><span class='line'>Docker version 1.6.2, build 7c8fca2/1.6.2
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>centos              centos6             62068de82c82        4 months ago        250.7 MB
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# docker run -d --name hadoop-master1 -h hadoop-master1 centos:centos6 /usr/sbin/sshd -D
</span><span class='line'>c975b0e41429a3c214e86552f2a9f599ba8ee7487e8fbdc25fd59d29adacca4f
</span><span class='line'>[root@cu2 ~]# docker run -d --name hadoop-master2 -h hadoop-master2 centos:centos6 /usr/sbin/sshd -D
</span><span class='line'>fac1d2ee4a05ab8457f4bd6756622ac8236f64423544150d355f9e3091764d8f
</span><span class='line'>[root@cu2 ~]# docker run -d --name hadoop-slaver1 -h hadoop-slaver1 centos:centos6 /usr/sbin/sshd -D
</span><span class='line'>cc8734f2a0963a030b994f69be697308a13e511557eaefc7d4aca7e300950ded
</span><span class='line'>[root@cu2 ~]# docker run -d --name hadoop-slaver2 -h hadoop-slaver2 centos:centos6 /usr/sbin/sshd -D
</span><span class='line'>7e4b5410a7cb8585436775f15609708b309a5b83930da74d6571533251c26355
</span><span class='line'>[root@cu2 ~]# docker run -d --name hadoop-slaver3 -h hadoop-slaver3 centos:centos6 /usr/sbin/sshd -D
</span><span class='line'>26018b256403d956b4272b6bda09a58d1fc6938591d18f9892ba72782c41880b
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# docker ps -a
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND               CREATED              STATUS              PORTS               NAMES
</span><span class='line'>26018b256403        centos:centos6      "/usr/sbin/sshd -D"   About a minute ago   Up About a minute                       hadoop-slaver3      
</span><span class='line'>7e4b5410a7cb        centos:centos6      "/usr/sbin/sshd -D"   About a minute ago   Up About a minute                       hadoop-slaver2      
</span><span class='line'>cc8734f2a096        centos:centos6      "/usr/sbin/sshd -D"   About a minute ago   Up About a minute                       hadoop-slaver1      
</span><span class='line'>fac1d2ee4a05        centos:centos6      "/usr/sbin/sshd -D"   About a minute ago   Up About a minute                       hadoop-master2      
</span><span class='line'>c975b0e41429        centos:centos6      "/usr/sbin/sshd -D"   8 minutes ago        Up 8 minutes                            hadoop-master1      
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# docker ps | grep hadoop | awk '{print $1}' | xargs -I{} docker inspect -f ' ' {}
</span><span class='line'>172.17.0.6 hadoop-slaver3
</span><span class='line'>172.17.0.5 hadoop-slaver2
</span><span class='line'>172.17.0.4 hadoop-slaver1
</span><span class='line'>172.17.0.3 hadoop-master2
</span><span class='line'>172.17.0.2 hadoop-master1</span></code></pre></td></tr></table></div></figure>


<p>重启docker后，可以直接通过名称启动即可：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# service docker start
</span><span class='line'>Starting docker:                                           [  OK  ]
</span><span class='line'>[root@cu2 ~]# docker start hadoop-master1 hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3
</span><span class='line'>hadoop-master1
</span><span class='line'>hadoop-master2
</span><span class='line'>hadoop-slaver1
</span><span class='line'>hadoop-slaver2
</span><span class='line'>hadoop-slaver3</span></code></pre></td></tr></table></div></figure>


<p>重启后，hosts文件会被重置！最好就是测试好之前不要重启docker！</p>

<h2>机器配置</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# ssh root@172.17.0.2
</span><span class='line'>root@172.17.0.2's password: 
</span><span class='line'>Last login: Thu Jan  7 06:17:11 2016 from 172.17.42.1
</span><span class='line'>[root@hadoop-master1 ~]# 
</span><span class='line'>[root@hadoop-master1 ~]# vi /etc/hosts
</span><span class='line'>127.0.0.1       localhost
</span><span class='line'>::1     localhost ip6-localhost ip6-loopback
</span><span class='line'>fe00::0 ip6-localnet
</span><span class='line'>ff00::0 ip6-mcastprefix
</span><span class='line'>ff02::1 ip6-allnodes
</span><span class='line'>ff02::2 ip6-allrouters
</span><span class='line'>
</span><span class='line'>172.17.0.6 hadoop-slaver3
</span><span class='line'>172.17.0.5 hadoop-slaver2
</span><span class='line'>172.17.0.4 hadoop-slaver1
</span><span class='line'>172.17.0.3 hadoop-master2
</span><span class='line'>172.17.0.2 hadoop-master1
</span><span class='line'>
</span><span class='line'>[root@hadoop-master1 ~]# ssh-keygen
</span><span class='line'>[root@hadoop-master1 ~]# 
</span><span class='line'>[root@hadoop-master1 ~]# ssh-copy-id hadoop-master1
</span><span class='line'>[root@hadoop-master1 ~]# ssh-copy-id hadoop-master2
</span><span class='line'>[root@hadoop-master1 ~]# ssh-copy-id hadoop-slaver1
</span><span class='line'>[root@hadoop-master1 ~]# ssh-copy-id hadoop-slaver2
</span><span class='line'>[root@hadoop-master1 ~]# ssh-copy-id hadoop-slaver3
</span><span class='line'>
</span><span class='line'># 拷贝hosts
</span><span class='line'>[root@hadoop-master1 ~]# for h in hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do scp /etc/hosts $h:/etc/ ; done
</span><span class='line'>
</span><span class='line'># 安装需要的软件
</span><span class='line'>[root@hadoop-master1 ~]# for h in hadoop-master1 hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do ssh $h "yum install man rsync curl wget tar" ; done
</span><span class='line'>
</span><span class='line'># 创建用户
</span><span class='line'>[root@hadoop-master1 ~]# for h in hadoop-master1 hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do ssh $h useradd hadoop ; done
</span><span class='line'>
</span><span class='line'>#// 把要设置的密码拷贝一下，接下来直接右键（CRT）粘贴弄5次就可以了。如果是几十几百台机器可以使用expect来实现
</span><span class='line'>[root@hadoop-master1 ~]# for h in hadoop-master1 hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do ssh $h passwd hadoop ; done
</span><span class='line'>New password: hadoop
</span><span class='line'>BAD PASSWORD: it is based on a dictionary word
</span><span class='line'>BAD PASSWORD: is too simple
</span><span class='line'>Retype new password: hadoop
</span><span class='line'>Changing password for user hadoop.
</span><span class='line'>passwd: all authentication tokens updated successfully.
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'># 建立数据目录，赋权给hadoop用户
</span><span class='line'>[root@hadoop-master1 ~]# for h in hadoop-master1 hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do ssh $h "mkdir /data; chown hadoop:hadoop /data" ; done
</span><span class='line'>
</span><span class='line'>[root@hadoop-master1 ~]# su - hadoop
</span><span class='line'>[hadoop@hadoop-master1 ~]$ ssh-keygen 
</span><span class='line'>[hadoop@hadoop-master1 ~]$ ssh-copy-id hadoop-master1
</span><span class='line'>[hadoop@hadoop-master1 ~]$ ssh-copy-id hadoop-master2
</span><span class='line'>[hadoop@hadoop-master1 ~]$ ssh-copy-id hadoop-slaver1
</span><span class='line'>[hadoop@hadoop-master1 ~]$ ssh-copy-id hadoop-slaver2
</span><span class='line'>[hadoop@hadoop-master1 ~]$ ssh-copy-id hadoop-slaver3
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ ll
</span><span class='line'>total 139036
</span><span class='line'>drwxr-xr-x 9 hadoop hadoop      4096 Oct  7  2013 hadoop-2.2.0
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop 142362384 Jan  7 07:14 jdk-7u60-linux-x64.gz
</span><span class='line'>drwxr-xr-x 8 hadoop hadoop      4096 Jan  7 07:11 zookeeper-3.4.6
</span><span class='line'>[hadoop@hadoop-master1 ~]$ tar zxvf jdk-7u60-linux-x64.gz 
</span><span class='line'>[hadoop@hadoop-master1 ~]$ tar zxvf hadoop-2.2.0.tar.gz 
</span><span class='line'>[hadoop@hadoop-master1 ~]$ tar zxvf zookeeper-3.4.6.tar.gz 
</span><span class='line'>
</span><span class='line'># 清理生产上无用的数据
</span><span class='line'>[hadoop@hadoop-master1 ~]$ rm hadoop-2.2.0.tar.gz zookeeper-3.4.6.tar.gz jdk-7u60-linux-x64.gz 
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ cd zookeeper-3.4.6/
</span><span class='line'>[hadoop@hadoop-master1 zookeeper-3.4.6]$ rm -rf docs/ src/
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 zookeeper-3.4.6]$ cd ../hadoop-2.2.0/
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ cd share/
</span><span class='line'>[hadoop@hadoop-master1 share]$ rm -rf doc/</span></code></pre></td></tr></table></div></figure>


<h2>程序配置与启动</h2>

<ul>
<li>java</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 ~]$ cd
</span><span class='line'>[hadoop@hadoop-master1 ~]$ vi .bashrc 
</span><span class='line'>...
</span><span class='line'>JAVA_HOME=~/jdk1.7.0_60
</span><span class='line'>PATH=$JAVA_HOME/bin:$PATH
</span><span class='line'>
</span><span class='line'>export JAVA_HOME PATH</span></code></pre></td></tr></table></div></figure>


<p>退出shell再登录，或者source .bashrc！</p>

<ul>
<li>zookeeper</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 ~]$ cd zookeeper-3.4.6/conf
</span><span class='line'>[hadoop@hadoop-master1 conf]$ cp zoo_sample.cfg zoo.cfg
</span><span class='line'>[hadoop@hadoop-master1 conf]$ vi zoo.cfg 
</span><span class='line'>...
</span><span class='line'>dataDir=/data/zookeeper
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ mkdir /data/zookeeper
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ cd ~/zookeeper-3.4.6/
</span><span class='line'>[hadoop@hadoop-master1 zookeeper-3.4.6]$ bin/zkServer.sh start
</span><span class='line'>JMX enabled by default
</span><span class='line'>Using config: /home/hadoop/zookeeper-3.4.6/bin/../conf/zoo.cfg
</span><span class='line'>Starting zookeeper ... STARTED
</span><span class='line'>[hadoop@hadoop-master1 zookeeper-3.4.6]$ 
</span><span class='line'>[hadoop@hadoop-master1 zookeeper-3.4.6]$ jps
</span><span class='line'>244 QuorumPeerMain
</span><span class='line'>265 Jps
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 zookeeper-3.4.6]$ less zookeeper.out </span></code></pre></td></tr></table></div></figure>


<ul>
<li>hadoop</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 ~]$ cd ~/hadoop-2.2.0/etc/hadoop/
</span><span class='line'>[hadoop@hadoop-master1 hadoop]$ rm *.cmd
</span><span class='line'>[hadoop@hadoop-master1 hadoop]$ vi hadoop-env.sh 
</span><span class='line'># 修改java_home和pid
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop]$ vi core-site.xml 
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;fs.defaultFS&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hdfs://hadoop-master1:9000&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span><span class='line'>&lt;value&gt;/data/tmp&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop]$ vi hdfs-site.xml 
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.replication&lt;/name&gt;
</span><span class='line'>&lt;value&gt;1&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
</span><span class='line'>&lt;value&gt; &lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop]$ vi mapred-site.xml
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
</span><span class='line'>&lt;value&gt;yarn&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master1:10020&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master1:19888&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop]$ vi yarn-site.xml 
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
</span><span class='line'>&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
</span><span class='line'>&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master1:8032&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master1:8030&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master1:8031&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master1:8033&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
</span><span class='line'>&lt;value&gt;hadoop-master1:8080&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span></code></pre></td></tr></table></div></figure>


<p>启动Hadoop</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ bin/hadoop version
</span><span class='line'>Hadoop 2.2.0
</span><span class='line'>Subversion https://svn.apache.org/repos/asf/hadoop/common -r 1529768
</span><span class='line'>Compiled by hortonmu on 2013-10-07T06:28Z
</span><span class='line'>Compiled with protoc 2.5.0
</span><span class='line'>From source with checksum 79e53ce7994d1628b240f09af91e1af4
</span><span class='line'>This command was run using /home/hadoop/hadoop-2.2.0/share/hadoop/common/hadoop-common-2.2.0.jar
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ bin/hadoop namenode -format
</span><span class='line'>
</span><span class='line'># 默认自带的libhadoop有点问题，start-dfs.sh通过hdfs getconf -namenodes输出信息导致执行错误
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ rm lib/native/libh*
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ cd 
</span><span class='line'>[hadoop@hadoop-master1 ~]$ for h in hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do scp -r jdk1.7.0_60 $h:~/ ; done
</span><span class='line'>[hadoop@hadoop-master1 ~]$ for h in hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do scp -r hadoop-2.2.0 $h:~/ ; done
</span><span class='line'>[hadoop@hadoop-master1 ~]$ for h in hadoop-master2 hadoop-slaver1 hadoop-slaver2 hadoop-slaver3 ; do scp -r .bashrc $h:~/ ; done
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ cd hadoop-2.2.0/
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/start-dfs.sh
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/stop-dfs.sh
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ sbin/start-dfs.sh
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.2.0]$ jps
</span><span class='line'>244 QuorumPeerMain
</span><span class='line'>3995 NameNode
</span><span class='line'>4187 Jps</span></code></pre></td></tr></table></div></figure>


<p>通过CRT的Port Forwarding的dynamic socket5，浏览器配置socket5代理就可以通过50070端口查看hadoop hdfs集群的状态了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgresql入门]]></title>
    <link href="http://winseliu.com/blog/2015/12/13/postgresql-start-guide/"/>
    <updated>2015-12-13T23:19:55+08:00</updated>
    <id>http://winseliu.com/blog/2015/12/13/postgresql-start-guide</id>
    <content type="html"><![CDATA[<p>简单介绍下软件的安装，配置。同时实践下从mysql迁移到postgres。</p>

<h2>安装配置</h2>

<p>这里直接使用rpm包来安装。如果是centos6.6以下版本的系统需要更新openssl。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master1 postgres]# ll
</span><span class='line'>total 20708
</span><span class='line'>-rw-r--r-- 1 root root  1593932 Dec 11 10:02 openssl-1.0.1e-42.el6.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root  1085208 Dec 11 09:12 postgresql94-9.4.5-1PGDG.rhel6.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root   541376 Dec 11 09:12 postgresql94-contrib-9.4.5-1PGDG.rhel6.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root  1600736 Dec 11 09:12 postgresql94-devel-9.4.5-1PGDG.rhel6.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root 11485008 Dec 11 09:13 postgresql94-docs-9.4.5-1PGDG.rhel6.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root   198968 Dec 11 09:12 postgresql94-libs-9.4.5-1PGDG.rhel6.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root    60688 Dec 11 09:12 postgresql94-plperl-9.4.5-1PGDG.rhel6.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root    68884 Dec 11 09:12 postgresql94-plpython-9.4.5-1PGDG.rhel6.x86_64.rpm
</span><span class='line'>-rw-r--r-- 1 root root  4556880 Dec 11 09:11 postgresql94-server-9.4.5-1PGDG.rhel6.x86_64.rpm</span></code></pre></td></tr></table></div></figure>


<ul>
<li>安装命令：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># yum install -y openssl-1.0.1e-42.el6.x86_64.rpm 
</span><span class='line'>
</span><span class='line'># useradd postgres
</span><span class='line'># rpm -i postgresql94-*</span></code></pre></td></tr></table></div></figure>


<ul>
<li>配置环境变量、初始化数据库、启动数据库：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># su - postgres
</span><span class='line'>$ vi .bash_profile
</span><span class='line'>
</span><span class='line'>export PGDATA=/var/lib/pgsql/9.4/data
</span><span class='line'>PG_HOME=/usr/pgsql-9.4
</span><span class='line'>PATH=$PG_HOME/bin:$PATH
</span><span class='line'>export PATH
</span><span class='line'>
</span><span class='line'>$ initdb
</span><span class='line'>
</span><span class='line'>$ vi $PGDATA/pg_hba.conf
</span><span class='line'>  host    all             all              192.168.0.0/16          md5
</span><span class='line'>
</span><span class='line'>$ vi /var/lib/pgsql/9.4/data/postgresql.conf
</span><span class='line'>  listen_addresses = '*'
</span><span class='line'>
</span><span class='line'># 切回root
</span><span class='line'>
</span><span class='line'># service postgresql-9.4 start
</span><span class='line'># chkconfig postgresql-9.4 on --level 2345</span></code></pre></td></tr></table></div></figure>


<p>pg_hba.conf用来控制什么用于可以被远程访问。而postgresql.conf修改的监听的地址，默认是localhost改成*后就可以所有地址都可以访问了。</p>

<ul>
<li>建立库，创建数据库用户</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-bash-4.1$ psql 
</span><span class='line'>
</span><span class='line'> create user dpi;
</span><span class='line'> create database dpi owner dpi;
</span><span class='line'> alter user dpi with password 'XXXX';</span></code></pre></td></tr></table></div></figure>


<p>建表：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>CREATE TABLE t_dta_illegalweb (
</span><span class='line'>...
</span><span class='line'>  day varchar(10) DEFAULT NULL,
</span><span class='line'>...
</span><span class='line'>);
</span><span class='line'>
</span><span class='line'>create or replace function t_dta_illegalweb_insert_trigger()
</span><span class='line'>returns trigger as $$
</span><span class='line'>begin
</span><span class='line'>  return null;
</span><span class='line'>end; 
</span><span class='line'>$$ language plpgsql;
</span><span class='line'>
</span><span class='line'>CREATE TRIGGER trigger_t_dta_illegalweb_insert
</span><span class='line'>    BEFORE INSERT ON t_dta_illegalweb
</span><span class='line'>    FOR EACH ROW EXECUTE PROCEDURE t_dta_illegalweb_insert_trigger();
</span></code></pre></td></tr></table></div></figure>


<p>后面会使用分区表，先把触发器都建好。把框框搭好，后面修改就行了。</p>

<h2>数据迁移</h2>

<ol>
<li>postgres创建表：</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>CREATE TABLE IF NOT EXISTS t_dta_illegalweb20151211 (check(day = '2015-12-11')) INHERITS (t_dta_illegalweb);
</span><span class='line'>CREATE TABLE IF NOT EXISTS t_dta_illegalweb20151210 (check(day = '2015-12-10')) INHERITS (t_dta_illegalweb);
</span><span class='line'>CREATE TABLE IF NOT EXISTS t_dta_illegalweb20151209 (check(day = '2015-12-09')) INHERITS (t_dta_illegalweb);
</span><span class='line'>CREATE TABLE IF NOT EXISTS t_dta_illegalweb20151208 (check(day = '2015-12-08')) INHERITS (t_dta_illegalweb);
</span><span class='line'>CREATE TABLE IF NOT EXISTS t_dta_illegalweb20151207 (check(day = '2015-12-07')) INHERITS (t_dta_illegalweb);</span></code></pre></td></tr></table></div></figure>


<ol>
<li>mysql导出数据：</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>select * from t_dta_illegalweb where day='2015-12-09' into outfile '/tmp/etl/t_dta_illegalweb20151209.sql'  fields terminated by '|';
</span><span class='line'>select * from t_dta_illegalweb where day='2015-12-08' into outfile '/tmp/etl/t_dta_illegalweb20151208.sql'  fields terminated by '|';
</span><span class='line'>select * from t_dta_illegalweb where day='2015-12-07' into outfile '/tmp/etl/t_dta_illegalweb20151207.sql'  fields terminated by '|';</span></code></pre></td></tr></table></div></figure>


<p>数据在mysql服务器的/tmp/etl目录下面。如果mysql和postgres不在同一台机，需要把这些文件拷贝到postgres的服务器。</p>

<ol>
<li>导入数据到postgres:</li>
</ol>


<p>用psql登录dpi，然后执行copy命令把数据导入到对应的表。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>\copy  t_dta_illegalweb20151209 from  '/tmp/etl/t_dta_illegalweb20151209.sql' using delimiters '|' ;
</span><span class='line'>\copy  t_dta_illegalweb20151208 from  '/tmp/etl/t_dta_illegalweb20151208.sql' using delimiters '|' ;
</span><span class='line'>\copy  t_dta_illegalweb20151207 from  '/tmp/etl/t_dta_illegalweb20151207.sql' using delimiters '|' ;</span></code></pre></td></tr></table></div></figure>


<h2>程序修改</h2>

<p>程序修改是一件头痛的事情，虽然大部分都是SQL，但是MYSQL的比较宽泛，很多语句都兼容不报错也能出来想要的结果。但是这些语句在postgres下面执行是会报错的。比如说，select count(*)对所有数据count的时刻不能加order by（提示要groupby）；再比如，mysql遇到字符串字段和数字比较会统一转换成数字比较，等等这些在postgres中都需要在SQL中显示的转换的。</p>

<p>那么postgres的类型转换怎么实现呢？两种形式cast(X as TYPE) 或者 X::TYPE。</p>

<p>由于程序是用hibernate来做数据库访问的，会遇到如下的问题</p>

<ul>
<li>如果用hql的话CAST函数hibernate首先会进行转换。（转换类型与hibernate对象的类型不匹配）</li>
<li>而用X::TYPE会把:TYPE作为一个name parameter。</li>
<li>不用hql用sql的话，要自己做对象转换，这是我们不愿意去做的事情（不然用hibernate干嘛）</li>
</ul>


<p>各种尝试过后，修改PostgreSQLDialect来实现，添加一个自定义的hibernate函数，把字符串转成bigint即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import java.sql.Types;
</span><span class='line'>
</span><span class='line'>import org.hibernate.Hibernate;
</span><span class='line'>import org.hibernate.dialect.function.SQLFunctionTemplate;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>public class PostgreSQLDialect extends org.hibernate.dialect.PostgreSQLDialect {
</span><span class='line'>  
</span><span class='line'>  public PostgreSQLDialect() {
</span><span class='line'>      super();
</span><span class='line'>      registerFunction( "bigint", new SQLFunctionTemplate(Hibernate.BIG_INTEGER, "cast(?1 as bigint)") );
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>使用如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>StringBuilder hql = new StringBuilder("from IllegalWebInfo where 1=1 ");
</span><span class='line'>List&lt;Object&gt; params = new ArrayList&lt;&gt;();
</span><span class='line'>
</span><span class='line'>String domain = queryBean.getDomain();
</span><span class='line'>if (StringUtils.isNotBlank(domain)) {
</span><span class='line'>  hql.append(" and ").append("domain=?");
</span><span class='line'>  params.add(domain.toLowerCase());
</span><span class='line'>}
</span><span class='line'>String houseId = queryBean.getHouseId();
</span><span class='line'>if (StringUtils.isNotBlank(houseId)) {
</span><span class='line'>  hql.append(" and ").append("houseId=?");
</span><span class='line'>  params.add(houseId);
</span><span class='line'>}
</span><span class='line'>String day = queryBean.getDay();
</span><span class='line'>if (StringUtils.isNotBlank(day)) {
</span><span class='line'>  hql.append(" and ").append("day=?");
</span><span class='line'>  params.add(day);
</span><span class='line'>}
</span><span class='line'>int threshold = queryBean.getThreshold();
</span><span class='line'>if(threshold &gt; 0){
</span><span class='line'>  hql.append(" and ").append("bigint(visitsCount) &gt;= ?");
</span><span class='line'>  params.add(BigInteger.valueOf(threshold)); // 注意这里的类型转换，把int装成bigint
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>Object[] paramArray = params.toArray();
</span><span class='line'>String detailHQL = hql.toString(); // + " order by bigint(visitsCount) desc ";
</span><span class='line'>List&lt;ActiveResourcesDomainInfo&gt; hist = activeResourcesDomainDao.findPageable(detailHQL, currentPage, pageSize, paramArray);
</span><span class='line'>
</span><span class='line'>String countHQL = "select count(*) " + hql;
</span><span class='line'>long count = (long) illegalWebDao.findByHql(countHQL, paramArray).iterator().next();</span></code></pre></td></tr></table></div></figure>


<h2>定时任务，创建和更新触发器函数</h2>

<p>函数：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>create or replace function create_partition_table_everyday (t TEXT) returns timestamp as $$
</span><span class='line'>declare 
</span><span class='line'>  i int;
</span><span class='line'>  cnt int;
</span><span class='line'>  stmt text;
</span><span class='line'>  select_stmt text;
</span><span class='line'>  day date;
</span><span class='line'>  isInherit BOOLEAN;
</span><span class='line'>begin
</span><span class='line'>
</span><span class='line'>  day := now() + interval '-1 day';
</span><span class='line'>  stmt := 'CREATE TABLE IF NOT EXISTS ' || t || to_char(day, 'YYYYMMDD') || '(check(day = ''' || to_char(day, 'YYYY-MM-DD') || ''')) INHERITS (' || t || ')';
</span><span class='line'>  RAISE INFO '[DEBUG] %', stmt;
</span><span class='line'>  EXECUTE stmt;
</span><span class='line'>
</span><span class='line'>  day := now() + interval '-183 day';
</span><span class='line'>  stmt := 'DROP TABLE IF EXISTS ' || t || to_char(day, 'YYYYMMDD');
</span><span class='line'>  RAISE INFO '[DEBUG] %', stmt;
</span><span class='line'>  EXECUTE stmt;
</span><span class='line'>
</span><span class='line'>BEGIN
</span><span class='line'>  day := now() + interval '-32 day';
</span><span class='line'>  stmt := 'ALTER TABLE IF EXISTS ' || t || to_char(day, 'YYYYMMDD') || ' NO INHERIT ' || t;
</span><span class='line'>  RAISE INFO '[DEBUG] %', stmt;
</span><span class='line'>  EXECUTE stmt;
</span><span class='line'>EXCEPTION WHEN OTHERS THEN
</span><span class='line'>  RAISE INFO '[WARN] % %', SQLERRM, SQLSTATE;
</span><span class='line'>END;
</span><span class='line'>
</span><span class='line'>  i := 0;
</span><span class='line'>  cnt := 6; -- 用于生成触发器分发最近几天的insert功能
</span><span class='line'>
</span><span class='line'>  day := now() + interval '-1 day';
</span><span class='line'>  stmt :=         ' create or replace function ' || t || '_insert_trigger() returns trigger as $' || '$ ';
</span><span class='line'>  stmt := stmt || ' begin ';
</span><span class='line'>  stmt := stmt || ' if (new.day = ''' || to_char(day, 'YYYY-MM-DD') || ''') then INSERT INTO ' || t || to_char(day, 'YYYYMMDD') || ' VALUES (new.*); ';
</span><span class='line'>  while i &lt; cnt 
</span><span class='line'>  loop
</span><span class='line'>      day := day + interval '-1 day';
</span><span class='line'>      stmt := stmt || ' elsif (new.day = ''' || to_char(day, 'YYYY-MM-DD') || ''') then INSERT INTO ' || t || to_char(day, 'YYYYMMDD') || ' VALUES (new.*); ';
</span><span class='line'>
</span><span class='line'>      i := i + 1;
</span><span class='line'>  end loop;
</span><span class='line'>  stmt := stmt || ' else raise exception ''DATE out of range. Fix the ' || t || '_insert_trigger() func!!''; ';
</span><span class='line'>  stmt := stmt || ' end if; ';
</span><span class='line'>  stmt := stmt || ' return null; ';
</span><span class='line'>  stmt := stmt || ' end;  ';
</span><span class='line'>  stmt := stmt || ' $' || '$ language plpgsql; ';
</span><span class='line'>  RAISE INFO '[DEBUG] %', stmt;
</span><span class='line'>  EXECUTE stmt;
</span><span class='line'>
</span><span class='line'>  return now();
</span><span class='line'>end;
</span><span class='line'>$$ language plpgsql;
</span></code></pre></td></tr></table></div></figure>


<p>脚本：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>vi update_dta_postgres.sh
</span><span class='line'>
</span><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'>source ~/.bash_profile
</span><span class='line'>
</span><span class='line'>psql -d dpi -c "select create_partition_table_everyday('t_dta_illegalweb')"
</span><span class='line'>psql -d dpi -c "select create_partition_table_everyday('t_dta_activeresources_domain')"
</span><span class='line'>psql -d dpi -c "select create_partition_table_everyday('t_dta_activeresources_ip')"
</span><span class='line'>
</span><span class='line'>$ 
</span><span class='line'>chmod +x update_dta_postgres.sh 
</span><span class='line'>crontab -e
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>10 0 * * * sh ~/scripts/update_dta_postgres.sh &gt;~/scripts/update_dta_postgres.log 2&gt;&1</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/22648597/linux-centos-yum-error-package-requires-libcrypto-so-10openssl-1-0-1-ec64bi">http://stackoverflow.com/questions/22648597/linux-centos-yum-error-package-requires-libcrypto-so-10openssl-1-0-1-ec64bi</a></li>
<li><a href="http://twpug.net/docs/postgresql-doc-8.0-zh_TW/functions-comparison.html">http://twpug.net/docs/postgresql-doc-8.0-zh_TW/functions-comparison.html</a></li>
<li><a href="http://stackoverflow.com/questions/7690329/check-if-table-inherits-from-other-table-in-postgresql">http://stackoverflow.com/questions/7690329/check-if-table-inherits-from-other-table-in-postgresql</a></li>
<li><a href="http://www.jaredlog.com/?p=137">http://www.jaredlog.com/?p=137</a></li>
<li><a href="http://www.anicehumble.com/2011/08/postgresql-catch-exception-rocks.html">http://www.anicehumble.com/2011/08/postgresql-catch-exception-rocks.html</a></li>
<li><a href="http://stackoverflow.com/questions/4877637/postgresql-exception-handling">http://stackoverflow.com/questions/4877637/postgresql-exception-handling</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搭梯笔记]]></title>
    <link href="http://winseliu.com/blog/2015/11/22/gfw-ladder/"/>
    <updated>2015-11-22T20:51:35+08:00</updated>
    <id>http://winseliu.com/blog/2015/11/22/gfw-ladder</id>
    <content type="html"><![CDATA[<h2>准备一个SSH账号</h2>

<ul>
<li><a href="http://www.99ssh.net/">http://www.99ssh.net/</a></li>
</ul>


<h2>SSH -N -D 或者MyEnTunnel</h2>

<ul>
<li><code>ssh -N -D [PORT] [USER@IP]</code></li>
<li><a href="http://www.99ssh.net/help/newsshow.php?cid=19&amp;id=21">http://www.99ssh.net/help/newsshow.php?cid=19&amp;id=21</a></li>
</ul>


<p>使用MyEnTunel的话，设置程序为【启动软件时自动连接】，同时把程序的快捷方式加到【C:\Users\winse\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Startup\】目录下。</p>

<h2>Chrome + SwitchyOmega + gfwlist</h2>

<ul>
<li><a href="https://github.com/FelisCatus/SwitchyOmega/releases">https://github.com/FelisCatus/SwitchyOmega/releases</a> SwitchySharp升级版本</li>
<li><a href="https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt">https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</a></li>
</ul>


<p>可以做到智能代理功能，gfwlist的才会走代理。加速访问国内网站，同时减少不必要的流量。</p>

<h2>Firefox + FoxyProxy + gfwlist</h2>

<ul>
<li><a href="http://mozilla.com.cn/thread-230260-1-1.html">http://mozilla.com.cn/thread-230260-1-1.html</a></li>
<li><a href="https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt">https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</a></li>
</ul>


<p>Chrome的版本速度快一点。配置好后，等待一段时间就智能的适配了，firefox的等的时间略长。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx再折腾---统一访问入口]]></title>
    <link href="http://winseliu.com/blog/2015/11/11/nginx-build-unified-access/"/>
    <updated>2015-11-11T11:04:04+08:00</updated>
    <id>http://winseliu.com/blog/2015/11/11/nginx-build-unified-access</id>
    <content type="html"><![CDATA[<p>快照目录文件太多，准备安装一个方式分目录。但是又要能保证原来的访问方式不变化！使用rewrite和try_files成功实现。</p>

<h2>目录结构:</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC /cygdrive/f/temp
</span><span class='line'>$ ls -R
</span><span class='line'>.:
</span><span class='line'>1.jpg  snapshot  snapshot-1  snapshot-2  snapshot-3  snapshot-4
</span><span class='line'>
</span><span class='line'>./snapshot:
</span><span class='line'>0.html
</span><span class='line'>
</span><span class='line'>./snapshot-1:
</span><span class='line'>1.html
</span><span class='line'>
</span><span class='line'>./snapshot-2:
</span><span class='line'>2.html
</span><span class='line'>
</span><span class='line'>./snapshot-3:
</span><span class='line'>3.html
</span><span class='line'>
</span><span class='line'>./snapshot-4:
</span><span class='line'>4.html</span></code></pre></td></tr></table></div></figure>


<h2>Nginx配置尝试一:</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    location /snapshot {
</span><span class='line'>        root   /home/hadoop/html-snapshot;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>
</span><span class='line'>        rewrite ^/snapshot/.*/(.*)$  /snapshot/$1 break ;
</span><span class='line'>        
</span><span class='line'>        try_files $uri /snapshot-1/$uri /snapshot-3/$uri;
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    location ~ /snapshot-\d+ {
</span><span class='line'>        root   /home/hadoop/html-snapshot;
</span><span class='line'>
</span><span class='line'>        rewrite ^/(.*)/.*/(.*)$ /$1/$2 break;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<p>这种方式是不行的，try_files要求除最后一个配置外其他都是文件！</p>

<blockquote><p>It is possible to check directory’s existence by specifying a slash at the end of a name, e.g. “$uri/”. If none of the files were found, an internal redirect to the uri specified in the last parameter is made.  [<a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#try_files">http://nginx.org/en/docs/http/ngx_http_core_module.html#try_files</a>]</p></blockquote>

<p>也就是说，中间配置路径，nginx只把他们当做本地的去看待！文件存在就返回结果，否则直接重定向到最后一个路径！！</p>

<h2>Nginx配置尝试二：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    location /snapshot {
</span><span class='line'>        root   F:/temp;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>
</span><span class='line'>        rewrite ^/snapshot/.*/(.*)$  /snapshot/$1 break ;
</span><span class='line'>
</span><span class='line'>        try_files $uri @backup;
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    location ~ /snapshot-\d+ {
</span><span class='line'>        root   F:/temp;
</span><span class='line'>
</span><span class='line'>      try_files $uri @backup;
</span><span class='line'>    }
</span><span class='line'>  
</span><span class='line'>    location @backup {
</span><span class='line'>      # 这里的顺序不能颠倒，[.*]会匹配所有的！
</span><span class='line'>        rewrite ^/(.*)-3/(.*)$ /$1-4/$2 last;
</span><span class='line'>        rewrite ^/(.*)-2/(.*)$ /$1-3/$2 last;
</span><span class='line'>        rewrite ^/(.*)-1/(.*)$ /$1-2/$2 last;
</span><span class='line'>        rewrite ^/(.*)/(.*)$ /$1-1/$2 last;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<p>这里使用循环的方式在backup的location中进行处理，一个个的循环查找。使用了正则表达式和一个统一rewrite的location。</p>

<h2>Nginx配置尝试三：</h2>

<p>上面发现，其实try_files都是去查找文件，其实目录结构和访问路径是匹配的，只是请求一开始就带snaphost，倒是每次都需要处理。如果请求过来的就没有带snaphost的话！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    location / {
</span><span class='line'>        root   F:/temp;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>
</span><span class='line'>        try_files /snapshot/$uri /snapshot-1/$uri  /snapshot-2/$uri  /snapshot-3/$uri  /snapshot-4/$uri;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<p>一个location配置就行了！</p>

<h2>Nginx配置完善版：</h2>

<p>转变思路后，最开始就把请求的前置snapshot去掉rewrite去掉就行了！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    location /snapshot {
</span><span class='line'>        root   F:/temp;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>      
</span><span class='line'>      rewrite ^/snapshot/.*/(.*)$  /$1 break ;
</span><span class='line'>
</span><span class='line'>        try_files /snapshot/$uri /snapshot-1/$uri  /snapshot-2/$uri  /snapshot-3/$uri  /snapshot-4/$uri;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<h2>nginx添加模块</h2>

<p>当我们启用 &ndash;with-debug 选项重新构建好调试版的 Nginx 之后，还需要同时在配置文件中通过标准的 error_log 配置指令为错误日志使用 debug 日志级别（这同时也是最低的日志级别）：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>error_log logs/error.log debug;</span></code></pre></td></tr></table></div></figure>


<p>添加echo模块：</p>

<p>下载zlib、pcre、echo：</p>

<ul>
<li><a href="http://www.zlib.net/">http://www.zlib.net/</a></li>
<li><a href="http://www.pcre.org/">http://www.pcre.org/</a></li>
<li><a href="https://github.com/openresty/echo-nginx-module">https://github.com/openresty/echo-nginx-module</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxvf zlib-1.2.8.tar.gz 
</span><span class='line'>mv zlib-1.2.8 src/zlib
</span><span class='line'>tar zxvf pcre-8.36.tar.gz 
</span><span class='line'>mv pcre-8.36 src/pcre
</span><span class='line'>
</span><span class='line'>./configure --prefix=/home/hadoop/nginx --add-module=/home/hadoop/echo-nginx-module-0.58  --with-pcre=src/pcre --with-zlib=src/zlib --with-debug 
</span><span class='line'>#[hadoop@cu2 nginx-1.7.10]$ ./configure --prefix=/home/hadoop/nginx --with-http_ssl_module --with-pcre=src/pcre/ --with-zlib=src/zlib/ --with-debug
</span><span class='line'>make -j2
</span><span class='line'>make install</span></code></pre></td></tr></table></div></figure>


<p>编译成功后，就能在location里面直接echo，页面访问时就能看到echo内容了。</p>

<h2>参考</h2>

<ul>
<li><a href="http://www.cnblogs.com/cgli/archive/2011/05/16/2047920.html">http://www.cnblogs.com/cgli/archive/2011/05/16/2047920.html</a></li>
<li><a href="http://www.cnblogs.com/tohilary/archive/2012/08/24/2653904.html">http://www.cnblogs.com/tohilary/archive/2012/08/24/2653904.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[树莓派raspberrypi2简单使用]]></title>
    <link href="http://winseliu.com/blog/2015/11/04/raspberrypi-start-guide/"/>
    <updated>2015-11-04T11:52:18+08:00</updated>
    <id>http://winseliu.com/blog/2015/11/04/raspberrypi-start-guide</id>
    <content type="html"><![CDATA[<h2>买的东西地址：</h2>

<p>现在想来，其实可以多加100，买一整套的比较方便。内存卡还有外壳都在里面。</p>

<ul>
<li><p><a href="https://item.taobao.com/item.htm?id=520179324500&amp;ali_refid=a3_420434_1006:1103723226:N:%E6%A0%91%E8%8E%93%E6%B4%BE2+b%2B:fc636846d2212679077e26f5f9f14118&amp;ali_trackid=1_fc636846d2212679077e26f5f9f14118&amp;spm=a230r.1.0.0.vhiQsb&amp;qq-pf-to=pcqq.c2c">树莓派2b 树莓派raspberry Pi 2代B型四核开发板 官方正品 树莓派</a></p></li>
<li><p><a href="http://item.jd.com/679773.html">闪迪（SanDisk）至尊高速移动MicroSDHC UHS-I存储卡 TF卡 32GB Class10 读速48Mb/s</a></p></li>
<li><a href="http://item.jd.com/667570.html">迅捷（FAST）FW150US 超小型150M无线USB网卡</a></li>
<li><p><a href="https://detail.tmall.com/item.htm?_u=4jgup6l1c31&amp;id=45729451918">USB转TTL PL2303HX模块 STC单片机下载线刷机线 升级串口模块</a></p></li>
<li><p><a href="http://item.jd.com/629794.html">雷柏（Rapoo）1860 无线光学键鼠套装</a>  质量一般。临时用用，可以考虑不买！</p></li>
</ul>


<h2>安装系统</h2>

<ul>
<li>显示器： 恰好同时有HDMI的接口和显示器。如果没有，那就要考虑直接把系统写到SD卡了！！</li>
<li>无线键盘、鼠标</li>
<li>电源： 一般的手机充电器都可以，用充电宝也是OK的</li>
<li><a href="https://www.raspberrypi.org/help/noobs-setup/">NOOBS</a>

<ul>
<li>NOOBS_v1_4_2.zip</li>
<li>SDFormatterv4.zip</li>
</ul>
</li>
</ul>


<p>插上SD卡，安装系统就行了。</p>

<p><img src="http://winseliu.com/images/blogs/raspberrypi-os-install.png" alt="" /></p>

<h2>无线网卡</h2>

<p>网站上没说有linux的驱动，但是直接插上后是能检测到设备的！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>pi@raspberrypi:~$ lsusb
</span><span class='line'>Bus 001 Device 004: ID 0bda:8179 Realtek Semiconductor Corp. 
</span><span class='line'>Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet Adapter
</span><span class='line'>Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. 
</span><span class='line'>Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span></code></pre></td></tr></table></div></figure>


<p>ifconfig也能查看到wlan0的无线网卡。接下来修改配置，添加用户密码即可。</p>

<p>配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>pi@raspberrypi:~$ cat /etc/network/interfaces
</span><span class='line'># Please note that this file is written to be used with dhcpcd.
</span><span class='line'># For static IP, consult /etc/dhcpcd.conf and 'man dhcpcd.conf'.
</span><span class='line'>
</span><span class='line'>auto lo
</span><span class='line'>iface lo inet loopback
</span><span class='line'>
</span><span class='line'>auto eth0
</span><span class='line'>allow-hotplug eth0
</span><span class='line'>#iface eth0 inet manual
</span><span class='line'>iface eth0 inet dhcp
</span><span class='line'>
</span><span class='line'>auto wlan0
</span><span class='line'>allow-hotplug wlan0
</span><span class='line'>#iface wlan0 inet manual
</span><span class='line'>iface wlan0 inet dhcp
</span><span class='line'>wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf
</span><span class='line'>
</span><span class='line'>auto wlan1
</span><span class='line'>allow-hotplug wlan1
</span><span class='line'>iface wlan1 inet manual
</span><span class='line'>wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf
</span><span class='line'>
</span><span class='line'>pi@raspberrypi:~$ sudo cat /etc/wpa_supplicant/wpa_supplicant.conf
</span><span class='line'>ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
</span><span class='line'>update_config=1
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>network={
</span><span class='line'>ssid="winse.liu"
</span><span class='line'>psk="MIMA"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>network={
</span><span class='line'>ssid="1108"
</span><span class='line'>psk="MIMA"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>USB转串口使用COM控制raspberry</h2>

<p>在淘宝上面买的，我系统是win10，抱着尝试下的心态也很便宜就买了一个。买来后，安装了win7的驱动，能用。</p>

<p>配置见图：</p>

<p><img src="http://winseliu.com/images/blogs/raspberrypi-gpio.png" alt="" /></p>

<p><img src="http://winseliu.com/images/blogs/raspberrypi-usb-com-install.jpg" alt="" /></p>

<p><img src="http://winseliu.com/images/blogs/raspberrypi-usb-com-config.jpg" alt="" /></p>

<p>最终效果：</p>

<p><img src="http://winseliu.com/images/blogs/raspberrypi-finished.jpg" alt="" /></p>

<h2>用到的一些命令</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo raspi-config
</span><span class='line'>修改配置，默认使用图形界面登录，可以使用`Boot Options`修改为文本console模式
</span><span class='line'>
</span><span class='line'>sudo iwlist wlan0 scan | grep ESSID
</span><span class='line'>查看可用的无线
</span><span class='line'>
</span><span class='line'>systemctl list-units
</span><span class='line'>
</span><span class='line'>sudo apt-get install screen
</span><span class='line'>
</span><span class='line'>sudo apt-get -y install vim
</span><span class='line'>sudo apt-get install nginx
</span><span class='line'>systemctl status nginx.service
</span><span class='line'>sudo cp /etc/skel/.* /home/robot/</span></code></pre></td></tr></table></div></figure>


<p>手机放一个热点出来，然后手机安装一个ssh的工具(JuiceSSH v2.0.2)就可以控制树莓派了。</p>

<p>网上有用手机当屏幕，然后键盘连树莓派usb，结合来控制树莓派。一开始挺新奇的，后来感觉挺扯淡的！不过学习到了screen的程序，自动登录啥的没弄成，直接输入用户密码登录也行了。在boot选项看到有自动登录，不知道有没有用。现在有无线网卡和com来控制，感觉已经够用了。</p>

<h2>参考</h2>

<ul>
<li><a href="https://www.raspberrypi.org/forums/viewtopic.php?f=91&amp;t=4751&amp;sid=661d1a59e4f85f333b40e6e46db58d32">Getting Started with the Raspberry Pi</a></li>
<li><a href="http://blog.csdn.net/c80486/article/details/8545307">树莓派(raspberry pi)学习15: 使用WIFI网卡连接无线网络</a></li>
<li><a href="http://blog.csdn.net/cugbabybear/article/details/23048741">windows下 用串行连接控制树莓派</a></li>
<li><a href="http://www.alsrobot.cn/article-141.html">【创客学堂】如何在windows系统下用串口通信完爆raspberry pi（树莓派）</a></li>
<li><a href="http://shumeipai.nxez.com/2013/10/10/raspberry-pi-pick-kindle-display.html">视频详解树莓派如何外接Kindle显示器</a></li>
<li><p><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-screen/index.html">linux 技巧：使用 screen 管理你的远程会话</a></p></li>
<li><p><a href="http://www.bkjia.com/Pythonjc/818142.html">RASPBERRY PI wifi配置</a></p></li>
<li><p><a href="http://blog.sina.com.cn/s/blog_3cb6a78c0101a0fe.html">Raspberry Pi 连接无线网卡</a></p></li>
<li><p><a href="http://davidrobot.com/2014/11/raspberry_pi_model_b_plus_startup.html">开机篇 – 树莓派 Raspberry Pi Model B+ 入手折腾记 (1)</a></p></li>
<li><p><a href="http://shumeipai.nxez.com/2013/09/07/no-screen-unknow-ip-login-pi.html#more-184">没有显示器且IP未知的情况下登录树莓派</a></p></li>
<li><p><a href="http://www.pc6.com/az/104761.html">JuiceSSH v2.0.2</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cacti批量添加配置]]></title>
    <link href="http://winseliu.com/blog/2015/10/13/cacti-batch-adding-configurations/"/>
    <updated>2015-10-13T08:35:50+08:00</updated>
    <id>http://winseliu.com/blog/2015/10/13/cacti-batch-adding-configurations</id>
    <content type="html"><![CDATA[<h2>所有机器SNMP配置同步</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for h in `cat /etc/hosts| grep hadoop|awk '{print $2}'` ; do scp -r /etc/snmp/snmpd.conf $h:/etc/snmp/ ; done
</span><span class='line'>
</span><span class='line'>for h in `cat /etc/hosts| grep hadoop|awk '{print $2}'` ; do ssh $h "service snmpd start" ; done</span></code></pre></td></tr></table></div></figure>


<h2>Cacti批量添加配置</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>批量添加device。ip也可以为hostname；template为机器模板；version为SNMP的版本
</span><span class='line'>[root@cu-omc1 cacti]# for h in `cat /etc/hosts | grep hadoop | awk '{print $2}'` ; do php cli/add_device.php --description="$h" --ip="$h" --template=9 --version=2 ; done
</span><span class='line'>
</span><span class='line'>了解参数
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-hosts
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-graph-templates --host-template-id=9
</span><span class='line'>Known Graph Templates:(id, name)
</span><span class='line'>29      Host MIB - Processes
</span><span class='line'>35      ucd/net - Users Logged On
</span><span class='line'>36      ucd/net - TCP Current Established
</span><span class='line'>37      ucd/net - Uptime
</span><span class='line'>38      ucd/net - TCP Counters
</span><span class='line'>39      ucd/net - Memory Usage (enhanced)
</span><span class='line'>40      ucd/net - Load Average (enhanced)
</span><span class='line'>41      ucd/net - CPU Usage (enhanced)
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-snmp-queries
</span><span class='line'>Known SNMP Queries:(id, name)
</span><span class='line'>1       SNMP - Interface Statistics
</span><span class='line'>2       ucd/net -  Get Monitored Partitions
</span><span class='line'>3       Karlnet - Wireless Bridge Statistics
</span><span class='line'>4       Netware - Get Available Volumes
</span><span class='line'>6       Unix - Get Mounted Partitions
</span><span class='line'>7       Netware - Get Processor Information
</span><span class='line'>8       SNMP - Get Mounted Partitions
</span><span class='line'>9       SNMP - Get Processor Information
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-snmp-fields --host-id=2 --snmp-query-id=1
</span><span class='line'>Known SNMP Fields for host-id 2: (name)
</span><span class='line'>ifAlias
</span><span class='line'>ifDescr
</span><span class='line'>ifHighSpeed
</span><span class='line'>ifHwAddr
</span><span class='line'>ifIndex
</span><span class='line'>ifIP
</span><span class='line'>ifName
</span><span class='line'>ifOperStatus
</span><span class='line'>ifSpeed
</span><span class='line'>ifType
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-snmp-values --host-id=2 --snmp-query-id=1 --snmp-field=ifIP
</span><span class='line'>Known values for ifIP for host 2: (name)
</span><span class='line'>127.0.0.1
</span><span class='line'>192.168.20.11
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php  --list-graph-templates 
</span><span class='line'>Known Graph Templates:(id, name)
</span><span class='line'>2       Interface - Traffic (bits/sec)
</span><span class='line'>3       ucd/net - Available Disk Space
</span><span class='line'>4       ucd/net - CPU Usage
</span><span class='line'>5       Karlnet - Wireless Levels
</span><span class='line'>6       Karlnet - Wireless Transmissions
</span><span class='line'>7       Unix - Ping Latency
</span><span class='line'>8       Unix - Processes
</span><span class='line'>9       Unix - Load Average
</span><span class='line'>10      Unix - Logged in Users
</span><span class='line'>11      ucd/net - Load Average
</span><span class='line'>12      Linux - Memory Usage
</span><span class='line'>13      ucd/net - Memory Usage
</span><span class='line'>14      Netware - File System Cache
</span><span class='line'>15      Netware - CPU Utilization
</span><span class='line'>16      Netware - File System Activity
</span><span class='line'>17      Netware - Logged In Users
</span><span class='line'>18      Cisco - CPU Usage
</span><span class='line'>19      Netware - Volume Information
</span><span class='line'>20      Netware - Directory Information
</span><span class='line'>21      Unix - Available Disk Space
</span><span class='line'>22      Interface - Errors/Discards
</span><span class='line'>23      Interface - Unicast Packets
</span><span class='line'>24      Interface - Non-Unicast Packets
</span><span class='line'>25      Interface - Traffic (bytes/sec)
</span><span class='line'>26      Host MIB - Available Disk Space
</span><span class='line'>27      Host MIB - CPU Utilization
</span><span class='line'>28      Host MIB - Logged in Users
</span><span class='line'>29      Host MIB - Processes
</span><span class='line'>30      Netware - Open Files
</span><span class='line'>31      Interface - Traffic (bits/sec, 95th Percentile)
</span><span class='line'>32      Interface - Traffic (bits/sec, Total Bandwidth)
</span><span class='line'>33      Interface - Traffic (bytes/sec, Total Bandwidth)
</span><span class='line'>34      SNMP - Generic OID Template
</span><span class='line'>35      ucd/net - Users Logged On
</span><span class='line'>36      ucd/net - TCP Current Established
</span><span class='line'>37      ucd/net - Uptime
</span><span class='line'>38      ucd/net - TCP Counters
</span><span class='line'>39      ucd/net - Memory Usage (enhanced)
</span><span class='line'>40      ucd/net - Load Average (enhanced)
</span><span class='line'>41      ucd/net - CPU Usage (enhanced)
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php  --list-query-types  --snmp-query-id=1
</span><span class='line'>Known SNMP Query Types: (id, name)
</span><span class='line'>2       In/Out Errors/Discarded Packets
</span><span class='line'>3       In/Out Non-Unicast Packets
</span><span class='line'>4       In/Out Unicast Packets
</span><span class='line'>9       In/Out Bytes (64-bit Counters)
</span><span class='line'>13      In/Out Bits
</span><span class='line'>14      In/Out Bits (64-bit Counters)
</span><span class='line'>16      In/Out Bytes
</span><span class='line'>20      In/Out Bits with 95th Percentile
</span><span class='line'>21      In/Out Bits with Total Bandwidth
</span><span class='line'>22      In/Out Bytes with Total Bandwidth
</span><span class='line'>
</span><span class='line'>先测试单机添加，对应到Device页面点击`Create Graphs for this Host`添加图像的操作
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --host-id=2 --graph-type=cg --graph-template-id=40
</span><span class='line'>Graph Added - graph-id: (5) - data-source-ids: (8, 9, 10)
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --host-id=2 --graph-type=cg --graph-template-id=41
</span><span class='line'>Graph Added - graph-id: (6) - data-source-ids: (11, 12, 13, 14)
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --host-id=2 --graph-type=cg --graph-template-id=39
</span><span class='line'>Graph Added - graph-id: (7) - data-source-ids: (15, 16, 17, 18, 19)
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --host-id=2 --graph-type=cg --graph-template-id=38
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --host-id="2" --graph-type=ds  --graph-template-id=2 --snmp-query-id=1 --snmp-query-type-id=16 --snmp-field=ifIP --snmp-value="192.168.20.11"
</span><span class='line'>Graph Added - graph-id: (9) - data-source-ids: (24, 24)
</span><span class='line'>
</span><span class='line'>批量操作
</span><span class='line'>添加Graph Templates
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-hosts | awk '{print $1}' | while read line ; do 
</span><span class='line'>&gt;  php cli/add_graphs.php --host-id=$line --graph-type=cg --graph-template-id=41
</span><span class='line'>&gt;  php cli/add_graphs.php --host-id=$line --graph-type=cg --graph-template-id=40
</span><span class='line'>&gt;  php cli/add_graphs.php --host-id=$line --graph-type=cg --graph-template-id=39
</span><span class='line'>&gt;  php cli/add_graphs.php --host-id=$line --graph-type=cg --graph-template-id=38
</span><span class='line'>&gt; done
</span><span class='line'>
</span><span class='line'>添加Data Query。比较复杂点，需要查询匹配
</span><span class='line'>php cli/add_graphs.php --list-hosts | awk '{print $1}' | while read line ; do 
</span><span class='line'>  php cli/add_graphs.php --host-id=$line --graph-type=ds  --graph-template-id=2 --snmp-query-id=1 --snmp-query-type-id=16 --snmp-field=ifIP --snmp-value=$(grep "`php cli/add_graphs.php --list-hosts | grep "^$line\s" | awk '{print $2}'`\s" /etc/hosts | awk '{print $1}')
</span><span class='line'>done</span></code></pre></td></tr></table></div></figure>


<h2>其他命令</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>dmesg |grep eth0
</span><span class='line'>
</span><span class='line'>iftop –i eth0 –B
</span><span class='line'>
</span><span class='line'>sar -n DEV 1 100 
</span><span class='line'>
</span><span class='line'>ethtool eth0
</span><span class='line'>
</span><span class='line'>[omc@cu-omc1 ~]$ sort -k 2 /tmp/cacti.list &gt; /tmp/cacti.sort.list
</span><span class='line'>[omc@cu-omc1 ~]$ grep hadoop /etc/hosts | sort -k 2 | join -j 2 - /tmp/cacti.sort.list </span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.educity.cn/net/1619986.html">http://www.educity.cn/net/1619986.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nagios监控主机]]></title>
    <link href="http://winseliu.com/blog/2015/09/25/nagios-start-guide/"/>
    <updated>2015-09-25T17:40:58+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/25/nagios-start-guide</id>
    <content type="html"><![CDATA[<p>和Cacti查看时间序列图形不同，Nagios更多的是状态的预警。</p>

<ul>
<li>下载应用</li>
</ul>


<p>到<a href="http://sourceforge.net/projects/nagios/files/?source=navbar">sourceforge</a>下面最新版的应用。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.6 (Final)
</span><span class='line'>[root@cu2 nagios]# ll
</span><span class='line'>-rw-r--r--  1 root root 11206656 Sep 23 12:47 nagios-4.1.1.tar.gz
</span><span class='line'>-rw-r--r--  1 root root  2677352 Sep 23 12:47 nagios-plugins-2.1.1.tar.gz
</span><span class='line'>-rw-r--r--  1 root root   419695 Sep 23 15:18 nrpe-2.15.tar.gz</span></code></pre></td></tr></table></div></figure>


<ul>
<li>新增nagios用户</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# useradd nagios
</span><span class='line'>[root@cu2 nagios]# groupadd nagcmd
</span><span class='line'>[root@cu2 nagios]# usermod -G nagcmd nagios
</span><span class='line'>[root@cu2 nagios]# usermod -G nagcmd apache 
</span><span class='line'># 如果已经安装了httpd，查看下是哪个用户进程，把该用户加入到nagcmd组。修改后需要重启httpd</span></code></pre></td></tr></table></div></figure>


<ul>
<li>编译Nagios服务端</li>
</ul>


<p>由于前面安装Cacti已经把依赖都安装了，如gcc、gd、httpd、php php-devel php-mysql php-pear php-common php-gd php-mbstring php-cli。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# cd nagios-4.1.1/
</span><span class='line'>[root@cu2 nagios-4.1.1]# ./configure --with-command-group=nagcmd
</span><span class='line'>...
</span><span class='line'>Creating sample config files in sample-config/ ...
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>*** Configuration summary for nagios 4.1.1 08-19-2015 ***:
</span><span class='line'>
</span><span class='line'> General Options:
</span><span class='line'> -------------------------
</span><span class='line'>        Nagios executable:  nagios
</span><span class='line'>        Nagios user/group:  nagios,nagios
</span><span class='line'>       Command user/group:  nagios,nagcmd
</span><span class='line'>             Event Broker:  yes
</span><span class='line'>        Install ${prefix}:  /usr/local/nagios
</span><span class='line'>    Install ${includedir}:  /usr/local/nagios/include/nagios
</span><span class='line'>                Lock file:  ${prefix}/var/nagios.lock
</span><span class='line'>   Check result directory:  ${prefix}/var/spool/checkresults
</span><span class='line'>           Init directory:  /etc/rc.d/init.d
</span><span class='line'>  Apache conf.d directory:  /etc/httpd/conf.d
</span><span class='line'>             Mail program:  /bin/mail
</span><span class='line'>                  Host OS:  linux-gnu
</span><span class='line'>          IOBroker Method:  epoll
</span><span class='line'>
</span><span class='line'> Web Interface Options:
</span><span class='line'> ------------------------
</span><span class='line'>                 HTML URL:  http://localhost/nagios/
</span><span class='line'>                  CGI URL:  http://localhost/nagios/cgi-bin/
</span><span class='line'> Traceroute (used by WAP):  
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Review the options above for accuracy.  If they look okay,
</span><span class='line'>type 'make all' to compile the main program and CGIs.
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make all
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>*** Compile finished ***
</span><span class='line'>
</span><span class='line'>If the main program and CGIs compiled without any errors, you
</span><span class='line'>can continue with installing Nagios as follows (type 'make'
</span><span class='line'>without any arguments for a list of all possible options):
</span><span class='line'>
</span><span class='line'>  make install
</span><span class='line'>     - This installs the main program, CGIs, and HTML files
</span><span class='line'>
</span><span class='line'>  make install-init
</span><span class='line'>     - This installs the init script in /etc/rc.d/init.d
</span><span class='line'>
</span><span class='line'>  make install-commandmode
</span><span class='line'>     - This installs and configures permissions on the
</span><span class='line'>       directory for holding the external command file
</span><span class='line'>
</span><span class='line'>  make install-config
</span><span class='line'>     - This installs *SAMPLE* config files in /usr/local/nagios/etc
</span><span class='line'>       You'll have to modify these sample files before you can
</span><span class='line'>       use Nagios.  Read the HTML documentation for more info
</span><span class='line'>       on doing this.  Pay particular attention to the docs on
</span><span class='line'>       object configuration files, as they determine what/how
</span><span class='line'>       things get monitored!
</span><span class='line'>
</span><span class='line'>  make install-webconf
</span><span class='line'>     - This installs the Apache config file for the Nagios
</span><span class='line'>       web interface
</span><span class='line'>
</span><span class='line'>  make install-exfoliation
</span><span class='line'>     - This installs the Exfoliation theme for the Nagios
</span><span class='line'>       web interface
</span><span class='line'>
</span><span class='line'>  make install-classicui
</span><span class='line'>     - This installs the classic theme for the Nagios
</span><span class='line'>       web interface
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>*** Support Notes *******************************************
</span><span class='line'>
</span><span class='line'>If you have questions about configuring or running Nagios,
</span><span class='line'>please make sure that you:
</span><span class='line'>
</span><span class='line'>     - Look at the sample config files
</span><span class='line'>     - Read the documentation on the Nagios Library at:
</span><span class='line'>           https://library.nagios.com
</span><span class='line'>
</span><span class='line'>before you post a question to one of the mailing lists.
</span><span class='line'>Also make sure to include pertinent information that could
</span><span class='line'>help others help you.  This might include:
</span><span class='line'>
</span><span class='line'>     - What version of Nagios you are using
</span><span class='line'>     - What version of the plugins you are using
</span><span class='line'>     - Relevant snippets from your config files
</span><span class='line'>     - Relevant error messages from the Nagios log file
</span><span class='line'>
</span><span class='line'>For more information on obtaining support for Nagios, visit:
</span><span class='line'>
</span><span class='line'>       https://support.nagios.com
</span><span class='line'>
</span><span class='line'>*************************************************************
</span><span class='line'>
</span><span class='line'>Enjoy.
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make install
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>*** Main program, CGIs and HTML files installed ***
</span><span class='line'>
</span><span class='line'>You can continue with installing Nagios as follows (type 'make'
</span><span class='line'>without any arguments for a list of all possible options):
</span><span class='line'>
</span><span class='line'>  make install-init
</span><span class='line'>     - This installs the init script in /etc/rc.d/init.d
</span><span class='line'>
</span><span class='line'>  make install-commandmode
</span><span class='line'>     - This installs and configures permissions on the
</span><span class='line'>       directory for holding the external command file
</span><span class='line'>
</span><span class='line'>  make install-config
</span><span class='line'>     - This installs sample config files in /usr/local/nagios/etc
</span><span class='line'>
</span><span class='line'>make[1]: Leaving directory `/data/nagios/nagios-4.1.1'
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make install-init
</span><span class='line'>/usr/bin/install -c -m 755 -d -o root -g root /etc/rc.d/init.d
</span><span class='line'>/usr/bin/install -c -m 755 -o root -g root daemon-init /etc/rc.d/init.d/nagios
</span><span class='line'>
</span><span class='line'>*** Init script installed ***
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make install-config
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>*** Config files installed ***
</span><span class='line'>
</span><span class='line'>Remember, these are *SAMPLE* config files.  You'll need to read
</span><span class='line'>the documentation for more information on how to actually define
</span><span class='line'>services, hosts, etc. to fit your particular needs.
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make install-commandmode
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagcmd -d /usr/local/nagios/var/rw
</span><span class='line'>chmod g+s /usr/local/nagios/var/rw
</span><span class='line'>
</span><span class='line'>*** External command directory configured ***
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# vi /usr/local/nagios/etc/objects/contacts.cfg 
</span><span class='line'>...修改define contact定义中的mail为你邮件。
</span><span class='line'>define contact{
</span><span class='line'>        contact_name                    nagiosadmin             ; Short name of user
</span><span class='line'>        use                             generic-contact         ; Inherit default values from generic-contact template (defined above)
</span><span class='line'>        alias                           Nagios Admin            ; Full name of user
</span><span class='line'>
</span><span class='line'>        email                           1234@XXX.cn      ; &lt;&lt;***** CHANGE THIS TO YOUR EMAIL ADDRESS ******
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>      
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make install-webconf
</span><span class='line'>/usr/bin/install -c -m 644 sample-config/httpd.conf /etc/httpd/conf.d/nagios.conf
</span><span class='line'>
</span><span class='line'>*** Nagios/Apache conf file installed ***
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# htpasswd -c /usr/local/nagios/etc/htpasswd.users nagiosadmin
</span><span class='line'>New password: 
</span><span class='line'>Re-type new password: 
</span><span class='line'>Adding password for user nagiosadmin
</span><span class='line'>#[root@cu-omc1 nagios-4.1.1]# chmod 644 /usr/local/nagios/etc/htpasswd.users 
</span><span class='line'>[root@cu2 nagios-4.1.1]# service httpd restart
</span><span class='line'>Stopping httpd:                                            [  OK  ]
</span><span class='line'>Starting httpd: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.0.214 for ServerName
</span><span class='line'>                                                           [  OK  ]
</span><span class='line'>                                             </span></code></pre></td></tr></table></div></figure>


<ul>
<li>编译nagios-plugin</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>如果configure没有mysql和ssl，先安装依赖
</span><span class='line'>yum -y install mysql-devel openssl
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# cd nagios-plugins-2.1.1/
</span><span class='line'>[root@cu2 nagios-plugins-2.1.1]# ./configure --with-nagios-user=nagios --with-nagios-group=nagios
</span><span class='line'>...
</span><span class='line'>            --with-apt-get-command: 
</span><span class='line'>              --with-ping6-command: /bin/ping6 -n -U -w %d -c %d %s
</span><span class='line'>               --with-ping-command: /bin/ping -n -U -w %d -c %d %s
</span><span class='line'>                       --with-ipv6: yes
</span><span class='line'>                      --with-mysql: /usr/bin/mysql_config
</span><span class='line'>                    --with-openssl: yes
</span><span class='line'>                     --with-gnutls: no
</span><span class='line'>               --enable-extra-opts: yes
</span><span class='line'>                       --with-perl: /usr/bin/perl
</span><span class='line'>             --enable-perl-modules: no
</span><span class='line'>                     --with-cgiurl: /nagios/cgi-bin
</span><span class='line'>               --with-trusted-path: /usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
</span><span class='line'>                   --enable-libtap: no
</span><span class='line'>[root@cu2 nagios-plugins-2.1.1]# make 
</span><span class='line'>[root@cu2 nagios-plugins-2.1.1]# make install</span></code></pre></td></tr></table></div></figure>


<p>打开浏览器，使用nagiosadmin和刚刚用htpasswd设置的密码登录就可以localhost的状态了。</p>

<p>手动制造一点异常，如登录用户超过50个。然后看刚刚设置的邮箱是否收到邮件提醒。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# service nagios status
</span><span class='line'>nagios (pid 53764) is running...
</span><span class='line'>[root@cu2 nagios]# lsof -p 53764
</span><span class='line'>nagios  53764 nagios    4u   REG                8,3    12715   280762 /usr/local/nagios/var/nagios.log
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# less /usr/local/nagios/var/nagios.log
</span><span class='line'>看看是否有错误，然后做相应的处理
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# yum install mail -y</span></code></pre></td></tr></table></div></figure>


<ul>
<li>被监控机器安装nrpe</li>
</ul>


<p>程序编译都在cu2上面操作，编译完后，把编译安装的程序直接scp到其他机器就可以了。被监控机器需要nagios-plugin和nrpe。</p>

<p>编译之前可以先看看nrpe-2.15/docs/NRPE.pdf，讲的很详细和清楚。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios-plugins-2.1.1]# cd ../nrpe-2.15
</span><span class='line'>[root@cu2 nrpe-2.15]# ./configure 
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>*** Configuration summary for nrpe 2.15 09-06-2013 ***:
</span><span class='line'>
</span><span class='line'> General Options:
</span><span class='line'> -------------------------
</span><span class='line'> NRPE port:    5666
</span><span class='line'> NRPE user:    nagios
</span><span class='line'> NRPE group:   nagios
</span><span class='line'> Nagios user:  nagios
</span><span class='line'> Nagios group: nagios
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Review the options above for accuracy.  If they look okay,
</span><span class='line'>type 'make all' to compile the NRPE daemon and client.
</span><span class='line'>
</span><span class='line'>[root@cu2 nrpe-2.15]# make all
</span><span class='line'>cd ./src/; make ; cd ..
</span><span class='line'>make[1]: Entering directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>gcc -g -O2 -I/usr/include/openssl -I/usr/include -DHAVE_CONFIG_H -I ../include -I ./../include -o nrpe ./nrpe.c ./utils.c ./acl.c -L/usr/lib64  -lssl -lcrypto -lnsl -lwrap  
</span><span class='line'>gcc -g -O2 -I/usr/include/openssl -I/usr/include -DHAVE_CONFIG_H -I ../include -I ./../include -o check_nrpe ./check_nrpe.c ./utils.c -L/usr/lib64  -lssl -lcrypto -lnsl 
</span><span class='line'>make[1]: Leaving directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>
</span><span class='line'>*** Compile finished ***
</span><span class='line'>
</span><span class='line'>If the NRPE daemon and client compiled without any errors, you
</span><span class='line'>can continue with the installation or upgrade process.
</span><span class='line'>
</span><span class='line'>Read the PDF documentation (NRPE.pdf) for information on the next
</span><span class='line'>steps you should take to complete the installation or upgrade.
</span><span class='line'>
</span><span class='line'>[root@cu2 nrpe-2.15]# make install-plugin
</span><span class='line'>cd ./src/ && make install-plugin
</span><span class='line'>make[1]: Entering directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagios -d /usr/local/nagios/libexec
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagios check_nrpe /usr/local/nagios/libexec
</span><span class='line'>make[1]: Leaving directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>[root@cu2 nrpe-2.15]# make install-daemon
</span><span class='line'>cd ./src/ && make install-daemon
</span><span class='line'>make[1]: Entering directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagios -d /usr/local/nagios/bin
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagios nrpe /usr/local/nagios/bin
</span><span class='line'>make[1]: Leaving directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>[root@cu2 nrpe-2.15]# make install-daemon-config
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagios -d /usr/local/nagios/etc
</span><span class='line'>/usr/bin/install -c -m 644 -o nagios -g nagios sample-config/nrpe.cfg /usr/local/nagios/etc
</span><span class='line'>
</span><span class='line'># 启动nrpe服务
</span><span class='line'>[root@cu2 nrpe-2.15]#  /usr/local/nagios/bin/nrpe -c /usr/local/nagios/etc/nrpe.cfg -d
</span><span class='line'>
</span><span class='line'>[root@cu2 nrpe-2.15]# /usr/local/nagios/libexec/check_nrpe -H localhost
</span><span class='line'>CHECK_NRPE: Error - Could not complete SSL handshake.
</span><span class='line'>[root@cu2 nrpe-2.15]# /usr/local/nagios/libexec/check_nrpe -H 127.0.0.1
</span><span class='line'>NRPE v2.15</span></code></pre></td></tr></table></div></figure>


<p>注意： 如果需要启用传递参数功能需要添加参数<code>--enable-command-args</code>，同时修改配置<code>dont_blame_nrpe=1</code>。<a href="http://www.cppblog.com/fwxjj/archive/2011/10/28/159262.aspx">http://www.cppblog.com/fwxjj/archive/2011/10/28/159262.aspx</a></p>

<p>拷贝程序到其他机器：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 local]# scp -r nagios cu3:/usr/local/
</span><span class='line'>
</span><span class='line'>[root@cu3 ~]# cd /usr/local/nagios/
</span><span class='line'>[root@cu3 nagios]# bin/nrpe -c etc/nrpe.cfg -d
</span><span class='line'>[root@cu3 nagios]# 
</span><span class='line'>[root@cu3 nagios]# libexec/check_nrpe -H 127.0.0.1
</span><span class='line'>NRPE v2.15</span></code></pre></td></tr></table></div></figure>


<p>修改配置，添加可以访问nrpe的白名单：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu3 nagios]# vi etc/nrpe.cfg 
</span><span class='line'>...修改
</span><span class='line'>allowed_hosts=127.0.0.1,192.168.0.0/24
</span><span class='line'>...查看
</span><span class='line'>command[check_users]=/usr/local/nagios/libexec/check_users -w 5 -c 10
</span><span class='line'>command[check_load]=/usr/local/nagios/libexec/check_load -w 15,10,5 -c 30,25,20
</span><span class='line'>command[check_hda1]=/usr/local/nagios/libexec/check_disk -w 20% -c 10% -p /dev/hda1
</span><span class='line'>command[check_zombie_procs]=/usr/local/nagios/libexec/check_procs -w 5 -c 10 -s Z
</span><span class='line'>command[check_total_procs]=/usr/local/nagios/libexec/check_procs -w 150 -c 200 
</span><span class='line'>
</span><span class='line'>nrpe.cfg最下面是nrpe的command，nagios配置中会用到
</span><span class='line'>
</span><span class='line'>[root@cu3 nagios]# pkill nrpe
</span><span class='line'>[root@cu3 nagios]# bin/nrpe -c etc/nrpe.cfg -d</span></code></pre></td></tr></table></div></figure>


<p>再回到cu2，把cu3加入nagios管理列表中：</p>

<p>首先，nagios的配置入口为etc/nagios.cfg。其他配置文件都是通过cfg_file去定位的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# vi etc/nagios.cfg 
</span><span class='line'>...添加
</span><span class='line'>cfg_file=/usr/local/nagios/etc/objects/hosts.cfg
</span><span class='line'>cfg_file=/usr/local/nagios/etc/objects/localhost.cfg
</span><span class='line'>cfg_file=/usr/local/nagios/etc/objects/cu3.cfg
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# vi etc/objects/commands.cfg 
</span><span class='line'>...添加
</span><span class='line'># 'check_nrpe' command definition
</span><span class='line'>define command{
</span><span class='line'>        command_name check_nrpe
</span><span class='line'>        command_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# vi etc/objects/cu3.cfg 
</span><span class='line'>...新增
</span><span class='line'>
</span><span class='line'>define host{
</span><span class='line'>        use                     linux-server            ; Name of host template to use
</span><span class='line'>                                                        ; This host definition will inherit all variables that are defined
</span><span class='line'>                                                        ; in (or inherited by) the linux-server host template definition.
</span><span class='line'>        host_name               cu3
</span><span class='line'>        alias                   cu3
</span><span class='line'>        address                 192.168.0.148
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu3
</span><span class='line'>        service_description             PING
</span><span class='line'>        check_command                   check_ping!100.0,20%!500.0,60%
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu3
</span><span class='line'>        service_description             Root Partition
</span><span class='line'>        check_command                   check_nrpe!check_hda1
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu3
</span><span class='line'>        service_description             Current Users
</span><span class='line'>        check_command                   check_nrpe!check_users
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu3
</span><span class='line'>        service_description             Total Processes
</span><span class='line'>        check_command                   check_nrpe!check_total_procs
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu3
</span><span class='line'>        service_description             Current Load
</span><span class='line'>        check_command                   check_nrpe!check_load
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# vi etc/objects/hosts.cfg 
</span><span class='line'>... 新增
</span><span class='line'>define hostgroup{
</span><span class='line'>        hostgroup_name  linux-servers ; The name of the hostgroup
</span><span class='line'>        alias           Linux Servers ; Long name of the group
</span><span class='line'>        members         localhost,cu3,cu4,cu5,cu1     ; Comma separated list of hosts that belong to this group
</span><span class='line'>        }
</span></code></pre></td></tr></table></div></figure>


<p>配置完后，校验配置，然后重启nagios。然后就可以打开浏览器查看cu3状态。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# bin/nagios -v etc/nagios.cfg 
</span><span class='line'>
</span><span class='line'>Nagios Core 4.1.1
</span><span class='line'>Copyright (c) 2009-present Nagios Core Development Team and Community Contributors
</span><span class='line'>Copyright (c) 1999-2009 Ethan Galstad
</span><span class='line'>Last Modified: 08-19-2015
</span><span class='line'>License: GPL
</span><span class='line'>
</span><span class='line'>Website: https://www.nagios.org
</span><span class='line'>Reading configuration data...
</span><span class='line'>   Read main config file okay...
</span><span class='line'>   Read object config files okay...
</span><span class='line'>
</span><span class='line'>Running pre-flight check on configuration data...
</span><span class='line'>
</span><span class='line'>Checking objects...
</span><span class='line'>        Checked 23 services.
</span><span class='line'>        Checked 4 hosts.
</span><span class='line'>        Checked 1 host groups.
</span><span class='line'>        Checked 0 service groups.
</span><span class='line'>        Checked 1 contacts.
</span><span class='line'>        Checked 1 contact groups.
</span><span class='line'>        Checked 25 commands.
</span><span class='line'>        Checked 5 time periods.
</span><span class='line'>        Checked 0 host escalations.
</span><span class='line'>        Checked 0 service escalations.
</span><span class='line'>Checking for circular paths...
</span><span class='line'>        Checked 4 hosts
</span><span class='line'>        Checked 0 service dependencies
</span><span class='line'>        Checked 0 host dependencies
</span><span class='line'>        Checked 5 timeperiods
</span><span class='line'>Checking global event handlers...
</span><span class='line'>Checking obsessive compulsive processor commands...
</span><span class='line'>Checking misc settings...
</span><span class='line'>
</span><span class='line'>Total Warnings: 0
</span><span class='line'>Total Errors:   0
</span><span class='line'>
</span><span class='line'>Things look okay - No serious problems were detected during the pre-flight check
</span><span class='line'>[root@cu2 nagios]# service nagios restart
</span><span class='line'>Running configuration check...
</span><span class='line'>Stopping nagios: done.
</span><span class='line'>Starting nagios: done.</span></code></pre></td></tr></table></div></figure>


<h2>后记</h2>

<p>技巧：配置服务的时刻，制定host_name可以使用正则表达式，一个服务通吃。对于功能类似的机器，可以减少很多工作量：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>vi etc/nagios.cfg 
</span><span class='line'>... 修改
</span><span class='line'>use_regexp_matching=1
</span><span class='line'>use_true_regexp_matching=1
</span><span class='line'>
</span><span class='line'>vi cu.cfg
</span><span class='line'>... 
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu.*
</span><span class='line'>        service_description             Current Load
</span><span class='line'>        check_command                   check_nrpe!check_load
</span><span class='line'>        }</span></code></pre></td></tr></table></div></figure>


<p>基本功能就算配置好了，如果出现异常就能得到邮件提醒。</p>

<h2>后后后记(2015-12-7 21:42:03)</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxvf nagios-plugins-2.1.1.tar.gz 
</span><span class='line'>tar zxvf nrpe-2.15.tar.gz 
</span><span class='line'>
</span><span class='line'>ps axu|grep httpd
</span><span class='line'>useradd nagios
</span><span class='line'>groupadd nagcmd
</span><span class='line'>usermod -G nagcmd nagios
</span><span class='line'>usermod -G nagcmd apache 
</span><span class='line'>
</span><span class='line'>cd nagios-4.1.1/
</span><span class='line'>yum -y install mysql-devel openssl
</span><span class='line'>cd nagios-plugins-2.1.1/
</span><span class='line'>./configure --with-nagios-user=nagios --with-nagios-group=nagios
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>cd ../nrpe-2.15
</span><span class='line'>./configure 
</span><span class='line'>make all
</span><span class='line'>make install-plugin
</span><span class='line'>make install-daemon
</span><span class='line'>make install-daemon-config
</span><span class='line'>
</span><span class='line'>cd /usr/local/
</span><span class='line'>for h in `cat /etc/hosts | grep hadoop | awk '{print $2}'` ; do rsync -vaz nagios root@$h:/usr/local/  ; done 
</span><span class='line'>
</span><span class='line'>rsync --dry-run -vaz nagios nagios-client &gt; nagios-client.list
</span><span class='line'>rsync --dry-run --include-from=nagios-client.list --exclude=* -vaz nagios nagios-clint
</span><span class='line'>
</span><span class='line'>for h in `cat /etc/hosts | grep "-" | grep -v omc1 | awk '{print $2}'` ; do rsync  --include-from=nagios-client.list --exclude=* -vaz nagios $h:/usr/local/  ; done 
</span><span class='line'>
</span><span class='line'>for h in `cat /etc/hosts | grep "-" | grep -v omc1 | awk '{print $2}'` ; do ssh $h "pkill nrpe; cd /usr/local/nagios ; bin/nrpe -c etc/nrpe.cfg -d"  ; done
</span><span class='line'>for h in `cat /etc/hosts | grep "-" | grep -v omc1 | awk '{print $2}'` ; do echo $h; ssh $h "if ! ps aux|grep nrpe | grep -v grep  | grep nrpe ; then cd /usr/local/nagios ; bin/nrpe -c etc/nrpe.cfg -d ; fi"   ; done
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>cd 
</span><span class='line'>tar zxvf nagios-4.1.1.tar.gz 
</span><span class='line'>cd nagios-4.1.1
</span><span class='line'>./configure --with-command-group=nagcmd
</span><span class='line'>make all
</span><span class='line'>make install
</span><span class='line'> make install-init
</span><span class='line'>make install-config
</span><span class='line'>make install-commandmode
</span><span class='line'>make install-webconf
</span><span class='line'>htpasswd -c /usr/local/nagios/etc/htpasswd.users nagiosadmin
</span><span class='line'>chmod 644 /usr/local/nagios/etc/htpasswd.users 
</span><span class='line'>service httpd restart
</span><span class='line'>service nagios start
</span><span class='line'>
</span><span class='line'>##======================================================================
</span><span class='line'>
</span><span class='line'>修改 vi nagios.cfg
</span><span class='line'>
</span><span class='line'>  cfg_file=/usr/local/nagios/etc/objects/hosts.cfg
</span><span class='line'>  cfg_file=/usr/local/nagios/etc/objects/cu.cfg
</span><span class='line'>
</span><span class='line'>  use_regexp_matching=1
</span><span class='line'>
</span><span class='line'>添加 for h in `cat /etc/hosts | grep "-" | grep -v omc1 | awk '{print $2}'` ; do echo "
</span><span class='line'>&gt; define host {
</span><span class='line'>&gt; use linux-server
</span><span class='line'>&gt; host_name HN-${h#*-}
</span><span class='line'>&gt; alias HN-${h#*-}
</span><span class='line'>&gt; address $h
</span><span class='line'>&gt; }
</span><span class='line'>&gt; " ; done &gt;hosts.cfg
</span><span class='line'>
</span><span class='line'>define hostgroup{
</span><span class='line'>        hostgroup_name  cu servers
</span><span class='line'>        alias           cu servers
</span><span class='line'>        members         HN-omc*, HN-uc*, HN-ud*, HN-db*
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>define hostgroup{
</span><span class='line'>        hostgroup_name  hadoop slavers
</span><span class='line'>        alias           hadoop slavers
</span><span class='line'>        members         HN-slaver*, HN-master*
</span><span class='line'>        }
</span><span class='line'>      
</span><span class='line'>添加 vi commands.cfg    
</span><span class='line'>
</span><span class='line'># 'check_nrpe' command definition
</span><span class='line'>define command{
</span><span class='line'>        command_name check_nrpe
</span><span class='line'>        command_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>编辑 vi /etc/mail.rc 
</span><span class='line'>
</span><span class='line'>set from=eshore_notify@189.cn
</span><span class='line'>set smtp=smtp.189.cn
</span><span class='line'>set smtp-auth-user=XXX
</span><span class='line'>set smtp-auth-password=XXX
</span><span class='line'>set smtp-auth=login
</span><span class='line'>
</span><span class='line'>echo test  | /bin/mail -s "** Service Alert **" XXX@189.cn
</span><span class='line'>
</span><span class='line'>##-----------------------------------
</span><span class='line'>[root@dr01 ~]# umount /root/.gvfs
</span><span class='line'>
</span><span class='line'>command_line  /usr/lib/nagios/plugins/check_disk -w $ARG1$ -c $ARG2$  -u GB -A -i .gvfs
</span><span class='line'>   </span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://nagios-cn.sourceforge.net/nagios-cn/beginning.html#quickstart-fedora">http://nagios-cn.sourceforge.net/nagios-cn/beginning.html#quickstart-fedora</a></li>
<li><a href="http://skypegnu1.blog.51cto.com/8991766/1532948">http://skypegnu1.blog.51cto.com/8991766/1532948</a></li>
<li><a href="http://nagios-cn.sourceforge.net/nagios-cn/beginning.html#monitoring-linux">http://nagios-cn.sourceforge.net/nagios-cn/beginning.html#monitoring-linux</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cacti监控主机]]></title>
    <link href="http://winseliu.com/blog/2015/09/22/cacti-start-guide/"/>
    <updated>2015-09-22T14:33:36+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/22/cacti-start-guide</id>
    <content type="html"><![CDATA[<p>其实通过yum好依赖的php、rrdtool、snmp后，安装配置Cacti其实很简单。</p>

<h2>环境说明</h2>

<p>五台机器：cu1~cu5(centos6.6)， 其中仅cu2作为cacti服务器，所有服务器都安装snmp服务。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cu1     192.168.0.37 
</span><span class='line'>cu2     192.168.0.214 
</span><span class='line'>cu3     192.168.0.148 
</span><span class='line'>cu4     192.168.0.30 
</span><span class='line'>cu5     192.168.0.174 </span></code></pre></td></tr></table></div></figure>


<h2>软件安装</h2>

<p>版本信息在贴的内容中体现。PHP不会，仅仅作为一个工具来使用。</p>

<h3>Cacti服务器机器安装</h3>

<p>mysql数据库5.1</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# rpm -q mysql 
</span><span class='line'>mysql-5.1.73-5.el6_6.x86_64</span></code></pre></td></tr></table></div></figure>


<p>首先用yum安装依赖软件php，httpd，snmp和<strong>rrdtool</strong>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# yum install epel-release
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# yum install httpd php php-devel php-mysql php-pear php-common php-gd php-mbstring php-cli php-snmp net-snmp net-snmp-utils net-snmp-libs rrdtool 
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>Installed:
</span><span class='line'>  net-snmp-libs.x86_64 1:5.5-54.el6_7.1 net-snmp-utils.x86_64 1:5.5-54.el6_7.1 php.x86_64 0:5.3.3-46.el6_6          php-cli.x86_64 0:5.3.3-46.el6_6   php-common.x86_64 0:5.3.3-46.el6_6
</span><span class='line'>  php-devel.x86_64 0:5.3.3-46.el6_6     php-gd.x86_64 0:5.3.3-46.el6_6         php-mbstring.x86_64 0:5.3.3-46.el6_6 php-mysql.x86_64 0:5.3.3-46.el6_6 php-pear.noarch 1:1.9.4-4.el6     
</span><span class='line'>  php-snmp.x86_64 0:5.3.3-46.el6_6      rrdtool.x86_64 0:1.3.8-7.el6          
</span><span class='line'>
</span><span class='line'>Dependency Installed:
</span><span class='line'>  autoconf.noarch 0:2.63-5.1.el6               automake.noarch 0:1.11.1-4.el6                  dejavu-fonts-common.noarch 0:2.33-1.el6   dejavu-lgc-sans-mono-fonts.noarch 0:2.33-1.el6  
</span><span class='line'>  dejavu-sans-mono-fonts.noarch 0:2.33-1.el6   fontpackages-filesystem.noarch 0:1.41-1.1.el6   lm_sensors-libs.x86_64 0:3.1.1-17.el6     net-snmp.x86_64 1:5.5-54.el6_7.1                
</span><span class='line'>  php-pdo.x86_64 0:5.3.3-46.el6_6             
</span><span class='line'>
</span><span class='line'>Updated:
</span><span class='line'>  httpd.x86_64 0:2.2.15-47.el6.centos                                                                                                                                                       
</span><span class='line'>
</span><span class='line'>Dependency Updated:
</span><span class='line'>  httpd-tools.x86_64 0:2.2.15-47.el6.centos                                                                                                                                                 
</span><span class='line'>
</span><span class='line'>Complete!
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# service httpd start
</span><span class='line'>Starting httpd: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.0.214 for ServerName
</span><span class='line'>                                                           [  OK  ]
</span><span class='line'>[root@cu2 ~]# service snmpd start
</span><span class='line'>Starting snmpd:                                            [  OK  ]
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# vi /etc/snmp/snmpd.conf 
</span><span class='line'>     41 #com2sec notConfigUser  default       public
</span><span class='line'>     42 com2sec notConfigUser  192.168.0.214       public
</span><span class='line'>   ...
</span><span class='line'>     63 #access  notConfigGroup ""      any       noauth    exact  systemview none none
</span><span class='line'>     64 access  notConfigGroup ""      any       noauth    exact  all none none
</span><span class='line'>   ...
</span><span class='line'>     86 ##           incl/excl subtree                          mask
</span><span class='line'>     87 view all    included  .1                               80  
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# service snmpd restart
</span><span class='line'>
</span><span class='line'># 使用snmpwalk可以得到数据
</span><span class='line'>[root@cu2 ~]# snmpwalk -Os -c public -v 1 cu2 system
</span><span class='line'>[root@cu2 ~]# snmpwalk -v 1 -c public localhost IP-MIB::ipAdEntIfIndex</span></code></pre></td></tr></table></div></figure>


<p>然后，把Cacti应用解压到httpd默认目录下/var/www/html。同时配置cacti连接到数据库。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# cd /var/www/html/
</span><span class='line'>[root@cu2 html]# tar zxvf cacti-0.8.8f.tar.gz 
</span><span class='line'>
</span><span class='line'>[root@cu2 html]# ln -s cacti-0.8.8f cacti
</span><span class='line'>
</span><span class='line'>[root@cu2 html]$ mysql -u root -p -h 127.0.0.1
</span><span class='line'>Enter password: 
</span><span class='line'>mysql&gt; 
</span><span class='line'>mysql&gt; create database cacti character set UTF8;
</span><span class='line'>mysql&gt; grant all on cacti.* to cacti@'%' identified by 'cacti';
</span><span class='line'>mysql&gt; flush privileges;
</span><span class='line'>mysql&gt; source /var/www/html/cacti/cacti.sql;
</span><span class='line'>
</span><span class='line'>[root@cu2 html]# vi cacti/include/config.php 
</span><span class='line'>$database_type = "mysql";
</span><span class='line'>$database_default = "cacti";
</span><span class='line'>$database_hostname = "127.0.0.1";
</span><span class='line'>$database_username = "cacti";
</span><span class='line'>$database_password = "cacti";
</span><span class='line'>$database_port = "3306";
</span><span class='line'>
</span><span class='line'>[root@cu2 html]$ vi /etc/php.ini 
</span><span class='line'>date.timezone = "Asia/Shanghai"
</span><span class='line'>
</span><span class='line'># 重启httpd服务
</span><span class='line'>
</span><span class='line'>[root@cu2 cacti]# php poller.php 
</span><span class='line'>
</span><span class='line'>[root@cu2 cacti]# crontab -e
</span><span class='line'>* * * * * php /var/www/html/cacti/poller.php &gt; /var/www/html/cacti/log/cron.log 2&gt;&1
</span></code></pre></td></tr></table></div></figure>


<p>打开浏览器访问：<a href="http://cu2/cacti/">http://cu2/cacti/</a> 首先会进入到install步骤，按照提示一步下一步，最后输入admin/admin登录。点击右上角的Preview View就可以看到图了。</p>

<p>如果启动错误，查看日志文件看日志：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 cacti]# less /var/log/httpd/error_log 
</span><span class='line'>[root@cu2 cacti]# less log/cacti.log </span></code></pre></td></tr></table></div></figure>


<h3>添加插件</h3>

<p>（网上很多文章都要打补丁，我这里的版本是最新的，同时plugin的补丁没有对应的版本，这里直接安装插件）</p>

<p>从<a href="http://docs.cacti.net/plugins">http://docs.cacti.net/plugins</a>下载<a href="http://docs.cacti.net/plugin:monitor">monitor</a>。把下载文件解压到plugins目录下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 plugins]# pwd
</span><span class='line'>/var/www/html/cacti/plugins
</span><span class='line'>[root@cu2 plugins]# ll
</span><span class='line'>-rw-r--r-- 1 1000 users   44 Jul 20 21:42 index.php
</span><span class='line'>drwxr-xr-x 4 root root  4096 Oct  6  2011 monitor</span></code></pre></td></tr></table></div></figure>


<p>然后进入Plugin Management页面<a href="http://cu2/cacti/plugins.php">http://cu2/cacti/plugins.php</a>，就能看到Monitor插件。点击表格Actions列的<strong>安装和启用</strong>图标（按钮），启用后，最上面页签会增加新的页签项monitor。</p>

<p>点击monitor页签，可以查看机器存活的状态。</p>

<p>同时Settings页面多了Misc选项卡，可以配置修改monitor属性。</p>

<p>注意：网上版本资料都有配置config.php添加plugins变量。我这里没进行这个操作也是ok的，安装-启用成功后会把monitor下面的sql更新到数据库，不需要手动执行。</p>

<h3>安装spine</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>注意：设置下umask避免不需要的麻烦： umask 0022
</span><span class='line'>[root@cu2 ~] tar zxvf cacti-spine-0.8.8f.tar.gz
</span><span class='line'>
</span><span class='line'>[root@cu2 cacti-spine-0.8.8f]# yum install -y mysql-devel net-snmp-devel
</span><span class='line'>
</span><span class='line'>[root@cu2 cacti-spine-0.8.8f]# ./configure --prefix=/usr/local/cacti-spine
</span><span class='line'>[root@cu2 cacti-spine-0.8.8f]# make && make install
</span><span class='line'># 如果make缺少报了错，需要重新configuration一遍
</span><span class='line'>
</span><span class='line'>[root@cu2 cacti-spine-0.8.8f]# cd /usr/local/cacti-spine/etc
</span><span class='line'>[root@cu2 etc]# mv spine.conf.dist spine.conf
</span><span class='line'>[root@cu2 etc]# vi spine.conf 
</span><span class='line'>DB_Host         127.0.0.1
</span><span class='line'>DB_Database     cacti
</span><span class='line'>DB_User         cacti
</span><span class='line'>DB_Pass         cacti
</span><span class='line'>DB_Port         3306
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>然后修改<a href="http://hadoop-master2/cacti/settings.php">Cacti</a>使用spine来获取信息。</li>
</ul>


<p>在[Settings]-[Paths]添加Spine Poller File Path为<code>/usr/local/cacti-spine/bin/spine</code>。在[Poller]选项卡，[Poller Type]修改为spine，[Poller Interval]和[Cron Interval]修改为一分钟即Every Minute。</p>

<ul>
<li>添加“每分钟”流量视图:</li>
</ul>


<p>点击Console -> Data Templates -> [Interface -> Traffic ] 添加“每分钟”流量视图，将轮询时间设置为60秒，Heartbeat时间设置为120秒(traffic_in/traffic_out里面的Heartbeat时间也设置为120秒)</p>

<h2>被监控机器配置</h2>

<p>被监控的机器，仅仅需要安装snmp即可。然后配置snpmd.conf即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu5 ~]# yum install  net-snmp net-snmp-utils net-snmp-libs -y
</span><span class='line'>[root@cu5 ~]# vi /etc/snmp/snmpd.conf 
</span><span class='line'>     41 #com2sec notConfigUser  default       public
</span><span class='line'>     42 com2sec notConfigUser  192.168.0.214       public
</span><span class='line'>   ...
</span><span class='line'>     63 #access  notConfigGroup ""      any       noauth    exact  systemview none none
</span><span class='line'>     64 access  notConfigGroup ""      any       noauth    exact  all none none
</span><span class='line'>   ...
</span><span class='line'>     86 ##           incl/excl subtree                          mask
</span><span class='line'>     87 view all    included  .1                               80  
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# service snmpd restart</span></code></pre></td></tr></table></div></figure>


<p>然后在Cacti的web页面添加Device(主机)：</p>

<ul>
<li>点击Console->Devices，打开设备管理页面。</li>
<li>点击右上角的add，添加一个新的机器</li>
<li>当主机的信息填好之后，点击Create

<ul>
<li>Host Template就是一个模板，会事先建立一些Associated Graph Templates和Associated Data Queries的数据，如Load Average，Memory Uages等。如果不确定直接选None即可。</li>
<li>SNMP Version选<code>Version 2</code>，SNMP Community与snmpd.conf中对应，如果安装上面操作，默认即可。</li>
</ul>
</li>
<li>此时你的页面左上角应该显示：Save Successful，并且已经显示出了主机信息和SNMP信息，如果SNMP信息显示 SNMP error，请查看最后的问题综述。</li>
<li>这时我们就可以添加相应的监控项了，在页面最下方的Associated Graph Templates中添加图形模板，在Associated Data Queries中添加数据模板。</li>
<li>保存，点击右上角的Create Graphs for this Host，来为刚才通过模板所获得到的数据进行画图。</li>
<li>选择好需要画图的项目后，点击右下角的Create，左上角会出现被创建出来的画图项。</li>
</ul>


<p>总结就是添加设备，然后生成图形，最后等待生成画图查看。</p>

<p>在Graphs界面左边显示树新添加主机。</p>

<ul>
<li>在Cacti界面Graph Trees中，选择进入节点(或者系统默认的Default Tree)。</li>
<li>添加一个新的显示项，在Tree Item Type中选择Host，然后在下面的Host中选择我们刚才创建的主机。点击Create。</li>
</ul>


<p><a href="http://docs.cacti.net/templates">http://docs.cacti.net/templates</a></p>

<h2>进阶</h2>

<p><a href="http://skypegnu1.blog.51cto.com/8991766/1537374">http://skypegnu1.blog.51cto.com/8991766/1537374</a></p>

<blockquote><p>cacti是如何获取数据呢？  <br/>
    其实cacti获取数据的方式是多样化的，通过周期性的执行某个脚本，或者使用snmp，更或者是ssh，这些都是根据实际需要以及方便性来抉择。cacti需要周期性的驱动这些获取数据的脚本执行，并把取得的数据保存至相应的rrd数据库中。
cacti是如何保存数据（创建rrd，并更新数据）呢？
    这就是数据模板的功能。
cacti是如何展示数据（绘图）呢？
    这就是图形模板的功能。</p></blockquote>

<p><a href="http://skypegnu1.blog.51cto.com/8991766/1537615">http://skypegnu1.blog.51cto.com/8991766/1537615</a>
<a href="http://skypegnu1.blog.51cto.com/8991766/1538459">http://skypegnu1.blog.51cto.com/8991766/1538459</a>
<a href="http://skypegnu1.blog.51cto.com/8991766/1547029">http://skypegnu1.blog.51cto.com/8991766/1547029</a></p>

<h2>资料</h2>

<p>入门的文档不错，可以到<a href="http://vdisk.weibo.com/u/1554831624">微盘</a>下载。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Cacti.0.8_Beginner.Guide.pdf
</span><span class='line'>
</span><span class='line'>Cacti实战指南--备份还原.pdf
</span><span class='line'>Cacti实战指南-完美部署.pdf
</span><span class='line'>Cacti实战指南-巧设轮询.pdf
</span><span class='line'>Cacti实战指南-插件安装.pdf
</span><span class='line'>Cacti实战指南-用户权限.pdf
</span><span class='line'>Cacti实战指南-邮件预警.pdf
</span><span class='line'>Cacti实战指南-阀值预警.pdf
</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><p>先看这个文档 <a href="http://blog.chinaunix.net/attachment/attach/21/08/97/212108972176206e1112f29600926449bdeedb3970.pdf">http://blog.chinaunix.net/attachment/attach/21/08/97/212108972176206e1112f29600926449bdeedb3970.pdf</a></p></li>
<li><p><a href="http://blog.csdn.net/chen3888015/article/details/8233125">http://blog.csdn.net/chen3888015/article/details/8233125</a></p></li>
<li><p><a href="http://www.cacti.net/downloads/docs/pdf/manual.pdf">http://www.cacti.net/downloads/docs/pdf/manual.pdf</a></p></li>
<li><p><a href="http://wenku.baidu.com/view/57aa69487fd5360cba1adb40.html?re=view">http://wenku.baidu.com/view/57aa69487fd5360cba1adb40.html?re=view</a></p></li>
<li><p><a href="http://wenku.baidu.com/view/b2d1f6c689eb172ded63b7f9.html?re=view">http://wenku.baidu.com/view/b2d1f6c689eb172ded63b7f9.html?re=view</a></p></li>
<li><p><a href="http://www.ehowstuff.com/how-to-install-and-configure-epel-repository-on-centos-6-2/">http://www.ehowstuff.com/how-to-install-and-configure-epel-repository-on-centos-6-2/</a></p></li>
<li><a href="http://www.ehowstuff.com/how-to-install-cacti-on-centos-6-2-using-epel-repository/">http://www.ehowstuff.com/how-to-install-cacti-on-centos-6-2-using-epel-repository/</a></li>
<li><a href="http://www.ehowstuff.com/how-to-setup-and-configure-cacti-on-centos-6-2/">http://www.ehowstuff.com/how-to-setup-and-configure-cacti-on-centos-6-2/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[解决SecureCRT【zmodem Transfer Canceled by Remote Side】问题]]></title>
    <link href="http://winseliu.com/blog/2015/09/21/solve-securecrt-zmodem-transfer-canceled-by-remote-side/"/>
    <updated>2015-09-21T16:02:18+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/21/solve-securecrt-zmodem-transfer-canceled-by-remote-side</id>
    <content type="html"><![CDATA[<p>处理方法很简单，解决后，对于ssh机器之间多次跳转的文件传输操作会方便很多。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>写到/etc/profile以后就可以直接使用了
</span><span class='line'>[root@af042cb4b34c ~]# alias rz="rz -e"
</span><span class='line'>
</span><span class='line'>[root@af042cb4b34c ~]# cd /usr/share/cacti/
</span><span class='line'>[root@af042cb4b34c cacti]# cd plugins
</span><span class='line'>[root@af042cb4b34c plugins]# rz
</span><span class='line'>rz waiting to receive.
</span><span class='line'>Starting zmodem transfer.  Press Ctrl+C to cancel.
</span><span class='line'>Transferring monitor-v1.3-1.tgz...
</span><span class='line'>  100%     231 KB     231 KB/sec    00:00:01       0 Errors  
</span><span class='line'>
</span><span class='line'>[root@af042cb4b34c plugins]# ll
</span><span class='line'>total 236
</span><span class='line'>-rw-r--r-- 1 root root     44 Aug  7  2013 index.php
</span><span class='line'>-rw-r--r-- 1 root root 236682 Sep 21 07:54 monitor-v1.3-1.tgz</span></code></pre></td></tr></table></div></figure>


<p>zmoden还是有比较多的限制。sftp还是不能少啊！！</p>

<h2>参考</h2>

<ul>
<li><a href="http://iamhere1.blog.163.com/blog/static/23612284201372322622902/">http://iamhere1.blog.163.com/blog/static/23612284201372322622902/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【linux 101 Hacks】读后感]]></title>
    <link href="http://winseliu.com/blog/2015/09/13/review-linux-101-hacks/"/>
    <updated>2015-09-13T13:12:53+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/13/review-linux-101-hacks</id>
    <content type="html"><![CDATA[<p>本书讲了linux维护和管理过程中常用的命令。</p>

<p>分12个章节，分别将了目录切换、日期、SSH远程登录、常用linux命令、PS1-4操作提示符、解压缩、命令历史记录、系统管理、容器服务器Apache、脚本环境变量、性能监控等。</p>

<p>介绍的命令有：cd, dirs, pushd, popd, cdpath, <strong>alias</strong>, mkdir, eval, date, hwclock, ssh, grep, find, 输出重定向, join, tr, xargs, sort, uniq, cut, stat, diff, ac, ps1, ps2, ps3, ps4, PROMPT_COMMAND, zip, unzip, tar, gzip, bzip2, HISTTIMEFORMAT, HISTSIZE, HISTIGNORE, fdisk, mke2fsk, mount, tune2fs, useradd, adduser, passwd, groupadd, id, ssh-copy-id, ssh-agent, crontab, apachectl, httpd, <strong>.bash_rc</strong>, .bash_profile, 单引号, 双引号, free, top, ps, df, kill, du, lsof, sar, vmstat, netstat, sysctl, nice, renice等等。</p>

<p>下面结合工作中的一些实践，谈一谈</p>

<h2>技巧一：登录服务器</h2>

<p>不管是正式环境还是云端的测试环境，一般提供给我们访问的只有一个入口（也就是常说的跳板机），登录跳板机后然后才能连接其他服务器。常用的工具有【SecureCRT】和【Xshell】，它们的使用方式基本相同。</p>

<p>最佳实践：连接跳板机的同时，建立自己机器和内网机器之间的隧道，即可以方便浏览器的访问，同时也可以使用sftp直接传输文件到内网机器。</p>

<p><img src="http://winseliu.com/images/blogs/linux-101-hacks-review-securecrt-config.png" alt="" /></p>

<p><img src="http://winseliu.com/images/blogs/linux-101-hacks-review-securecrt-web.png" alt="" /></p>

<h2>技巧二：ssh-copy-id【hack 72】</h2>

<p>想不通，现在的教程都使用【复制-添加-修改权限】公钥的方式来进行无密钥登录配置。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 ~]$ scp .ssh/id_rsa.pub 172.17.0.3:~/master_id_rsa.pub
</span><span class='line'>hadoop@172.17.0.3's password: 
</span><span class='line'>id_rsa.pub                                                                                                                     100%  403     0.4KB/s   00:00    
</span><span class='line'>[hadoop@hadoop-master1 ~]$ ssh 172.17.0.3
</span><span class='line'>hadoop@172.17.0.3's password: 
</span><span class='line'>Last login: Sun Sep 13 11:41:17 2015 from 172.17.0.2
</span><span class='line'>[hadoop@hadoop-slaver1 ~]$ cat master_id_rsa.pub &gt;&gt; .ssh/authorized_keys 
</span><span class='line'>[hadoop@hadoop-slaver1 ~]$ ll -d .ssh
</span><span class='line'>drwx------. 2 hadoop hadoop 4096 Mar 10  2015 .ssh
</span><span class='line'>[hadoop@hadoop-slaver1 ~]$ ll .ssh/authorized_keys 
</span><span class='line'>-rw-------. 1 hadoop hadoop 403 Sep 13 11:58 .ssh/authorized_keys</span></code></pre></td></tr></table></div></figure>


<p>处理一个ssh无密钥登录搞N多的步骤，还不一定能成功！其实使用ssh-copy-id的命令就行了，不知道各类书籍上面都使用老旧的方法，都是抄来的吗？！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver1 ~]$ ssh-copy-id -i .ssh/id_rsa.pub 172.17.0.2
</span><span class='line'>The authenticity of host '172.17.0.2 (172.17.0.2)' can't be established.
</span><span class='line'>RSA key fingerprint is aa:41:79:6d:9d:c2:ec:f1:29:71:43:24:39:09:58:b6.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added '172.17.0.2' (RSA) to the list of known hosts.
</span><span class='line'>hadoop@172.17.0.2's password: 
</span><span class='line'>Now try logging into the machine, with "ssh '172.17.0.2'", and check in:
</span><span class='line'>
</span><span class='line'>  .ssh/authorized_keys
</span><span class='line'>
</span><span class='line'>to make sure we haven't added extra keys that you weren't expecting.</span></code></pre></td></tr></table></div></figure>


<h2>技巧三：查看机器</h2>

<p>碰到不认识的人，我们都会上下打量。机器也一样，首先要了解机器，才能充分的发挥自己的性能。存储不够要么删点，要么加磁盘等等。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>uname -a
</span><span class='line'>cat /etc/redhat-release 
</span><span class='line'>ifconfig
</span><span class='line'>
</span><span class='line'>date
</span><span class='line'>
</span><span class='line'>df -h , df -Tha
</span><span class='line'>free -m 
</span><span class='line'>uptime
</span><span class='line'>top
</span><span class='line'>ps aux , ps auxf
</span><span class='line'>netstat -atp
</span><span class='line'>du -h --max-depth=1
</span><span class='line'>lsof -i:[PORT]
</span><span class='line'>
</span><span class='line'>cat /etc/hosts</span></code></pre></td></tr></table></div></figure>


<h2>技巧四：管道</h2>

<p>一个命令的结果直接输出给另一个命令。就像水从一个结头通过管子直接流向下一个结头一样。中间不需要落地，直接立即用于下一个命令，直到结果输出。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat /etc/hosts | grep 'hadoop' | awk '{print $2}' | while read h ; do echo $h ; done</span></code></pre></td></tr></table></div></figure>


<p>shell的命令那么多，简单功能的材料都准备好了，就像堆积木一样，叠加后总能实现你想得到的效果。</p>

<p>在进行一次性文件拷贝时，如果文件数量过多，可以先打包然后传到远程机器再解压：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zc nginx | ssh bigdata1 'tar zx'</span></code></pre></td></tr></table></div></figure>


<h2>技巧N：查看帮助</h2>

<p>写java的没看过开源项目不要说自己会java，写shell没用过man不要说自己会shell！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>man [CMD]
</span><span class='line'>info [CMD]
</span><span class='line'>[CMD] help
</span><span class='line'>[CMD] -h
</span><span class='line'>[CMD] -help</span></code></pre></td></tr></table></div></figure>


<p>总有一款适合你，带着实践和问题的目的去学/写，能更好的把握它。（shell的命令太多，不要寄希望于看一个宝典就能写好！实践出真知，真正用到的才是实用的）</p>

<h2>技巧N-1：调试Shell脚本</h2>

<p>Shell脚本/命令在执行前会对变量进行解析、处理。查看最终执行的命令，能让我们了解到脚本不正确的地方，然后及时进行更正。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set命令的参数说明
</span><span class='line'>-v      Print shell input lines as they are read.
</span><span class='line'>-x      After  expanding each simple command, for command, case command, select command, or arithmetic for command, 
</span><span class='line'>display the expanded value of PS4, followed by the command and its expanded arguments or associated word list.
</span><span class='line'>    
</span><span class='line'>[hadoop@hadoop-master2 ~]$ set -x
</span><span class='line'>[hadoop@hadoop-master2 1]$ cmd=*
</span><span class='line'>+ cmd='*'
</span><span class='line'>[hadoop@hadoop-master2 1]$ echo $cmd
</span><span class='line'>+ echo 2 file
</span><span class='line'>2 file
</span><span class='line'>[hadoop@hadoop-master2 1]$ echo "$cmd" # 双引号
</span><span class='line'>+ echo '*'
</span><span class='line'>*
</span><span class='line'>[hadoop@hadoop-master2 1]# echo '$cmd' # 单引号
</span><span class='line'>+ echo '$cmd'
</span><span class='line'>$cmd</span></code></pre></td></tr></table></div></figure>


<p>调试脚本</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 1]# vi run.sh
</span><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'>bin=$(dir=`dirname $0`; cd $dir ; pwd)
</span><span class='line'>
</span><span class='line'>cd $bin
</span><span class='line'>ls -l
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 1]# sh -x run.sh 
</span><span class='line'>+++ dirname run.sh
</span><span class='line'>++ dir=.
</span><span class='line'>++ cd .
</span><span class='line'>++ pwd
</span><span class='line'>+ bin=/tmp/1
</span><span class='line'>+ cd /tmp/1
</span><span class='line'>+ ls -l
</span><span class='line'>total 8
</span><span class='line'>drwxrwxr-x 2 hadoop hadoop 4096 Sep 13 20:33 2
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop    0 Sep 13 20:33 file
</span><span class='line'>-rw-r--r-- 1 root   root     66 Sep 13 21:12 run.sh</span></code></pre></td></tr></table></div></figure>


<h2>技巧N-2：历史history</h2>

<p>历史如足迹。如果你要学习前辈的经验，理着他的足迹，一步步的走！</p>

<p>很多书上说的，<code>CTRL+R, !!, !-1, CTRL+P, ![CMD], !!:$, !^, ![CMD]:2, ![CMD]:$</code>用于获取以后执行的命令或者参数，多半好看不实用。会写的也就前面1-2个命令重复用一下，上下方向键就可以了，不会写的用history查看全部慢慢学更实际点。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>历史记录执行时间
</span><span class='line'>export HISTTIMEFORMAT='%F %T '
</span><span class='line'>输出最近10条历史
</span><span class='line'>alias hist10='history 10'
</span><span class='line'>
</span><span class='line'>持久化保存的历史记录数
</span><span class='line'>vi ~/.bash_profile
</span><span class='line'>HISTSIZE=450
</span><span class='line'>HISTFILESIZE=450
</span><span class='line'># HISTFILE
</span><span class='line'>
</span><span class='line'>忽略连续重复的命令
</span><span class='line'>export HISTCONTROL=ignoredups
</span><span class='line'>
</span><span class='line'>忽略重复的命令
</span><span class='line'>export HISTCONTROL=erasedups
</span><span class='line'>
</span><span class='line'>忽略指定的命令
</span><span class='line'>export HISTIGNORE='pwd:ls'</span></code></pre></td></tr></table></div></figure>


<h2>技巧N-3：shell之grep awk sed vi</h2>

<p>这些就不是看看man就能上手的，细嚼慢咽找几本书翻翻！！</p>

<p>推荐两本书： [sed与awk(第二版)], [Shell脚本学习指南]</p>

<h2>技巧N-4：批量处理之神：expect/for/while</h2>

<p>传入用户（与ssh的用户一致）密码，进行SSH无密钥认证：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 1]# vi ssh-copy-id.expect
</span><span class='line'>#!/usr/bin/expect  
</span><span class='line'>
</span><span class='line'>## Usage $0 [user@]host password
</span><span class='line'>
</span><span class='line'>set host [lrange $argv 0 0];
</span><span class='line'>set password [lrange $argv 1 1] ;
</span><span class='line'>
</span><span class='line'>set timeout 30;
</span><span class='line'>
</span><span class='line'>spawn ssh-copy-id $host ;
</span><span class='line'>
</span><span class='line'>expect {
</span><span class='line'>  "(yes/no)?" { send yes\n; exp_continue; }
</span><span class='line'>  "password:" { send $password\n; exp_continue; }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>exec sleep 1;
</span><span class='line'>
</span><span class='line'>批量处理
</span><span class='line'>[root@hadoop-master2 1]# for h in `cat /etc/hosts | grep hadoop | awk '{print $2}' ` ; do ./ssh-copy-id.expect $h root-password ; done</span></code></pre></td></tr></table></div></figure>


<p>传入新用户名称和密码，新建用户：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 1]# vi passwd.expect
</span><span class='line'>#!/usr/bin/expect  
</span><span class='line'>
</span><span class='line'>## Usage $0 host username password
</span><span class='line'>
</span><span class='line'>set host [lrange $argv 0 0];
</span><span class='line'>set username [lrange $argv 1 1];
</span><span class='line'>set password [lrange $argv 2 2] ;
</span><span class='line'>
</span><span class='line'>set timeout 30;
</span><span class='line'>
</span><span class='line'>##
</span><span class='line'>
</span><span class='line'>spawn ssh $host useradd $username ;
</span><span class='line'>
</span><span class='line'>exec sleep 1;
</span><span class='line'>
</span><span class='line'>##
</span><span class='line'>
</span><span class='line'>spawn ssh $host passwd $username ;
</span><span class='line'>
</span><span class='line'>## password and repasswd all use this
</span><span class='line'>expect {
</span><span class='line'>  "password:" { send $password\n; exp_continue; }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>exec sleep 1;
</span><span class='line'>
</span><span class='line'>批量处理
</span><span class='line'>[root@hadoop-master2 1]# for h in `cat /etc/hosts | grep hadoop | awk '{print $2}' ` ; do ./passwd.expect $h hadoop hadoop-password ; done</span></code></pre></td></tr></table></div></figure>


<h2>最后</h2>

<p>当然还有很多命令，xargs, if等需要在实践中慢慢积累，shell博大精深继续码字！cdpath眼前一亮，alias还可以这么用！！</p>

<p>在linux把xml转成properties键值对形式的命令，觉得也挺有意思的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ vi format.xslt
</span><span class='line'>&lt;xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform"&gt;
</span><span class='line'>&lt;xsl:output method="text" encoding="iso-8859-1"/&gt;
</span><span class='line'>
</span><span class='line'>&lt;xsl:strip-space elements="*" /&gt;
</span><span class='line'>
</span><span class='line'>&lt;xsl:template match="/*/child::*"&gt;
</span><span class='line'>&lt;xsl:for-each select="child::*"&gt;
</span><span class='line'>&lt;xsl:if test="position() != last()"&gt;&lt;xsl:value-of select="normalize-space(.)"/&gt;=&lt;/xsl:if&gt;
</span><span class='line'>&lt;xsl:if test="position() = last()"&gt;&lt;xsl:value-of select="normalize-space(.)"/&gt; &lt;xsl:text&gt;&#xa;&lt;/xsl:text&gt; &lt;/xsl:if&gt;
</span><span class='line'>&lt;/xsl:for-each&gt;
</span><span class='line'>&lt;/xsl:template&gt;
</span><span class='line'>
</span><span class='line'>&lt;/xsl:stylesheet&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 ~]$ xsltproc format.xslt ~/hadoop-2.2.0/etc/hadoop/yarn-site.xml
</span><span class='line'>yarn.nodemanager.aux-services=mapreduce_shuffle
</span><span class='line'>yarn.nodemanager.aux-services.mapreduce.shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
</span><span class='line'>yarn.resourcemanager.address=hadoop-master1:8032
</span><span class='line'>yarn.resourcemanager.scheduler.address=hadoop-master1:8030
</span><span class='line'>yarn.resourcemanager.resource-tracker.address=hadoop-master1:8031
</span><span class='line'>yarn.resourcemanager.admin.address=hadoop-master1:8033
</span><span class='line'>yarn.resourcemanager.webapp.address=hadoop-master1:8088
</span><span class='line'>yarn.nodemanager.resource.memory-mb=51200=yarn-default.xml
</span><span class='line'>yarn.scheduler.minimum-allocation-mb=1024=yarn-default.xml</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Oozie Start Guide]]></title>
    <link href="http://winseliu.com/blog/2015/09/08/oozie-start-guide/"/>
    <updated>2015-09-08T11:15:14+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/08/oozie-start-guide</id>
    <content type="html"><![CDATA[<h2>步骤记录</h2>

<p>说明：cu2就是hadoop-master2</p>

<ol>
<li>打包</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi bin/mkdistro.sh 
</span><span class='line'>MVN_OPTS="-Dbuild.time=${DATETIME} -Dvc.revision=${VC_REV} -Dvc.url=${VC_URL} "
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/mkdistro.sh -DskipTests -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<ol>
<li>依赖</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>打包后，文件的位置
</span><span class='line'>[hadoop@cu2 ~]$ tar zxvf sources/oozie-4.2.0/distro/target/oozie-4.2.0-distro.tar.gz
</span><span class='line'>
</span><span class='line'>下载 &lt;http://dev.sencha.com/deploy/ext-2.2.zip&gt;
</span><span class='line'>
</span><span class='line'>yum install zip
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ mkdir libext
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ cd libext/
</span><span class='line'>[hadoop@cu2 libext]$ ll
</span><span class='line'>total 7584
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop 6800612 Sep  7 16:00 ext-2.2.zip
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop  960372 Feb 28  2015 mysql-connector-java-5.1.34.jar</span></code></pre></td></tr></table></div></figure>


<ol>
<li>安装</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/oozie-setup.sh prepare-war
</span><span class='line'>
</span><span class='line'>setup后，生成的war的位置：/home/hadoop/oozie-4.2.0/oozie-server/webapps/oozie.war</span></code></pre></td></tr></table></div></figure>


<ol>
<li>初始化数据库</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>创建数据库用户
</span><span class='line'>
</span><span class='line'>CREATE DATABASE oozie;
</span><span class='line'>GRANT ALL ON oozie.* TO 'oozie'@'%' IDENTIFIED BY 'oozie';
</span><span class='line'>FLUSH PRIVILEGES;
</span><span class='line'>GRANT ALL ON oozie.* TO 'oozie'@'localhost'  IDENTIFIED BY 'oozie';
</span><span class='line'>FLUSH PRIVILEGES;
</span><span class='line'>
</span><span class='line'>show grants for oozie;
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi conf/oozie-site.xml 
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;oozie.service.JPAService.jdbc.driver&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;oozie.service.JPAService.jdbc.url&lt;/name&gt;&lt;value&gt;jdbc:mysql://localhost:3306/oozie&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;oozie.service.JPAService.jdbc.username&lt;/name&gt;&lt;value&gt;oozie&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;oozie.service.JPAService.jdbc.password&lt;/name&gt;&lt;value&gt;oozie&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>这里直接把hadoop的jar添加到脚本中，不拷贝到libext下面
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi bin/ooziedb.sh
</span><span class='line'>OOZIECPPATH=""
</span><span class='line'>if [ ! -z ${HADOOP_HOME} ] ; then
</span><span class='line'>  OOZIECPPATH="${OOZIECPPATH}:$($HADOOP_HOME/bin/hadoop classpath)"
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>照着写就行了，不必考虑sql文件的存在与否
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/ooziedb.sh create -sqlfile oozie.sql -run
</span><span class='line'>  setting CATALINA_OPTS="$CATALINA_OPTS -Xmx1024m"
</span><span class='line'>
</span><span class='line'>Validate DB Connection
</span><span class='line'>DONE
</span><span class='line'>DB schema does not exist
</span><span class='line'>Check OOZIE_SYS table does not exist
</span><span class='line'>DONE
</span><span class='line'>Create SQL schema
</span><span class='line'>DONE
</span><span class='line'>Create OOZIE_SYS table
</span><span class='line'>DONE
</span><span class='line'>
</span><span class='line'>Oozie DB has been created for Oozie version '4.2.0'
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>The SQL commands have been written to: oozie.sql</span></code></pre></td></tr></table></div></figure>


<ol>
<li>启动服务</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>由于war中没有hadoop的jar，所以这里也需要把它们添加到tomcat
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ $HADOOP_HOME/bin/hadoop classpath | sed 's/:/,/g'
</span><span class='line'>/home/hadoop/hadoop-2.7.1/etc/hadoop,/home/hadoop/hadoop-2.7.1/share/hadoop/common/lib/*,/home/hadoop/hadoop-2.7.1/share/hadoop/common/*,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs/lib/*,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs/*,/home/hadoop/hadoop-2.7.1/share/hadoop/yarn/lib/*,/home/hadoop/hadoop-2.7.1/share/hadoop/yarn/*,/home/hadoop/hadoop-2.7.1/share/hadoop/mapreduce/lib/*,/home/hadoop/hadoop-2.7.1/share/hadoop/mapreduce/*,/home/hadoop/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
</span><span class='line'>
</span><span class='line'>处理下把*改成*.jar
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi oozie-server/conf/catalina.properties 
</span><span class='line'>common.loader=${catalina.base}/lib,${catalina.base}/lib/*.jar,${catalina.home}/lib,${catalina.home}/lib/*.jar,/home/hadoop/hadoop-2.7.1/etc/hadoop,/home/hadoop/hadoop-2.7.1/share/hadoop/common/lib/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/common/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs/lib/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/yarn/lib/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/yarn/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/mapreduce/lib/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/mapreduce/*.jar,/home/hadoop/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
</span><span class='line'>
</span><span class='line'># 前台运行 bin/oozied.sh run
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/oozied.sh start
</span><span class='line'>
</span><span class='line'>http://localhost:11000/</span></code></pre></td></tr></table></div></figure>


<ol>
<li>测试</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi bin/oozie
</span><span class='line'>OOZIECPPATH=""
</span><span class='line'>if [ ! -z ${HADOOP_HOME} ] ; then
</span><span class='line'>  OOZIECPPATH="${OOZIECPPATH}:$($HADOOP_HOME/bin/hadoop classpath)"
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/oozie admin -oozie http://localhost:11000/oozie -status
</span><span class='line'>System mode: NORMAL</span></code></pre></td></tr></table></div></figure>


<ol>
<li>跑个helloworld</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 oozie-4.2.0]$ tar zxvf oozie-sharelib-4.2.0.tar.gz 
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ ~/hadoop-2.7.1/bin/hadoop fs -rmr share
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ ~/hadoop-2.7.1/bin/hadoop fs -put share share
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ tar zxvf oozie-examples.tar.gz 
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ ~/hadoop-2.7.1/bin/hadoop fs -put examples examples
</span><span class='line'>
</span><span class='line'>修改share后重启下oozie，sharelib在应用中会缓冲，中间上传程序不能识别，会报`Could not locate Oozie sharelib`的错。
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi examples/apps/map-reduce/job.properties 
</span><span class='line'>nameNode=hdfs://hadoop-master2:9000
</span><span class='line'>jobTracker=hadoop-master2:8032
</span><span class='line'>queueName=default
</span><span class='line'>examplesRoot=examples
</span><span class='line'>
</span><span class='line'>oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/map-reduce/workflow.xml
</span><span class='line'>outputDir=map-reduce
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/oozie job -oozie http://localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run
</span><span class='line'>Error: E0501 : E0501: Could not perform authorization operation, User: hadoop is not allowed to impersonate hadoop
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.7.1]$ vi etc/hadoop/core-site.xml 
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 ~]$ for h in `cat /etc/hosts | grep slaver | awk '{print $2}' ` ; do rsync -vaz hadoop-2.7.1 $h:~/ --exclude=logs ; done
</span><span class='line'>
</span><span class='line'>同步重启集群
</span><span class='line'>
</span><span class='line'>注：增加以上配置后，无需重启集群，可以直接用hadoop管理员账号重新加载这两个属性值，命令为：
</span><span class='line'>    hdfs dfsadmin -refreshSuperUserGroupsConfiguration
</span><span class='line'>    yarn rmadmin -refreshSuperUserGroupsConfiguration
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/oozie job -oozie http://localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run
</span><span class='line'>job: 0000000-150908082015741-oozie-hado-W
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.7.1]$ bin/hadoop fs -cat /user/hadoop/examples/output-data/map-reduce/part-00000
</span><span class='line'>
</span><span class='line'>尽管能看到结果了，但是不算任务执行成功。任务是有报错的`JA006: Call From cu2/192.168.0.214 to hadoop-master2:10020 failed on connection exception`
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.7.1]$ sbin/mr-jobhistory-daemon.sh start historyserver
</span><span class='line'>
</span><span class='line'>在运行一次就ok了。</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="https://oozie.apache.org/docs/4.2.0/DG_QuickStart.html">https://oozie.apache.org/docs/4.2.0/DG_QuickStart.html</a></li>
<li><a href="http://ju.outofmemory.cn/entry/65688">http://ju.outofmemory.cn/entry/65688</a></li>
<li><a href="http://stackoverflow.com/questions/30926357/oozie-on-yarn-oozie-is-not-allowed-to-impersonate-hadoop">http://stackoverflow.com/questions/30926357/oozie-on-yarn-oozie-is-not-allowed-to-impersonate-hadoop</a></li>
<li><a href="http://oozie.apache.org/docs/4.0.0/DG_QuickStart.html#Oozie_Share_Lib_Installation">http://oozie.apache.org/docs/4.0.0/DG_QuickStart.html#Oozie_Share_Lib_Installation</a></li>
<li><a href="https://oozie.apache.org/docs/4.2.0/DG_Examples.html">https://oozie.apache.org/docs/4.2.0/DG_Examples.html</a></li>
<li><p><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/ClusterSetup.html">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/ClusterSetup.html</a></p></li>
<li><p><a href="http://blog.csdn.net/wngn123/article/details/41380013">http://blog.csdn.net/wngn123/article/details/41380013</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装http代理服务器squid]]></title>
    <link href="http://winseliu.com/blog/2015/09/06/squid-http-proxy-server-install/"/>
    <updated>2015-09-06T23:22:50+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/06/squid-http-proxy-server-install</id>
    <content type="html"><![CDATA[<h2>环境说明</h2>

<ul>
<li>squid-3.3.14.tar.gz</li>
<li>centos6.6</li>
</ul>


<h2>安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install gcc gcc-c++
</span><span class='line'>cd squid-3.3.14
</span><span class='line'>./configure
</span><span class='line'>make
</span><span class='line'>make install
</span><span class='line'>
</span><span class='line'>cd /usr/local/squid
</span><span class='line'>#不修改会有权限的问题
</span><span class='line'>chmod 777 var/logs
</span><span class='line'>sbin/squid 
</span><span class='line'>sbin/squid -k shutdown</span></code></pre></td></tr></table></div></figure>


<p>或者：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum -y install squid
</span><span class='line'>chkconfig squid on</span></code></pre></td></tr></table></div></figure>


<p>改下squid.conf的配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># And finally deny all other access to this proxy
</span><span class='line'>#http_access deny all
</span><span class='line'>http_access allow all</span></code></pre></td></tr></table></div></figure>


<h2>使用</h2>

<p>在浏览器中设置Http代理。端口为3128</p>

<h2>参考</h2>

<ul>
<li><a href="ftp://ftp.cuhk.edu.hk/pub/packages/info-systems/www/squid/">ftp://ftp.cuhk.edu.hk/pub/packages/info-systems/www/squid/</a></li>
<li><a href="http://www.educity.cn/linux/517165.html">http://www.educity.cn/linux/517165.html</a></li>
<li><a href="http://www.ajaxstu.com/Proxyfuwuqi/283731.html">http://www.ajaxstu.com/Proxyfuwuqi/283731.html</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_537b9caa010185xo.html">http://blog.sina.com.cn/s/blog_537b9caa010185xo.html</a></li>
<li><a href="http://blog.163.com/sword_111/blog/static/6658941620114163458435/">http://blog.163.com/sword_111/blog/static/6658941620114163458435/</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-squid-proxy-on-centos-6">https://www.digitalocean.com/community/tutorials/how-to-install-squid-proxy-on-centos-6</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[手动安装supervisor]]></title>
    <link href="http://winseliu.com/blog/2015/08/24/manual-install-supervisor/"/>
    <updated>2015-08-24T16:24:25+08:00</updated>
    <id>http://winseliu.com/blog/2015/08/24/manual-install-supervisor</id>
    <content type="html"><![CDATA[<h2>下载依赖以及安装包，并安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>python -V
</span><span class='line'>
</span><span class='line'>tar zxvf setuptools-18.2.tar.gz 
</span><span class='line'>cd setuptools-18.2
</span><span class='line'>python setup.py install
</span><span class='line'>
</span><span class='line'>tar zxvf meld3-0.6.5.tar.gz 
</span><span class='line'>cd meld3-0.6.5
</span><span class='line'>python setup.py install
</span><span class='line'>
</span><span class='line'>tar zxvf elementtree-1.2.6-20050316.tar.gz 
</span><span class='line'>cd elementtree-1.2.6-20050316
</span><span class='line'>python setup.py install
</span><span class='line'>
</span><span class='line'>tar zxvf supervisor-3.1.3.tar.gz 
</span><span class='line'>cd supervisor-3.1.3
</span><span class='line'>python setup.py  install</span></code></pre></td></tr></table></div></figure>


<h2>配置启动</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@master supervisor-3.1.3]# echo_supervisord_conf &gt;/etc/supervisord.conf
</span><span class='line'>[root@master supervisor-3.1.3]# supervisord
</span><span class='line'>
</span><span class='line'>[root@master supervisor-3.1.3]# ps aux | grep supervisor | grep -v grep
</span><span class='line'>root      6123  0.0  0.7 207292 13296 ?        Ss   08:15   0:00 /usr/bin/python /usr/bin/supervisord
</span><span class='line'>
</span><span class='line'>[root@master supervisor-3.1.3]# supervisorctl shutdown
</span><span class='line'>Shut down
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://supervisord.org/installing.html#installing-to-a-system-without-internet-access">http://supervisord.org/installing.html#installing-to-a-system-without-internet-access</a></li>
<li><a href="http://supervisord.org/configuration.html#programx-section">http://supervisord.org/configuration.html#programx-section</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logstash Elasticsearch Kibana日志采集查询系统搭建]]></title>
    <link href="http://winseliu.com/blog/2015/08/21/logstash-elasticsearch-kibana-startguide/"/>
    <updated>2015-08-21T14:42:30+08:00</updated>
    <id>http://winseliu.com/blog/2015/08/21/logstash-elasticsearch-kibana-startguide</id>
    <content type="html"><![CDATA[<h2>软件版本</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@master opt]# ll
</span><span class='line'>total 20
</span><span class='line'>drwxr-xr-x 7 root root 4096 Aug 21 01:23 elasticsearch-1.7.1
</span><span class='line'>drwxr-xr-x 8 uucp  143 4096 Mar 18  2014 jdk1.8.0_05
</span><span class='line'>drwxrwxr-x 7 1000 1000 4096 Aug 21 01:09 kibana-4.1.1-linux-x64
</span><span class='line'>drwxr-xr-x 5 root root 4096 Aug 21 05:58 logstash-1.5.3
</span><span class='line'>drwxrwxr-x 6 root root 4096 Aug 21 06:44 redis-3.0.3</span></code></pre></td></tr></table></div></figure>


<h2>安装运行脚本</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># java
</span><span class='line'>vi /etc/profile
</span><span class='line'>source /etc/profile
</span><span class='line'>
</span><span class='line'>cd /opt/elasticsearch-1.7.1
</span><span class='line'>bin/elasticsearch -p elasticsearch.pid -d
</span><span class='line'>
</span><span class='line'>curl localhost:9200/_cluster/nodes/172.17.0.4
</span><span class='line'>
</span><span class='line'>cd /opt/kibana-4.1.1-linux-x64/
</span><span class='line'>bin/kibana 
</span><span class='line'># http://master:5601
</span><span class='line'>
</span><span class='line'>cd /opt/redis-3.0.3
</span><span class='line'>yum install gcc
</span><span class='line'>yum install bzip2
</span><span class='line'>make MALLOC=jemalloc
</span><span class='line'>
</span><span class='line'># 也可以修改配置的daemon属性
</span><span class='line'>nohup src/redis-server & 
</span><span class='line'>
</span><span class='line'>cd /opt/logstash-1.5.3/
</span><span class='line'>bin/logstash -e 'input { stdin { } } output { stdout {} }'
</span><span class='line'>
</span><span class='line'>vi index.conf
</span><span class='line'>vi agent.conf
</span><span class='line'>
</span><span class='line'># agent可不加
</span><span class='line'>bin/logstash agent -f agent.conf &
</span><span class='line'>bin/logstash agent -f index.conf &</span></code></pre></td></tr></table></div></figure>


<h2>logstash配置</h2>

<p>由于程序都运行在一台机器(localhost)，redis、elasticsearch和kibana都使用默认配置。下面贴的是logstash的采集和过滤的配置：</p>

<p>(kibaba的配置config/kibana.yml, elasticsearch的配置config/elasticsearch.yml)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@master logstash-1.5.3]# cat agent.conf 
</span><span class='line'>input {
</span><span class='line'>  file {
</span><span class='line'>    path =&gt; "/var/log/yum.log"
</span><span class='line'>    start_position =&gt; beginning
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  redis {
</span><span class='line'>    key =&gt; "logstash.redis"
</span><span class='line'>    data_type =&gt; list
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  # 便于查看调试
</span><span class='line'>  stdout { }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@master logstash-1.5.3]# cat index.conf 
</span><span class='line'>input {
</span><span class='line'>  redis {
</span><span class='line'>    data_type =&gt; list
</span><span class='line'>    key =&gt; "logstash.redis"
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  elasticsearch {
</span><span class='line'>    host =&gt; "localhost"
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>注意要改动下被采集的原始文件！！然后启动相应的程序，打开浏览器<a href="http://master:5601">http://master:5601</a>配置一下索引项，就可以查看了。</p>

<p>至于input/output/filter(map,reduce)怎么配置，查看官方文档<a href="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html">filter-plugins</a></p>

<h2>filter</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu1 logstash-1.5.3]$ bin/logstash -e "
</span><span class='line'>input {
</span><span class='line'>stdin {}
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>filter {
</span><span class='line'>grok { 
</span><span class='line'>match =&gt; {\"message\" =&gt; \"%{WORD:content}\"}
</span><span class='line'>add_field =&gt; { \"foo_%{content}\" =&gt; \"helloworld\" }
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>stdout { codec =&gt; json }
</span><span class='line'>}
</span><span class='line'>"
</span><span class='line'>
</span><span class='line'>abc
</span><span class='line'>{"message":"abc","@version":"1","@timestamp":"2015-09-10T08:02:52.024Z","host":"cu1","content":"abc","foo_abc":"helloworld"}</span></code></pre></td></tr></table></div></figure>


<p>grok-pattern文件的位置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 logstash-1.5.3]$ less ./vendor/bundle/jruby/1.9/gems/logstash-patterns-core-0.1.10/patterns/grok-patterns 
</span><span class='line'>
</span><span class='line'>2015-09-06 15:23:53,027 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
</span><span class='line'>%{TIMESTAMP_ISO8601:time} %{LOGLEVEL:loglevel} %{GREEDYDATA:content}
</span><span class='line'>
</span><span class='line'>[2015-09-10 08:00:46,539][INFO ][cluster.metadata         ] [Jumbo Carnation] [logstash-2015.09.10] update_mapping [hbase-logs] (dynamic)
</span><span class='line'>\[%{TIMESTAMP_ISO8601:time}\]\[%{LOGLEVEL:loglevel}%{SPACE}\]%{GREEDYDATA:content}</span></code></pre></td></tr></table></div></figure>


<h2>学习</h2>

<p>过滤DEBUG/INFO日志</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu1 logstash-1.5.3]$ bin/logstash -e "
</span><span class='line'> input {
</span><span class='line'> stdin {}
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> filter {
</span><span class='line'> grok {
</span><span class='line'> match =&gt; { \"message\" =&gt; \"%{TIMESTAMP_ISO8601:time} %{LOGLEVEL:loglevel} %{GREEDYDATA:content}\" }
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> if [loglevel] == \"INFO\" { drop {} }
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> output {
</span><span class='line'> stdout {}
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> "</span></code></pre></td></tr></table></div></figure>


<p>用shell先预处理</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>    stdin {
</span><span class='line'>        type =&gt; "nginx"
</span><span class='line'>        format =&gt; "json_event"
</span><span class='line'>    }
</span><span class='line'>} 
</span><span class='line'>output {
</span><span class='line'>    amqp {
</span><span class='line'>        type =&gt; "nginx"
</span><span class='line'>        host =&gt; "10.10.10.10"
</span><span class='line'>        key  =&gt; "cdn"
</span><span class='line'>        name =&gt; "logstash"
</span><span class='line'>        exchange_type =&gt; "direct"
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>#!/bin/sh
</span><span class='line'>      tail -F /data/nginx/logs/access.json \
</span><span class='line'>    | sed 's/upstreamtime":-/upstreamtime":0/' \
</span><span class='line'>    | /usr/local/logstash/bin/logstash -f /usr/local/logstash/etc/agent.conf &</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.cnblogs.com/buzzlight/p/logstash_elasticsearch_kibana_log.html">http://www.cnblogs.com/buzzlight/p/logstash_elasticsearch_kibana_log.html</a></li>
<li><a href="http://www.cnblogs.com/ibook360/archive/2013/03/15/2961428.html">http://www.cnblogs.com/ibook360/archive/2013/03/15/2961428.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/index.html">https://www.elastic.co/guide/en/logstash/current/index.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/first-event.html">https://www.elastic.co/guide/en/logstash/current/first-event.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/working-with-plugins.html">https://www.elastic.co/guide/en/logstash/current/working-with-plugins.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html">https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html</a></li>
<li><p><a href="https://www.elastic.co/guide/en/logstash/current/codec-plugins.html">https://www.elastic.co/guide/en/logstash/current/codec-plugins.html</a></p></li>
<li><p><a href="http://blog.csdn.net/yeasy/article/details/45332493">http://blog.csdn.net/yeasy/article/details/45332493</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop不同版本yarn和hdfs混搭，spark-yarn环境配置]]></title>
    <link href="http://winseliu.com/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs/"/>
    <updated>2015-06-10T18:48:19+08:00</updated>
    <id>http://winseliu.com/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs</id>
    <content type="html"><![CDATA[<p>hadoop分为存储和计算两个主要的功能，hdfs步入hadoop2后不论稳定性还是HA等等功能都比hadoop1要更吸引人。hadoop-2.2.0的hdfs已经比较稳定，但是yarn高版本有更加丰富的功能。本文主要关注spark-yarn下日志的查看，以及spark-yarn-dynamic的配置。</p>

<p>hadoop-2.2.0的hdfs原本已经在使用的环境，在这基础上搭建运行yarn-2.6.0，以及spark-1.3.0-bin-2.2.0。</p>

<ul>
<li>编译</li>
</ul>


<p>我是在虚拟机里面编译，共享了host主机的maven库。参考【VMware共享目录】，【VMware-Centos6 Build hadoop-2.6】注意<strong>cmake_symlink_library的异常，由于共享的windows目录下不能创建linux的软链接</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxvf ~/hadoop-2.6.0-src.tar.gz 
</span><span class='line'>cd hadoop-2.6.0-src/
</span><span class='line'>mvn package -Pdist,native -DskipTests -Dtar -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'># 由于hadoop-hdfs还是2.2的，这里编译spark需要用2.2版本！
</span><span class='line'># 如果用2.6会遇到[UnsatisfiedLinkError:org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray ](http://blog.csdn.net/zeng_84_long/article/details/44340441)
</span><span class='line'>cd spark-1.3.0
</span><span class='line'>export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=1g -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>mvn clean package -Phadoop-2.2 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</span><span class='line'>
</span><span class='line'>vi make-distribution.sh #注释掉BUILD_COMMAND那一行，不重复执行package！
</span><span class='line'>./make-distribution.sh  --mvn `which mvn` --tgz  --skip-java-test -Phadoop-2.6 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests</span></code></pre></td></tr></table></div></figure>


<ul>
<li><p>配置注意点</p></li>
<li><p>core-site不要全部拷贝原来的，只要一些主要的配置即可。</p></li>
<li>yarn-site的<code>yarn.resourcemanager.webapp.address</code>需要填写具体的地址，不能写<code>0.0.0.0</code>。</li>
<li>yarn-site的<code>yarn.nodemanager.aux-services</code>添加spark_shuffle服务。<a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation</a></li>
<li>把hive-site的文件拷贝/链接到spark的conf目录下。</li>
<li>spark-yarn-dynamic配置: <a href="https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation">https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ cat conf/spark-defaults.conf 
</span><span class='line'># spark.master                     spark://bigdatamgr1:7077,bigdata8:7077
</span><span class='line'># spark.eventLog.enabled           true
</span><span class='line'># spark.eventLog.dir               hdfs://namenode:8021/directory
</span><span class='line'># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
</span><span class='line'># spark.executor.extraJavaOptions       -Xmx16g -Xms16g -Xmn256m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:ParallelGCThreads=10
</span><span class='line'>spark.driver.memory              48g
</span><span class='line'>spark.executor.memory            48g
</span><span class='line'>spark.sql.shuffle.partitions     200
</span><span class='line'>
</span><span class='line'>#spark.scheduler.mode FAIR
</span><span class='line'>spark.serializer  org.apache.spark.serializer.KryoSerializer
</span><span class='line'>spark.driver.maxResultSize 8g
</span><span class='line'>#spark.kryoserializer.buffer.max.mb 2048
</span><span class='line'>
</span><span class='line'>spark.dynamicAllocation.enabled true
</span><span class='line'>spark.dynamicAllocation.minExecutors 4
</span><span class='line'>spark.shuffle.service.enabled true
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 conf]$ cat spark-env.sh 
</span><span class='line'>#!/usr/bin/env bash
</span><span class='line'>
</span><span class='line'>JAVA_HOME=/home/eshore/jdk1.7.0_60
</span><span class='line'>
</span><span class='line'># log4j
</span><span class='line'>
</span><span class='line'>__add_to_classpath() {
</span><span class='line'>
</span><span class='line'>  root=$1
</span><span class='line'>
</span><span class='line'>  if [ -d "$root" ] ; then
</span><span class='line'>    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
</span><span class='line'>      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
</span><span class='line'>      else
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$f
</span><span class='line'>      fi
</span><span class='line'>    done
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'># this add tail of SPARK_CLASSPATH
</span><span class='line'>__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"
</span><span class='line'>
</span><span class='line'>#export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
</span><span class='line'>export HADOOP_CONF_DIR=/home/eshore/hadoop-2.6.0/etc/hadoop
</span><span class='line'>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'># HA
</span><span class='line'>SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark" 
</span><span class='line'>
</span><span class='line'>SPARK_PID_DIR=${SPARK_HOME}/pids
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>同步</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for h in `cat slaves ` ; do rsync -vaz hadoop-2.6.0 $h:~/ --delete --exclude=work --exclude=logs --exclude=metastore_db --exclude=data --exclude=pids ; done</span></code></pre></td></tr></table></div></figure>


<ul>
<li>启动spark-hive-thrift</li>
</ul>


<p>./sbin/start-thriftserver.sh &ndash;executor-memory 29g &ndash;master yarn-client</p>

<p>对于多任务的集群来说，配置自动动态分配（类似资源池）更有利于资源的使用。可以通过【All Applications】-【ApplicationMaster】-【Executors】来观察执行进程的变化。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tachyon剖析]]></title>
    <link href="http://winseliu.com/blog/2015/04/18/tachyon-deep-source/"/>
    <updated>2015-04-18T23:13:01+08:00</updated>
    <id>http://winseliu.com/blog/2015/04/18/tachyon-deep-source</id>
    <content type="html"><![CDATA[<p>要了解一个框架，一般都是从框架提供/开放的接口入手。先从最直接的方式下手，可以通过<code>tachyon tfs</code>和浏览器19999端口获取集群以及文件的相关信息。</p>

<ul>
<li>要了解tachyon首先就是其文件系统，可以从两个功能开始：命令行tachyon.command.TFsShell和web-servlet。</li>
<li>然后会深入tachyon.client包，了解<strong>TachyonFS</strong>和TachyonFile处理io的方式。以及tachyon.hadoop的包。</li>
<li>io处理：

<ul>
<li>写：BlockOutStream（#getLocalBlockTemporaryPath； MappedByteBuffer）、FileOutStream</li>
<li>读：RemoteBlockInStream、LocalBlockInStream</li>
</ul>
</li>
<li>了解thrift：MasterClient、MasterServiceHandler、WorkerClient、WorkerServiceHandler、ClientBlockInfo、ClientFileInfo。</li>
<li>看tachyon.example，巩固</li>
</ul>


<p>注：MappedByteBuffer在windows存在资源占用的bug！参见<a href="http://www.th7.cn/Program/java/2012/01/31/57401.shtml">http://www.th7.cn/Program/java/2012/01/31/57401.shtml</a>，</p>

<p>把整个io流理清之后，然后需要了解tachyon是怎么维护这些信息的：</p>

<ul>
<li>配置：WorkerConf、MasterConf、UserConf</li>
<li>了解thrift：MasterClient、MasterServiceHandler、ClientWorkerInfo、MasterInfo</li>
<li>TachyonMaster和TachyonWorker的启动</li>
<li>Checkpoint、Image、Journal</li>
<li>内存淘汰策略</li>
<li>DataServer在哪里用到（nio/netty）：TachyonFile#readRemoteByteBuffer、RemoteBlockInStream#read(byte[], int, int)</li>
<li>HA</li>
<li>Dependency（不知道怎么用）</li>
</ul>


<p>远程调试Worker以及tfs：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ cat conf/tachyon-env.sh 
</span><span class='line'>export TACHYON_WORKER_JAVA_OPTS="$TACHYON_JAVA_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070"
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ export TACHYON_JAVA_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8070"
</span><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ bin/tachyon tfs lsr /</span></code></pre></td></tr></table></div></figure>


<h2>IO/client</h2>

<ul>
<li>TachyonFS是client与Master/Worker的纽带，请求<strong>文件系统和文件</strong>的元数据CRUD操作。其中的WorkerClient仅用于写（保存）文件。</li>
<li>TachyonFile是文件的抽象处理集合，可以获取文件的基本属性（元数据），同时提供了IO的接口用于文件内容的读写。</li>
<li>InStream读、获取文件内容

<ul>
<li>EmptyBlockInStream(文件包括的块为0）</li>
<li>BlockInStream(文件包括的块为1）

<ul>
<li>LocalBlockInStream 仅当是localworker且该块在本机时，通过MappedByteBuffer获取数据（数据在ramdisk也就是内存盘上哦）。</li>
<li>RemoteBlockInStream （通过nio从远程的worker#DataServer机器获取数据#retrieveByteBufferFromRemoteMachine，如果readtype设置为cache同时把数据缓冲到本地worker）</li>
</ul>
</li>
<li>FileInStream(文件包括的块为1+，可以理解为BlocksInStream。通过mCurrentPosition / mBlockCapacity来获取当前的blockindex最终还是调用BlockInStream）</li>
</ul>
</li>
<li>FileOutStream写，写数据入口就是只有FileOutStream

<ul>
<li>BlockOutStream（WriteType设置了需要缓冲，会写到本地localworker。<strong>由于需要进行分块，会复杂些#appendCurrentBuffer</strong>）</li>
<li>UnderFileSystem（如果WriteType设置了Through，则把数据写一份到underfs文件系统）</li>
</ul>
</li>
</ul>


<h2>Master</h2>

<p>TachyonMaster是master的启动类，所有的服务都是在这个类里面初始化启动的。</p>

<ul>
<li>HA：LeaderSelectorClient</li>
<li>EditLog：EditLogProcessor、Journal。

<ul>
<li>如果是HA模式，leader调用setup方法把EditLogProcessor停掉。也就是说在HA模式下，standby才会运行EditLogProcessor实时处理editlog。</li>
<li>leader和非HA master则仅在启动时通过调用MasterInfo#init处理editlog一次。</li>
</ul>
</li>
<li>Thrift: TServer、MasterServiceHandler；与MasterClient对应的服务端。</li>
<li>Web: UIWebServer，使用jetty的内嵌服务器。</li>
<li>MasterInfo</li>
</ul>


<h2>Worker</h2>

<h2>Thrift</h2>

<h2>HA</h2>

<p>当配置<code>tachyon.usezookeeper</code>设置为true时，启动master时会初始化LeaderSelectorClient对象。使用curator连接到zookeeper服务器进行leader的选举。</p>

<p><strong>LeaderSelectorClient</strong>实现了LeaderSelectorListener接口，创建LeaderSelector并传入当前实例作为监听实例，当选举完成后，被选leader触发takeLeadership事件。</p>

<blockquote><p>public void takeLeadership(CuratorFramework client) throws Exception
Called when your instance has been granted leadership. This method should not return until you wish to release leadership</p></blockquote>

<p>takeLeadership方法中把<code>mIsLeader</code>设置为true（master自己判断）、创建<code>mLeaderFolder + mName</code>路径（客户端获取master leader）；然后隔5s的死循环（运行在LeaderSelector单独的线程池）。</p>

<h2>Checkpoint</h2>

<h2>Journal</h2>

<hr />

<h2>问题</h2>

<ul>
<li>程序没有返回内容，没有响应</li>
</ul>


<p>tfs 默认是CACHE_THROUGH，会缓冲同时写ufs。如果改成must则只写cache，然后清理内存，再获取数据，一直没有内容返回，不知道为什么？</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ export TACHYON_JAVA_OPTS="-Dtachyon.user.file.writetype.default=MUST_CACHE "
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs copyFromLocal LICENSE /LICENSE2
</span><span class='line'>Copied LICENSE to /LICENSE2
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs free /LICENSE2
</span><span class='line'>/LICENSE2 was successfully freed from memory.
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs cat /LICENSE2</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tachyon入门指南]]></title>
    <link href="http://winseliu.com/blog/2015/04/15/tachyon-quickstart/"/>
    <updated>2015-04-15T22:56:09+08:00</updated>
    <id>http://winseliu.com/blog/2015/04/15/tachyon-quickstart</id>
    <content type="html"><![CDATA[<p>tachyon程序是在HDFS与程序之间缓冲，相当于CPU与磁盘设备之间内存的功能。tachyon提供了TachyonFS、TachyonFile等API使操作起来更像一个文件系统；同时实现了HDFS的FileSystem接口，方便原有程序的迁移，只要把url的模式（schema）hdfs改成tachyon。</p>

<p>tachyon和HDFS一样也是master-slaver（worker）结构：master保存元数据，worker节点使用内存盘缓冲数据。</p>

<h2>部署集群</h2>

<p>下载tachyon的编译文件后，按下面的步骤部署：</p>

<ul>
<li>解压</li>
<li>修改conf/tachyon-env.sh（JAVA_HOME，TACHYON_UNDERFS_ADDRESS，TACHYON_MASTER_ADDRESS）</li>
<li>修改conf/worker</li>
<li>同步代码到workers子节点</li>
<li>格式化tachyon（建立master和worker所需的各种目录）</li>
<li>挂载内存盘</li>
<li>启动集群</li>
<li>通过19999端口访问</li>
</ul>


<p>如果hadoop集群的版本不是最新的2.6.0，需要手工编译源码：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mvn clean package assembly:single -Dhadoop.version=2.2.0 -DskipTests -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<p>同步程序的脚本如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do  rsync -vaz tachyon-0.6.1 $h:~/ --exclude=logs --exclude=underfs --exclude=journal ; done</span></code></pre></td></tr></table></div></figure>


<p>用tachyon用户格式化：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/tachyon format</span></code></pre></td></tr></table></div></figure>


<p>使用root挂载内存盘：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/tachyon-mount.sh Mount workers
</span><span class='line'>for h in `cat slaves ` ; do  ssh $h "chmod 777 /mnt/ramdisk; chmod 777 /mnt/tachyon_default_home"  ; done</span></code></pre></td></tr></table></div></figure>


<p>确认下worker节点是否有underfs/tmp/tachyon/data，如果没有手动创建下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do ssh $h mkdir -p ~/tachyon-0.6.1/underfs/tmp/tachyon/data ; done</span></code></pre></td></tr></table></div></figure>


<p>启动集群：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 tachyon-0.6.1]$ bin/tachyon-start.sh all NoMount</span></code></pre></td></tr></table></div></figure>


<p>上传文件到tachyon：（注意，这里是在worker节点！）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs copyFromLocal README.md /
</span><span class='line'>Copied README.md to /</span></code></pre></td></tr></table></div></figure>


<h2>集成到Spark</h2>

<p>注意，这里是在worker节点，使用local本地集群的方式（spark集群资源全部被spark-sql占用了，导致提交的任务分配不到资源！）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 spark-1.3.0-bin-2.2.0]$ export SPARK_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar 
</span><span class='line'>[eshore@bigdata1 spark-1.3.0-bin-2.2.0]$ bin/spark-shell --master local[1] -Dspark.ui.port=4041
</span><span class='line'>scala&gt; val s = sc.textFile("tachyon://bigdatamgr1:19998/README.md")
</span><span class='line'>s: org.apache.spark.rdd.RDD[String] = tachyon://bigdatamgr1:19998/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:21
</span><span class='line'>
</span><span class='line'>scala&gt; s.count()
</span><span class='line'>15/04/03 11:13:09 WARN : tachyon.home is not set. Using /mnt/tachyon_default_home as the default value.
</span><span class='line'>res0: Long = 45
</span><span class='line'>
</span><span class='line'>scala&gt; val wordCounts = s.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
</span><span class='line'>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:23
</span><span class='line'>
</span><span class='line'>scala&gt; wordCounts.saveAsTextFile("tachyon://bigdatamgr1:19998/wordcount-README")
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 tachyon-0.6.1]$ bin/tachyon tfs ls /wordcount-README/
</span><span class='line'>1407.00 B 04-03-2015 11:16:05:483  In Memory      /wordcount-README/part-00000
</span><span class='line'>0.00 B    04-03-2015 11:16:05:787  In Memory      /wordcount-README/_SUCCESS</span></code></pre></td></tr></table></div></figure>


<p>为啥要在worker节点运行呢？不能在master节点运行？运行肯定是可以的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ export SPARK_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ bin/spark-shell --master local[1] --jars /home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>
</span><span class='line'>scala&gt; val s = sc.textFile("tachyon://bigdatamgr1:19998/NOTICE")
</span><span class='line'>s: org.apache.spark.rdd.RDD[String] = tachyon://bigdatamgr1:19998/NOTICE MapPartitionsRDD[1] at textFile at &lt;console&gt;:15
</span><span class='line'>
</span><span class='line'>scala&gt; s.count()
</span><span class='line'>15/04/13 16:05:45 WARN BlockReaderLocal: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
</span><span class='line'>15/04/13 16:05:45 WARN : tachyon.home is not set. Using /mnt/tachyon_default_home as the default value.
</span><span class='line'>java.io.IOException: The machine does not have any local worker.
</span><span class='line'>        at tachyon.client.BlockOutStream.&lt;init&gt;(BlockOutStream.java:94)
</span><span class='line'>        at tachyon.client.BlockOutStream.&lt;init&gt;(BlockOutStream.java:65)
</span><span class='line'>        at tachyon.client.RemoteBlockInStream.read(RemoteBlockInStream.java:204)
</span><span class='line'>        at tachyon.hadoop.HdfsFileInputStream.read(HdfsFileInputStream.java:142)
</span><span class='line'>        at java.io.DataInputStream.read(DataInputStream.java:100)
</span><span class='line'>        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)
</span><span class='line'>        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
</span><span class='line'>        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)
</span><span class='line'>        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)
</span><span class='line'>        at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:245)
</span><span class='line'>        at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:212)
</span><span class='line'>        at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
</span><span class='line'>        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
</span><span class='line'>        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
</span><span class='line'>        at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1466)
</span><span class='line'>        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
</span><span class='line'>        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
</span><span class='line'>        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
</span><span class='line'>        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
</span><span class='line'>        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
</span><span class='line'>        at org.apache.spark.scheduler.Task.run(Task.scala:64)
</span><span class='line'>        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span><span class='line'>res0: Long = 2</span></code></pre></td></tr></table></div></figure>


<p>两个点：</p>

<ul>
<li>这里是运行的spark local集群；</li>
<li>运行当然没有问题，但是会打印不和谐的<strong>The machine does not have any local worker</strong>警告日志。这与FileSystem的获取输入流<code>ReadType.CACHE</code>实现有关（见源码HdfsFileInputStream）。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mTachyonFileInputStream = mTachyonFile.getInStream(ReadType.CACHE);</span></code></pre></td></tr></table></div></figure>


<p>如果master为spark集群，spark-driver不管运行在哪台集群都没有问题。因为，此时运行任务的spark-worker就是tachyon-worker节点啊，当然就有local worker了。</p>

<p>为了更深入的了解，还可以试验一下<code>ReadType.CACHE</code>的作用：原本不在内存的数据，计算后就会被载入到缓冲（内存）！！</p>

<p>可以再试一次，先从内存中删掉（此处underfs配置存储在HDFS）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs free /NOTICE
</span><span class='line'>/NOTICE was successfully freed from memory.
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs fileinfo /NOTICE
</span><span class='line'>/NOTICE with file id 2 has the following blocks: 
</span><span class='line'>ClientBlockInfo(blockId:2147483648, offset:0, length:62, locations:[NetAddress(mHost:bigdata8, mPort:-1, mSecondaryPort:-1), NetAddress(bigdata6, mPort:-1, mSecondaryPort:-1), NetAddress(mHost:bigdata5, mPort:-1, mSecondaryPort:-1)])</span></code></pre></td></tr></table></div></figure>


<p>再次运行count：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; s.count()
</span><span class='line'>res1: Long = 2</span></code></pre></td></tr></table></div></figure>


<p>再次查看文件状态：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs fileinfo /NOTICE
</span><span class='line'>/NOTICE with file id 2 has the following blocks: 
</span><span class='line'>ClientBlockInfo(blockId:2147483648, offset:0, length:62, locations:[NetAddress(mHost:bigdata1, mPort:29998, mSecondaryPort:29999)])</span></code></pre></td></tr></table></div></figure>


<p>此时文件对应的block所在机器变成了bigdata1，也就是spark-worker运行的节点（这里用local，worker和driver都在bigdata1上）。</p>

<p>参考</p>

<ul>
<li><a href="http://tachyon-project.org/Running-Tachyon-on-a-Cluster.html">http://tachyon-project.org/Running-Tachyon-on-a-Cluster.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html">http://spark.apache.org/docs/latest/configuration.html</a></li>
<li><a href="http://tachyon-project.org/Running-Spark-on-Tachyon.html">http://tachyon-project.org/Running-Spark-on-Tachyon.html</a></li>
</ul>


<h2>集成到Hadoop集群</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ export HADOOP_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount -libjars /home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar tachyon://bigdatamgr1:19998/NOTICE tachyon://bigdatamgr1:19998/NOTICE-wordcount
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs cat /NOTICE-wordcount/part-r-00000
</span><span class='line'>2012-2014       1
</span><span class='line'>Berkeley        1
</span><span class='line'>California,     1
</span><span class='line'>Copyright       1
</span><span class='line'>Tachyon 1
</span><span class='line'>University      1
</span><span class='line'>of      1</span></code></pre></td></tr></table></div></figure>


<h2>后记</h2>

<p>当前apache开源大部分集群的部署都是同一种模式，源码也基本都是用maven来进行构建。部署其实没有什么难度，如果是应用到spark、hadoop这样的平台，其实只要部署，然后用FileSystem的接口就一切ok了。但是要了解其原理，官网的文档也不是很全，那得需要深入源码。</p>

<p>入门写到这里，差不多了，下一篇从TachyonFS角度解析tachyon。</p>

<h2>附录</h2>

<ul>
<li>spark-env.sh</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>JAVA_HOME=/home/eshore/jdk1.7.0_60
</span><span class='line'>
</span><span class='line'># log4j
</span><span class='line'>
</span><span class='line'>__add_to_classpath() {
</span><span class='line'>
</span><span class='line'>  root=$1
</span><span class='line'>
</span><span class='line'>  if [ -d "$root" ] ; then
</span><span class='line'>    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
</span><span class='line'>      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
</span><span class='line'>      else
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$f
</span><span class='line'>      fi
</span><span class='line'>    done
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>__add_to_classpath "/home/eshore/tez-0.4.0-incubating"
</span><span class='line'>__add_to_classpath "/home/eshore/tez-0.4.0-incubating/lib"
</span><span class='line'>__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"
</span><span class='line'>
</span><span class='line'>export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
</span><span class='line'>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'># HA
</span><span class='line'>SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark"
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do rsync -vaz spark-1.3.0-bin-2.2.0 $h:~/ --exclude=logs --exclude=metastore_db --exclude=work --delete ; done</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
</feed>
