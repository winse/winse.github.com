<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Winse Blog]]></title>
  <link href="http://winseliu.com/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2015-08-21T16:48:26+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Logstash Elasticsearch Kibana日志采集查询系统搭建]]></title>
    <link href="http://winseliu.com/blog/2015/08/21/logstash-elasticsearch-kibana-startguide/"/>
    <updated>2015-08-21T14:42:30+08:00</updated>
    <id>http://winseliu.com/blog/2015/08/21/logstash-elasticsearch-kibana-startguide</id>
    <content type="html"><![CDATA[<h2>软件版本</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@master opt]# ll
</span><span class='line'>total 20
</span><span class='line'>drwxr-xr-x 7 root root 4096 Aug 21 01:23 elasticsearch-1.7.1
</span><span class='line'>drwxr-xr-x 8 uucp  143 4096 Mar 18  2014 jdk1.8.0_05
</span><span class='line'>drwxrwxr-x 7 1000 1000 4096 Aug 21 01:09 kibana-4.1.1-linux-x64
</span><span class='line'>drwxr-xr-x 5 root root 4096 Aug 21 05:58 logstash-1.5.3
</span><span class='line'>drwxrwxr-x 6 root root 4096 Aug 21 06:44 redis-3.0.3</span></code></pre></td></tr></table></div></figure>


<h2>安装运行脚本</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># java
</span><span class='line'>vi /etc/profile
</span><span class='line'>source /etc/profile
</span><span class='line'>
</span><span class='line'>cd /opt/elasticsearch-1.7.1
</span><span class='line'>bin/elasticsearch -p elasticsearch.pid -d
</span><span class='line'>
</span><span class='line'>curl localhost:9200/_cluster/nodes/172.17.0.4
</span><span class='line'>
</span><span class='line'>cd /opt/kibana-4.1.1-linux-x64/
</span><span class='line'>bin/kibana 
</span><span class='line'># http://master:5601
</span><span class='line'>
</span><span class='line'>cd /opt/redis-3.0.3
</span><span class='line'>yum install gcc
</span><span class='line'>yum install bzip2
</span><span class='line'>make MALLOC=jemalloc
</span><span class='line'>
</span><span class='line'>nohup src/redis-server & 
</span><span class='line'>
</span><span class='line'>cd /opt/logstash-1.5.3/
</span><span class='line'>vi index.conf
</span><span class='line'>vi agent.conf
</span><span class='line'>
</span><span class='line'># agent可不加
</span><span class='line'>bin/logstash agent -f agent.conf &
</span><span class='line'>bin/logstash agent -f index.conf &</span></code></pre></td></tr></table></div></figure>


<h2>logstash配置</h2>

<p>由于程序都运行在一台机器(localhost)，redis、elasticsearch和kibana都使用默认配置。下面贴的是logstash的采集和过滤的配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@master logstash-1.5.3]# cat agent.conf 
</span><span class='line'>input {
</span><span class='line'>  file {
</span><span class='line'>    path =&gt; "/var/log/yum.log"
</span><span class='line'>    start_position =&gt; beginning
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  redis {
</span><span class='line'>    key =&gt; "logstash.redis"
</span><span class='line'>    data_type =&gt; list
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  # 便于查看调试
</span><span class='line'>  stdout { }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@master logstash-1.5.3]# cat index.conf 
</span><span class='line'>input {
</span><span class='line'>  redis {
</span><span class='line'>    data_type =&gt; list
</span><span class='line'>    key =&gt; "logstash.redis"
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  elasticsearch {
</span><span class='line'>    host =&gt; "localhost"
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>注意要改动下采集的文件！！然后启动相应的程序，打开浏览器<a href="http://master:5601">http://master:5601</a>配置一下索引项，就可以查看了。</p>

<p>至于input/output/filter(map,reduce)怎么配置，查看官方文档<a href="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html">filter-plugins</a></p>

<h2>参考</h2>

<ul>
<li><a href="http://www.cnblogs.com/buzzlight/p/logstash_elasticsearch_kibana_log.html">http://www.cnblogs.com/buzzlight/p/logstash_elasticsearch_kibana_log.html</a></li>
<li><a href="http://www.cnblogs.com/ibook360/archive/2013/03/15/2961428.html">http://www.cnblogs.com/ibook360/archive/2013/03/15/2961428.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/index.html">https://www.elastic.co/guide/en/logstash/current/index.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/first-event.html">https://www.elastic.co/guide/en/logstash/current/first-event.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html">https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/codec-plugins.html">https://www.elastic.co/guide/en/logstash/current/codec-plugins.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop不同版本yarn和hdfs混搭，spark-yarn环境配置]]></title>
    <link href="http://winseliu.com/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs/"/>
    <updated>2015-06-10T18:48:19+08:00</updated>
    <id>http://winseliu.com/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs</id>
    <content type="html"><![CDATA[<p>hadoop分为存储和计算两个主要的功能，hdfs步入hadoop2后不论稳定性还是HA等等功能都比hadoop1要更吸引人。hadoop-2.2.0的hdfs已经比较稳定，但是yarn高版本有更加丰富的功能。本文主要关注spark-yarn下日志的查看，以及spark-yarn-dynamic的配置。</p>

<p>hadoop-2.2.0的hdfs原本已经在使用的环境，在这基础上搭建运行yarn-2.6.0，以及spark-1.3.0-bin-2.2.0。</p>

<ul>
<li>编译</li>
</ul>


<p>我是在虚拟机里面编译，共享了host主机的maven库。参考【VMware共享目录】，【VMware-Centos6 Build hadoop-2.6】注意<strong>cmake_symlink_library的异常，由于共享的windows目录下不能创建linux的软链接</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxvf ~/hadoop-2.6.0-src.tar.gz 
</span><span class='line'>cd hadoop-2.6.0-src/
</span><span class='line'>mvn package -Pdist,native -DskipTests -Dtar -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'># 由于hadoop-hdfs还是2.2的，这里编译spark需要用2.2版本！
</span><span class='line'># 如果用2.6会遇到[UnsatisfiedLinkError:org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray ](http://blog.csdn.net/zeng_84_long/article/details/44340441)
</span><span class='line'>cd spark-1.3.0
</span><span class='line'>export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=1g -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>mvn clean package -Phadoop-2.2 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</span><span class='line'>
</span><span class='line'>vi make-distribution.sh #注释掉BUILD_COMMAND那一行，不重复执行package！
</span><span class='line'>./make-distribution.sh  --mvn `which mvn` --tgz  --skip-java-test -Phadoop-2.6 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests</span></code></pre></td></tr></table></div></figure>


<ul>
<li><p>配置注意点</p></li>
<li><p>core-site不要全部拷贝原来的，只要一些主要的配置即可。</p></li>
<li>yarn-site的<code>yarn.resourcemanager.webapp.address</code>需要填写具体的地址，不能写<code>0.0.0.0</code>。</li>
<li>yarn-site的<code>yarn.nodemanager.aux-services</code>添加spark_shuffle服务。<a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation</a></li>
<li>把hive-site的文件拷贝/链接到spark的conf目录下。</li>
<li>spark-yarn-dynamic配置: <a href="https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation">https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ cat conf/spark-defaults.conf 
</span><span class='line'># spark.master                     spark://bigdatamgr1:7077,bigdata8:7077
</span><span class='line'># spark.eventLog.enabled           true
</span><span class='line'># spark.eventLog.dir               hdfs://namenode:8021/directory
</span><span class='line'># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
</span><span class='line'># spark.executor.extraJavaOptions       -Xmx16g -Xms16g -Xmn256m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:ParallelGCThreads=10
</span><span class='line'>spark.driver.memory              48g
</span><span class='line'>spark.executor.memory            48g
</span><span class='line'>spark.sql.shuffle.partitions     200
</span><span class='line'>
</span><span class='line'>#spark.scheduler.mode FAIR
</span><span class='line'>spark.serializer  org.apache.spark.serializer.KryoSerializer
</span><span class='line'>spark.driver.maxResultSize 8g
</span><span class='line'>#spark.kryoserializer.buffer.max.mb 2048
</span><span class='line'>
</span><span class='line'>spark.dynamicAllocation.enabled true
</span><span class='line'>spark.dynamicAllocation.minExecutors 4
</span><span class='line'>spark.shuffle.service.enabled true
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 conf]$ cat spark-env.sh 
</span><span class='line'>#!/usr/bin/env bash
</span><span class='line'>
</span><span class='line'>JAVA_HOME=/home/eshore/jdk1.7.0_60
</span><span class='line'>
</span><span class='line'># log4j
</span><span class='line'>
</span><span class='line'>__add_to_classpath() {
</span><span class='line'>
</span><span class='line'>  root=$1
</span><span class='line'>
</span><span class='line'>  if [ -d "$root" ] ; then
</span><span class='line'>    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
</span><span class='line'>      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
</span><span class='line'>      else
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$f
</span><span class='line'>      fi
</span><span class='line'>    done
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'># this add tail of SPARK_CLASSPATH
</span><span class='line'>__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"
</span><span class='line'>
</span><span class='line'>#export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
</span><span class='line'>export HADOOP_CONF_DIR=/home/eshore/hadoop-2.6.0/etc/hadoop
</span><span class='line'>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'># HA
</span><span class='line'>SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark" 
</span><span class='line'>
</span><span class='line'>SPARK_PID_DIR=${SPARK_HOME}/pids
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>同步</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for h in `cat slaves ` ; do rsync -vaz hadoop-2.6.0 $h:~/ --delete --exclude=work --exclude=logs --exclude=metastore_db --exclude=data --exclude=pids ; done</span></code></pre></td></tr></table></div></figure>


<ul>
<li>启动spark-hive-thrift</li>
</ul>


<p>./sbin/start-thriftserver.sh &ndash;executor-memory 29g &ndash;master yarn-client</p>

<p>对于多任务的集群来说，配置自动动态分配（类似资源池）更有利于资源的使用。可以通过【All Applications】-【ApplicationMaster】-【Executors】来观察执行进程的变化。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tachyon剖析]]></title>
    <link href="http://winseliu.com/blog/2015/04/18/tachyon-deep-source/"/>
    <updated>2015-04-18T23:13:01+08:00</updated>
    <id>http://winseliu.com/blog/2015/04/18/tachyon-deep-source</id>
    <content type="html"><![CDATA[<p>要了解一个框架，一般都是从框架提供/开放的接口入手。先从最直接的方式下手，可以通过<code>tachyon tfs</code>和浏览器19999端口获取集群以及文件的相关信息。</p>

<ul>
<li>要了解tachyon首先就是其文件系统，可以从两个功能开始：命令行tachyon.command.TFsShell和web-servlet。</li>
<li>然后会深入tachyon.client包，了解<strong>TachyonFS</strong>和TachyonFile处理io的方式。以及tachyon.hadoop的包。</li>
<li>io处理：

<ul>
<li>写：BlockOutStream（#getLocalBlockTemporaryPath； MappedByteBuffer）、FileOutStream</li>
<li>读：RemoteBlockInStream、LocalBlockInStream</li>
</ul>
</li>
<li>了解thrift：MasterClient、MasterServiceHandler、WorkerClient、WorkerServiceHandler、ClientBlockInfo、ClientFileInfo。</li>
<li>看tachyon.example，巩固</li>
</ul>


<p>注：MappedByteBuffer在windows存在资源占用的bug！参见<a href="http://www.th7.cn/Program/java/2012/01/31/57401.shtml">http://www.th7.cn/Program/java/2012/01/31/57401.shtml</a>，</p>

<p>把整个io流理清之后，然后需要了解tachyon是怎么维护这些信息的：</p>

<ul>
<li>配置：WorkerConf、MasterConf、UserConf</li>
<li>了解thrift：MasterClient、MasterServiceHandler、ClientWorkerInfo、MasterInfo</li>
<li>TachyonMaster和TachyonWorker的启动</li>
<li>Checkpoint、Image、Journal</li>
<li>内存淘汰策略</li>
<li>DataServer在哪里用到（nio/netty）：TachyonFile#readRemoteByteBuffer、RemoteBlockInStream#read(byte[], int, int)</li>
<li>HA</li>
<li>Dependency（不知道怎么用）</li>
</ul>


<p>远程调试Worker以及tfs：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ cat conf/tachyon-env.sh 
</span><span class='line'>export TACHYON_WORKER_JAVA_OPTS="$TACHYON_JAVA_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070"
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ export TACHYON_JAVA_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8070"
</span><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ bin/tachyon tfs lsr /</span></code></pre></td></tr></table></div></figure>


<h2>IO/client</h2>

<ul>
<li>TachyonFS是client与Master/Worker的纽带，请求<strong>文件系统和文件</strong>的元数据CRUD操作。其中的WorkerClient仅用于写（保存）文件。</li>
<li>TachyonFile是文件的抽象处理集合，可以获取文件的基本属性（元数据），同时提供了IO的接口用于文件内容的读写。</li>
<li>InStream读、获取文件内容

<ul>
<li>EmptyBlockInStream(文件包括的块为0）</li>
<li>BlockInStream(文件包括的块为1）

<ul>
<li>LocalBlockInStream 仅当是localworker且该块在本机时，通过MappedByteBuffer获取数据（数据在ramdisk也就是内存盘上哦）。</li>
<li>RemoteBlockInStream （通过nio从远程的worker#DataServer机器获取数据#retrieveByteBufferFromRemoteMachine，如果readtype设置为cache同时把数据缓冲到本地worker）</li>
</ul>
</li>
<li>FileInStream(文件包括的块为1+，可以理解为BlocksInStream。通过mCurrentPosition / mBlockCapacity来获取当前的blockindex最终还是调用BlockInStream）</li>
</ul>
</li>
<li>FileOutStream写，写数据入口就是只有FileOutStream

<ul>
<li>BlockOutStream（WriteType设置了需要缓冲，会写到本地localworker。<strong>由于需要进行分块，会复杂些#appendCurrentBuffer</strong>）</li>
<li>UnderFileSystem（如果WriteType设置了Through，则把数据写一份到underfs文件系统）</li>
</ul>
</li>
</ul>


<h2>Master</h2>

<p>TachyonMaster是master的启动类，所有的服务都是在这个类里面初始化启动的。</p>

<ul>
<li>HA：LeaderSelectorClient</li>
<li>EditLog：EditLogProcessor、Journal。

<ul>
<li>如果是HA模式，leader调用setup方法把EditLogProcessor停掉。也就是说在HA模式下，standby才会运行EditLogProcessor实时处理editlog。</li>
<li>leader和非HA master则仅在启动时通过调用MasterInfo#init处理editlog一次。</li>
</ul>
</li>
<li>Thrift: TServer、MasterServiceHandler；与MasterClient对应的服务端。</li>
<li>Web: UIWebServer，使用jetty的内嵌服务器。</li>
<li>MasterInfo</li>
</ul>


<h2>Worker</h2>

<h2>Thrift</h2>

<h2>HA</h2>

<p>当配置<code>tachyon.usezookeeper</code>设置为true时，启动master时会初始化LeaderSelectorClient对象。使用curator连接到zookeeper服务器进行leader的选举。</p>

<p><strong>LeaderSelectorClient</strong>实现了LeaderSelectorListener接口，创建LeaderSelector并传入当前实例作为监听实例，当选举完成后，被选leader触发takeLeadership事件。</p>

<blockquote><p>public void takeLeadership(CuratorFramework client) throws Exception
Called when your instance has been granted leadership. This method should not return until you wish to release leadership</p></blockquote>

<p>takeLeadership方法中把<code>mIsLeader</code>设置为true（master自己判断）、创建<code>mLeaderFolder + mName</code>路径（客户端获取master leader）；然后隔5s的死循环（运行在LeaderSelector单独的线程池）。</p>

<h2>Checkpoint</h2>

<h2>Journal</h2>

<hr />

<h2>问题</h2>

<ul>
<li>程序没有返回内容，没有响应</li>
</ul>


<p>tfs 默认是CACHE_THROUGH，会缓冲同时写ufs。如果改成must则只写cache，然后清理内存，再获取数据，一直没有内容返回，不知道为什么？</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ export TACHYON_JAVA_OPTS="-Dtachyon.user.file.writetype.default=MUST_CACHE "
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs copyFromLocal LICENSE /LICENSE2
</span><span class='line'>Copied LICENSE to /LICENSE2
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs free /LICENSE2
</span><span class='line'>/LICENSE2 was successfully freed from memory.
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs cat /LICENSE2</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tachyon入门指南]]></title>
    <link href="http://winseliu.com/blog/2015/04/15/tachyon-quickstart/"/>
    <updated>2015-04-15T22:56:09+08:00</updated>
    <id>http://winseliu.com/blog/2015/04/15/tachyon-quickstart</id>
    <content type="html"><![CDATA[<p>tachyon程序是在HDFS与程序之间缓冲，相当于CPU与磁盘设备之间内存的功能。tachyon提供了TachyonFS、TachyonFile等API使操作起来更像一个文件系统；同时实现了HDFS的FileSystem接口，方便原有程序的迁移，只要把url的模式（schema）hdfs改成tachyon。</p>

<p>tachyon和HDFS一样也是master-slaver（worker）结构：master保存元数据，worker节点使用内存盘缓冲数据。</p>

<h2>部署集群</h2>

<p>下载tachyon的编译文件后，按下面的步骤部署：</p>

<ul>
<li>解压</li>
<li>修改conf/tachyon-env.sh（JAVA_HOME，TACHYON_UNDERFS_ADDRESS，TACHYON_MASTER_ADDRESS）</li>
<li>修改conf/worker</li>
<li>同步代码到workers子节点</li>
<li>格式化tachyon（建立master和worker所需的各种目录）</li>
<li>挂载内存盘</li>
<li>启动集群</li>
<li>通过19999端口访问</li>
</ul>


<p>如果hadoop集群的版本不是最新的2.6.0，需要手工编译源码：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mvn clean package assembly:single -Dhadoop.version=2.2.0 -DskipTests -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<p>同步程序的脚本如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do  rsync -vaz tachyon-0.6.1 $h:~/ --exclude=logs --exclude=underfs --exclude=journal ; done</span></code></pre></td></tr></table></div></figure>


<p>用tachyon用户格式化：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/tachyon format</span></code></pre></td></tr></table></div></figure>


<p>使用root挂载内存盘：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/tachyon-mount.sh Mount workers
</span><span class='line'>for h in `cat slaves ` ; do  ssh $h "chmod 777 /mnt/ramdisk; chmod 777 /mnt/tachyon_default_home"  ; done</span></code></pre></td></tr></table></div></figure>


<p>确认下worker节点是否有underfs/tmp/tachyon/data，如果没有手动创建下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do ssh $h mkdir -p ~/tachyon-0.6.1/underfs/tmp/tachyon/data ; done</span></code></pre></td></tr></table></div></figure>


<p>启动集群：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 tachyon-0.6.1]$ bin/tachyon-start.sh all NoMount</span></code></pre></td></tr></table></div></figure>


<p>上传文件到tachyon：（注意，这里是在worker节点！）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs copyFromLocal README.md /
</span><span class='line'>Copied README.md to /</span></code></pre></td></tr></table></div></figure>


<h2>集成到Spark</h2>

<p>注意，这里是在worker节点，使用local本地集群的方式（spark集群资源全部被spark-sql占用了，导致提交的任务分配不到资源！）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 spark-1.3.0-bin-2.2.0]$ export SPARK_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar 
</span><span class='line'>[eshore@bigdata1 spark-1.3.0-bin-2.2.0]$ bin/spark-shell --master local[1] -Dspark.ui.port=4041
</span><span class='line'>scala&gt; val s = sc.textFile("tachyon://bigdatamgr1:19998/README.md")
</span><span class='line'>s: org.apache.spark.rdd.RDD[String] = tachyon://bigdatamgr1:19998/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:21
</span><span class='line'>
</span><span class='line'>scala&gt; s.count()
</span><span class='line'>15/04/03 11:13:09 WARN : tachyon.home is not set. Using /mnt/tachyon_default_home as the default value.
</span><span class='line'>res0: Long = 45
</span><span class='line'>
</span><span class='line'>scala&gt; val wordCounts = s.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
</span><span class='line'>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:23
</span><span class='line'>
</span><span class='line'>scala&gt; wordCounts.saveAsTextFile("tachyon://bigdatamgr1:19998/wordcount-README")
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 tachyon-0.6.1]$ bin/tachyon tfs ls /wordcount-README/
</span><span class='line'>1407.00 B 04-03-2015 11:16:05:483  In Memory      /wordcount-README/part-00000
</span><span class='line'>0.00 B    04-03-2015 11:16:05:787  In Memory      /wordcount-README/_SUCCESS</span></code></pre></td></tr></table></div></figure>


<p>为啥要在worker节点运行呢？不能在master节点运行？运行肯定是可以的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ export SPARK_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ bin/spark-shell --master local[1] --jars /home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>
</span><span class='line'>scala&gt; val s = sc.textFile("tachyon://bigdatamgr1:19998/NOTICE")
</span><span class='line'>s: org.apache.spark.rdd.RDD[String] = tachyon://bigdatamgr1:19998/NOTICE MapPartitionsRDD[1] at textFile at &lt;console&gt;:15
</span><span class='line'>
</span><span class='line'>scala&gt; s.count()
</span><span class='line'>15/04/13 16:05:45 WARN BlockReaderLocal: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
</span><span class='line'>15/04/13 16:05:45 WARN : tachyon.home is not set. Using /mnt/tachyon_default_home as the default value.
</span><span class='line'>java.io.IOException: The machine does not have any local worker.
</span><span class='line'>        at tachyon.client.BlockOutStream.&lt;init&gt;(BlockOutStream.java:94)
</span><span class='line'>        at tachyon.client.BlockOutStream.&lt;init&gt;(BlockOutStream.java:65)
</span><span class='line'>        at tachyon.client.RemoteBlockInStream.read(RemoteBlockInStream.java:204)
</span><span class='line'>        at tachyon.hadoop.HdfsFileInputStream.read(HdfsFileInputStream.java:142)
</span><span class='line'>        at java.io.DataInputStream.read(DataInputStream.java:100)
</span><span class='line'>        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)
</span><span class='line'>        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
</span><span class='line'>        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)
</span><span class='line'>        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)
</span><span class='line'>        at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:245)
</span><span class='line'>        at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:212)
</span><span class='line'>        at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
</span><span class='line'>        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
</span><span class='line'>        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
</span><span class='line'>        at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1466)
</span><span class='line'>        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
</span><span class='line'>        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
</span><span class='line'>        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
</span><span class='line'>        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
</span><span class='line'>        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
</span><span class='line'>        at org.apache.spark.scheduler.Task.run(Task.scala:64)
</span><span class='line'>        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span><span class='line'>res0: Long = 2</span></code></pre></td></tr></table></div></figure>


<p>两个点：</p>

<ul>
<li>这里是运行的spark local集群；</li>
<li>运行当然没有问题，但是会打印不和谐的<strong>The machine does not have any local worker</strong>警告日志。这与FileSystem的获取输入流<code>ReadType.CACHE</code>实现有关（见源码HdfsFileInputStream）。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mTachyonFileInputStream = mTachyonFile.getInStream(ReadType.CACHE);</span></code></pre></td></tr></table></div></figure>


<p>如果master为spark集群，spark-driver不管运行在哪台集群都没有问题。因为，此时运行任务的spark-worker就是tachyon-worker节点啊，当然就有local worker了。</p>

<p>为了更深入的了解，还可以试验一下<code>ReadType.CACHE</code>的作用：原本不在内存的数据，计算后就会被载入到缓冲（内存）！！</p>

<p>可以再试一次，先从内存中删掉（此处underfs配置存储在HDFS）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs free /NOTICE
</span><span class='line'>/NOTICE was successfully freed from memory.
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs fileinfo /NOTICE
</span><span class='line'>/NOTICE with file id 2 has the following blocks: 
</span><span class='line'>ClientBlockInfo(blockId:2147483648, offset:0, length:62, locations:[NetAddress(mHost:bigdata8, mPort:-1, mSecondaryPort:-1), NetAddress(bigdata6, mPort:-1, mSecondaryPort:-1), NetAddress(mHost:bigdata5, mPort:-1, mSecondaryPort:-1)])</span></code></pre></td></tr></table></div></figure>


<p>再次运行count：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; s.count()
</span><span class='line'>res1: Long = 2</span></code></pre></td></tr></table></div></figure>


<p>再次查看文件状态：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs fileinfo /NOTICE
</span><span class='line'>/NOTICE with file id 2 has the following blocks: 
</span><span class='line'>ClientBlockInfo(blockId:2147483648, offset:0, length:62, locations:[NetAddress(mHost:bigdata1, mPort:29998, mSecondaryPort:29999)])</span></code></pre></td></tr></table></div></figure>


<p>此时文件对应的block所在机器变成了bigdata1，也就是spark-worker运行的节点（这里用local，worker和driver都在bigdata1上）。</p>

<p>参考</p>

<ul>
<li><a href="http://tachyon-project.org/Running-Tachyon-on-a-Cluster.html">http://tachyon-project.org/Running-Tachyon-on-a-Cluster.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html">http://spark.apache.org/docs/latest/configuration.html</a></li>
<li><a href="http://tachyon-project.org/Running-Spark-on-Tachyon.html">http://tachyon-project.org/Running-Spark-on-Tachyon.html</a></li>
</ul>


<h2>集成到Hadoop集群</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ export HADOOP_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount -libjars /home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar tachyon://bigdatamgr1:19998/NOTICE tachyon://bigdatamgr1:19998/NOTICE-wordcount
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs cat /NOTICE-wordcount/part-r-00000
</span><span class='line'>2012-2014       1
</span><span class='line'>Berkeley        1
</span><span class='line'>California,     1
</span><span class='line'>Copyright       1
</span><span class='line'>Tachyon 1
</span><span class='line'>University      1
</span><span class='line'>of      1</span></code></pre></td></tr></table></div></figure>


<h2>后记</h2>

<p>当前apache开源大部分集群的部署都是同一种模式，源码也基本都是用maven来进行构建。部署其实没有什么难度，如果是应用到spark、hadoop这样的平台，其实只要部署，然后用FileSystem的接口就一切ok了。但是要了解其原理，官网的文档也不是很全，那得需要深入源码。</p>

<p>入门写到这里，差不多了，下一篇从TachyonFS角度解析tachyon。</p>

<h2>附录</h2>

<ul>
<li>spark-env.sh</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>JAVA_HOME=/home/eshore/jdk1.7.0_60
</span><span class='line'>
</span><span class='line'># log4j
</span><span class='line'>
</span><span class='line'>__add_to_classpath() {
</span><span class='line'>
</span><span class='line'>  root=$1
</span><span class='line'>
</span><span class='line'>  if [ -d "$root" ] ; then
</span><span class='line'>    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
</span><span class='line'>      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
</span><span class='line'>      else
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$f
</span><span class='line'>      fi
</span><span class='line'>    done
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>__add_to_classpath "/home/eshore/tez-0.4.0-incubating"
</span><span class='line'>__add_to_classpath "/home/eshore/tez-0.4.0-incubating/lib"
</span><span class='line'>__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"
</span><span class='line'>
</span><span class='line'>export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
</span><span class='line'>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'># HA
</span><span class='line'>SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark"
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do rsync -vaz spark-1.3.0-bin-2.2.0 $h:~/ --exclude=logs --exclude=metastore_db --exclude=work --delete ; done</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用RamDisk来优化系统]]></title>
    <link href="http://winseliu.com/blog/2015/04/12/optimize-system-ramdisk/"/>
    <updated>2015-04-12T16:56:09+08:00</updated>
    <id>http://winseliu.com/blog/2015/04/12/optimize-system-ramdisk</id>
    <content type="html"><![CDATA[<p>最近加了一条8G的内存，总共16G。暂时来说很难全部用起来。如果能够实现类似linux的shm分区的话，那就完美了，把临时的数据全部放到这个内存分区中。最好是免费的，通过一阵折腾搜索，整理如下：</p>

<p>去到官网<a href="http://www.ltr-data.se/opencode.html/#ImDisk">http://www.ltr-data.se/opencode.html/#ImDisk</a>直接下载<code>ImDisk Toolkit</code><a href="http://reboot.pro/files/file/284-imdisk-toolkit/">http://reboot.pro/files/file/284-imdisk-toolkit/</a>，toolkit里面已经集成了ImDisk软件。（新版本的toolkit可以节省很多事情，参考最后的两个链接看看即可）</p>

<p>配置：填写大小<code>5</code>、盘符<code>S</code>、磁盘格式<code>NTFS</code>，然后点击【确定】格式化磁盘，然后就可以使用了。</p>

<p><img src="http://winseliu.com/images/blogs/ramdisk-config.png" alt="" /></p>

<p>把临时的文件目录指定到ramdisk，重启系统。</p>

<p><img src="http://winseliu.com/images/blogs/ramdisk-temp.png" alt="" /></p>

<p>上面仅仅是把用户和系统的临时目录移到<strong>内存盘</strong>中。由于rar，java一些软件都是用用户的临时目录，已经可以体验到加速的快感了！！直接拖拽解压rar情况下速度明显快了很多。</p>

<p>还有一个问题，重启后，内存盘的数据会被全部清掉。默认情况下只建立了Temp目录，没有我们指定的Cache目录。Chrome启动的时刻如果发现Cache目录为不可用状态会重建该目录。</p>

<p>在Advanced页签，<strong>Load Content from Image File or Folder</strong>选项可以选择初始化加载的内容。我们只要先把目录结构建立后，然后在初始化后加载该路径一切都解决了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\local\home\RamDiskInit&gt;find .
</span><span class='line'>.
</span><span class='line'>./Temp
</span><span class='line'>./Temp/Chrome
</span><span class='line'>./Temp/Chrome/Cache</span></code></pre></td></tr></table></div></figure>


<p>然后在<code>RamDisk Config</code>的Advanced页签选择<strong>E:\local\home\RamDiskInit</strong>作为<strong>Load Content</strong>即可。</p>

<h2>参考</h2>

<ul>
<li><a href="http://zohead.com/archives/rsync-performance-linux-cygwin-msys/">http://zohead.com/archives/rsync-performance-linux-cygwin-msys/</a> 从这里看到ramdisk-imdisk</li>
<li><a href="http://www.appinn.com/imdisk/">http://www.appinn.com/imdisk/</a> 安装简单使用，以及两篇核心文章的链接</li>
<li><a href="http://www.ltr-data.se/opencode.html/#ImDisk">http://www.ltr-data.se/opencode.html/#ImDisk</a></li>
<li><a href="http://www.kenming.idv.tw/super_lighweight_ramdisk_imdisk_setup#more-1995">超小巧效能强悍的穷人版 Ramdisk－ImDisk (设定篇) </a></li>
<li><a href="http://www.mobile01.com/topicdetail.php?f=300&amp;t=2200352">Win7 x64 下使用 ImDisk 当作RamDisk的小小心得与改良方法</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[已有HDFS上部署yarn]]></title>
    <link href="http://winseliu.com/blog/2015/03/25/deploy-separated-yarn-on-exists-hdfs-cluster/"/>
    <updated>2015-03-25T21:22:59+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/25/deploy-separated-yarn-on-exists-hdfs-cluster</id>
    <content type="html"><![CDATA[<h2>原有环境</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[biadmin@bigdatamgr1 IHC]$ pwd
</span><span class='line'>/data/opt/ibm/biginsights/IHC
</span><span class='line'>
</span><span class='line'>[biadmin@bigdatamgr1 biginsights]$ ll conf/ hadoop-conf
</span><span class='line'>conf/:
</span><span class='line'>total 64
</span><span class='line'>-rwxr-xr-x 1 biadmin biadmin  2886 Jan 30 15:09 biginsights-env.sh
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>hadoop-conf:
</span><span class='line'>total 108
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  7698 Mar 12 17:57 capacity-scheduler.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   535 Mar 12 17:57 configuration.xsl
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   872 Mar 12 17:57 console-site.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  3744 Mar 24 16:51 core-site.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   569 Mar 12 17:57 fair-scheduler.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   410 Mar 12 17:57 flex-scheduler.xml
</span><span class='line'>-rwxrwxr-x 1 biadmin biadmin  5027 Mar 12 17:57 hadoop-env.sh
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  1859 Mar 12 17:57 hadoop-metrics2.properties
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  4886 Mar 12 17:57 hadoop-policy.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  3836 Mar 12 17:57 hdfs-site.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  2678 Mar 12 17:57 ibm-hadoop.properties
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   207 Mar 12 17:57 includes
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin 10902 Mar 12 17:57 log4j.properties
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   610 Mar 12 17:57 mapred-queue-acls.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  6951 Mar 23 17:24 mapred-site.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin    44 Mar 12 17:57 masters
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   207 Mar 12 17:57 slaves
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  1243 Mar 12 17:57 ssl-client.xml.example
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  1195 Mar 12 17:57 ssl-server.xml.example
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   301 Mar 12 17:57 taskcontroller.cfg
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   172 Mar 12 17:57 zk-jaas.conf
</span><span class='line'>
</span><span class='line'>[root@bigdatamgr1 ~]# cat /etc/profile
</span><span class='line'>...
</span><span class='line'>for i in /etc/profile.d/*.sh ; do
</span><span class='line'>    if [ -r "$i" ]; then
</span><span class='line'>        if [ "${-#*i}" != "$-" ]; then
</span><span class='line'>            . "$i"
</span><span class='line'>        else
</span><span class='line'>            . "$i" &gt;/dev/null 2&gt;&1
</span><span class='line'>        fi
</span><span class='line'>    fi
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@bigdatamgr1 ~]# ll /etc/profile.d/
</span><span class='line'>total 60
</span><span class='line'>lrwxrwxrwx  1 root root   49 Jan 30 15:10 biginsights-env.sh -&gt; /data/opt/ibm/biginsights/conf/biginsights-env.sh
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[biadmin@bigdatamgr1 biginsights]$ cat hadoop-conf/hadoop-env.sh
</span><span class='line'>...
</span><span class='line'># include biginsights-env.sh
</span><span class='line'>if [ -r "/data/opt/ibm/biginsights/hdm/../conf/biginsights-env.sh" ]; then
</span><span class='line'>        source "/data/opt/ibm/biginsights/hdm/../conf/biginsights-env.sh"
</span><span class='line'>fi
</span><span class='line'>...
</span><span class='line'>export HADOOP_LOG_DIR=/data/var/ibm/biginsights/hadoop/logs
</span><span class='line'>...
</span><span class='line'>export HADOOP_PID_DIR=/data/var/ibm/biginsights/hadoop/pids
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>hdfs用的是2.x的，但是mr是1.x。真心坑爹！！</p>

<h2>单独部署新的yarn</h2>

<p>由于biginsights整了一套的环境变量，在加载profile的时刻就会进行初始化。所以需要搞一个<strong>新的用户</strong>在加载用户的环境变量的时刻把这些值清理掉。同时也为了与原来的有所区分。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ cat .bash_profile 
</span><span class='line'>...
</span><span class='line'>for i in ~/conf/*.sh ; do
</span><span class='line'>  if [ -r "$i" ] ; then
</span><span class='line'>    . "$i"
</span><span class='line'>  fi
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ ll conf/
</span><span class='line'>total 4
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 292 Mar 24 20:48 reset-biginsights-env.sh</span></code></pre></td></tr></table></div></figure>


<p>使用biadmin停掉原来的jobtracker-tasktracker。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[biadmin@bigdatamgr1 IHC]$ ssh `hdfs getconf -confKey mapreduce.jobtracker.address | sed 's/:.*//' ` "sudo -u mapred /data/opt/ibm/biginsights/IHC/sbin/hadoop-daemon.sh  stop jobtracker"
</span><span class='line'>
</span><span class='line'>[biadmin@bigdatamgr1 biginsights]$ for h in `cat hadoop-conf/slaves ` ; do ssh $h "sudo -u mapred /data/opt/ibm/biginsights/IHC/sbin/hadoop-daemon.sh  stop tasktracker" ; done
</span></code></pre></td></tr></table></div></figure>


<p>这里使用while不行，不知道为啥!?</p>

<p>部署新的hadoop-2.2.0。使用超级管理员新建目录给eshore用户：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>usermod -g biadmin eshore
</span><span class='line'>mkdir /data/opt/ibm/biginsights/hadoop-2.2.0
</span><span class='line'>chown eshore:biadmin hadoop-2.2.0</span></code></pre></td></tr></table></div></figure>


<p>使用超级管理员同步到各个slaver节点：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@bigdatamgr1 biginsights]# for line in `cat hadoop-conf/slaves` ; do ssh $line "usermod -g biadmin eshore" ; done
</span><span class='line'>
</span><span class='line'>[root@bigdatamgr1 biginsights]# cat hadoop-conf/slaves | while read line ; do rsync -vazXog hadoop-2.2.0 $line:/data/opt/ibm/biginsights/ ; done
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ cd etc/hadoop/
</span><span class='line'>[eshore@bigdatamgr1 hadoop]$ ll
</span><span class='line'>total 116
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 3560 Feb 15  2014 capacity-scheduler.xml
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 1335 Feb 15  2014 configuration.xsl
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  318 Feb 15  2014 container-executor.cfg
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  713 Mar 24 23:31 core-site.xml
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 3614 Mar 24 22:45 hadoop-env.sh
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 1774 Feb 15  2014 hadoop-metrics2.properties
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 2490 Feb 15  2014 hadoop-metrics.properties
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 9257 Feb 15  2014 hadoop-policy.xml
</span><span class='line'>lrwxrwxrwx 1 eshore biadmin   51 Mar 24 21:33 hdfs-site.xml -&gt; /data/opt/ibm/biginsights/hadoop-conf/hdfs-site.xml
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 1180 Feb 15  2014 httpfs-env.sh
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 1657 Feb 15  2014 httpfs-log4j.properties
</span><span class='line'>-rw-r--r-- 1 eshore biadmin   21 Feb 15  2014 httpfs-signature.secret
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  620 Feb 15  2014 httpfs-site.xml
</span><span class='line'>-rw-rw-r-- 1 eshore biadmin   75 Feb 15  2014 journalnodes
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 9116 Feb 15  2014 log4j.properties
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 1383 Feb 15  2014 mapred-env.sh
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 4113 Feb 15  2014 mapred-queues.xml.template
</span><span class='line'>-rw-rw-r-- 1 eshore biadmin 1508 Mar 24 21:42 mapred-site.xml
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  758 Feb 15  2014 mapred-site.xml.template
</span><span class='line'>lrwxrwxrwx 1 eshore biadmin   44 Mar 24 21:34 slaves -&gt; /data/opt/ibm/biginsights/hadoop-conf/slaves
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 2316 Feb 15  2014 ssl-client.xml.example
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 2251 Feb 15  2014 ssl-server.xml.example
</span><span class='line'>lrwxrwxrwx 1 eshore biadmin   16 Mar 25 16:10 tez-site.xml -&gt; tez-site.xml-0.4
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  282 Mar 25 15:37 tez-site.xml-0.4
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  347 Mar 25 15:49 tez-site.xml-0.6
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 4039 Mar 24 22:26 yarn-env.sh
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 1826 Mar 24 21:42 yarn-site.xml</span></code></pre></td></tr></table></div></figure>


<p>把属性配置好（hdfs，slaves<strong>可以用原来</strong>的就建立一个软链即可），然后用sbin/start-yarn.sh启动即可。</p>

<h2>其他命令</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ for line in `cat etc/hadoop/slaves` ; do echo "================$line" ; ssh $line "top -u eshore -n 1 -b | grep java | xargs -I{}  kill {} "   ; done</span></code></pre></td></tr></table></div></figure>


<h2>部署值得鉴戒学习的IBM bigsql套件：</h2>

<ul>
<li>一个管理用户部署，各个引用使用各自的用户</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@bigdatamgr1 ~]# cat /etc/sudoers
</span><span class='line'>biadmin ALL=(ALL)   NOPASSWD: ALL
</span><span class='line'>
</span><span class='line'>[root@bigdatamgr1 ~]# cat /etc/passwd
</span><span class='line'>biadmin:x:200:501::/home/biadmin:/bin/bash
</span><span class='line'>avahi-autoipd:x:170:170:Avahi IPv4LL Stack:/var/lib/avahi-autoipd:/sbin/nologin
</span><span class='line'>hive:x:205:501::/home/hive:/bin/bash
</span><span class='line'>oozie:x:206:501::/home/oozie:/bin/bash
</span><span class='line'>monitoring:x:220:501::/home/monitoring:/bin/bash
</span><span class='line'>alert:x:225:501::/home/alert:/bin/bash
</span><span class='line'>catalog:x:224:501::/home/catalog:/bin/bash
</span><span class='line'>hdfs:x:201:501::/home/hdfs:/bin/bash
</span><span class='line'>httpfs:x:221:501::/home/httpfs:/bin/bash
</span><span class='line'>bigsql:x:222:501::/home/bigsql:/bin/bash
</span><span class='line'>console:x:223:501::/home/console:/bin/bash
</span><span class='line'>mapred:x:202:501::/home/mapred:/bin/bash
</span><span class='line'>orchestrator:x:226:501::/home/orchestrator:/bin/bash
</span><span class='line'>hbase:x:204:501::/home/hbase:/bin/bash
</span><span class='line'>zookeeper:x:203:501::/home/zookeeper:/bin/bash</span></code></pre></td></tr></table></div></figure>


<p>启用时管理员用户使用<code>sudo -u XXX COMMAND</code>操作。</p>

<ul>
<li>所有应用部署/启动管理</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[biadmin@bigdatamgr1 biginsights]$ bin/start.sh -h
</span><span class='line'>Usage: start.sh &lt;component&gt;...
</span><span class='line'>    Start one or more BigInsights components. Start all components if 'all' is
</span><span class='line'>    specified. If a component is already started, this command does nothing to it.
</span><span class='line'>    
</span><span class='line'>    For example:
</span><span class='line'>        start.sh all
</span><span class='line'>          - Starts all components.
</span><span class='line'>        start.sh hadoop zookeeper
</span><span class='line'>          - Starts hadoop and zookeeper daemons.
</span><span class='line'>
</span><span class='line'>OPTIONS:
</span><span class='line'>    -ex=&lt;component&gt;
</span><span class='line'>        Exclude a component, often used together with 'all'. I.e. 
</span><span class='line'>        `stop.sh all -ex=console` stops all components but the mgmt console.
</span><span class='line'>
</span><span class='line'>    -h, --help
</span><span class='line'>        Get help information.</span></code></pre></td></tr></table></div></figure>


<ul>
<li>反复依赖的包，通过软链来管理</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[biadmin@bigdatamgr1 lib]$ ll
</span><span class='line'>total 50336
</span><span class='line'>-rw-r--r-- 1 biadmin biadmin   303042 Jan 30 15:22 avro-1.7.4.jar
</span><span class='line'>lrwxrwxrwx 1 biadmin biadmin       60 Jan 30 15:22 biginsights-gpfs-2.2.0.jar -&gt; /data/opt/ibm/biginsights/IHC/lib/biginsights-gpfs-2.2.0.jar
</span><span class='line'>-rw-r--r-- 1 biadmin biadmin    15322 Jan 30 15:22 findbugs-annotations-1.3.9-1.jar
</span><span class='line'>lrwxrwxrwx 1 biadmin biadmin       48 Jan 30 15:22 guardium-proxy.jar -&gt; /data/opt/ibm/biginsights/lib/guardium-proxy.jar
</span><span class='line'>-rw-r--r-- 1 biadmin biadmin  1795932 Jan 30 15:22 guava-12.0.1.jar
</span><span class='line'>-rw-r--r-- 1 biadmin biadmin   710492 Jan 30 15:22 guice-3.0.jar
</span><span class='line'>-rw-r--r-- 1 biadmin biadmin    65012 Jan 30 15:22 guice-servlet-3.0.jar
</span><span class='line'>lrwxrwxrwx 1 biadmin biadmin       45 Jan 30 15:22 hadoop-core.jar -&gt; /data/opt/ibm/biginsights/IHC/hadoop-core.jar
</span><span class='line'>lrwxrwxrwx 1 biadmin biadmin       76 Jan 30 15:22 hadoop-distcp-2.2.0.jar -&gt; /data/opt/ibm/biginsights/IHC/share/hadoop/tools/lib/hadoop-distcp-2.2.0.jar</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Distcp]]></title>
    <link href="http://winseliu.com/blog/2015/03/13/hadoop-distcp/"/>
    <updated>2015-03-13T20:38:23+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/13/hadoop-distcp</id>
    <content type="html"><![CDATA[<p>HDFS提供的CP是单线程的，对于大数据量的拷贝操作希望能并行的复制。Hadoop Tools提供了DistCp工具，通过调用MapRed来实现并行的拷贝。</p>

<h2>先来了解下hdfs cp的功能：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage: hdfs dfs -cp [-f] [-p | -p[topax]] URI [URI ...] &lt;dest&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-not-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -mkdir /cp-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-not-exists2/
</span><span class='line'>cp: `/cp-not-exists2/': No such file or directory
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -ls -R /
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 19:55 /cp
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 19:55 /cp/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 19:54 /cp/README.txt
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-exists
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-exists/cp
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-exists/cp/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-exists/cp/README.txt
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-not-exists
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-not-exists/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-not-exists/README.txt</span></code></pre></td></tr></table></div></figure>


<h2>DistCp(distributed copy)分布式拷贝简单使用方式：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ bin/hadoop distcp /cp /cp-distcp</span></code></pre></td></tr></table></div></figure>


<p>用到分布式一般就说明规模不少，且数据量大，操作时间长。DistCp提供了一些参数来控制程序：</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> DistCpOptionSwitch选项    </th>
<th style="text-align:center;"> 命令行参数                      </th>
<th style="text-align:left;"> 描述                                        </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> LOG_PATH                  </td>
<td style="text-align:center;"> <code>-log &lt;logdir&gt;               </code> </td>
<td style="text-align:left;"> map结果输出的目录。默认为<code>JobStagingDir/_logs</code>，在DistCp#configureOutputFormat把该路径设置给CopyOutputFormat#setOutputPath。</td>
</tr>
<tr>
<td style="text-align:left;"> SOURCE_FILE_LISTING       </td>
<td style="text-align:center;"> <code>-f &lt;urilist_uri&gt;            </code> </td>
<td style="text-align:left;"> 需要拷贝的source-path&hellip;从改文件获取。</td>
</tr>
<tr>
<td style="text-align:left;"> MAX_MAPS                  </td>
<td style="text-align:center;"> <code>-m &lt;num_maps&gt;               </code> </td>
<td style="text-align:left;"> 默认20个，创建job时通过<code>JobContext.NUM_MAPS</code>添加到配置。</td>
</tr>
<tr>
<td style="text-align:left;"> ATOMIC_COMMIT             </td>
<td style="text-align:center;"> <code>-atomic                     </code> </td>
<td style="text-align:left;"> 原子操作。要么全部拷贝成功，那么失败。与<code>SYNC_FOLDERS</code> &amp; <code>DELETE_MISSING</code>选项不兼容。</td>
</tr>
<tr>
<td style="text-align:left;"> WORK_PATH                 </td>
<td style="text-align:center;"> <code>-tmp &lt;tmp_dir&gt;              </code> </td>
<td style="text-align:left;"> 与atomic一起使用，中间过程存储数据目录。成功后在CopyCommitter一次性移动到target-path下。</td>
</tr>
<tr>
<td style="text-align:left;"> SYNC_FOLDERS              </td>
<td style="text-align:center;"> <code>-update                     </code> </td>
<td style="text-align:left;"> 新建或更新文件。当文件大小和blockSize（以及crc）一样忽略。</td>
</tr>
<tr>
<td style="text-align:left;"> DELETE_MISSING            </td>
<td style="text-align:center;"> <code>-delete                     </code> </td>
<td style="text-align:left;"> 针对target-path目录，清理source-paths目录下没有的文件。常和<code>SYNC_FOLDERS</code>选项一起使用。</td>
</tr>
<tr>
<td style="text-align:left;"> BLOCKING                  </td>
<td style="text-align:center;"> <code>-async                      </code> </td>
<td style="text-align:left;"> 异步运行。其实就是job提交后，不打印日志了没有调用<code>job.waitForCompletion(true)</code>罢了。</td>
</tr>
<tr>
<td style="text-align:left;"> BANDWIDTH                 </td>
<td style="text-align:center;"> <code>-bandwidth num(M)           </code> </td>
<td style="text-align:left;"> 获取数据的最大速度。结合ThrottledInputStream来进行控制，在RetriableFileCopyCommand中初始化。</td>
</tr>
<tr>
<td style="text-align:left;"> COPY_STRATEGY             </td>
<td style="text-align:center;"> <code>-strategy dynamic/uniformsize</code> </td>
<td style="text-align:left;"> 复制的时刻分组策略，即每个Map到底处理写什么数据。后面会讲到，分为静态和动态。</td>
</tr>
</tbody>
</table>


<p>还有新增的两个属性skipcrccheck（SKIP_CRC），append（APPEND）。保留Preserve 属性和ssl选项由于暂时没用到，这里不表，以后用到再补充。</p>

<h2>DistCp的源码</h2>

<p>放在<code>hadoop-2.6.0-src\hadoop-tools\hadoop-distcp</code>目录下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn eclipse:eclipse </span></code></pre></td></tr></table></div></figure>


<p>网络没问题的话，一般都能成功生成.classpath和.project两个Eclipse需要的项目文件。然后把项目导入eclipse即可。包括4个目录。</p>

<p>还是先说说整个distcp的实现流程，看看distcp怎么跑的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ export HADOOP_CLIENT_OPTS="-Dhadoop.root.logger=debug,console -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8071"
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop distcp /cp /cp-distcp
</span><span class='line'>Listening for transport dt_socket at address: 8071</span></code></pre></td></tr></table></div></figure>


<p>运行eclipse远程调试，连接服务器的8071端口，在DistCp的run方法打个断点，就可以调试了解其运行方式。修改log4j为debug，可以查看更详细的日志，了解执行的流程。</p>

<p>服务器的jdk版本和本地eclipse的jdk版本最好一致，这样调试的时刻比较顺畅。</p>

<h3>Driver</h3>

<p>首先进到DistCp(Driver)的main方法，DistCp继承Configured实现了Tool接口，</p>

<p>第一步解析参数</p>

<ol>
<li>使用<code>ToolRunner.run</code>运行会调用GenericOptionsParser解析<code>-D</code>的属性到Configuration实例；</li>
<li>进到run方法后，通过<code>OptionsParser.parse</code>来解析配置为DistCpOptions实例；功能比较单一，主要涉及到DistCpOptionSwitch和DistCpOptions两个类。</li>
</ol>


<p>第二步准备MapRed的Job实例</p>

<ol>
<li>创建metaFolderPath（后面的 待拷贝文件seq文件存取的位置：StagingDir/_distcp[RAND]），对应<code>CONF_LABEL_META_FOLDER</code>属性；</li>
<li>创建Job，设置名称、InputFormat(UniformSizeInputFormat|DynamicInputFormat)、Map类CopyMapper、Map个数（默认20个）、Reduce个数（0个）、OutputKey|ValueClass、MAP_SPECULATIVE（使用RetriableCommand代替）、CopyOutputFormat</li>
<li>把命令行的配置写入Configuration。</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>metaFolderPath /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp-1344594636</span></code></pre></td></tr></table></div></figure>


<p>此处有话题，设置InputFormat时通过<code>DistCpUtils#getStrategy</code>获取，代码中并没有<code>strategy.impl</code>的键加入到configuration。why？此处也是我们可以学习的，这个设置项在distcp-default.xml配置文件中，这种方式可以实现代码的解耦。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public static Class&lt;? extends InputFormat&gt; getStrategy(Configuration conf,
</span><span class='line'>                                                                 DistCpOptions options) {
</span><span class='line'>    String confLabel = "distcp." +
</span><span class='line'>        options.getCopyStrategy().toLowerCase(Locale.getDefault()) + ".strategy.impl";
</span><span class='line'>    return conf.getClass(confLabel, UniformSizeInputFormat.class, InputFormat.class);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>// 配置
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>        &lt;name&gt;distcp.dynamic.strategy.impl&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;org.apache.hadoop.tools.mapred.lib.DynamicInputFormat&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;Implementation of dynamic input format&lt;/description&gt;
</span><span class='line'>    &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>        &lt;name&gt;distcp.static.strategy.impl&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;org.apache.hadoop.tools.mapred.UniformSizeInputFormat&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;Implementation of static input format&lt;/description&gt;
</span><span class='line'>    &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>配置CopyOutputFormat时，设置了三个路径：</p>

<ul>
<li>WorkingDirectory（中间临时存储目录，atomic选项时为tmp路径，否则为target-path路径）；</li>
<li>CommitDirectory（文件拷贝最终目录，即target-path）；</li>
<li>OutputPath（map write记录输出路径）。</li>
</ul>


<p>关于命令行选项有一个疑问，用eclipse查看<code>Call Hierachy</code>调用关系的时刻，并没有发现调用<code>DistCpOptions#getXXX</code>的方法，那么是通过什么方式把这些配置项设置到Configuration的呢？ 在DistCpOptionSwitch的枚举类中定义了每个选项的confLabel，在<code>DistCpOptions#appendToConf</code>方法中一起把这些属性填充到Configuration中。 [统一配置] ！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public void appendToConf(Configuration conf) {
</span><span class='line'>    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.ATOMIC_COMMIT,
</span><span class='line'>        String.valueOf(atomicCommit));
</span><span class='line'>    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.IGNORE_FAILURES,
</span><span class='line'>        String.valueOf(ignoreFailures));
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>第三步整理需要拷贝的文件列表</p>

<p>这个真tmd的独到，提前把要做的事情规划好。需要拷贝的列表数据最终写入<code>[metaFolder]/fileList.seq</code>（key：与source-path的相对路径，value：该文件的CopyListingFileStatus），对应<code>CONF_LABEL_LISTING_FILE_PATH</code>，也就是map的输入（在自定义的InputFormat中处理）。</p>

<p>涉及CopyList的三个实现FileBasedCopyListing（-f）、GlobbedCopyListing、SimpleCopyListing。最终都调用SimpleCopyListing把文件和空目录列表写入到fileList.seq；最后校验否有重复的文件名，如果存在会抛出DuplicateFileException。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/tmp/hadoop-yarn/staging/hadoop/.staging/_distcp179796572/fileList.seq</span></code></pre></td></tr></table></div></figure>


<p>同时计算需要拷贝的个数和大小（Byte），对应<code>CONF_LABEL_TOTAL_BYTES_TO_BE_COPIED</code>和<code>CONF_LABEL_TOTAL_NUMBER_OF_RECORDS</code>。</p>

<p>第四步提交任务，等待等待无尽的等待。</p>

<p>也可以设置async选项，提交成功后直接完成Driver。</p>

<h3>Mapper</h3>

<p>首先，setup从Configuration中获取配置属性：sync(update)/忽略错误(i)/校验码/overWrite/workPath/finalPath</p>

<p>然后，从CONF_LABEL_LISTING_FILE_PATH路径获取准备好的sourcepath->CopyListingFileStatus键值对作为map的输入。</p>

<p>其实CopyListingFileStatus这个对象真正用到的就是原始Path的路径，真心不知道搞这么多属性干嘛！获取原始路径后又重新实例CopyListingFileStatus为sourceCurrStatus。</p>

<ul>
<li>如果源路径为文件夹，调用createTargetDirsWithRetry（RetriableDirectoryCreateCommand）创建路径，COPY计数加1，return。</li>
<li>如果源路径为文件，但是checkUpdate（文件大小和块大小一致）为skip，SKIP计数加1，BYTESSKIPPED计数加上sourceCurrStatus的长度，把改条记录写入map输出，return。</li>
<li>如果源路径为文件，且检查后不是skip则调用copyFileWithRetry（RetriableFileCopyCommand）拷贝文件，BYTESEXPECTED计数加上sourceCurrStatus的长度，BYTESCOPIED计数加上拷贝文件的大小，COPY计数加1，再return。</li>
<li>如果配置有保留文件/文件夹属性，对目标进行属性修改。</li>
</ul>


<p>从CopyListing获取数据，调用FileSystem-IO接口进行数据的拷贝（在原有IO的基础上封装了ThrottledInputStream来进行限流处理）。于此同时会涉及到source路径是文件夹但是target不是文件夹等的检查；更新还是覆盖；文件属性的保留和Map计数值的更新操作。</p>

<h3>InputFormat</h3>

<p>自定义了InputFormat来UniformSizeInputFormat进行拆分构造FileSplit，对CONF_LABEL_LISTING_FILE_PATH文件的每个键值的文件大小平均分成Map num
个数小块，根据键值的位置构造Map num个FileSplit对象。执行map时，RecordReader根据FileSplit来获取键值对，然后传递给map。</p>

<p>新版本的增加了DynamicInputFormat，实现能者多难的功能。先通过实际的日志，看看运行效果：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ export HADOOP_CLIENT_OPTS="-Dhadoop.root.logger=debug,console -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8071"
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop distcp "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090" -strategy dynamic -m 2 /cp /cp-distcp-dynamic
</span><span class='line'>
</span><span class='line'># 创建的chunk
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        198 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00000
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        224 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00001
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        220 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00002
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>
</span><span class='line'># 分配后的chunk
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        220 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00002
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        198 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/task_1426773672048_0006_m_000000
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        224 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/task_1426773672048_0006_m_000001
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>
</span><span class='line'># map获取后
</span><span class='line'>[hadoop@hadoop-master2 ~]$  ssh -g -L 8090:hadoop-slaver1:8090 hadoop-slaver1
</span><span class='line'># 每拷贝完一个chunk/最后map结束，会把上一个跑完的chunk文件删除
</span><span class='line'># job跑完后，临时目录的数据就被清楚了
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>ls: `/tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446': No such file or directory</span></code></pre></td></tr></table></div></figure>


<p>由于设置的map num为2，还有一个chunk没有分配出去，等到真正执行的时刻再进行分配。体现了策略的动态性。这个<strong>chunkm_000000分配给map0(其他类似)</strong>，其他没有分配出去的chunk让给map去<strong>抢</strong>。</p>

<p>首先InputFormat创建FileSplit，在此过程中把原来的<code>CONF_LABEL_LISTING_FILE_PATH</code>中的需要处理的文件根据个数等份成chunk。（具体实现看源码，其中numEntriesPerChunk计算一个chunk几个文件比较复杂点）</p>

<p>chunk中的也是sourcepath->CopyListingFileStatus键值对，以seq格式的存储文件中。<code>DynamicInputChunk#acquire(TaskAttemptContext)</code>读取数据的时刻比较有意思，在Driver阶段分配的chunk处理完后，就会动态的取处理余下的chunk，能者多劳。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public static DynamicInputChunk acquire(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
</span><span class='line'>    if (!areInvariantsInitialized())
</span><span class='line'>        initializeChunkInvariants(taskAttemptContext.getConfiguration());
</span><span class='line'>
</span><span class='line'>    String taskId = taskAttemptContext.getTaskAttemptID().getTaskID().toString();
</span><span class='line'>    Path acquiredFilePath = new Path(chunkRootPath, taskId);
</span><span class='line'>
</span><span class='line'>    if (fs.exists(acquiredFilePath)) {
</span><span class='line'>      LOG.info("Acquiring pre-assigned chunk: " + acquiredFilePath);
</span><span class='line'>      return new DynamicInputChunk(acquiredFilePath, taskAttemptContext);
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    for (FileStatus chunkFile : getListOfChunkFiles()) {
</span><span class='line'>      if (fs.rename(chunkFile.getPath(), acquiredFilePath)) {
</span><span class='line'>        LOG.info(taskId + " acquired " + chunkFile.getPath());
</span><span class='line'>        return new DynamicInputChunk(acquiredFilePath, taskAttemptContext);
</span><span class='line'>      }
</span><span class='line'>      else
</span><span class='line'>        LOG.warn(taskId + " could not acquire " + chunkFile.getPath());
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    return null;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<h3>OutputFormat &amp; Committer</h3>

<p>自定义的CopyOutputFormat包括了working/commit/output路径的get/set方法，同时指定了自定义的OutputCommitter：CopyCommitter。</p>

<p>正常情况为app-master调用CopyCommitter#commitJob处理善后的事情：保留文件属性的情况下更新文件的属性，atomic情况下把working转到commit路径，delete情况下删除target目录多余的文件。最后清理临时目录。</p>

<p>看完DistCp然后再去看DistCpV1，尽管说功能上类似，但是要和新版本对上仍然要去看distcp的代码。好的代码就是这样吧，让人很自然轻松的理解，而不必反复来回的折腾，甚至于为了免得来回折腾而记住该代码块。（类太大，方法太长，变量定义和使用的位置相隔很远！一个变量作用域太长赋值变更次数太多）</p>

<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/FileSystemShell.html#cp">FileSystemShell cp</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistCp.html">DistCp官方文档</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Windows Build hadoop-2.6]]></title>
    <link href="http://winseliu.com/blog/2015/03/09/windows-build-hadoop-2-dot-6/"/>
    <updated>2015-03-09T12:01:55+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/09/windows-build-hadoop-2-dot-6</id>
    <content type="html"><![CDATA[<h2>环境</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>C:\Users\winse&gt;java -version
</span><span class='line'>java version "1.7.0_02"
</span><span class='line'>Java(TM) SE Runtime Environment (build 1.7.0_02-b13)
</span><span class='line'>Java HotSpot(TM) Client VM (build 22.0-b10, mixed mode, sharing)
</span><span class='line'>
</span><span class='line'>C:\Users\winse&gt;protoc --version
</span><span class='line'>libprotoc 2.5.0
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ cygcheck -c cygwin
</span><span class='line'>Cygwin Package Information
</span><span class='line'>Package              Version        Status
</span><span class='line'>cygwin               1.7.33-1       OK</span></code></pre></td></tr></table></div></figure>


<h2>具体步骤</h2>

<p>在windows下，hadoop-2.6还不能直接编译java-x86的dll。需要自己处理/打patch<a href="https://issues.apache.org/jira/browse/HADOOP-9922">HADOOP-9922</a>，但是官网jira-patch给出来的和2.6.0-src对不上。自己动手丰衣足食，把x64的全部改成Win32即可，附编译成功的patch<a href="http://yunpan.cn/cJaZzSu6DIibg">下载hadoop-2.6.0-common-native-win32-diff.patch（提取码：08fd）</a>。</p>

<ul>
<li>用visual studio2010的x86命令行进入：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Visual Studio 命令提示(2010)
</span><span class='line'>
</span><span class='line'>Setting environment for using Microsoft Visual Studio 2010 x86 tools.</span></code></pre></td></tr></table></div></figure>


<ul>
<li>切换到hadoop源码目录，打补丁和编译。同时protobuf目录和cygwin\bin目录加入PATH：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd hadoop-2.6.0-src
</span><span class='line'>cd hadoop-common-project\hadoop-common
</span><span class='line'>patch -p0 &lt; hadoop-2.6.0-common-native-win32-diff.patch
</span><span class='line'>
</span><span class='line'>set PATH=%PATH%;E:\local\home\Administrator\bin;c:\cygwin\bin
</span><span class='line'>
</span><span class='line'>mvn package -Pdist,native-win -DskipTests -Dtar -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<ul>
<li>编译完成后，直接把<code>hadoop-common\target\bin</code>目录下的内容拷贝到程序的bin目录下。</li>
</ul>


<p>在windows下，执行java程序java.library.path默认到PATH路径找。这也是需要定义<strong>环境变量HADOOP_HOME</strong>，以及把<code>%HADOOP_HOME%\bin</code>加入到PATH的原因！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>HADOOP_HOME=E:\local\libs\big\hadoop-2.2.0 
</span><span class='line'>PATH=%HADOOP_HOME%\bin;%PATH%</span></code></pre></td></tr></table></div></figure>


<ul>
<li>配置坑：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0
</span><span class='line'>$ find . -name "*-default.xml" | xargs -I{} grep "hadoop.tmp.dir" {}
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/local&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/system&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/staging&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/temp&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/history/recoverystore&lt;/value&gt;
</span><span class='line'>  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/io/local&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/s3&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/s3a&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/name&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/data&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/namesecondary&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn/system/rmstore&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/nm-local-dir&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn-nm-recovery&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn/timeline&lt;/value&gt;</span></code></pre></td></tr></table></div></figure>


<p>就dfs的在前面加了<code>file://</code>前缀！</p>

<p>所以，在windows下如果你只配置hadoop.tmp.dir（<code>file:///e:/tmp/hadoop</code>）的话还得同时配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/name&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/data&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/namesecondary&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>接下来格式化，启动都和同时一样。</p>

<h2>其他</h2>

<p>调试，下载maven源码等</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set HADOOP_NAMENODE_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"
</span><span class='line'>
</span><span class='line'>mvn dependency:resolve -Dclassifier=sources
</span><span class='line'>
</span><span class='line'>mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs 
</span><span class='line'>
</span><span class='line'>mvn dependency:sources 
</span><span class='line'>mvn dependency:resolve -Dclassifier=javadoc
</span><span class='line'>
</span><span class='line'>/* 操作HDFS */
</span><span class='line'>set HADOOP_ROOT_LOGGER=DEBUG,console</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VMware-Centos6 Build hadoop-2.6]]></title>
    <link href="http://winseliu.com/blog/2015/03/08/vmware-build-hadoop2-dot-6/"/>
    <updated>2015-03-08T08:22:14+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/08/vmware-build-hadoop2-dot-6</id>
    <content type="html"><![CDATA[<p>每次编译hadoop（-common）都是惊心动魄，没一次顺顺当当的！由于作者的偷懒，引发的有一起血案~~~</p>

<h2>环境说明</h2>

<ul>
<li>操作系统</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# uname -a
</span><span class='line'>Linux localhost.localdomain 2.6.32-431.el6.x86_64 #1 SMP Fri Nov 22 03:15:09 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
</span><span class='line'>[root@localhost ~]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.5 (Final)</span></code></pre></td></tr></table></div></figure>


<ul>
<li>使用VMware的<strong>Shared Folders</strong>建立了maven和hadoop-2.6.0-src到宿主机器的映射：(不要直接在源码映射的目录下编译，先拷贝到linux的硬盘下！！)</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# ll -a hadoop-2.6.0-src maven
</span><span class='line'>lrwxrwxrwx. 1 root root 26 Mar  7 22:47 hadoop-2.6.0-src -&gt; /mnt/hgfs/hadoop-2.6.0-src
</span><span class='line'>lrwxrwxrwx. 1 root root 15 Mar  7 22:47 maven -&gt; /mnt/hgfs/maven</span></code></pre></td></tr></table></div></figure>


<h2>具体操作</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 安装maven，jdk
</span><span class='line'>cat apache-maven-3.2.3-bin.tar.gz | ssh root@192.168.154.130 "cat - | tar zxv "
</span><span class='line'>
</span><span class='line'>tar zxvf jdk-7u60-linux-x64.gz -C ~/
</span><span class='line'>vi .bash_profile 
</span><span class='line'>
</span><span class='line'># 开发环境
</span><span class='line'>yum install gcc glibc-headers gcc-c++ zlib-devel
</span><span class='line'>yum install openssl-devel
</span><span class='line'>
</span><span class='line'># 安装protobuf
</span><span class='line'>tar zxvf protobuf-2.5.0.tar.gz 
</span><span class='line'>cd protobuf-2.5.0
</span><span class='line'>./configure 
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>## 编译hadoop-common
</span><span class='line'># 从映射文件中拷贝hadoop-common到linux文件系统，然后在编译hadoop-common
</span><span class='line'>cd hadoop-2.6.0-src/hadoop-common-project/hadoop-common/
</span><span class='line'>cd ..
</span><span class='line'>cp -r  hadoop-common ~/  #Q:为啥要拷贝一份，【遇到的问题】中有进行解析
</span><span class='line'>cd ~/hadoop-common
</span><span class='line'>mvn install
</span><span class='line'>mvn -X package -Pdist,native -Dmaven.test.skip=true -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'>## 编译全部，耗时比较久，可以先去吃个饭^v^
</span><span class='line'>cp -r /mnt/hgfs/hadoop-2.6.0-src ~/
</span><span class='line'>mvn package -Pdist,native -DskipTests -Dmaven.javadoc.skip=true #Q:这里为啥不能用maven.test.skip?</span></code></pre></td></tr></table></div></figure>


<h2>遇到的问题</h2>

<ul>
<li><p>第一个问题肯定是没有<strong>c</strong>的编译环境，安装gcc即可。</p></li>
<li><p><code>configure: error: C++ preprocessor "/lib/cpp" fails sanity check</code>，安装c++。</p></li>
</ul>


<p>-> <a href="http://www.cnblogs.com/niocai/archive/2011/11/04/2236458.html">configure: error: C++ preprocessor &ldquo;/lib/cpp&rdquo; fails sanity check</a></p>

<ul>
<li><code>Unknown lifecycle phase "c"</code>，点击错误提示最后的链接查看解决方法，即执行<code>mvn install</code>。</li>
</ul>


<p>-> <a href="http://blog.csdn.net/kamemo/article/details/6523992">执行第一maven用例出错：Unknown lifecycle phase &ldquo;complile&rdquo;.</a>
-> <a href="https://cwiki.apache.org/confluence/display/MAVEN/LifecyclePhaseNotFoundException">LifecyclePhaseNotFoundException</a></p>

<ul>
<li><code>CMake Error at /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:108 (message): Could NOT find ZLIB (missing: ZLIB_INCLUDE_DIR)</code>， 缺少zlib-devel。</li>
</ul>


<p>-> <a href="http://ask.csdn.net/questions/62307">Cmake时报错：Could NOT find ImageMagick</a></p>

<ul>
<li><code>cmake_symlink_library: System Error: Operation not supported</code>， 共享的windows目录下不能创建linux的软链接。</li>
</ul>


<p>-> <a href="http://bbs.chinaunix.net/forum.php?mod=viewthread&amp;tid=3595245&amp;fromuid=26971268">参见9楼回复</a></p>

<blockquote><p>创建链接不成功，要确认当前帐户下是否有权限在编译的目录中有创建链接的权限</p>

<p>比如，你如果是在一个WINDOWS机器上的共享目录中编译，就没法创建链接，就会失败。把源码复制到本地的目录中再编译就不会有这问题。</p></blockquote>

<ul>
<li>全部编译时需使用skipTests。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>main:
</span><span class='line'>     [echo] Running test_libhdfs_threaded
</span><span class='line'>     [exec] nmdCreate: NativeMiniDfsCluster#Builder#Builder error:
</span><span class='line'>     [exec] java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/MiniDFSCluster$Builder
</span><span class='line'>     [exec] Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdfs.MiniDFSCluster$Builder
</span><span class='line'>     [exec]     at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
</span><span class='line'>     [exec]     at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
</span><span class='line'>     [exec]     at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>     [exec]     at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
</span><span class='line'>     [exec]     at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
</span><span class='line'>     [exec]     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
</span><span class='line'>     [exec]     at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</span><span class='line'>     [exec] TEST_ERROR: failed on /root/hadoop-2.6.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c:326 (errno: 2): got NULL from tlhCluster</span></code></pre></td></tr></table></div></figure>


<ul>
<li><code>Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the</code>，安装openssl-devel。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>main:
</span><span class='line'>    [mkdir] Created dir: /root/hadoop-2.6.0-src/hadoop-tools/hadoop-pipes/target/native
</span><span class='line'>     [exec] -- The C compiler identification is GNU 4.4.7
</span><span class='line'>     [exec] -- The CXX compiler identification is GNU 4.4.7
</span><span class='line'>     [exec] -- Check for working C compiler: /usr/bin/cc
</span><span class='line'>     [exec] -- Check for working C compiler: /usr/bin/cc -- works
</span><span class='line'>     [exec] -- Detecting C compiler ABI info
</span><span class='line'>     [exec] -- Detecting C compiler ABI info - done
</span><span class='line'>     [exec] -- Check for working CXX compiler: /usr/bin/c++
</span><span class='line'>     [exec] -- Check for working CXX compiler: /usr/bin/c++ -- works
</span><span class='line'>     [exec] -- Detecting CXX compiler ABI info
</span><span class='line'>     [exec] -- Detecting CXX compiler ABI info - done
</span><span class='line'>     [exec] -- Configuring incomplete, errors occurred!
</span><span class='line'>     [exec] See also "/root/hadoop-2.6.0-src/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeOutput.log".
</span><span class='line'>     [exec] CMake Error at /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:108 (message):
</span><span class='line'>     [exec]   Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the
</span><span class='line'>     [exec]   system variable OPENSSL_ROOT_DIR (missing: OPENSSL_LIBRARIES
</span><span class='line'>     [exec]   OPENSSL_INCLUDE_DIR)
</span><span class='line'>     [exec] Call Stack (most recent call first):
</span><span class='line'>     [exec]   /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:315 (_FPHSA_FAILURE_MESSAGE)
</span><span class='line'>     [exec]   /usr/share/cmake/Modules/FindOpenSSL.cmake:313 (find_package_handle_standard_args)
</span><span class='line'>     [exec]   CMakeLists.txt:20 (find_package)
</span><span class='line'>     [exec] 
</span><span class='line'>     [exec] </span></code></pre></td></tr></table></div></figure>


<h2>成功</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[INFO] Executed tasks
</span><span class='line'>[INFO] 
</span><span class='line'>[INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---
</span><span class='line'>[INFO] Skipping javadoc generation
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Reactor Summary:
</span><span class='line'>[INFO] 
</span><span class='line'>[INFO] Apache Hadoop Main ................................. SUCCESS [ 43.005 s]
</span><span class='line'>[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 25.511 s]
</span><span class='line'>[INFO] Apache Hadoop Annotations .......................... SUCCESS [ 21.177 s]
</span><span class='line'>[INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 11.728 s]
</span><span class='line'>[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 51.274 s]
</span><span class='line'>[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 35.625 s]
</span><span class='line'>[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 21.936 s]
</span><span class='line'>[INFO] Apache Hadoop Auth ................................. SUCCESS [ 24.665 s]
</span><span class='line'>[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 17.058 s]
</span><span class='line'>[INFO] Apache Hadoop Common ............................... SUCCESS [06:07 min]
</span><span class='line'>[INFO] Apache Hadoop NFS .................................. SUCCESS [ 41.279 s]
</span><span class='line'>[INFO] Apache Hadoop KMS .................................. SUCCESS [ 59.186 s]
</span><span class='line'>[INFO] Apache Hadoop Common Project ....................... SUCCESS [  7.216 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS ................................. SUCCESS [04:29 min]
</span><span class='line'>[INFO] Apache Hadoop HttpFS ............................... SUCCESS [ 52.883 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.972 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 24.901 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  7.486 s]
</span><span class='line'>[INFO] hadoop-yarn ........................................ SUCCESS [  7.466 s]
</span><span class='line'>[INFO] hadoop-yarn-api .................................... SUCCESS [ 32.970 s]
</span><span class='line'>[INFO] hadoop-yarn-common ................................. SUCCESS [ 25.549 s]
</span><span class='line'>[INFO] hadoop-yarn-server ................................. SUCCESS [  6.709 s]
</span><span class='line'>[INFO] hadoop-yarn-server-common .......................... SUCCESS [ 25.292 s]
</span><span class='line'>[INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 29.555 s]
</span><span class='line'>[INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [ 12.800 s]
</span><span class='line'>[INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 14.025 s]
</span><span class='line'>[INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 21.121 s]
</span><span class='line'>[INFO] hadoop-yarn-server-tests ........................... SUCCESS [ 24.019 s]
</span><span class='line'>[INFO] hadoop-yarn-client ................................. SUCCESS [ 18.949 s]
</span><span class='line'>[INFO] hadoop-yarn-applications ........................... SUCCESS [  7.586 s]
</span><span class='line'>[INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  8.428 s]
</span><span class='line'>[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [ 12.671 s]
</span><span class='line'>[INFO] hadoop-yarn-site ................................... SUCCESS [  7.518 s]
</span><span class='line'>[INFO] hadoop-yarn-registry ............................... SUCCESS [ 18.518 s]
</span><span class='line'>[INFO] hadoop-yarn-project ................................ SUCCESS [ 38.781 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client ............................ SUCCESS [ 13.133 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 23.772 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 22.815 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [ 16.810 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.404 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [ 18.157 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 14.637 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  9.190 s]
</span><span class='line'>[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  9.037 s]
</span><span class='line'>[INFO] hadoop-mapreduce ................................... SUCCESS [ 59.280 s]
</span><span class='line'>[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 26.724 s]
</span><span class='line'>[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 31.503 s]
</span><span class='line'>[INFO] Apache Hadoop Archives ............................. SUCCESS [ 19.867 s]
</span><span class='line'>[INFO] Apache Hadoop Rumen ................................ SUCCESS [ 27.401 s]
</span><span class='line'>[INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 20.102 s]
</span><span class='line'>[INFO] Apache Hadoop Data Join ............................ SUCCESS [ 20.382 s]
</span><span class='line'>[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [ 12.207 s]
</span><span class='line'>[INFO] Apache Hadoop Extras ............................... SUCCESS [ 24.069 s]
</span><span class='line'>[INFO] Apache Hadoop Pipes ................................ SUCCESS [ 31.975 s]
</span><span class='line'>[INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 32.225 s]
</span><span class='line'>[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [02:45 min]
</span><span class='line'>[INFO] Apache Hadoop Client ............................... SUCCESS [01:38 min]
</span><span class='line'>[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 15.450 s]
</span><span class='line'>[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 46.489 s]
</span><span class='line'>[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [01:31 min]
</span><span class='line'>[INFO] Apache Hadoop Tools ................................ SUCCESS [  7.603 s]
</span><span class='line'>[INFO] Apache Hadoop Distribution ......................... SUCCESS [ 32.967 s]
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] BUILD SUCCESS
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Total time: 39:30 min
</span><span class='line'>[INFO] Finished at: 2015-03-08T10:55:47+08:00
</span><span class='line'>[INFO] Final Memory: 102M/340M
</span><span class='line'>[INFO] ------------------------------------------------------------------------</span></code></pre></td></tr></table></div></figure>


<p>把src编译出来的native下面的文件拷贝到hadoop集群程序目录下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 lib]$ scp -r root@172.17.42.1:~/hadoop-2.6.0-src/hadoop-dist/target/hadoop-2.6.0/lib/native ./
</span><span class='line'>[hadoop@hadoop-master1 lib]$ cd native/
</span><span class='line'>[hadoop@hadoop-master1 native]$ ll
</span><span class='line'>total 4356
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop 1119518 Mar  8 03:11 libhadoop.a
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop 1486964 Mar  8 03:11 libhadooppipes.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop      18 Mar  3 21:08 libhadoop.so -&gt; libhadoop.so.1.0.0
</span><span class='line'>-rwxr-xr-x. 1 hadoop hadoop  671237 Mar  8 03:11 libhadoop.so.1.0.0
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop  581944 Mar  8 03:11 libhadooputils.a
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop  359490 Mar  8 03:11 libhdfs.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop      16 Mar  3 21:08 libhdfs.so -&gt; libhdfs.so.0.0.0
</span><span class='line'>-rwxr-xr-x. 1 hadoop hadoop  228451 Mar  8 03:11 libhdfs.so.0.0.0</span></code></pre></td></tr></table></div></figure>


<p>添加编译的native包前后对比：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.0]$ hadoop fs -ls /
</span><span class='line'>15/03/08 03:09:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Found 3 items
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-06 16:49 /README.txt
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-06 16:54 /tmp
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-06 16:54 /user
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.0]$ hadoop fs -ls /
</span><span class='line'>Found 3 items
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-06 16:49 /README.txt
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-06 16:54 /tmp
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-06 16:54 /user</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VMware共享目录]]></title>
    <link href="http://winseliu.com/blog/2015/03/07/vmware-sharefolder/"/>
    <updated>2015-03-07T22:25:52+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/07/vmware-sharefolder</id>
    <content type="html"><![CDATA[<p>VMware提供了与主机共享目录的功能，可以在虚拟机访问宿主机器的文件。</p>

<ol>
<li>选择映射目录
 选择[Edit virtual machine settings]，在弹出的对话框中选择[Options]页签，选择[Shared Folders]，点击右边的[Add]按钮添加需要映射(maven)的本地目录。</li>
<li>安装VMware Tools

<ul>
<li>启动linux虚拟机，选择[VM]菜单，再选择[Install VMware Tools&hellip;]菜单。下载完成后，会自动通过cdrom加载到虚拟机。</li>
<li>登录linux虚拟机，执行以下命令：</li>
</ul>
</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /mnt
</span><span class='line'>mkdir cdrom
</span><span class='line'>mount /dev/cdrom cdrom
</span><span class='line'>cd cdrom/
</span><span class='line'>mkdir ~/vmware
</span><span class='line'>tar zxvf VMwareTools-9.2.0-799703.tar.gz -C ~/vmware
</span><span class='line'>
</span><span class='line'>cd ~/vmware
</span><span class='line'>cd vmware-tools-distrib/
</span><span class='line'>./vmware-install.pl 
</span><span class='line'>reboot
</span><span class='line'>
</span><span class='line'>cd /mnt/hgfs/maven</span></code></pre></td></tr></table></div></figure>


<p>当前的maven目录是映射到宿主的机器目录。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost maven]# ll -a
</span><span class='line'>total 3
</span><span class='line'>drwxrwxrwx. 1 root root    0 Dec 28  2012 .
</span><span class='line'>dr-xr-xr-x. 1 root root 4192 Mar  7 22:41 ..
</span><span class='line'>drwxrwxrwx. 1 root root    0 Dec 28  2012 .m2</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[开年2015]]></title>
    <link href="http://winseliu.com/blog/2015/02/19/starting-2015-spring-festival/"/>
    <updated>2015-02-19T01:42:27+08:00</updated>
    <id>http://winseliu.com/blog/2015/02/19/starting-2015-spring-festival</id>
    <content type="html"><![CDATA[<p>红包把整个2015春节捣腾的。。。</p>

<p>到了凌晨，春晚谢幕，寒气开始袭人，陆陆续续大家都开始休息，除夕的红包大战也告一段落。</p>

<p>静下来，方能更好的反思总结。</p>

<p>2014年过的还算顺当，任务上不仅仅是一些繁琐的应付事的工作。提供了足够的空间余地，可以做做自己喜欢的事情，捣腾一阵后，有时间可以给自己思考。</p>

<p>工作4年，到新的环境已然不是新人，时间的积淀和历练让我们更成熟，更有资本的同时，承担更多的责任。在项目组中，可能需要做一些表率。我觉得也是最尴尬的时间，没有傲立群雄的能耐，也不是自甘堕落的无能者，后有追兵前有猛狼。真实的感觉到85后尴尬一代的现实！</p>

<p>工作一段时间后，越来越容易被身边的事情干扰，时不时就会被这样那样的事情打断头绪！在工作之余，学习的时间越来越少，被电视剧和游戏霸占，觉得很是不应该！对于好胜心强的自己来说，接下来还是应该一心一意的去做一件事情！这样自己才能提升的明显，不至于自己觉得碌碌无为！</p>

<p>2014也帮师兄做了些事情，遇到了一些不一样的人和事情，改变了自己原来而一些看法（或者说被现实打败了）。原来总认为别人做了类似的东西，咋用就好了，原来的自己不屑一顾这些东西。还要自己去再造轮子，不希望也觉得浪费时间精力。</p>

<p>其实已有的东西，自己实现一遍后，才是你自己的东西！！不要觉得是在做无用功，当你改进或者添加新的功能时，你会发现自己写的自己实践过的才是自己的，才能驾轻就熟！</p>

<p>好基友都结婚的勾搭的，知名而不认命，什么时刻能踏上点呢！？</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[学习btrace]]></title>
    <link href="http://winseliu.com/blog/2015/02/06/start-btrace/"/>
    <updated>2015-02-06T20:38:33+08:00</updated>
    <id>http://winseliu.com/blog/2015/02/06/start-btrace</id>
    <content type="html"><![CDATA[<p>到官网下载<a href="https://kenai.com/projects/btrace/downloads/directory/releases">btrace</a>，在btrace-bin.zip压缩包中包括了usersguide.html入门教程。源码用hg管理<a href="https://kenai.com/projects/btrace/sources/hg/show">点击下载</a>。</p>

<p>Btrace程序是一个普通的java类，由<strong>BTrace annotations</strong>标注的带有的<code>public static void</code>方法组成。从代码上面来看，类似于spring aop的写法。标注用于指定需要监控的位置（方法，类）。</p>

<p>Btrace程序是只读的，以及只能执行有限制的操作。一般的限制：</p>

<ul>
<li>不能创建新对象（new objects）</li>
<li>不能创建数组（new arrays）</li>
<li>不能抛出异常（throw）</li>
<li>不能捕获异常（catch）</li>
<li>不能调用实例/静态方法。仅仅能调用com.sun.btrace.BTraceUtils的static方法。</li>
<li>不能给被监测的程序的实例或者静态字段复制。但是Btrace程序可以给自己的类的静态字段复制。</li>
<li>不能有实例字段和方法。Btrace类中仅能包括static字段和<code>public static void</code>的方法。</li>
<li>不能有内部类，嵌套类（outer, inner, nested or local classes）</li>
<li>不能有同步块或同步方法</li>
<li>不能包括循环（for，while，do&hellip;while）</li>
<li>不能继承（父类只能是默认的java.lang.Object）</li>
<li>不能实现接口</li>
<li>不能包括assert语句</li>
<li>不能使用类常量</li>
</ul>


<h2>helloworld</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// import all BTrace annotations
</span><span class='line'>import com.sun.btrace.annotations.*;
</span><span class='line'>// import statics from BTraceUtils class
</span><span class='line'>import static com.sun.btrace.BTraceUtils.*;
</span><span class='line'>
</span><span class='line'>// @BTrace annotation tells that this is a BTrace program
</span><span class='line'>@BTrace
</span><span class='line'>public class HelloWorld {
</span><span class='line'> 
</span><span class='line'>    // @OnMethod annotation tells where to probe.
</span><span class='line'>    // In this example, we are interested in entry 
</span><span class='line'>    // into the Thread.start() method. 
</span><span class='line'>    @OnMethod(
</span><span class='line'>        clazz="java.lang.Thread",
</span><span class='line'>        method="start"
</span><span class='line'>    )
</span><span class='line'>    public static void func() {
</span><span class='line'>        // println is defined in BTraceUtils
</span><span class='line'>        // you can only call the static methods of BTraceUtils
</span><span class='line'>        println("about to start a thread!");
</span><span class='line'>    }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>通过命令行脚本<code>btrace &lt;PID&gt; &lt;btrace-script&gt;</code>脚本运行。script可以是java源文件，或者已经编译好的class字节码文件。</p>

<p>btracec提供了类似于javac的功能，额外会对include的文件中定义的变量进行替换。如果你的btrace类就是一个普通功能的java类的话，直接用javac编译及可以了。</p>

<p>编写一个测试类，然后监控这个java程序的线程启动：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>public class HelloTest {
</span><span class='line'>
</span><span class='line'>  @Test
</span><span class='line'>  public void test() throws Exception {
</span><span class='line'>      testNewThread();
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  public void testNewThread() throws InterruptedException {
</span><span class='line'>      Thread.sleep(20 * 1000); // 最佳方式就是使用Scanner，手动输入一个操作后执行后面的操作。scanner.nextLine()
</span><span class='line'>
</span><span class='line'>      for (int i = 0; i &lt; 100; i++) {
</span><span class='line'>          final int index = i;
</span><span class='line'>          new Thread(//
</span><span class='line'>                  new Runnable() {
</span><span class='line'>                      public void run() {
</span><span class='line'>                          System.out.println("my order: " + index);
</span><span class='line'>                      }
</span><span class='line'>                  } //
</span><span class='line'>          ).start();
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>然后，启动btrace程序：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd src\test\script
</span><span class='line'>
</span><span class='line'>#下面的内容是一个批处理文件
</span><span class='line'>set PATH=%PATH%;C:\cygwin\bin;C:\cygwin\usr\local\bin
</span><span class='line'>
</span><span class='line'>set BTRACE_HOME=E:\local\opt\btrace-bin
</span><span class='line'>set CUR_ROOT=%cd%\..\..\..
</span><span class='line'>set SCRIPT=%CUR_ROOT%\src\main\java\com\github\winse\btrace\HelloWorld.java
</span><span class='line'>set SCRIPT=%CUR_ROOT%\target\classes\com\github\winse\btrace\HelloWorld.class
</span><span class='line'>
</span><span class='line'>jps -m  | findstr HelloTest | gawk '{print $1}' | xargs -I {} %BTRACE_HOME%\bin\btrace.bat {} %SCRIPT%</span></code></pre></td></tr></table></div></figure>


<p>上面的主程序启动后sleep了20s，等btrace程序启动。如果是程序一启动就要进行监控记录，可vm的参数添加javaagent：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-javaagent:E:\local\opt\btrace-bin\build\btrace-agent.jar=noServer=true,scriptOutputFile=C:\Temp\test.txt,script=F:\workspaces\cms_hadoop\btrace\target\classes\com\github\winse\btrace\HelloWorld.class</span></code></pre></td></tr></table></div></figure>


<p>添加到eclipse的运行配置（Debug Configurations）参数（Arguments）的VM arguments输入框内。</p>

<p>启动主程序，就可以在C:\Temp\test.txt文件看到btrace程序输出的内容了。</p>

<h2>注解</h2>

<ul>
<li>参数</li>
</ul>


<p>@Self获取this对象
@Return用于获取方法的返回值对象
@TargetInstance和@TargetMethodOrField用来查看被监控的方法内部调用那些实例的方法
@ProbeClassName和@ProbeMethodName用来检测获取当前被监控实例和方法（在OnMethod中使用通配符时，查看到底有那些方法被调用）</p>

<ul>
<li>方法</li>
</ul>


<p>@OnMethod
@OnTimer
@OnError
@OnExist
@OnLowMemory
@OnEvent
@OnProbe</p>

<h2>源码</h2>

<ul>
<li><a href="https://github.com/winse/helloJ/tree/hello/btrace">https://github.com/winse/helloJ/tree/hello/btrace</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Windows Gif]]></title>
    <link href="http://winseliu.com/blog/2015/02/04/windows-gif/"/>
    <updated>2015-02-04T15:18:20+08:00</updated>
    <id>http://winseliu.com/blog/2015/02/04/windows-gif</id>
    <content type="html"><![CDATA[<p>看到linux上各种录制gif的工具：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install byzanz
</span><span class='line'>
</span><span class='line'>byzanz-record -d 10 -x 0 -y 0 -w 1363 -h 758 byzanz-demo.gif</span></code></pre></td></tr></table></div></figure>


<p>还有各种包装的工具：<a href="https://github.com/KeyboardFire/mkcast">mkcast</a></p>

<p>本来想在cygwin中安装byzanz，但是编译需要各种库，最终放弃了。</p>

<p>其实在windows下面，也有很好的gif录制的工具：<a href="http://www.cockos.com/licecap/">LICEcap</a></p>

<p><img src="http://winseliu.com/images/blogs/gif-capture-helloworld.gif" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://v2ex.com/t/139035">windows 下有没有什么录制 gif 截屏质量较好的软件可推荐?</a></li>
<li><a href="http://v5b7.com/other/ubuntu_byzanz.html">Ubuntu录制GIF图</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Build redis-2.8]]></title>
    <link href="http://winseliu.com/blog/2015/01/22/build-redis/"/>
    <updated>2015-01-22T09:59:13+08:00</updated>
    <id>http://winseliu.com/blog/2015/01/22/build-redis</id>
    <content type="html"><![CDATA[<h2>jemalloc</h2>

<p>默认make使用的libc，在内存方面会产生比较多的碎片。可以使用jemalloc要进行内存的分配管理。</p>

<p>如果报<code>make cc Command not found</code>，需要先安装gcc。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxvf redis-2.8.13.bin.tar.gz 
</span><span class='line'>cd redis-2.8.13
</span><span class='line'>cd deps/jemalloc/
</span><span class='line'># 用于产生h头文件
</span><span class='line'>./configure 
</span><span class='line'>
</span><span class='line'>cd redis-2.8.13
</span><span class='line'>make MALLOC=jemalloc
</span><span class='line'>src/redis-server </span></code></pre></td></tr></table></div></figure>


<p>查看jemalloc的include的内容如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@localhost jemalloc]$ cd include/jemalloc/
</span><span class='line'>internal/              jemalloc_defs.h.in     jemalloc_macros.h      jemalloc_mangle.h      jemalloc_mangle.sh     jemalloc_protos.h.in   jemalloc_rename.h      jemalloc.sh
</span><span class='line'>jemalloc_defs.h        jemalloc.h             jemalloc_macros.h.in   jemalloc_mangle_jet.h  jemalloc_protos.h      jemalloc_protos_jet.h  jemalloc_rename.sh  </span></code></pre></td></tr></table></div></figure>


<p>查看内存使用：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@localhost redis-2.8.13]$ src/redis-cli info
</span><span class='line'>...
</span><span class='line'># Memory
</span><span class='line'>used_memory:503576
</span><span class='line'>used_memory_human:491.77K
</span><span class='line'>used_memory_rss:2158592
</span><span class='line'>used_memory_peak:503576
</span><span class='line'>used_memory_peak_human:491.77K
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:4.29
</span><span class='line'>mem_allocator:jemalloc-3.6.0
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>redis在使用过程中，会产生碎片。重启以及libc和jemalloc的对比如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 运行中实例
</span><span class='line'># Memory
</span><span class='line'>used_memory:4623527744
</span><span class='line'>used_memory_human:4.31G
</span><span class='line'>used_memory_rss:48304705536
</span><span class='line'>used_memory_peak:38217543280
</span><span class='line'>used_memory_peak_human:35.59G
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:10.45
</span><span class='line'>mem_allocator:libc
</span><span class='line'>
</span><span class='line'>51616 hadoop    20   0 45.1g  44g 1136 S  0.0 35.7   3410:42 /home/hadoop/redis-2.8.13/src/redis-server *:6371
</span><span class='line'>
</span><span class='line'># 序列化为rdb的文件大小
</span><span class='line'>[hadoop@hadoop-master1 18111]$ ll
</span><span class='line'>总用量 1183116
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1210319541 1月  14 11:28 dump.rdb
</span><span class='line'>
</span><span class='line'># 重启后的实例
</span><span class='line'>[hadoop@hadoop-master1 18111]$  ~/redis-2.8.13/src/redis-server --port 18111
</span><span class='line'>[77484] 14 Jan 14:33:17.910 * DB loaded from disk: 218.337 seconds
</span><span class='line'>
</span><span class='line'># Memory
</span><span class='line'>used_memory:4763158704
</span><span class='line'>used_memory_human:4.44G
</span><span class='line'>used_memory_rss:6217580544
</span><span class='line'>used_memory_peak:4763158704
</span><span class='line'>used_memory_peak_human:4.44G
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:1.31
</span><span class='line'>mem_allocator:libc
</span><span class='line'>
</span><span class='line'>77484 hadoop    20   0 6052m 5.8g 1200 S  0.0  4.6   3:38.39 /home/hadoop/redis-2.8.13/src/redis-server *:18111
</span><span class='line'>
</span><span class='line'># 使用jemalloc替换libc的实例
</span><span class='line'>[hadoop@hadoop-master1 18111]$ ~/redis-jemalloc/redis-2.8.13/src/redis-server --port 18888
</span><span class='line'>[14793] 14 Jan 14:50:11.250 * DB loaded from disk: 209.839 seconds
</span><span class='line'>
</span><span class='line'># Memory
</span><span class='line'>used_memory:4527760088
</span><span class='line'>used_memory_human:4.22G
</span><span class='line'>used_memory_rss:4625887232
</span><span class='line'>used_memory_peak:4527760088
</span><span class='line'>used_memory_peak_human:4.22G
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:1.02
</span><span class='line'>mem_allocator:jemalloc-3.6.0
</span><span class='line'>
</span><span class='line'>14793 hadoop    20   0 4538m 4.3g 1360 S  0.0  3.4   3:28.10 /home/hadoop/redis-jemalloc/redis-2.8.13/src/redis-server *:18888                                                                                                                       </span></code></pre></td></tr></table></div></figure>


<h2>tcmalloc</h2>

<ul>
<li>root安装</li>
</ul>


<p>如果有root用户的话操作比较简单。现在<a href="https://code.google.com/p/gperftools/">gperftools</a>和<a href="http://download.savannah.gnu.org/releases/libunwind/libunwind-0.99-beta.tar.gz">libunwind-0.99-beta</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd libunwind-0.99-beta
</span><span class='line'>./configure 
</span><span class='line'>make && make install
</span><span class='line'>cd /home/hadoop/gperftools-2.4
</span><span class='line'>./configure 
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>cd redis-2.8.13
</span><span class='line'>make MALLOC=tcmalloc</span></code></pre></td></tr></table></div></figure>


<p>如果出现<strong>./libtool: line 1125: g++: command not found</strong>的错误，缺少编译环境；</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost gperftools-2.4]# yum -y install gcc+ gcc-c++</span></code></pre></td></tr></table></div></figure>


<p>编译后，运行报错<strong>src/redis-server: error while loading shared libraries: libtcmalloc.so.4: cannot open shared object file: No such file or directory</strong>，需要配置环境变量：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@localhost redis-2.8.13]$ export LD_LIBRARY_PATH=/usr/local/lib
</span><span class='line'>[hadoop@localhost redis-2.8.13]$ src/redis-server </span></code></pre></td></tr></table></div></figure>


<p>或者按照网上的做法：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "/usr/local/lib" &gt; /etc/ld.so.conf.d/usr_local_lib.conf  
</span><span class='line'>/sbin/ldconfig  </span></code></pre></td></tr></table></div></figure>


<p>检查tcmalloc是否生效<code>lsof -n | grep tcmalloc</code>，出现以下信息说明生效</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>redis-ser 1716    hadoop  mem       REG  253,0  2201976  936349 /usr/local/lib/libtcmalloc.so.4.2.6</span></code></pre></td></tr></table></div></figure>


<p>修改配置文件找到daemonize，将后面的no改为yes，让其可以以服务方式运行。</p>

<ul>
<li>普通用户安装</li>
</ul>


<p>考虑到可以各台机器上面复制，指定编译目录这种方式会比较方便。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd libunwind-0.99-beta
</span><span class='line'>CFLAGS=-fPIC ./configure --prefix=/home/hadoop/redis
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>cd gperftools-2.4
</span><span class='line'>./configure -h
</span><span class='line'>export LDFLAGS="-L/home/hadoop/redis/lib"
</span><span class='line'>export CPPFLAGS="-I/home/hadoop/redis/include"
</span><span class='line'>./configure --prefix=/home/hadoop/redis
</span><span class='line'>make && make install</span></code></pre></td></tr></table></div></figure>


<p>编译好后，把东西redis目录内容移到redis-2.8.13/src下。然后修改src/Makefile：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 redis-2.8.13]$ vi src/Makefile
</span><span class='line'># Include paths to dependencies
</span><span class='line'>FINAL_CFLAGS+= -I../deps/hiredis -I../deps/linenoise -I../deps/lua/src
</span><span class='line'>    
</span><span class='line'>ifeq ($(MALLOC),tcmalloc)
</span><span class='line'>        #FINAL_CFLAGS+= -DUSE_TCMALLOC
</span><span class='line'>        #FINAL_LIBS+= -ltcmalloc
</span><span class='line'>        FINAL_CFLAGS+= -DUSE_TCMALLOC -I./include
</span><span class='line'>        FINAL_LIBS+= -L./lib  -ltcmalloc -ldl
</span><span class='line'>
</span><span class='line'>endif
</span><span class='line'>
</span><span class='line'>ifeq ($(MALLOC),tcmalloc_minimal)
</span><span class='line'>        FINAL_CFLAGS+= -DUSE_TCMALLOC
</span><span class='line'>        FINAL_LIBS+= -ltcmalloc_minimal
</span><span class='line'>endif</span></code></pre></td></tr></table></div></figure>


<p>然后编译：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 redis-2.8.13]$ export LD_LIBRARY_PATH=/home/hadoop/redis-2.8.13/src/lib
</span><span class='line'>[hadoop@master1 redis-2.8.13]$ make MALLOC=tcmalloc
</span><span class='line'>cd src && make all
</span><span class='line'>make[1]: Entering directory `/home/hadoop/redis-2.8.13/src'
</span><span class='line'>    LINK redis-server
</span><span class='line'>    INSTALL redis-sentinel
</span><span class='line'>    CC redis-cli.o
</span><span class='line'>In file included from zmalloc.h:40,
</span><span class='line'>                 from redis-cli.c:50:
</span><span class='line'>./include/google/tcmalloc.h:35:2: warning: #warning is a GCC extension
</span><span class='line'>./include/google/tcmalloc.h:35:2: warning: #warning "google/tcmalloc.h is deprecated. Use gperftools/tcmalloc.h instead"
</span><span class='line'>    LINK redis-cli
</span><span class='line'>    CC redis-benchmark.o
</span><span class='line'>In file included from zmalloc.h:40,
</span><span class='line'>                 from redis-benchmark.c:47:
</span><span class='line'>./include/google/tcmalloc.h:35:2: warning: #warning is a GCC extension
</span><span class='line'>./include/google/tcmalloc.h:35:2: warning: #warning "google/tcmalloc.h is deprecated. Use gperftools/tcmalloc.h instead"
</span><span class='line'>    LINK redis-benchmark
</span><span class='line'>    CC redis-check-dump.o
</span><span class='line'>    LINK redis-check-dump
</span><span class='line'>    CC redis-check-aof.o
</span><span class='line'>    LINK redis-check-aof
</span><span class='line'>
</span><span class='line'>Hint: To run 'make test' is a good idea ;)
</span><span class='line'>
</span><span class='line'>make[1]: Leaving directory `/home/hadoop/redis-2.8.13/src'
</span><span class='line'>[hadoop@master1 redis-2.8.13]$ </span></code></pre></td></tr></table></div></figure>


<h2>redis3集群安装cluster</h2>

<p>编译安装和2.8一样，configuration/make/makeinstall即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 cluster-test]$ cat cluster.conf 
</span><span class='line'>port .
</span><span class='line'>cluster-enabled yes
</span><span class='line'>cluster-config-file nodes.conf
</span><span class='line'>cluster-node-timeout 5000
</span><span class='line'>appendonly yes</span></code></pre></td></tr></table></div></figure>


<p>比较苦逼的是需要安装ruby，服务器不能上网！其实ruby在能访问的机器上面安装就可以了！初始化集群的脚本其实就是客户端连接服务端，初始化集群而已。
还有就是在调用命令的时刻要加上<code>-c</code>，这样才是使用集群模式，不然仅仅连单机，读写其他集群服务会报错！</p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVx5-Ab4jaAAA9_Lg7l-I862.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVyXSAC5iEAABfrrHCfuI114.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVzBSAc3KOAADvQfFIPrs908.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCc-AXZ3EAAHoKZnb1nQ426.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCkWAZYuLAAAWM5VoXJI861.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCuSAAuXxAABB-LpH1nQ340.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/05/5F/wKhkA1PZBrSAPaMTAAAcSnjmhXE093.png" alt="" /></p>

<h2>Cygwin</h2>

<p>开发环境系统都是在windows，想调试一步步的看源码就得编译下redis。由于cygwin环境，模拟的linux，有部分的变量没有定义，需要进行修改。修改如下:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git log -1
</span><span class='line'>commit 0c211a1953afeda3d0d45126653e2d4c38bd88cb
</span><span class='line'>Author: antirez &lt;antirez@gmail.com&gt;
</span><span class='line'>Date:   Fri Dec 5 10:51:09 2014 +010
</span><span class='line'>
</span><span class='line'>$ git branch
</span><span class='line'>* 2.8
</span><span class='line'>
</span><span class='line'>$ git diff
</span><span class='line'>diff --git a/deps/hiredis/net.c b/deps/hiredis/net.c
</span><span class='line'>index bdb84ce..6e95f22 100644
</span><span class='line'>--- a/deps/hiredis/net.c
</span><span class='line'>+++ b/deps/hiredis/net.c
</span><span class='line'>@@ -51,6 +51,13 @@
</span><span class='line'> #include "net.h"
</span><span class='line'> #include "sds.h"
</span><span class='line'>
</span><span class='line'>+/* Cygwin Fix */
</span><span class='line'>+#ifdef __CYGWIN__
</span><span class='line'>+#define TCP_KEEPCNT 8
</span><span class='line'>+#define TCP_KEEPINTVL 150
</span><span class='line'>+#define TCP_KEEPIDLE 14400
</span><span class='line'>+#endif
</span><span class='line'>+
</span><span class='line'> /* Defined in hiredis.c */
</span><span class='line'> void __redisSetError(redisContext *c, int type, const char *str);
</span><span class='line'>
</span><span class='line'>diff --git a/src/Makefile b/src/Makefile
</span><span class='line'>index 8b3e959..a72b2f2 100644
</span><span class='line'>--- a/src/Makefile
</span><span class='line'>+++ b/src/Makefile
</span><span class='line'>@@ -63,6 +63,9 @@ else
</span><span class='line'> ifeq ($(uname_S),Darwin)
</span><span class='line'>        # Darwin (nothing to do)
</span><span class='line'> else
</span><span class='line'>+ifeq ($(uname_S),CYGWIN_NT-6.3-WOW64)
</span><span class='line'>+       # cygwin (nothing to do)
</span><span class='line'>+else
</span><span class='line'> ifeq ($(uname_S),AIX)
</span><span class='line'>         # AIX
</span><span class='line'>         FINAL_LDFLAGS+= -Wl,-bexpall
</span><span class='line'>@@ -75,6 +78,7 @@ else
</span><span class='line'> endif
</span><span class='line'> endif
</span><span class='line'> endif
</span><span class='line'>+endif
</span><span class='line'> # Include paths to dependencies
</span><span class='line'> FINAL_CFLAGS+= -I../deps/hiredis -I../deps/linenoise -I../deps/lua/src
</span></code></pre></td></tr></table></div></figure>


<p>然后编译：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd deps/
</span><span class='line'>make lua hiredis linenoise
</span><span class='line'>
</span><span class='line'>cd ..
</span><span class='line'>make</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>编译成功后，把程序导入eclipse CDT环境进行运行调试。导入后需要重新构建一下，不然调试的时刻会按照/cygwin的路径来查找源码。</p>

<ul>
<li>Import，然后选择C/C++目录下的[Existing Code as Makefile project]</li>
<li>在[Existing Code Location]填入redis程序对应的目录，在[Toolchain for Indexer Settings]选择<strong>Cygwin GCC</strong></li>
<li>导入完成后，右键选择[Build Configuration]->[Build All]</li>
<li>Run然后选择执行redis-server即可。</li>
</ul>


<p>好像也可以远程调试</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@Frankzfz]$gdbserver 10.27.10.48:9000 ./test_seg_fault</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://blog.sina.com.cn/s/blog_71954d8a0100nixe.html">tcp Keepalive</a></li>
<li><a href="http://jiangzhixiang123.blog.163.com/blog/static/2780206220115643822896/">setsockopt之 TCP_KEEPIDLE/TCP_KEEPINTVL/TCP_KEEPCNT - [Linux]</a></li>
<li><a href="http://blog.csdn.net/ce123_zhouwei/article/details/6625486">GDB+GdbServer: ARM程序调试</a></li>
<li><a href="http://my.oschina.net/shelllife/blog/167914">使用gdbserver远程调试</a></li>
<li><p><a href="http://qingfengju.com/article.asp?id=303">用gdb,gdbserver,eclipse+cdt在windows上远程调试linux程序</a></p></li>
<li><p><a href="http://www.cnblogs.com/kernel_hcy/archive/2011/05/15/2046963.html">redis源码分析（1）内存管理</a></p></li>
<li><a href="http://blog.csdn.net/unix21/article/details/12119059">利用TCMalloc替换Nginx和Redis默认glibc库的malloc内存分配</a>)</li>
<li><a href="http://blog.nosqlfan.com/html/3490.html">Redis采用不同内存分配器碎片率对比</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka快速入门]]></title>
    <link href="http://winseliu.com/blog/2015/01/08/kafka-guide/"/>
    <updated>2015-01-08T22:02:21+08:00</updated>
    <id>http://winseliu.com/blog/2015/01/08/kafka-guide</id>
    <content type="html"><![CDATA[<p>年前的时刻就听过kafka的大名，但是一直没有机会亲手尝试。数据写入HDFS然后再MapReduce去处理数据，这样会多出很多中间过程，浪费系统资源。实践下kafka+spark分析是否会更高效。首先了解kafka的基本操作。</p>

<p><a href="http://kafka.apache.org/documentation.html">文档</a>先进行简单的介绍。kafka是一个分布式的、分区的、冗余的日志服务，提供消息系统类似的功能。主要的概念： Topic，Producers，Consumers，Partition，Distribution（replicated）；producers通过TCP发送消息给Kafka集群，然后consumer从Kafka集群获取信息。</p>

<p>Kafka遵循：</p>

<ul>
<li>对于同一个生产者产生的消息有序。</li>
<li>消费者看到的消息顺序和消息存储的顺序一致</li>
<li>一个主题冗余为N的，可以容忍N-1个服务器失败而不会丢失任何消息。</li>
</ul>


<p>下载<a href="http://kafka.apache.org/downloads.html">kafka</a>，当前稳定版本为<a href="https://archive.apache.org/dist/kafka/0.8.1.1/RELEASE_NOTES.html">kafka_2.10-0.8.1.1</a>。下载后解压就可以运行了。</p>

<h2>启动单实例</h2>

<p>由于windows运行的程序放在<code>bin\windows</code>下面。需要对kafka-run-class.bat批处理文件进行稍稍修改：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>rem set BASE_DIR=%CD%\..
</span><span class='line'>set BASE_DIR=%CD%\..\..
</span><span class='line'>
</span><span class='line'>rem for %%i in (%BASE_DIR%\core\lib\*.jar) do (
</span><span class='line'>for %%i in (%BASE_DIR%\libs\*.jar) do (</span></code></pre></td></tr></table></div></figure>


<p>运行程序：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin\windows&gt;zookeeper-server-start.bat ..\..\config\zookeeper.properties
</span><span class='line'>
</span><span class='line'>rem 再打开一个cmd窗口运行
</span><span class='line'>bin\windows&gt;kafka-server-start.bat ..\..\config\server.properties
</span></code></pre></td></tr></table></div></figure>


<p>整合成一个脚本<code>start-all.bat</code>，方便以后使用：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>start zookeeper-server-start.bat ..\..\config\zookeeper.properties
</span><span class='line'>timeout 5
</span><span class='line'>start kafka-server-start.bat ..\..\config\server.properties
</span><span class='line'>exit</span></code></pre></td></tr></table></div></figure>


<h2>Topic</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand --create --zookeeper localhost:2181 --replication 1 --partitions 1 --topic hello
</span><span class='line'>Created topic "hello".
</span><span class='line'>
</span><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand --list --zookeeper localhost:2181
</span><span class='line'>hello
</span><span class='line'>
</span><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand  --describe --zookeeper localhost:2181 --topic hello
</span><span class='line'>Topic:hello     PartitionCount:1        ReplicationFactor:1     Configs:
</span><span class='line'>        Topic: hello    Partition: 0    Leader: 0       Replicas: 0     Isr: 0</span></code></pre></td></tr></table></div></figure>


<p>如果是在linux下，可以运行kafka-topics.sh来创建和查询。如果觉得打印的日志很不爽，可以修改config目录下的log4j.properties（在脚本中通过环境变量log4j.configuration指定为该文件）。</p>

<h2>发送接受消息</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>rem 生产者
</span><span class='line'>bin\windows&gt;kafka-console-producer.bat --broker-list localhost:9092 --topic hello
</span><span class='line'>
</span><span class='line'>rem 消费者（新开一个窗口）
</span><span class='line'>bin\windows&gt;kafka-console-consumer.bat --zookeeper localhost:2181 --topic hello --from-beginning</span></code></pre></td></tr></table></div></figure>


<p>都启动后，在producer的窗口输入信息。同一时刻，consumer也会打印输入的内容。</p>

<p>这个两个命令都有很多参数，直接输入命令不加任何参数可以输出帮助，了解各个参数的含义及其用法。</p>

<h2>Kafka集群</h2>

<p>集群的配置和zookeeper的集群配置方式很类似。只要修改broker.id和数据存储目录即可。</p>

<p>拷贝server.properties，然后修改下面的三个属性：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>broker.id=1
</span><span class='line'>port=9192
</span><span class='line'>log.dir=/tmp/kafka-logs-1</span></code></pre></td></tr></table></div></figure>


<p>然后启动：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set JMX_PORT=19999
</span><span class='line'>start kafka-server-start.bat ..\..\config\server-1.properties
</span><span class='line'>set JMX_PORT=29999
</span><span class='line'>start kafka-server-start.bat ..\..\config\server-2.properties
</span><span class='line'>set JMX_PORT=39999
</span><span class='line'>start kafka-server-start.bat ..\..\config\server-3.properties</span></code></pre></td></tr></table></div></figure>


<p>创建Topic</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic mhello
</span><span class='line'>Created topic "mhello".
</span><span class='line'>
</span><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand --describe --zookeeper localhost:2181 --topic mhello
</span><span class='line'>Topic:mhello    PartitionCount:1        ReplicationFactor:3     Configs:
</span><span class='line'>        Topic: mhello   Partition: 0    Leader: 0       Replicas: 0,3,1 Isr: 0,3,1
</span><span class='line'>
</span><span class='line'>bin\windows&gt;kafka-console-producer.bat --broker-list localhost:9092 --topic mhello
</span><span class='line'>bin\windows&gt;kafka-console-consumer.bat --zookeeper localhost:2181 --topic mhello --from-beginning
</span><span class='line'>      </span></code></pre></td></tr></table></div></figure>


<p>描述命令的第一行是所有分区的概要信息，接下来的每一行是每一个分区的信息。Leader后面的数字表示对应的broker-id的进程为当前分区的主节点，后面的Replicas是数据分布的情况（不管数据存在与否），Isr是当前存活的节点的数据分布情况。</p>

<p>把刚刚启动的1，2，3的节点都停掉，再查描述信息。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand --describe --zookeeper localhost:2181 --topic mhello
</span><span class='line'>Topic:mhello    PartitionCount:1        ReplicationFactor:3     Configs:
</span><span class='line'>        Topic: mhello   Partition: 0    Leader: 0       Replicas: 0,3,1 Isr: 0
</span><span class='line'>
</span><span class='line'>bin\windows&gt;kafka-console-consumer.bat --zookeeper localhost:2181 --topic mhello --from-beginning
</span><span class='line'>hello1
</span><span class='line'>hello2
</span><span class='line'>hello3        </span></code></pre></td></tr></table></div></figure>


<p>只要有一个节点存在，获取数据都没有问题。如果全部停了，就不能提供服务，但是查询describe命令，显示的还是0，囧！！</p>

<p>开启1，2，3节点后，mhello分区的状态：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Topic:mhello    PartitionCount:1        ReplicationFactor:3     Configs:
</span><span class='line'>        Topic: mhello   Partition: 0    Leader: 3       Replicas: 0,3,1 Isr: 3,1</span></code></pre></td></tr></table></div></figure>


<p>问题：当broker-id修改后，原来的数据，并不能透明的过渡。把broker-id为0的节点修改为1000，然后重启。mhello的数据仍然找不到。再次改回0，存活节点才都回来。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    Topic: mhello   Partition: 0    Leader: 3       Replicas: 0,3,1 Isr: 3,1,0</span></code></pre></td></tr></table></div></figure>


<h2>小结</h2>

<p>把基本的功能操作了一遍，都是使用命令行操作，接下来学习下和<a href="https://github.com/linkedin/camus/">hadoop结合</a>，使用java-api来操作Kafka。</p>

<h2>参考</h2>

<ul>
<li><a href="http://kafka.apache.org/documentation.html#gettingStarted">kafka gettingStarted</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redis维护]]></title>
    <link href="http://winseliu.com/blog/2014/12/31/redis-operations/"/>
    <updated>2014-12-31T23:14:57+08:00</updated>
    <id>http://winseliu.com/blog/2014/12/31/redis-operations</id>
    <content type="html"><![CDATA[<p>在使用过程中，接触最多的就是它的commands。除了string/hashmap/set/sortedlist的基本使用方式外，下面总结平时会经常使用的命令：</p>

<h2>启动，客户端连接</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# nohup src/redis-server --port 6370 &
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# src/redis-cli -p 6370
</span><span class='line'>127.0.0.1:6370&gt; </span></code></pre></td></tr></table></div></figure>


<h2>获取redis的整体状态</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; info
</span><span class='line'>...
</span><span class='line'># Memory
</span><span class='line'>used_memory:1415161160
</span><span class='line'>used_memory_human:1.32G
</span><span class='line'>used_memory_rss:0
</span><span class='line'>used_memory_peak:1415161160
</span><span class='line'>used_memory_peak_human:1.32G
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:0.00
</span><span class='line'>mem_allocator:jemalloc-3.6.0
</span><span class='line'>...
</span><span class='line'># CPU
</span><span class='line'>used_cpu_sys:52.47
</span><span class='line'>used_cpu_user:10.07
</span><span class='line'>used_cpu_sys_children:0.00
</span><span class='line'>used_cpu_user_children:0.00
</span><span class='line'>
</span><span class='line'># Keyspace
</span><span class='line'>db0:keys=4253125,expires=0,avg_ttl=0</span></code></pre></td></tr></table></div></figure>


<p>列出的信息，包括了版本、内存/CPU使用、请求数、键值对等信息。通过这些基本了解redis运行情况。</p>

<h2>清空数据库</h2>

<p>对于数据量少的情况下，可以使用flushall来清理记录。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; set abc 1234
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6370&gt; keys *
</span><span class='line'>1) "abc"
</span><span class='line'>127.0.0.1:6370&gt; flushall
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6370&gt; keys *
</span><span class='line'>(empty list or set)</span></code></pre></td></tr></table></div></figure>


<p>数据量大的情况不建议使用flushall，可以直接把rdb数据文件干掉，然后重启redis服务就可以了（找不到数据文件后，就是一个新的库）。</p>

<h2>随机获取一个键</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; mset a 1 b 2 c 3 d 4 e 5 f 6 
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"a"
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"f"
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"e"
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"a"</span></code></pre></td></tr></table></div></figure>


<h2>遍历获取键值</h2>

<p>一般情况下，我们会使用<code>keys PATTERN</code>来查找匹配的键值。但是，如果数据量很大，keys操作会很消耗系统资源，<code>stop the world</code>的事情不是我们想看到的！此时，可以通过scan/hscan/zscan/ssan命令依次获取。</p>

<ul>
<li>获取库中的键值</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; eval "for i=1,100000 do redis.call('set', 'a' .. i, i) end" 0
</span><span class='line'>(nil)
</span><span class='line'>(0.98s)</span></code></pre></td></tr></table></div></figure>


<p>正式环境我们无法预估匹配的键的数量，一根筋的使用keys命令可能并不明智。如果数据量很多，等不到结束应该就会ctrl+c了。这种情况下，可以使用scan命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; scan 0 match ismi:domain:*.upaiyun.com
</span><span class='line'>1) "6553600"
</span><span class='line'>2) 1) "ismi:domain:KunMing:1415646303170928524.test.b0.upaiyun.com"
</span><span class='line'>   2) "ismi:domain:KunMing:1415392926002699280.test.b0.upaiyun.com"
</span><span class='line'>   3) "ismi:domain:KunMing:141489373375899237.test.b0.upaiyun.com"
</span><span class='line'>127.0.0.1:6370&gt; scan 0 match ismi:domain:*.upaiyun.com count 10
</span><span class='line'>1) "6553600"
</span><span class='line'>2) 1) "ismi:domain:KunMing:1415646303170928524.test.b0.upaiyun.com"
</span><span class='line'>   2) "ismi:domain:KunMing:1415392926002699280.test.b0.upaiyun.com"
</span><span class='line'>   3) "ismi:domain:KunMing:141489373375899237.test.b0.upaiyun.com"
</span></code></pre></td></tr></table></div></figure>


<p>Basically with COUNT the user specified the amount of work that should be done at every call in order to retrieve elements from the collection. This is just an hint for the implementation, however generally speaking this is what you could expect most of the times from the implementation.</p>

<p>COUNT数值的意思应该是匹配操作的次数，而不是查询结果的个数。通过和<code>scan 0</code>对比可以得出来。</p>

<p>同理，对于set（smembers）可以使用sscan，sortedlist可以使用zcan等。</p>

<h2>lua脚本</h2>

<p>redis内置的脚本语言，直接使用脚本可以减少客户端和服务端连接（多次请求）的压力。例如要批量删除一些键值：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>src/redis-cli keys 'v2:*' | awk '{print $1}' | while read line; do src/redis-cli del $line ; done</span></code></pre></td></tr></table></div></figure>


<p>先获取匹配的key，然后使用shell再次调用redis的客户端进行删除。表面上看起来没啥问题，如果匹配的key很多，会产生很多的tcp连接，占用redis服务器的端口！最终端口不够用，请求报错。</p>

<p>此时，如果使用lua脚本的方式，就可以轻松处理。无需考虑端口等问题。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 量少时可以使用
</span><span class='line'>eval "local aks=redis.call('keys', 'v2:*'); if #aks &gt;0 then redis.call('del', unpack(aks)) end" 0
</span><span class='line'>
</span><span class='line'># 优美
</span><span class='line'>eval "local aks=redis.call('keys', 'v2:*'); for _,r in ipairs(aks) do redis.call('del', r) end" 0</span></code></pre></td></tr></table></div></figure>


<p>当然，如果键不多，还可以使用一次性全部删除：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>src/redis-cli -p $PORT del `~/redis-2.8.13/src/redis-cli -p $PORT 'keys' 'v2:*' | grep -v 'v2:ci:' | grep -v 'v2:ff' | grep -v "$(date +%Y-%m-%d)" | grep -v "$(date +%Y-%m-%d -d '-1 day')"`</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop查看作业状态Rest接口]]></title>
    <link href="http://winseliu.com/blog/2014/12/07/hadoop-mr-rest-api/"/>
    <updated>2014-12-07T10:09:49+08:00</updated>
    <id>http://winseliu.com/blog/2014/12/07/hadoop-mr-rest-api</id>
    <content type="html"><![CDATA[<p>hadoop yarn提供了web端查看任务状态，同时可以通过rest的方式获取任务的相关信息。rest接口和网页端的每个界面一一对应。</p>

<p><img src="http://file.bmob.cn/M00/D7/C0/oYYBAFSDxeaARA6AAAenwsLMShM027.png" alt="" /></p>

<p>上面的5个图的链接为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>http://hadoop-master1:8088/cluster/apps/RUNNING
</span><span class='line'>http://hadoop-master1:8088/cluster/app/application_1417676507722_1846
</span><span class='line'>http://hadoop-master1:8088/proxy/application_1417676507722_1846/
</span><span class='line'>http://hadoop-master1:8088/proxy/application_1417676507722_1846/mapreduce/job/job_1417676507722_1846
</span><span class='line'>http://hadoop-master1:19888/jobhistory/job/job_1417676507722_1846/mapreduce/job/job_1417676507722_1846</span></code></pre></td></tr></table></div></figure>


<h2>查看正在运行的任务</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://hadoop-master1:8088/ws/v1/cluster/apps?states=RUNNING
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/info
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/counters
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/conf
</span></code></pre></td></tr></table></div></figure>


<p>如果上面的任务是已经完成的，获取对应的信息时返回的值是空的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/counters</span></code></pre></td></tr></table></div></figure>


<h2>查看执行完成的任务</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://hadoop-master1:19888/ws/v1/history
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/info
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs?startedTimeBegin=$(date +%s -d '-1 hour')000
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867/counters
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867/conf
</span><span class='line'>
</span><span class='line'>curl -H "Accept: application/xml" "http://hadoop-master1:8088/ws/v1/cluster/apps?states=FINISHED&limit=1" | xmllint --format - </span></code></pre></td></tr></table></div></figure>


<p>后面的参数和运行任务一致，只是提供服务不同。</p>

<h2>xml转csv</h2>

<p><img src="http://file.bmob.cn/M00/D7/D5/oYYBAFSEHuKAaAgHAACb9_KkMEU331.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -H "Accept: application/xml" "http://hadoop-master1:8088/ws/v1/cluster/apps?startedTimeBegin=$(date +%s -d '-1 hour')000" 2&gt;/dev/null | xsltproc yarn.xslt -  | sort -r
</span><span class='line'>
</span><span class='line'>application_1417676507722_1973,AccessLogOnlyHiveJob,RUNNING,UNDEFINED,1417942144941,0,19416
</span><span class='line'>application_1417676507722_1972,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417942084278,1417942098184,13906
</span><span class='line'>application_1417676507722_1971,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417941603456,1417941617773,14317
</span><span class='line'>application_1417676507722_1970,AccessLogOnlyHiveJob,FINISHED,SUCCEEDED,1417941581080,1417942142287,561207
</span><span class='line'>application_1417676507722_1969,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417941422664,1417941436456,13792</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Hadoop YARN - Introduction to the web services REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">ResourceManager REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/MapredAppMasterRest.html">MapReduce Application Master REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html">History Server REST API&rsquo;s.</a></li>
<li><a href="http://blog.csdn.net/wypblog/article/details/21159795">Hadoop YARN中web服务的REST API介绍</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mysql分区]]></title>
    <link href="http://winseliu.com/blog/2014/11/14/mysql-partition/"/>
    <updated>2014-11-14T15:05:50+08:00</updated>
    <id>http://winseliu.com/blog/2014/11/14/mysql-partition</id>
    <content type="html"><![CDATA[<p>Windows8 Mysql安装后数据默认放在<code>C:\ProgramData\MySQL\MySQL Server 5.6\data</code>下。</p>

<blockquote><p>2、MyISAM数据库表文件：
.MYD文件：即MY Data，表数据文件
.MYI文件：即MY Index，索引文件
.log文件：日志文件</p>

<p>3、InnoDB采用表空间（tablespace）来管理数据，存储表数据和索引，
InnoDB数据库文件（即InnoDB文件集，ib-file set）：
  ibdata1、ibdata2等：系统表空间文件，存储InnoDB系统信息和用户数据库表数据和索引，所有表共用
  .ibd文件：单表表空间文件，每个表使用一个表空间文件（file per table），存放用户数据库表数据和索引
  日志文件： ib_logfile1、ib_logfile2</p></blockquote>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>create database hello;
</span><span class='line'>use hello;
</span><span class='line'>create table abc ( name varchar(1000), age int );
</span><span class='line'>insert into abc values ("1", 1);
</span><span class='line'>
</span><span class='line'>create table abc_myisam ( name varchar(100), age int ) engine=myisam;
</span><span class='line'>insert into abc_myisam values ( '1', 1), ('2',2);
</span><span class='line'>alter table abc_myisam partition by hash(age) partitions 4 ;
</span><span class='line'>
</span><span class='line'>insert into abc_myisam values ( '11', 10), ('2',20), ( '1', 11), ('2',21), ( '1', 21), ('2',22), ( '1', 31), ('2',32), ( '1', 41), ('2',24), ( '1', 15), ('2',23) ;</span></code></pre></td></tr></table></div></figure>


<p>最终库目录如下:</p>

<p><img src="http://file.bmob.cn/M00/D2/16/oYYBAFRlrMaAAAdoAADDsNJhdNs617.png" alt="" /></p>

<p>根据月份来进行分区：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>--最好按照月份分区(date需要为日期类型)
</span><span class='line'>alter table abc_myisam PARTITION BY RANGE (extract(YEAR_MONTH from date)) (  
</span><span class='line'>    PARTITION p410 VALUES LESS THAN (201411),  
</span><span class='line'>    PARTITION p411 VALUES LESS THAN (201412),  
</span><span class='line'>    PARTITION p412 VALUES LESS THAN (201501),  
</span><span class='line'>  PARTITION p501 VALUES LESS THAN (201502), 
</span><span class='line'>  PARTITION p502 VALUES LESS THAN (201503), 
</span><span class='line'>  PARTITION p503 VALUES LESS THAN (201504), 
</span><span class='line'>  PARTITION p504 VALUES LESS THAN (201505), 
</span><span class='line'>  PARTITION p505 VALUES LESS THAN (201506), 
</span><span class='line'>    PARTITION p0 VALUES LESS THAN MAXVALUE  
</span><span class='line'>)</span></code></pre></td></tr></table></div></figure>


<p>根据日期来分区：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>alter table t_dta_activeresources_ip PARTITION BY RANGE (to_days(day)) (  
</span><span class='line'>    PARTITION p0 VALUES LESS THAN (735926),  
</span><span class='line'>PARTITION p141124 VALUES LESS THAN (735927),
</span><span class='line'>PARTITION p141125 VALUES LESS THAN (735928),
</span><span class='line'>PARTITION p141126 VALUES LESS THAN (735929),
</span><span class='line'>PARTITION p88 VALUES LESS THAN MAXVALUE  
</span><span class='line'>)</span></code></pre></td></tr></table></div></figure>


<p>查询时执行计划带上partitions可以查看命中的是那个分区：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mysql&gt; explain select * from t_dta_illegalweb where day='2015-01-04';
</span><span class='line'>+----+-------------+------------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>| id | select_type | table            | type | possible_keys | key  | key_len | ref  | rows    | Extra       |
</span><span class='line'>+----+-------------+------------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>|  1 | SIMPLE      | t_dta_illegalweb | ALL  | NULL          | NULL | NULL    | NULL | 1335432 | Using where |
</span><span class='line'>+----+-------------+------------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>1 row in set
</span><span class='line'>
</span><span class='line'>mysql&gt; explain partitions
</span><span class='line'> select * from t_dta_illegalweb where day='2015-01-04';
</span><span class='line'>+----+-------------+------------------+------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>| id | select_type | table            | partitions | type | possible_keys | key  | key_len | ref  | rows    | Extra       |
</span><span class='line'>+----+-------------+------------------+------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>|  1 | SIMPLE      | t_dta_illegalweb | p150104    | ALL  | NULL          | NULL | NULL    | NULL | 1335432 | Using where |
</span><span class='line'>+----+-------------+------------------+------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>1 row in set</span></code></pre></td></tr></table></div></figure>


<p>如果清理掉分区的数据后，再查看执行计划：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mysql&gt; alter table t_dta_illegalweb truncate partition p150104;
</span><span class='line'>Query OK, 0 rows affected
</span><span class='line'>
</span><span class='line'>mysql&gt; explain partitions select * from t_dta_illegalweb where day='2015-01-04';
</span><span class='line'>+----+-------------+-------+------------+------+---------------+------+---------+------+------+-----------------------------------------------------+
</span><span class='line'>| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | Extra                                               |
</span><span class='line'>+----+-------------+-------+------------+------+---------------+------+---------+------+------+-----------------------------------------------------+
</span><span class='line'>|  1 | SIMPLE      | NULL  | NULL       | NULL | NULL          | NULL | NULL    | NULL | NULL | Impossible WHERE noticed after reading const tables |
</span><span class='line'>+----+-------------+-------+------------+------+---------------+------+---------+------+------+-----------------------------------------------------+
</span><span class='line'>1 row in set</span></code></pre></td></tr></table></div></figure>


<h2>打开日志开关</h2>

<p>默认mysql是没有打开记录一般日志的开关的，可以通过命令行修改参数。对于查看具体执行了那些sql语句，调试很有帮助。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mysql&gt; set global general_log = 1;
</span><span class='line'>Query OK, 0 rows affected
</span><span class='line'>
</span><span class='line'>mysql&gt; SHOW  GLOBAL VARIABLES LIKE '%log%';</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://blog.csdn.net/yaotinging/article/details/6671506">MySQL数据文件介绍及存放位置</a></li>
<li><a href="http://lehsyh.iteye.com/blog/732719">MySQL的表分区</a></li>
<li><a href="http://lobert.iteye.com/blog/1955841">http://lobert.iteye.com/blog/1955841</a></li>
<li><a href="http://blog.csdn.net/jiao_fuyou/article/details/14214213">http://blog.csdn.net/jiao_fuyou/article/details/14214213</a></li>
<li><a href="http://database.51cto.com/art/201002/184392.htm">http://database.51cto.com/art/201002/184392.htm</a></li>
<li><a href="http://dev.mysql.com/doc/refman/5.5/en/error-log.html">mysql不重启清理日志</a></li>
<li><a href="http://pangge.blog.51cto.com/6013757/1319304">http://pangge.blog.51cto.com/6013757/1319304</a></li>
<li><a href="http://bbs.csdn.net/topics/70096519">http://bbs.csdn.net/topics/70096519</a></li>
<li><a href="http://bbs.csdn.net/topics/350138520">http://bbs.csdn.net/topics/350138520</a></li>
<li><a href="http://www.iteye.com/topic/408701">http://www.iteye.com/topic/408701</a></li>
<li><a href="http://www.blogjava.net/allrounder/articles/323591.html">http://www.blogjava.net/allrounder/articles/323591.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx服务配置]]></title>
    <link href="http://winseliu.com/blog/2014/11/13/nginx-serving-static-content/"/>
    <updated>2014-11-13T10:16:17+08:00</updated>
    <id>http://winseliu.com/blog/2014/11/13/nginx-serving-static-content</id>
    <content type="html"><![CDATA[<p>配置nginx作为网页快照的服务，需要理解好配置<code>root</code>的涵义！</p>

<h2>安装、启动</h2>

<p>首先安装，然后修改配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install nginx 
</span><span class='line'>
</span><span class='line'>less /etc/nginx/nginx.conf
</span><span class='line'>less /etc/nginx/conf.d/default.conf 
</span><span class='line'>
</span><span class='line'>service nginx restart</span></code></pre></td></tr></table></div></figure>


<p>实际操作中没有root，只能自己编译了：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>下载nginx，pcre-8.36.zip，zlib-1.2.3.tar.gz解压到src下。
</span><span class='line'>cd nginx-1.7.7
</span><span class='line'>./configure --prefix=/home/omc/tools/nginx --with-pcre=src/pcre --with-zlib=src/zlib
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>cd /home/omc/tools/nginx
</span><span class='line'>vi conf/nginx.conf # 修改listen的端口，80要root才能起
</span><span class='line'>sbin/nginx
</span><span class='line'>sbin/nginx -s reload</span></code></pre></td></tr></table></div></figure>


<p>如果编译的目录和真正存放程序的路径不一致时，可以使用<code>-p</code>参数来指定。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd nginx
</span><span class='line'>sbin/nginx -p $PWD
</span><span class='line'>sbin/nginx -s reload -p $PWD</span></code></pre></td></tr></table></div></figure>


<h2>静态页面服务配置</h2>

<p>下面具体说说配置的涵义：</p>

<ul>
<li>root（不管在那个配置节点下）位置都对应 请求的根路径。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location /static {
</span><span class='line'>    root  /usr/share/static/html;
</span><span class='line'>    autoindex on;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>location / {
</span><span class='line'>    root   /usr/share/nginx/html;
</span><span class='line'>    index  index.html index.htm;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li>location的<code>/static</code>对应的是访问目录<code>/usr/share/static/html/static</code>下的内容，请求<code>/static/hello.html</code>对应到<code>/usr/share/static/html/static/hello.html</code>。也就是说节点下的root目录 对应 的是 访问地址的<code>/</code>。</li>
<li>autoindex可以用于list列出目录内容。</li>
</ul>


<p><img src="http://file.bmob.cn/M00/05/49/ooYBAFRkHkmAe3wcAACCDsZ0Oc8983.png" alt="" /></p>

<p>配置了两个路径后，问题来了：如果<code>/usr/share/nginx/html/</code>也有目录static，那nginx会访问谁？<strong>nginx来先匹配配置，访问/static定位到<code>/usr/share/static/html</code></strong>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location /static {
</span><span class='line'>    root  /usr/share/static/html;
</span><span class='line'>  try_files $uri /static/404.html;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li>try_files可以设置默认页面，如<code>/usr/share/static/html/static</code>目录下不存在abc.html，那么会内部重定向到<code>/static/404.html</code>。这里路径要<code>/static</code>下面。</li>
</ul>


<p>try_files还可以返回状态值，跳转到对应状态的页面：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location / {
</span><span class='line'>    try_files $uri $uri/ $uri.html =404;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/D1/D0/oYYBAFRkJ6eAc8UiAAEKid3ICHw052.png" alt="" /></p>

<p>如果try_files的所给出的地址不包括<code>$uri</code>时，请求会被重定向配置指向的新代理服务：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location / {
</span><span class='line'>    try_files $uri $uri/ @backend;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>location @backend {
</span><span class='line'>    proxy_pass http://backend.example.com;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>实践</h2>

<p>在实际操作遇到的不能访问的问题，配置本机的其他JavaWeb应用，但是在登录后，点其他链接总是跳转到登陆页。可以查看下真正请求的地址。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location /omc {
</span><span class='line'>      proxy_pass http://REAL-IP:9000/omc;
</span><span class='line'>      #proxy_pass http://localhost:9000/omc;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>填写localhost不能访问，但是填具体的外网IP时是可以访问的。查看后，在页面定义了<code>&lt;base href="${basedir}/&gt;</code>导致请求都跳转到localhost了。在客户端肯定就访问失败了。这个需要特别注意下。</p>

<p><img src="http://file.bmob.cn/M00/05/C3/ooYBAFRpn1qAKNKiAACUae7DmjY717.png" alt="" /></p>

<p>在特定的情况下，文件不一定是html后缀的（如：txt），如果要在浏览器解析html，需要配置content-type标题头。同时访问的url和真实存放的文件的路径有出处时，可以通过rewrite指令来进行适配。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>server {
</span><span class='line'>  ...
</span><span class='line'>    location /snapshot {
</span><span class='line'>        root   /home/ud/html-snapshot;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>        rewrite ^/snapshot/.*/(.*)$  /snapshot/$1   last;
</span><span class='line'>        try_files $uri $uri.html $uri.htm =404;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<h2>主备配置</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>upstream backend {
</span><span class='line'>    server backend1.example.com 
</span><span class='line'>    server backup1.example.com  backup;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>server {
</span><span class='line'>    location / {
</span><span class='line'>        proxy_pass http://backend;
</span><span class='line'>    }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li><a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#upstream">nginx upstream</a></li>
</ul>


<h2>防火墙跳转情况下nginx配置</h2>

<p>如在防火墙做了11111端口映射到9000端口，如果按照的配置，应用的redirect会被nginx转换为9000端口发给用户，而不是原始的用户访问的11111端口。导致不一致甚至不能访问。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  location ~ \.do$ {
</span><span class='line'>    proxy_pass              http://localhost:8080;
</span><span class='line'>    proxy_set_header        X-Real-IP $remote_addr;
</span><span class='line'>    proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
</span><span class='line'>    proxy_set_header        Host $http_host;
</span><span class='line'>  }                                                                                                       
</span><span class='line'>  location ~ \.jsp$ {
</span><span class='line'>    proxy_pass              http://localhost:8080;
</span><span class='line'>    proxy_set_header        X-Real-IP $remote_addr;
</span><span class='line'>    proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
</span><span class='line'>    proxy_set_header        Host $http_host;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<ul>
<li><a href="http://wiki.nginx.org/JavaServers">JavaServers</a></li>
<li><a href="http://wiki.nginx.org/LikeApache">Apache ProxyPassReverse</a></li>
<li><a href="http://wiki.nginx.org/NginxHttpProxyModule#proxy_pass">NginxHttpProxyModule</a></li>
</ul>


<h2>rewrite</h2>

<p>flag有两个last和break参数。last和break最大的不同在于</p>

<ul>
<li>break是终止当前location的rewrite检测,而且不再进行location匹配
– last是终止当前location的rewrite检测,但会继续重试location匹配并处理区块中的rewrite规则</li>
</ul>


<p>结果rewrite的结果重新命中了location /download/ 虽然这次并没有命中rewrite规则的正则表达式,但因为缺少终止rewrite的标志,其仍会不停重试download中rewrite规则直到达到10次上限返回HTTP 500。</p>

<p>配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    location / {
</span><span class='line'>       root   html;
</span><span class='line'>
</span><span class='line'>rewrite ^/snapshot/[^\/]*/(.*)$  /snapshot/$1 last;
</span><span class='line'>       index  index.html index.htm;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<p>日志：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2015/03/13 11:53:42 [error] 32395#0: *17 rewrite or internal redirection cycle while processing "/snapshot/45/c7/2f/45c72f9a926d2b72b0c705a125d2764a.txt", client: 132.122.237.189, server: localhost, request: "GET //snapshot/1/2/3/4/5/6/7/8/9/10/11/45/c7/2f/45c72f9a926d2b72b0c705a125d2764a.txt HTTP/1.1", host: "umcc97-44:8888"
</span><span class='line'>
</span><span class='line'>2015/03/13 11:54:14 [error] 32395#0: *20 open() "/home/hadoop/nginx/html/snapshot/45c72f9a926d2b72b0c705a125d2764a.txt" failed (2: No such file or directory), client: 132.122.237.189, server: localhost, request: "GET //snapshot/1/2/3/4/5/45c72f9a926d2b72b0c705a125d2764a.txt HTTP/1.1", host: "umcc97-44:8888"</span></code></pre></td></tr></table></div></figure>


<p>10次以上就报错，少于10次是ok的。</p>

<ul>
<li><a href="http://www.cnblogs.com/dami520/archive/2012/08/16/2642967.html">Nginx Rewrite详解</a></li>
</ul>


<h2>参考</h2>

<ul>
<li><a href="http://nginx.com/resources/admin-guide/serving-static-content/">Serving Static Content</a></li>
<li><a href="http://nginx.com/resources/admin-guide/web-server/">NGINX Web Server</a></li>
<li><a href="http://www.cnblogs.com/cgli/archive/2011/05/16/2047920.html">nginx rewrite规则</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为github Pages页面设置自定义域名]]></title>
    <link href="http://winseliu.com/blog/2014/10/24/github-custom-domain/"/>
    <updated>2014-10-24T00:17:19+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/24/github-custom-domain</id>
    <content type="html"><![CDATA[<ol>
<li>注册个域名（net.cn）</li>
<li>添加CNAME文件（github.com）</li>
<li>添加解析记录（net.cn）</li>
</ol>


<p><img src="http://file.bmob.cn/M00/20/C5/wKhkA1RJKuyAf6lWAACIJ28IFe8161.png" alt="" /></p>

<p>如果是使用子域名的话非常简单。在（pages）CNAME文件中填写www.winseliu.com，然后在（net.cn）解析页添加CNAME指向winse.github.io即可。</p>

<p>如果想默认顶级域名也能访问，需要添加的两个ip指向，参见上图。同时（pages）CNAME中使用winseliu.com。</p>

<h2>参考</h2>

<ul>
<li><a href="https://help.github.com/articles/my-custom-domain-isn-t-working/">My custom domain isn&rsquo;t working</a></li>
<li><a href="https://help.github.com/articles/tips-for-configuring-an-a-record-with-your-dns-provider/">Tips for configuring an A record with your DNS provider</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
