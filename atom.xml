<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Winse Blog]]></title>
  <link href="http://winseliu.com/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2015-11-22T21:14:26+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[搭梯笔记]]></title>
    <link href="http://winseliu.com/blog/2015/11/22/gfw-ladder/"/>
    <updated>2015-11-22T20:51:35+08:00</updated>
    <id>http://winseliu.com/blog/2015/11/22/gfw-ladder</id>
    <content type="html"><![CDATA[<h2>准备一个SSH账号</h2>

<ul>
<li><a href="http://www.99ssh.net/">http://www.99ssh.net/</a></li>
</ul>


<h2>SSH -N -D 或者MyEnTunnel</h2>

<ul>
<li><code>ssh -N -D [PORT] [USER@IP]</code></li>
<li><a href="http://www.99ssh.net/help/newsshow.php?cid=19&amp;id=21">http://www.99ssh.net/help/newsshow.php?cid=19&amp;id=21</a></li>
</ul>


<h2>Chrome + SwitchyOmega + gfwlist</h2>

<ul>
<li><a href="https://github.com/FelisCatus/SwitchyOmega/releases">https://github.com/FelisCatus/SwitchyOmega/releases</a> SwitchySharp升级版本</li>
<li><a href="https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt">https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</a></li>
</ul>


<p>可以做到智能代理功能，gfwlist的才会走代理。加速访问国内网站，同时减少不必要的流量。</p>

<h2>Firefox + FoxyProxy + gfwlist</h2>

<ul>
<li><a href="http://mozilla.com.cn/thread-230260-1-1.html">http://mozilla.com.cn/thread-230260-1-1.html</a></li>
<li><a href="https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt">https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</a></li>
</ul>


<p>Chrome的版本速度快一点。配置好后，等待一段时间就智能的适配了，firefox的等的时间略长。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx再折腾---统一访问入口]]></title>
    <link href="http://winseliu.com/blog/2015/11/11/nginx-build-unified-access/"/>
    <updated>2015-11-11T11:04:04+08:00</updated>
    <id>http://winseliu.com/blog/2015/11/11/nginx-build-unified-access</id>
    <content type="html"><![CDATA[<p>快照目录文件太多，准备安装一个方式分目录。但是又要能保证原来的访问方式不变化！使用rewrite和try_files成功实现。</p>

<h2>目录结构:</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC /cygdrive/f/temp
</span><span class='line'>$ ls -R
</span><span class='line'>.:
</span><span class='line'>1.jpg  snapshot  snapshot-1  snapshot-2  snapshot-3  snapshot-4
</span><span class='line'>
</span><span class='line'>./snapshot:
</span><span class='line'>0.html
</span><span class='line'>
</span><span class='line'>./snapshot-1:
</span><span class='line'>1.html
</span><span class='line'>
</span><span class='line'>./snapshot-2:
</span><span class='line'>2.html
</span><span class='line'>
</span><span class='line'>./snapshot-3:
</span><span class='line'>3.html
</span><span class='line'>
</span><span class='line'>./snapshot-4:
</span><span class='line'>4.html</span></code></pre></td></tr></table></div></figure>


<h2>Nginx配置尝试一:</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    location /snapshot {
</span><span class='line'>        root   /home/hadoop/html-snapshot;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>
</span><span class='line'>        rewrite ^/snapshot/.*/(.*)$  /snapshot/$1 break ;
</span><span class='line'>        
</span><span class='line'>        try_files $uri /snapshot-1/$uri /snapshot-3/$uri;
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    location ~ /snapshot-\d+ {
</span><span class='line'>        root   /home/hadoop/html-snapshot;
</span><span class='line'>
</span><span class='line'>        rewrite ^/(.*)/.*/(.*)$ /$1/$2 break;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<p>这种方式是不行的，try_files要求除最后一个配置外其他都是文件！</p>

<blockquote><p>It is possible to check directory’s existence by specifying a slash at the end of a name, e.g. “$uri/”. If none of the files were found, an internal redirect to the uri specified in the last parameter is made.  [<a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#try_files">http://nginx.org/en/docs/http/ngx_http_core_module.html#try_files</a>]</p></blockquote>

<p>也就是说，中间配置路径，nginx只把他们当做本地的去看待！文件存在就返回结果，否则直接重定向到最后一个路径！！</p>

<h2>Nginx配置尝试二：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    location /snapshot {
</span><span class='line'>        root   F:/temp;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>
</span><span class='line'>        rewrite ^/snapshot/.*/(.*)$  /snapshot/$1 break ;
</span><span class='line'>
</span><span class='line'>        try_files $uri @backup;
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    location ~ /snapshot-\d+ {
</span><span class='line'>        root   F:/temp;
</span><span class='line'>
</span><span class='line'>      try_files $uri @backup;
</span><span class='line'>    }
</span><span class='line'>  
</span><span class='line'>    location @backup {
</span><span class='line'>      # 这里的顺序不能颠倒，[.*]会匹配所有的！
</span><span class='line'>        rewrite ^/(.*)-3/(.*)$ /$1-4/$2 last;
</span><span class='line'>        rewrite ^/(.*)-2/(.*)$ /$1-3/$2 last;
</span><span class='line'>        rewrite ^/(.*)-1/(.*)$ /$1-2/$2 last;
</span><span class='line'>        rewrite ^/(.*)/(.*)$ /$1-1/$2 last;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<p>这里使用循环的方式在backup的location中进行处理，一个个的循环查找。使用了正则表达式和一个统一rewrite的location。</p>

<h2>Nginx配置尝试三：</h2>

<p>上面发现，其实try_files都是去查找文件，其实目录结构和访问路径是匹配的，只是请求一开始就带snaphost，倒是每次都需要处理。如果请求过来的就没有带snaphost的话！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    location / {
</span><span class='line'>        root   F:/temp;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>
</span><span class='line'>        try_files /snapshot/$uri /snapshot-1/$uri  /snapshot-2/$uri  /snapshot-3/$uri  /snapshot-4/$uri;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<p>一个location配置就行了！</p>

<h2>Nginx配置完善版：</h2>

<p>转变思路后，最开始就把请求的前置snapshot去掉rewrite去掉就行了！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    location /snapshot {
</span><span class='line'>        root   F:/temp;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>      
</span><span class='line'>      rewrite ^/snapshot/.*/(.*)$  /$1 break ;
</span><span class='line'>
</span><span class='line'>        try_files /snapshot/$uri /snapshot-1/$uri  /snapshot-2/$uri  /snapshot-3/$uri  /snapshot-4/$uri;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<h2>nginx添加模块</h2>

<p>当我们启用 &ndash;with-debug 选项重新构建好调试版的 Nginx 之后，还需要同时在配置文件中通过标准的 error_log 配置指令为错误日志使用 debug 日志级别（这同时也是最低的日志级别）：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>error_log logs/error.log debug;</span></code></pre></td></tr></table></div></figure>


<p>添加echo模块：</p>

<p>下载zlib、pcre、echo：</p>

<ul>
<li><a href="http://www.zlib.net/">http://www.zlib.net/</a></li>
<li><a href="http://www.pcre.org/">http://www.pcre.org/</a></li>
<li><a href="https://github.com/openresty/echo-nginx-module">https://github.com/openresty/echo-nginx-module</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxvf zlib-1.2.8.tar.gz 
</span><span class='line'>mv zlib-1.2.8 src/zlib
</span><span class='line'>tar zxvf pcre-8.36.tar.gz 
</span><span class='line'>mv pcre-8.36 src/pcre
</span><span class='line'>
</span><span class='line'>./configure --prefix=/home/hadoop/nginx --add-module=/home/hadoop/echo-nginx-module-0.58  --with-pcre=src/pcre --with-zlib=src/zlib --with-debug 
</span><span class='line'>make -j2
</span><span class='line'>make install</span></code></pre></td></tr></table></div></figure>


<p>编译成功后，就能在location里面直接echo，页面访问时就能看到echo内容了。</p>

<h2>参考</h2>

<ul>
<li><a href="http://www.cnblogs.com/cgli/archive/2011/05/16/2047920.html">http://www.cnblogs.com/cgli/archive/2011/05/16/2047920.html</a></li>
<li><a href="http://www.cnblogs.com/tohilary/archive/2012/08/24/2653904.html">http://www.cnblogs.com/tohilary/archive/2012/08/24/2653904.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[树莓派raspberrypi2简单使用]]></title>
    <link href="http://winseliu.com/blog/2015/11/04/raspberrypi-start-guide/"/>
    <updated>2015-11-04T11:52:18+08:00</updated>
    <id>http://winseliu.com/blog/2015/11/04/raspberrypi-start-guide</id>
    <content type="html"><![CDATA[<h2>买的东西地址：</h2>

<p>现在想来，其实可以多加100，买一整套的比较方便。内存卡还有外壳都在里面。</p>

<ul>
<li><p><a href="https://item.taobao.com/item.htm?id=520179324500&amp;ali_refid=a3_420434_1006:1103723226:N:%E6%A0%91%E8%8E%93%E6%B4%BE2+b%2B:fc636846d2212679077e26f5f9f14118&amp;ali_trackid=1_fc636846d2212679077e26f5f9f14118&amp;spm=a230r.1.0.0.vhiQsb&amp;qq-pf-to=pcqq.c2c">树莓派2b 树莓派raspberry Pi 2代B型四核开发板 官方正品 树莓派</a></p></li>
<li><p><a href="http://item.jd.com/679773.html">闪迪（SanDisk）至尊高速移动MicroSDHC UHS-I存储卡 TF卡 32GB Class10 读速48Mb/s</a></p></li>
<li><a href="http://item.jd.com/667570.html">迅捷（FAST）FW150US 超小型150M无线USB网卡</a></li>
<li><p><a href="https://detail.tmall.com/item.htm?_u=4jgup6l1c31&amp;id=45729451918">USB转TTL PL2303HX模块 STC单片机下载线刷机线 升级串口模块</a></p></li>
<li><p><a href="http://item.jd.com/629794.html">雷柏（Rapoo）1860 无线光学键鼠套装</a>  质量一般。临时用用，可以考虑不买！</p></li>
</ul>


<h2>安装系统</h2>

<ul>
<li>显示器： 恰好同时有HDMI的接口和显示器。如果没有，那就要考虑直接把系统写到SD卡了！！</li>
<li>无线键盘、鼠标</li>
<li>电源： 一般的手机充电器都可以，用充电宝也是OK的</li>
<li><a href="https://www.raspberrypi.org/help/noobs-setup/">NOOBS</a>

<ul>
<li>NOOBS_v1_4_2.zip</li>
<li>SDFormatterv4.zip</li>
</ul>
</li>
</ul>


<p>插上SD卡，安装系统就行了。</p>

<p><img src="http://winseliu.com/images/blogs/raspberrypi-os-install.png" alt="" /></p>

<h2>无线网卡</h2>

<p>网站上没说有linux的驱动，但是直接插上后是能检测到设备的！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>pi@raspberrypi:~$ lsusb
</span><span class='line'>Bus 001 Device 004: ID 0bda:8179 Realtek Semiconductor Corp. 
</span><span class='line'>Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet Adapter
</span><span class='line'>Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. 
</span><span class='line'>Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span></code></pre></td></tr></table></div></figure>


<p>ifconfig也能查看到wlan0的无线网卡。接下来修改配置，添加用户密码即可。</p>

<p>配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>pi@raspberrypi:~$ cat /etc/network/interfaces
</span><span class='line'># Please note that this file is written to be used with dhcpcd.
</span><span class='line'># For static IP, consult /etc/dhcpcd.conf and 'man dhcpcd.conf'.
</span><span class='line'>
</span><span class='line'>auto lo
</span><span class='line'>iface lo inet loopback
</span><span class='line'>
</span><span class='line'>auto eth0
</span><span class='line'>allow-hotplug eth0
</span><span class='line'>#iface eth0 inet manual
</span><span class='line'>iface eth0 inet dhcp
</span><span class='line'>
</span><span class='line'>auto wlan0
</span><span class='line'>allow-hotplug wlan0
</span><span class='line'>#iface wlan0 inet manual
</span><span class='line'>iface wlan0 inet dhcp
</span><span class='line'>wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf
</span><span class='line'>
</span><span class='line'>auto wlan1
</span><span class='line'>allow-hotplug wlan1
</span><span class='line'>iface wlan1 inet manual
</span><span class='line'>wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf
</span><span class='line'>
</span><span class='line'>pi@raspberrypi:~$ sudo cat /etc/wpa_supplicant/wpa_supplicant.conf
</span><span class='line'>ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
</span><span class='line'>update_config=1
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>network={
</span><span class='line'>ssid="winse.liu"
</span><span class='line'>psk="MIMA"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>network={
</span><span class='line'>ssid="1108"
</span><span class='line'>psk="MIMA"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>USB转串口使用COM控制raspberry</h2>

<p>在淘宝上面买的，我系统是win10，抱着尝试下的心态也很便宜就买了一个。买来后，安装了win7的驱动，能用。</p>

<p>配置见图：</p>

<p><img src="http://winseliu.com/images/blogs/raspberrypi-gpio.png" alt="" /></p>

<p><img src="http://winseliu.com/images/blogs/raspberrypi-usb-com-install.jpg" alt="" /></p>

<p><img src="http://winseliu.com/images/blogs/raspberrypi-usb-com-config.jpg" alt="" /></p>

<p>最终效果：</p>

<p><img src="http://winseliu.com/images/blogs/raspberrypi-finished.jpg" alt="" /></p>

<h2>用到的一些命令</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo raspi-config
</span><span class='line'>修改配置，默认使用图形界面登录，可以使用`Boot Options`修改为文本console模式
</span><span class='line'>
</span><span class='line'>sudo iwlist wlan0 scan | grep ESSID
</span><span class='line'>查看可用的无线
</span><span class='line'>
</span><span class='line'>systemctl list-units
</span><span class='line'>
</span><span class='line'>sudo apt-get install screen
</span><span class='line'>
</span><span class='line'>sudo apt-get -y install vim
</span><span class='line'>sudo apt-get install nginx
</span><span class='line'>systemctl status nginx.service
</span><span class='line'>sudo cp /etc/skel/.* /home/robot/</span></code></pre></td></tr></table></div></figure>


<p>手机放一个热点出来，然后手机安装一个ssh的工具(JuiceSSH v2.0.2)就可以控制树莓派了。</p>

<p>网上有用手机当屏幕，然后键盘连树莓派usb，结合来控制树莓派。一开始挺新奇的，后来感觉挺扯淡的！不过学习到了screen的程序，自动登录啥的没弄成，直接输入用户密码登录也行了。在boot选项看到有自动登录，不知道有没有用。现在有无线网卡和com来控制，感觉已经够用了。</p>

<h2>参考</h2>

<ul>
<li><a href="https://www.raspberrypi.org/forums/viewtopic.php?f=91&amp;t=4751&amp;sid=661d1a59e4f85f333b40e6e46db58d32">Getting Started with the Raspberry Pi</a></li>
<li><a href="http://blog.csdn.net/c80486/article/details/8545307">树莓派(raspberry pi)学习15: 使用WIFI网卡连接无线网络</a></li>
<li><a href="http://blog.csdn.net/cugbabybear/article/details/23048741">windows下 用串行连接控制树莓派</a></li>
<li><a href="http://www.alsrobot.cn/article-141.html">【创客学堂】如何在windows系统下用串口通信完爆raspberry pi（树莓派）</a></li>
<li><a href="http://shumeipai.nxez.com/2013/10/10/raspberry-pi-pick-kindle-display.html">视频详解树莓派如何外接Kindle显示器</a></li>
<li><p><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-screen/index.html">linux 技巧：使用 screen 管理你的远程会话</a></p></li>
<li><p><a href="http://www.bkjia.com/Pythonjc/818142.html">RASPBERRY PI wifi配置</a></p></li>
<li><p><a href="http://blog.sina.com.cn/s/blog_3cb6a78c0101a0fe.html">Raspberry Pi 连接无线网卡</a></p></li>
<li><p><a href="http://davidrobot.com/2014/11/raspberry_pi_model_b_plus_startup.html">开机篇 – 树莓派 Raspberry Pi Model B+ 入手折腾记 (1)</a></p></li>
<li><p><a href="http://shumeipai.nxez.com/2013/09/07/no-screen-unknow-ip-login-pi.html#more-184">没有显示器且IP未知的情况下登录树莓派</a></p></li>
<li><p><a href="http://www.pc6.com/az/104761.html">JuiceSSH v2.0.2</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cacti批量添加配置]]></title>
    <link href="http://winseliu.com/blog/2015/10/13/cacti-batch-adding-configurations/"/>
    <updated>2015-10-13T08:35:50+08:00</updated>
    <id>http://winseliu.com/blog/2015/10/13/cacti-batch-adding-configurations</id>
    <content type="html"><![CDATA[<h2>所有机器SNMP配置同步</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for h in `cat /etc/hosts| grep hadoop|awk '{print $2}'` ; do scp -r /etc/snmp/snmpd.conf $h:/etc/snmp/ ; done
</span><span class='line'>
</span><span class='line'>for h in `cat /etc/hosts| grep hadoop|awk '{print $2}'` ; do ssh $h "service snmpd start" ; done</span></code></pre></td></tr></table></div></figure>


<h2>Cacti批量添加配置</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>批量添加device。ip也可以为hostname；template为机器模板；version为SNMP的版本
</span><span class='line'>[root@cu-omc1 cacti]# for h in `cat /etc/hosts | grep hadoop | awk '{print $2}'` ; do php cli/add_device.php --description="$h" --ip="$h" --template=9 --version=2 ; done
</span><span class='line'>
</span><span class='line'>了解参数
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-hosts
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-graph-templates --host-template-id=9
</span><span class='line'>Known Graph Templates:(id, name)
</span><span class='line'>29      Host MIB - Processes
</span><span class='line'>35      ucd/net - Users Logged On
</span><span class='line'>36      ucd/net - TCP Current Established
</span><span class='line'>37      ucd/net - Uptime
</span><span class='line'>38      ucd/net - TCP Counters
</span><span class='line'>39      ucd/net - Memory Usage (enhanced)
</span><span class='line'>40      ucd/net - Load Average (enhanced)
</span><span class='line'>41      ucd/net - CPU Usage (enhanced)
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-snmp-queries
</span><span class='line'>Known SNMP Queries:(id, name)
</span><span class='line'>1       SNMP - Interface Statistics
</span><span class='line'>2       ucd/net -  Get Monitored Partitions
</span><span class='line'>3       Karlnet - Wireless Bridge Statistics
</span><span class='line'>4       Netware - Get Available Volumes
</span><span class='line'>6       Unix - Get Mounted Partitions
</span><span class='line'>7       Netware - Get Processor Information
</span><span class='line'>8       SNMP - Get Mounted Partitions
</span><span class='line'>9       SNMP - Get Processor Information
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-snmp-fields --host-id=2 --snmp-query-id=1
</span><span class='line'>Known SNMP Fields for host-id 2: (name)
</span><span class='line'>ifAlias
</span><span class='line'>ifDescr
</span><span class='line'>ifHighSpeed
</span><span class='line'>ifHwAddr
</span><span class='line'>ifIndex
</span><span class='line'>ifIP
</span><span class='line'>ifName
</span><span class='line'>ifOperStatus
</span><span class='line'>ifSpeed
</span><span class='line'>ifType
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-snmp-values --host-id=2 --snmp-query-id=1 --snmp-field=ifIP
</span><span class='line'>Known values for ifIP for host 2: (name)
</span><span class='line'>127.0.0.1
</span><span class='line'>192.168.20.11
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php  --list-graph-templates 
</span><span class='line'>Known Graph Templates:(id, name)
</span><span class='line'>2       Interface - Traffic (bits/sec)
</span><span class='line'>3       ucd/net - Available Disk Space
</span><span class='line'>4       ucd/net - CPU Usage
</span><span class='line'>5       Karlnet - Wireless Levels
</span><span class='line'>6       Karlnet - Wireless Transmissions
</span><span class='line'>7       Unix - Ping Latency
</span><span class='line'>8       Unix - Processes
</span><span class='line'>9       Unix - Load Average
</span><span class='line'>10      Unix - Logged in Users
</span><span class='line'>11      ucd/net - Load Average
</span><span class='line'>12      Linux - Memory Usage
</span><span class='line'>13      ucd/net - Memory Usage
</span><span class='line'>14      Netware - File System Cache
</span><span class='line'>15      Netware - CPU Utilization
</span><span class='line'>16      Netware - File System Activity
</span><span class='line'>17      Netware - Logged In Users
</span><span class='line'>18      Cisco - CPU Usage
</span><span class='line'>19      Netware - Volume Information
</span><span class='line'>20      Netware - Directory Information
</span><span class='line'>21      Unix - Available Disk Space
</span><span class='line'>22      Interface - Errors/Discards
</span><span class='line'>23      Interface - Unicast Packets
</span><span class='line'>24      Interface - Non-Unicast Packets
</span><span class='line'>25      Interface - Traffic (bytes/sec)
</span><span class='line'>26      Host MIB - Available Disk Space
</span><span class='line'>27      Host MIB - CPU Utilization
</span><span class='line'>28      Host MIB - Logged in Users
</span><span class='line'>29      Host MIB - Processes
</span><span class='line'>30      Netware - Open Files
</span><span class='line'>31      Interface - Traffic (bits/sec, 95th Percentile)
</span><span class='line'>32      Interface - Traffic (bits/sec, Total Bandwidth)
</span><span class='line'>33      Interface - Traffic (bytes/sec, Total Bandwidth)
</span><span class='line'>34      SNMP - Generic OID Template
</span><span class='line'>35      ucd/net - Users Logged On
</span><span class='line'>36      ucd/net - TCP Current Established
</span><span class='line'>37      ucd/net - Uptime
</span><span class='line'>38      ucd/net - TCP Counters
</span><span class='line'>39      ucd/net - Memory Usage (enhanced)
</span><span class='line'>40      ucd/net - Load Average (enhanced)
</span><span class='line'>41      ucd/net - CPU Usage (enhanced)
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php  --list-query-types  --snmp-query-id=1
</span><span class='line'>Known SNMP Query Types: (id, name)
</span><span class='line'>2       In/Out Errors/Discarded Packets
</span><span class='line'>3       In/Out Non-Unicast Packets
</span><span class='line'>4       In/Out Unicast Packets
</span><span class='line'>9       In/Out Bytes (64-bit Counters)
</span><span class='line'>13      In/Out Bits
</span><span class='line'>14      In/Out Bits (64-bit Counters)
</span><span class='line'>16      In/Out Bytes
</span><span class='line'>20      In/Out Bits with 95th Percentile
</span><span class='line'>21      In/Out Bits with Total Bandwidth
</span><span class='line'>22      In/Out Bytes with Total Bandwidth
</span><span class='line'>
</span><span class='line'>先测试单机添加，对应到Device页面点击`Create Graphs for this Host`添加图像的操作
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --host-id=2 --graph-type=cg --graph-template-id=40
</span><span class='line'>Graph Added - graph-id: (5) - data-source-ids: (8, 9, 10)
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --host-id=2 --graph-type=cg --graph-template-id=41
</span><span class='line'>Graph Added - graph-id: (6) - data-source-ids: (11, 12, 13, 14)
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --host-id=2 --graph-type=cg --graph-template-id=39
</span><span class='line'>Graph Added - graph-id: (7) - data-source-ids: (15, 16, 17, 18, 19)
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --host-id=2 --graph-type=cg --graph-template-id=38
</span><span class='line'>
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --host-id="2" --graph-type=ds  --graph-template-id=2 --snmp-query-id=1 --snmp-query-type-id=16 --snmp-field=ifIP --snmp-value="192.168.20.11"
</span><span class='line'>Graph Added - graph-id: (9) - data-source-ids: (24, 24)
</span><span class='line'>
</span><span class='line'>批量操作
</span><span class='line'>添加Graph Templates
</span><span class='line'>[root@cu-omc1 cacti]# php cli/add_graphs.php --list-hosts | awk '{print $1}' | while read line ; do 
</span><span class='line'>&gt;  php cli/add_graphs.php --host-id=$line --graph-type=cg --graph-template-id=41
</span><span class='line'>&gt;  php cli/add_graphs.php --host-id=$line --graph-type=cg --graph-template-id=40
</span><span class='line'>&gt;  php cli/add_graphs.php --host-id=$line --graph-type=cg --graph-template-id=39
</span><span class='line'>&gt;  php cli/add_graphs.php --host-id=$line --graph-type=cg --graph-template-id=38
</span><span class='line'>&gt; done
</span><span class='line'>
</span><span class='line'>添加Data Query。比较复杂点，需要查询匹配
</span><span class='line'>php cli/add_graphs.php --list-hosts | awk '{print $1}' | while read line ; do 
</span><span class='line'>  php cli/add_graphs.php --host-id=$line --graph-type=ds  --graph-template-id=2 --snmp-query-id=1 --snmp-query-type-id=16 --snmp-field=ifIP --snmp-value=$(grep "`php cli/add_graphs.php --list-hosts | grep "^$line\s" | awk '{print $2}'`\s" /etc/hosts | awk '{print $1}')
</span><span class='line'>done</span></code></pre></td></tr></table></div></figure>


<h2>其他命令</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>dmesg |grep eth0
</span><span class='line'>
</span><span class='line'>iftop –i eth0 –B
</span><span class='line'>
</span><span class='line'>sar -n DEV 1 100 
</span><span class='line'>
</span><span class='line'>ethtool eth0
</span><span class='line'>
</span><span class='line'>[omc@cu-omc1 ~]$ sort -k 2 /tmp/cacti.list &gt; /tmp/cacti.sort.list
</span><span class='line'>[omc@cu-omc1 ~]$ grep hadoop /etc/hosts | sort -k 2 | join -j 2 - /tmp/cacti.sort.list </span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.educity.cn/net/1619986.html">http://www.educity.cn/net/1619986.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nagios监控主机]]></title>
    <link href="http://winseliu.com/blog/2015/09/25/nagios-start-guide/"/>
    <updated>2015-09-25T17:40:58+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/25/nagios-start-guide</id>
    <content type="html"><![CDATA[<p>和Cacti查看时间序列图形不同，Nagios更多的是状态的预警。</p>

<ul>
<li>下载应用</li>
</ul>


<p>到<a href="http://sourceforge.net/projects/nagios/files/?source=navbar">sourceforge</a>下面最新版的应用。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.6 (Final)
</span><span class='line'>[root@cu2 nagios]# ll
</span><span class='line'>-rw-r--r--  1 root root 11206656 Sep 23 12:47 nagios-4.1.1.tar.gz
</span><span class='line'>-rw-r--r--  1 root root  2677352 Sep 23 12:47 nagios-plugins-2.1.1.tar.gz
</span><span class='line'>-rw-r--r--  1 root root   419695 Sep 23 15:18 nrpe-2.15.tar.gz</span></code></pre></td></tr></table></div></figure>


<ul>
<li>新增nagios用户</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# useradd nagios
</span><span class='line'>[root@cu2 nagios]# groupadd nagcmd
</span><span class='line'>[root@cu2 nagios]# usermod -G nagcmd nagios
</span><span class='line'>[root@cu2 nagios]# usermod -G nagcmd apache 
</span><span class='line'># 如果已经安装了httpd，查看下是哪个用户进程，把该用户加入到nagcmd组。修改后需要重启httpd</span></code></pre></td></tr></table></div></figure>


<ul>
<li>编译Nagios服务端</li>
</ul>


<p>由于前面安装Cacti已经把依赖都安装了，如gcc、gd、httpd、php php-devel php-mysql php-pear php-common php-gd php-mbstring php-cli。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# cd nagios-4.1.1/
</span><span class='line'>[root@cu2 nagios-4.1.1]# ./configure --with-command-group=nagcmd
</span><span class='line'>...
</span><span class='line'>Creating sample config files in sample-config/ ...
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>*** Configuration summary for nagios 4.1.1 08-19-2015 ***:
</span><span class='line'>
</span><span class='line'> General Options:
</span><span class='line'> -------------------------
</span><span class='line'>        Nagios executable:  nagios
</span><span class='line'>        Nagios user/group:  nagios,nagios
</span><span class='line'>       Command user/group:  nagios,nagcmd
</span><span class='line'>             Event Broker:  yes
</span><span class='line'>        Install ${prefix}:  /usr/local/nagios
</span><span class='line'>    Install ${includedir}:  /usr/local/nagios/include/nagios
</span><span class='line'>                Lock file:  ${prefix}/var/nagios.lock
</span><span class='line'>   Check result directory:  ${prefix}/var/spool/checkresults
</span><span class='line'>           Init directory:  /etc/rc.d/init.d
</span><span class='line'>  Apache conf.d directory:  /etc/httpd/conf.d
</span><span class='line'>             Mail program:  /bin/mail
</span><span class='line'>                  Host OS:  linux-gnu
</span><span class='line'>          IOBroker Method:  epoll
</span><span class='line'>
</span><span class='line'> Web Interface Options:
</span><span class='line'> ------------------------
</span><span class='line'>                 HTML URL:  http://localhost/nagios/
</span><span class='line'>                  CGI URL:  http://localhost/nagios/cgi-bin/
</span><span class='line'> Traceroute (used by WAP):  
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Review the options above for accuracy.  If they look okay,
</span><span class='line'>type 'make all' to compile the main program and CGIs.
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make all
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>*** Compile finished ***
</span><span class='line'>
</span><span class='line'>If the main program and CGIs compiled without any errors, you
</span><span class='line'>can continue with installing Nagios as follows (type 'make'
</span><span class='line'>without any arguments for a list of all possible options):
</span><span class='line'>
</span><span class='line'>  make install
</span><span class='line'>     - This installs the main program, CGIs, and HTML files
</span><span class='line'>
</span><span class='line'>  make install-init
</span><span class='line'>     - This installs the init script in /etc/rc.d/init.d
</span><span class='line'>
</span><span class='line'>  make install-commandmode
</span><span class='line'>     - This installs and configures permissions on the
</span><span class='line'>       directory for holding the external command file
</span><span class='line'>
</span><span class='line'>  make install-config
</span><span class='line'>     - This installs *SAMPLE* config files in /usr/local/nagios/etc
</span><span class='line'>       You'll have to modify these sample files before you can
</span><span class='line'>       use Nagios.  Read the HTML documentation for more info
</span><span class='line'>       on doing this.  Pay particular attention to the docs on
</span><span class='line'>       object configuration files, as they determine what/how
</span><span class='line'>       things get monitored!
</span><span class='line'>
</span><span class='line'>  make install-webconf
</span><span class='line'>     - This installs the Apache config file for the Nagios
</span><span class='line'>       web interface
</span><span class='line'>
</span><span class='line'>  make install-exfoliation
</span><span class='line'>     - This installs the Exfoliation theme for the Nagios
</span><span class='line'>       web interface
</span><span class='line'>
</span><span class='line'>  make install-classicui
</span><span class='line'>     - This installs the classic theme for the Nagios
</span><span class='line'>       web interface
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>*** Support Notes *******************************************
</span><span class='line'>
</span><span class='line'>If you have questions about configuring or running Nagios,
</span><span class='line'>please make sure that you:
</span><span class='line'>
</span><span class='line'>     - Look at the sample config files
</span><span class='line'>     - Read the documentation on the Nagios Library at:
</span><span class='line'>           https://library.nagios.com
</span><span class='line'>
</span><span class='line'>before you post a question to one of the mailing lists.
</span><span class='line'>Also make sure to include pertinent information that could
</span><span class='line'>help others help you.  This might include:
</span><span class='line'>
</span><span class='line'>     - What version of Nagios you are using
</span><span class='line'>     - What version of the plugins you are using
</span><span class='line'>     - Relevant snippets from your config files
</span><span class='line'>     - Relevant error messages from the Nagios log file
</span><span class='line'>
</span><span class='line'>For more information on obtaining support for Nagios, visit:
</span><span class='line'>
</span><span class='line'>       https://support.nagios.com
</span><span class='line'>
</span><span class='line'>*************************************************************
</span><span class='line'>
</span><span class='line'>Enjoy.
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make install
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>*** Main program, CGIs and HTML files installed ***
</span><span class='line'>
</span><span class='line'>You can continue with installing Nagios as follows (type 'make'
</span><span class='line'>without any arguments for a list of all possible options):
</span><span class='line'>
</span><span class='line'>  make install-init
</span><span class='line'>     - This installs the init script in /etc/rc.d/init.d
</span><span class='line'>
</span><span class='line'>  make install-commandmode
</span><span class='line'>     - This installs and configures permissions on the
</span><span class='line'>       directory for holding the external command file
</span><span class='line'>
</span><span class='line'>  make install-config
</span><span class='line'>     - This installs sample config files in /usr/local/nagios/etc
</span><span class='line'>
</span><span class='line'>make[1]: Leaving directory `/data/nagios/nagios-4.1.1'
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make install-init
</span><span class='line'>/usr/bin/install -c -m 755 -d -o root -g root /etc/rc.d/init.d
</span><span class='line'>/usr/bin/install -c -m 755 -o root -g root daemon-init /etc/rc.d/init.d/nagios
</span><span class='line'>
</span><span class='line'>*** Init script installed ***
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make install-config
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>*** Config files installed ***
</span><span class='line'>
</span><span class='line'>Remember, these are *SAMPLE* config files.  You'll need to read
</span><span class='line'>the documentation for more information on how to actually define
</span><span class='line'>services, hosts, etc. to fit your particular needs.
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make install-commandmode
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagcmd -d /usr/local/nagios/var/rw
</span><span class='line'>chmod g+s /usr/local/nagios/var/rw
</span><span class='line'>
</span><span class='line'>*** External command directory configured ***
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# vi /usr/local/nagios/etc/objects/contacts.cfg 
</span><span class='line'>...修改define contact定义中的mail为你邮件。
</span><span class='line'>define contact{
</span><span class='line'>        contact_name                    nagiosadmin             ; Short name of user
</span><span class='line'>        use                             generic-contact         ; Inherit default values from generic-contact template (defined above)
</span><span class='line'>        alias                           Nagios Admin            ; Full name of user
</span><span class='line'>
</span><span class='line'>        email                           1234@XXX.cn      ; &lt;&lt;***** CHANGE THIS TO YOUR EMAIL ADDRESS ******
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>      
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# make install-webconf
</span><span class='line'>/usr/bin/install -c -m 644 sample-config/httpd.conf /etc/httpd/conf.d/nagios.conf
</span><span class='line'>
</span><span class='line'>*** Nagios/Apache conf file installed ***
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios-4.1.1]# htpasswd -c /usr/local/nagios/etc/htpasswd.users nagiosadmin
</span><span class='line'>New password: 
</span><span class='line'>Re-type new password: 
</span><span class='line'>Adding password for user nagiosadmin
</span><span class='line'>#[root@cu-omc1 nagios-4.1.1]# chmod 644 /usr/local/nagios/etc/htpasswd.users 
</span><span class='line'>[root@cu2 nagios-4.1.1]# service httpd restart
</span><span class='line'>Stopping httpd:                                            [  OK  ]
</span><span class='line'>Starting httpd: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.0.214 for ServerName
</span><span class='line'>                                                           [  OK  ]
</span><span class='line'>                                             </span></code></pre></td></tr></table></div></figure>


<ul>
<li>编译nagios-plugin</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>如果configure没有mysql和ssl，先安装依赖
</span><span class='line'>yum -y install mysql-devel openssl
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# cd nagios-plugins-2.1.1/
</span><span class='line'>[root@cu2 nagios-plugins-2.1.1]# ./configure --with-nagios-user=nagios --with-nagios-group=nagios
</span><span class='line'>...
</span><span class='line'>            --with-apt-get-command: 
</span><span class='line'>              --with-ping6-command: /bin/ping6 -n -U -w %d -c %d %s
</span><span class='line'>               --with-ping-command: /bin/ping -n -U -w %d -c %d %s
</span><span class='line'>                       --with-ipv6: yes
</span><span class='line'>                      --with-mysql: /usr/bin/mysql_config
</span><span class='line'>                    --with-openssl: yes
</span><span class='line'>                     --with-gnutls: no
</span><span class='line'>               --enable-extra-opts: yes
</span><span class='line'>                       --with-perl: /usr/bin/perl
</span><span class='line'>             --enable-perl-modules: no
</span><span class='line'>                     --with-cgiurl: /nagios/cgi-bin
</span><span class='line'>               --with-trusted-path: /usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
</span><span class='line'>                   --enable-libtap: no
</span><span class='line'>[root@cu2 nagios-plugins-2.1.1]# make 
</span><span class='line'>[root@cu2 nagios-plugins-2.1.1]# make install</span></code></pre></td></tr></table></div></figure>


<p>打开浏览器，使用nagiosadmin和刚刚用htpasswd设置的密码登录就可以localhost的状态了。</p>

<p>手动制造一点异常，如登录用户超过50个。然后看刚刚设置的邮箱是否收到邮件提醒。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# service nagios status
</span><span class='line'>nagios (pid 53764) is running...
</span><span class='line'>[root@cu2 nagios]# lsof -p 53764
</span><span class='line'>nagios  53764 nagios    4u   REG                8,3    12715   280762 /usr/local/nagios/var/nagios.log
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# less /usr/local/nagios/var/nagios.log
</span><span class='line'>看看是否有错误，然后做相应的处理
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# yum install mail -y</span></code></pre></td></tr></table></div></figure>


<ul>
<li>被监控机器安装nrpe</li>
</ul>


<p>程序编译都在cu2上面操作，编译完后，把编译安装的程序直接scp到其他机器就可以了。被监控机器需要nagios-plugin和nrpe。</p>

<p>编译之前可以先看看nrpe-2.15/docs/NRPE.pdf，讲的很详细和清楚。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios-plugins-2.1.1]# cd ../nrpe-2.15
</span><span class='line'>[root@cu2 nrpe-2.15]# ./configure 
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>*** Configuration summary for nrpe 2.15 09-06-2013 ***:
</span><span class='line'>
</span><span class='line'> General Options:
</span><span class='line'> -------------------------
</span><span class='line'> NRPE port:    5666
</span><span class='line'> NRPE user:    nagios
</span><span class='line'> NRPE group:   nagios
</span><span class='line'> Nagios user:  nagios
</span><span class='line'> Nagios group: nagios
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>Review the options above for accuracy.  If they look okay,
</span><span class='line'>type 'make all' to compile the NRPE daemon and client.
</span><span class='line'>
</span><span class='line'>[root@cu2 nrpe-2.15]# make all
</span><span class='line'>cd ./src/; make ; cd ..
</span><span class='line'>make[1]: Entering directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>gcc -g -O2 -I/usr/include/openssl -I/usr/include -DHAVE_CONFIG_H -I ../include -I ./../include -o nrpe ./nrpe.c ./utils.c ./acl.c -L/usr/lib64  -lssl -lcrypto -lnsl -lwrap  
</span><span class='line'>gcc -g -O2 -I/usr/include/openssl -I/usr/include -DHAVE_CONFIG_H -I ../include -I ./../include -o check_nrpe ./check_nrpe.c ./utils.c -L/usr/lib64  -lssl -lcrypto -lnsl 
</span><span class='line'>make[1]: Leaving directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>
</span><span class='line'>*** Compile finished ***
</span><span class='line'>
</span><span class='line'>If the NRPE daemon and client compiled without any errors, you
</span><span class='line'>can continue with the installation or upgrade process.
</span><span class='line'>
</span><span class='line'>Read the PDF documentation (NRPE.pdf) for information on the next
</span><span class='line'>steps you should take to complete the installation or upgrade.
</span><span class='line'>
</span><span class='line'>[root@cu2 nrpe-2.15]# make install-plugin
</span><span class='line'>cd ./src/ && make install-plugin
</span><span class='line'>make[1]: Entering directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagios -d /usr/local/nagios/libexec
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagios check_nrpe /usr/local/nagios/libexec
</span><span class='line'>make[1]: Leaving directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>[root@cu2 nrpe-2.15]# make install-daemon
</span><span class='line'>cd ./src/ && make install-daemon
</span><span class='line'>make[1]: Entering directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagios -d /usr/local/nagios/bin
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagios nrpe /usr/local/nagios/bin
</span><span class='line'>make[1]: Leaving directory `/data/nagios/nrpe-2.15/src'
</span><span class='line'>[root@cu2 nrpe-2.15]# make install-daemon-config
</span><span class='line'>/usr/bin/install -c -m 775 -o nagios -g nagios -d /usr/local/nagios/etc
</span><span class='line'>/usr/bin/install -c -m 644 -o nagios -g nagios sample-config/nrpe.cfg /usr/local/nagios/etc
</span><span class='line'>
</span><span class='line'># 启动nrpe服务
</span><span class='line'>[root@cu2 nrpe-2.15]#  /usr/local/nagios/bin/nrpe -c /usr/local/nagios/etc/nrpe.cfg -d
</span><span class='line'>
</span><span class='line'>[root@cu2 nrpe-2.15]# /usr/local/nagios/libexec/check_nrpe -H localhost
</span><span class='line'>CHECK_NRPE: Error - Could not complete SSL handshake.
</span><span class='line'>[root@cu2 nrpe-2.15]# /usr/local/nagios/libexec/check_nrpe -H 127.0.0.1
</span><span class='line'>NRPE v2.15</span></code></pre></td></tr></table></div></figure>


<p>注意： 如果需要启用传递参数功能需要添加参数<code>--enable-command-args</code>，同时修改配置<code>dont_blame_nrpe=1</code>。<a href="http://www.cppblog.com/fwxjj/archive/2011/10/28/159262.aspx">http://www.cppblog.com/fwxjj/archive/2011/10/28/159262.aspx</a></p>

<p>拷贝程序到其他机器：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 local]# scp -r nagios cu3:/usr/local/
</span><span class='line'>
</span><span class='line'>[root@cu3 ~]# cd /usr/local/nagios/
</span><span class='line'>[root@cu3 nagios]# bin/nrpe -c etc/nrpe.cfg -d
</span><span class='line'>[root@cu3 nagios]# 
</span><span class='line'>[root@cu3 nagios]# libexec/check_nrpe -H 127.0.0.1
</span><span class='line'>NRPE v2.15</span></code></pre></td></tr></table></div></figure>


<p>修改配置，添加可以访问nrpe的白名单：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu3 nagios]# vi etc/nrpe.cfg 
</span><span class='line'>...修改
</span><span class='line'>allowed_hosts=127.0.0.1,192.168.0.0/24
</span><span class='line'>...查看
</span><span class='line'>command[check_users]=/usr/local/nagios/libexec/check_users -w 5 -c 10
</span><span class='line'>command[check_load]=/usr/local/nagios/libexec/check_load -w 15,10,5 -c 30,25,20
</span><span class='line'>command[check_hda1]=/usr/local/nagios/libexec/check_disk -w 20% -c 10% -p /dev/hda1
</span><span class='line'>command[check_zombie_procs]=/usr/local/nagios/libexec/check_procs -w 5 -c 10 -s Z
</span><span class='line'>command[check_total_procs]=/usr/local/nagios/libexec/check_procs -w 150 -c 200 
</span><span class='line'>
</span><span class='line'>nrpe.cfg最下面是nrpe的command，nagios配置中会用到
</span><span class='line'>
</span><span class='line'>[root@cu3 nagios]# pkill nrpe
</span><span class='line'>[root@cu3 nagios]# bin/nrpe -c etc/nrpe.cfg -d</span></code></pre></td></tr></table></div></figure>


<p>再回到cu2，把cu3加入nagios管理列表中：</p>

<p>首先，nagios的配置入口为etc/nagios.cfg。其他配置文件都是通过cfg_file去定位的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# vi etc/nagios.cfg 
</span><span class='line'>...添加
</span><span class='line'>cfg_file=/usr/local/nagios/etc/objects/hosts.cfg
</span><span class='line'>cfg_file=/usr/local/nagios/etc/objects/localhost.cfg
</span><span class='line'>cfg_file=/usr/local/nagios/etc/objects/cu3.cfg
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# vi etc/objects/commands.cfg 
</span><span class='line'>...添加
</span><span class='line'># 'check_nrpe' command definition
</span><span class='line'>define command{
</span><span class='line'>        command_name check_nrpe
</span><span class='line'>        command_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# vi etc/objects/cu3.cfg 
</span><span class='line'>...新增
</span><span class='line'>
</span><span class='line'>define host{
</span><span class='line'>        use                     linux-server            ; Name of host template to use
</span><span class='line'>                                                        ; This host definition will inherit all variables that are defined
</span><span class='line'>                                                        ; in (or inherited by) the linux-server host template definition.
</span><span class='line'>        host_name               cu3
</span><span class='line'>        alias                   cu3
</span><span class='line'>        address                 192.168.0.148
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu3
</span><span class='line'>        service_description             PING
</span><span class='line'>        check_command                   check_ping!100.0,20%!500.0,60%
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu3
</span><span class='line'>        service_description             Root Partition
</span><span class='line'>        check_command                   check_nrpe!check_hda1
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu3
</span><span class='line'>        service_description             Current Users
</span><span class='line'>        check_command                   check_nrpe!check_users
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu3
</span><span class='line'>        service_description             Total Processes
</span><span class='line'>        check_command                   check_nrpe!check_total_procs
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu3
</span><span class='line'>        service_description             Current Load
</span><span class='line'>        check_command                   check_nrpe!check_load
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>[root@cu2 nagios]# vi etc/objects/hosts.cfg 
</span><span class='line'>... 新增
</span><span class='line'>define hostgroup{
</span><span class='line'>        hostgroup_name  linux-servers ; The name of the hostgroup
</span><span class='line'>        alias           Linux Servers ; Long name of the group
</span><span class='line'>        members         localhost,cu3,cu4,cu5,cu1     ; Comma separated list of hosts that belong to this group
</span><span class='line'>        }
</span></code></pre></td></tr></table></div></figure>


<p>配置完后，校验配置，然后重启nagios。然后就可以打开浏览器查看cu3状态。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 nagios]# bin/nagios -v etc/nagios.cfg 
</span><span class='line'>
</span><span class='line'>Nagios Core 4.1.1
</span><span class='line'>Copyright (c) 2009-present Nagios Core Development Team and Community Contributors
</span><span class='line'>Copyright (c) 1999-2009 Ethan Galstad
</span><span class='line'>Last Modified: 08-19-2015
</span><span class='line'>License: GPL
</span><span class='line'>
</span><span class='line'>Website: https://www.nagios.org
</span><span class='line'>Reading configuration data...
</span><span class='line'>   Read main config file okay...
</span><span class='line'>   Read object config files okay...
</span><span class='line'>
</span><span class='line'>Running pre-flight check on configuration data...
</span><span class='line'>
</span><span class='line'>Checking objects...
</span><span class='line'>        Checked 23 services.
</span><span class='line'>        Checked 4 hosts.
</span><span class='line'>        Checked 1 host groups.
</span><span class='line'>        Checked 0 service groups.
</span><span class='line'>        Checked 1 contacts.
</span><span class='line'>        Checked 1 contact groups.
</span><span class='line'>        Checked 25 commands.
</span><span class='line'>        Checked 5 time periods.
</span><span class='line'>        Checked 0 host escalations.
</span><span class='line'>        Checked 0 service escalations.
</span><span class='line'>Checking for circular paths...
</span><span class='line'>        Checked 4 hosts
</span><span class='line'>        Checked 0 service dependencies
</span><span class='line'>        Checked 0 host dependencies
</span><span class='line'>        Checked 5 timeperiods
</span><span class='line'>Checking global event handlers...
</span><span class='line'>Checking obsessive compulsive processor commands...
</span><span class='line'>Checking misc settings...
</span><span class='line'>
</span><span class='line'>Total Warnings: 0
</span><span class='line'>Total Errors:   0
</span><span class='line'>
</span><span class='line'>Things look okay - No serious problems were detected during the pre-flight check
</span><span class='line'>[root@cu2 nagios]# service nagios restart
</span><span class='line'>Running configuration check...
</span><span class='line'>Stopping nagios: done.
</span><span class='line'>Starting nagios: done.</span></code></pre></td></tr></table></div></figure>


<h2>后记</h2>

<p>技巧：配置服务的时刻，制定host_name可以使用正则表达式，一个服务通吃。对于功能类似的机器，可以减少很多工作量：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>vi etc/nagios.cfg 
</span><span class='line'>... 修改
</span><span class='line'>use_regexp_matching=1
</span><span class='line'>use_true_regexp_matching=1
</span><span class='line'>
</span><span class='line'>vi cu.cfg
</span><span class='line'>... 
</span><span class='line'>define service{
</span><span class='line'>        use                             generic-service         ; Name of service template to use
</span><span class='line'>        host_name                       cu.*
</span><span class='line'>        service_description             Current Load
</span><span class='line'>        check_command                   check_nrpe!check_load
</span><span class='line'>        }</span></code></pre></td></tr></table></div></figure>


<p>基本功能就算配置好了，如果出现异常就能得到邮件提醒。</p>

<h2>参考</h2>

<ul>
<li><a href="http://nagios-cn.sourceforge.net/nagios-cn/beginning.html#quickstart-fedora">http://nagios-cn.sourceforge.net/nagios-cn/beginning.html#quickstart-fedora</a></li>
<li><a href="http://skypegnu1.blog.51cto.com/8991766/1532948">http://skypegnu1.blog.51cto.com/8991766/1532948</a></li>
<li><a href="http://nagios-cn.sourceforge.net/nagios-cn/beginning.html#monitoring-linux">http://nagios-cn.sourceforge.net/nagios-cn/beginning.html#monitoring-linux</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cacti监控主机]]></title>
    <link href="http://winseliu.com/blog/2015/09/22/cacti-start-guide/"/>
    <updated>2015-09-22T14:33:36+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/22/cacti-start-guide</id>
    <content type="html"><![CDATA[<p>其实通过yum好依赖的php、rrdtool、snmp后，安装配置Cacti其实很简单。</p>

<h2>环境说明</h2>

<p>五台机器：cu1~cu5(centos6.6)， 其中仅cu2作为cacti服务器，所有服务器都安装snmp服务。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cu1     192.168.0.37 
</span><span class='line'>cu2     192.168.0.214 
</span><span class='line'>cu3     192.168.0.148 
</span><span class='line'>cu4     192.168.0.30 
</span><span class='line'>cu5     192.168.0.174 </span></code></pre></td></tr></table></div></figure>


<h2>软件安装</h2>

<p>版本信息在贴的内容中体现。PHP不会，仅仅作为一个工具来使用。</p>

<h3>Cacti服务器机器安装</h3>

<p>mysql数据库5.1</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# rpm -q mysql 
</span><span class='line'>mysql-5.1.73-5.el6_6.x86_64</span></code></pre></td></tr></table></div></figure>


<p>首先用yum安装依赖软件php，httpd，snmp和<strong>rrdtool</strong>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# yum install epel-release
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# yum install httpd php php-devel php-mysql php-pear php-common php-gd php-mbstring php-cli php-snmp net-snmp net-snmp-utils net-snmp-libs rrdtool 
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>Installed:
</span><span class='line'>  net-snmp-libs.x86_64 1:5.5-54.el6_7.1 net-snmp-utils.x86_64 1:5.5-54.el6_7.1 php.x86_64 0:5.3.3-46.el6_6          php-cli.x86_64 0:5.3.3-46.el6_6   php-common.x86_64 0:5.3.3-46.el6_6
</span><span class='line'>  php-devel.x86_64 0:5.3.3-46.el6_6     php-gd.x86_64 0:5.3.3-46.el6_6         php-mbstring.x86_64 0:5.3.3-46.el6_6 php-mysql.x86_64 0:5.3.3-46.el6_6 php-pear.noarch 1:1.9.4-4.el6     
</span><span class='line'>  php-snmp.x86_64 0:5.3.3-46.el6_6      rrdtool.x86_64 0:1.3.8-7.el6          
</span><span class='line'>
</span><span class='line'>Dependency Installed:
</span><span class='line'>  autoconf.noarch 0:2.63-5.1.el6               automake.noarch 0:1.11.1-4.el6                  dejavu-fonts-common.noarch 0:2.33-1.el6   dejavu-lgc-sans-mono-fonts.noarch 0:2.33-1.el6  
</span><span class='line'>  dejavu-sans-mono-fonts.noarch 0:2.33-1.el6   fontpackages-filesystem.noarch 0:1.41-1.1.el6   lm_sensors-libs.x86_64 0:3.1.1-17.el6     net-snmp.x86_64 1:5.5-54.el6_7.1                
</span><span class='line'>  php-pdo.x86_64 0:5.3.3-46.el6_6             
</span><span class='line'>
</span><span class='line'>Updated:
</span><span class='line'>  httpd.x86_64 0:2.2.15-47.el6.centos                                                                                                                                                       
</span><span class='line'>
</span><span class='line'>Dependency Updated:
</span><span class='line'>  httpd-tools.x86_64 0:2.2.15-47.el6.centos                                                                                                                                                 
</span><span class='line'>
</span><span class='line'>Complete!
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# service httpd start
</span><span class='line'>Starting httpd: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.0.214 for ServerName
</span><span class='line'>                                                           [  OK  ]
</span><span class='line'>[root@cu2 ~]# service snmpd start
</span><span class='line'>Starting snmpd:                                            [  OK  ]
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# vi /etc/snmp/snmpd.conf 
</span><span class='line'>     41 #com2sec notConfigUser  default       public
</span><span class='line'>     42 com2sec notConfigUser  192.168.0.214       public
</span><span class='line'>   ...
</span><span class='line'>     63 #access  notConfigGroup ""      any       noauth    exact  systemview none none
</span><span class='line'>     64 access  notConfigGroup ""      any       noauth    exact  all none none
</span><span class='line'>   ...
</span><span class='line'>     86 ##           incl/excl subtree                          mask
</span><span class='line'>     87 view all    included  .1                               80  
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# service snmpd restart
</span><span class='line'>
</span><span class='line'># 使用snmpwalk可以得到数据
</span><span class='line'>[root@cu2 ~]# snmpwalk -Os -c public -v 1 cu2 system
</span><span class='line'>[root@cu2 ~]# snmpwalk -v 1 -c public localhost IP-MIB::ipAdEntIfIndex</span></code></pre></td></tr></table></div></figure>


<p>然后，把Cacti应用解压到httpd默认目录下/var/www/html。同时配置cacti连接到数据库。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 ~]# cd /var/www/html/
</span><span class='line'>[root@cu2 html]# tar zxvf cacti-0.8.8f.tar.gz 
</span><span class='line'>
</span><span class='line'>[root@cu2 html]# ln -s cacti-0.8.8f cacti
</span><span class='line'>
</span><span class='line'>[root@cu2 html]$ mysql -u root -p -h 127.0.0.1
</span><span class='line'>Enter password: 
</span><span class='line'>mysql&gt; 
</span><span class='line'>mysql&gt; create database cacti character set UTF8;
</span><span class='line'>mysql&gt; grant all on cacti.* to cacti@'%' identified by 'cacti';
</span><span class='line'>mysql&gt; flush privileges;
</span><span class='line'>mysql&gt; source /var/www/html/cacti/cacti.sql;
</span><span class='line'>
</span><span class='line'>[root@cu2 html]# vi cacti/include/config.php 
</span><span class='line'>$database_type = "mysql";
</span><span class='line'>$database_default = "cacti";
</span><span class='line'>$database_hostname = "127.0.0.1";
</span><span class='line'>$database_username = "cacti";
</span><span class='line'>$database_password = "cacti";
</span><span class='line'>$database_port = "3306";
</span><span class='line'>
</span><span class='line'>[root@cu2 html]$ vi /etc/php.ini 
</span><span class='line'>date.timezone = "Asia/Shanghai"
</span><span class='line'>
</span><span class='line'># 重启httpd服务
</span><span class='line'>
</span><span class='line'>[root@cu2 cacti]# php poller.php 
</span><span class='line'>
</span><span class='line'>[root@cu2 cacti]# crontab -e
</span><span class='line'>* * * * * php /var/www/html/cacti/poller.php &gt; /var/www/html/cacti/log/cron.log 2&gt;&1
</span></code></pre></td></tr></table></div></figure>


<p>打开浏览器访问：<a href="http://cu2/cacti/">http://cu2/cacti/</a> 首先会进入到install步骤，按照提示一步下一步，最后输入admin/admin登录。点击右上角的Preview View就可以看到图了。</p>

<p>如果启动错误，查看日志文件看日志：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 cacti]# less /var/log/httpd/error_log 
</span><span class='line'>[root@cu2 cacti]# less log/cacti.log </span></code></pre></td></tr></table></div></figure>


<h3>添加插件</h3>

<p>（网上很多文章都要打补丁，我这里的版本是最新的，同时plugin的补丁没有对应的版本，这里直接安装插件）</p>

<p>从<a href="http://docs.cacti.net/plugins">http://docs.cacti.net/plugins</a>下载<a href="http://docs.cacti.net/plugin:monitor">monitor</a>。把下载文件解压到plugins目录下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu2 plugins]# pwd
</span><span class='line'>/var/www/html/cacti/plugins
</span><span class='line'>[root@cu2 plugins]# ll
</span><span class='line'>-rw-r--r-- 1 1000 users   44 Jul 20 21:42 index.php
</span><span class='line'>drwxr-xr-x 4 root root  4096 Oct  6  2011 monitor</span></code></pre></td></tr></table></div></figure>


<p>然后进入Plugin Management页面<a href="http://cu2/cacti/plugins.php">http://cu2/cacti/plugins.php</a>，就能看到Monitor插件。点击表格Actions列的<strong>安装和启用</strong>图标（按钮），启用后，最上面页签会增加新的页签项monitor。</p>

<p>点击monitor页签，可以查看机器存活的状态。</p>

<p>同时Settings页面多了Misc选项卡，可以配置修改monitor属性。</p>

<p>注意：网上版本资料都有配置config.php添加plugins变量。我这里没进行这个操作也是ok的，安装-启用成功后会把monitor下面的sql更新到数据库，不需要手动执行。</p>

<h3>安装spine</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>注意：设置下umask避免不需要的麻烦： umask 0022
</span><span class='line'>[root@cu2 ~] tar zxvf cacti-spine-0.8.8f.tar.gz
</span><span class='line'>
</span><span class='line'>[root@cu2 cacti-spine-0.8.8f]# yum install -y mysql-devel net-snmp-devel
</span><span class='line'>
</span><span class='line'>[root@cu2 cacti-spine-0.8.8f]# ./configure --prefix=/usr/local/cacti-spine
</span><span class='line'>[root@cu2 cacti-spine-0.8.8f]# make && make install
</span><span class='line'># 如果make缺少报了错，需要重新configuration一遍
</span><span class='line'>
</span><span class='line'>[root@cu2 cacti-spine-0.8.8f]# cd /usr/local/cacti-spine/etc
</span><span class='line'>[root@cu2 etc]# mv spine.conf.dist spine.conf
</span><span class='line'>[root@cu2 etc]# vi spine.conf 
</span><span class='line'>DB_Host         127.0.0.1
</span><span class='line'>DB_Database     cacti
</span><span class='line'>DB_User         cacti
</span><span class='line'>DB_Pass         cacti
</span><span class='line'>DB_Port         3306
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>然后修改<a href="http://hadoop-master2/cacti/settings.php">Cacti</a>使用spine来获取信息。</li>
</ul>


<p>在[Settings]-[Paths]添加Spine Poller File Path为<code>/usr/local/cacti-spine/bin/spine</code>。在[Poller]选项卡，[Poller Type]修改为spine，[Poller Interval]和[Cron Interval]修改为一分钟即Every Minute。</p>

<ul>
<li>添加“每分钟”流量视图:</li>
</ul>


<p>点击Console -> Data Templates -> [Interface -> Traffic ] 添加“每分钟”流量视图，将轮询时间设置为60秒，Heartbeat时间设置为120秒(traffic_in/traffic_out里面的Heartbeat时间也设置为120秒)</p>

<h2>被监控机器配置</h2>

<p>被监控的机器，仅仅需要安装snmp即可。然后配置snpmd.conf即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@cu5 ~]# yum install  net-snmp net-snmp-utils net-snmp-libs -y
</span><span class='line'>[root@cu5 ~]# vi /etc/snmp/snmpd.conf 
</span><span class='line'>     41 #com2sec notConfigUser  default       public
</span><span class='line'>     42 com2sec notConfigUser  192.168.0.214       public
</span><span class='line'>   ...
</span><span class='line'>     63 #access  notConfigGroup ""      any       noauth    exact  systemview none none
</span><span class='line'>     64 access  notConfigGroup ""      any       noauth    exact  all none none
</span><span class='line'>   ...
</span><span class='line'>     86 ##           incl/excl subtree                          mask
</span><span class='line'>     87 view all    included  .1                               80  
</span><span class='line'>
</span><span class='line'>[root@cu2 ~]# service snmpd restart</span></code></pre></td></tr></table></div></figure>


<p>然后在Cacti的web页面添加Device(主机)：</p>

<ul>
<li>点击Console->Devices，打开设备管理页面。</li>
<li>点击右上角的add，添加一个新的机器</li>
<li>当主机的信息填好之后，点击Create

<ul>
<li>Host Template就是一个模板，会事先建立一些Associated Graph Templates和Associated Data Queries的数据，如Load Average，Memory Uages等。如果不确定直接选None即可。</li>
<li>SNMP Version选<code>Version 2</code>，SNMP Community与snmpd.conf中对应，如果安装上面操作，默认即可。</li>
</ul>
</li>
<li>此时你的页面左上角应该显示：Save Successful，并且已经显示出了主机信息和SNMP信息，如果SNMP信息显示 SNMP error，请查看最后的问题综述。</li>
<li>这时我们就可以添加相应的监控项了，在页面最下方的Associated Graph Templates中添加图形模板，在Associated Data Queries中添加数据模板。</li>
<li>保存，点击右上角的Create Graphs for this Host，来为刚才通过模板所获得到的数据进行画图。</li>
<li>选择好需要画图的项目后，点击右下角的Create，左上角会出现被创建出来的画图项。</li>
</ul>


<p>总结就是添加设备，然后生成图形，最后等待生成画图查看。</p>

<p>在Graphs界面左边显示树新添加主机。</p>

<ul>
<li>在Cacti界面Graph Trees中，选择进入节点(或者系统默认的Default Tree)。</li>
<li>添加一个新的显示项，在Tree Item Type中选择Host，然后在下面的Host中选择我们刚才创建的主机。点击Create。</li>
</ul>


<p><a href="http://docs.cacti.net/templates">http://docs.cacti.net/templates</a></p>

<h2>进阶</h2>

<p><a href="http://skypegnu1.blog.51cto.com/8991766/1537374">http://skypegnu1.blog.51cto.com/8991766/1537374</a></p>

<blockquote><p>cacti是如何获取数据呢？  <br/>
    其实cacti获取数据的方式是多样化的，通过周期性的执行某个脚本，或者使用snmp，更或者是ssh，这些都是根据实际需要以及方便性来抉择。cacti需要周期性的驱动这些获取数据的脚本执行，并把取得的数据保存至相应的rrd数据库中。
cacti是如何保存数据（创建rrd，并更新数据）呢？
    这就是数据模板的功能。
cacti是如何展示数据（绘图）呢？
    这就是图形模板的功能。</p></blockquote>

<p><a href="http://skypegnu1.blog.51cto.com/8991766/1537615">http://skypegnu1.blog.51cto.com/8991766/1537615</a>
<a href="http://skypegnu1.blog.51cto.com/8991766/1538459">http://skypegnu1.blog.51cto.com/8991766/1538459</a>
<a href="http://skypegnu1.blog.51cto.com/8991766/1547029">http://skypegnu1.blog.51cto.com/8991766/1547029</a></p>

<h2>资料</h2>

<p>入门的文档不错，可以到<a href="http://vdisk.weibo.com/u/1554831624">微盘</a>下载。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Cacti.0.8_Beginner.Guide.pdf
</span><span class='line'>
</span><span class='line'>Cacti实战指南--备份还原.pdf
</span><span class='line'>Cacti实战指南-完美部署.pdf
</span><span class='line'>Cacti实战指南-巧设轮询.pdf
</span><span class='line'>Cacti实战指南-插件安装.pdf
</span><span class='line'>Cacti实战指南-用户权限.pdf
</span><span class='line'>Cacti实战指南-邮件预警.pdf
</span><span class='line'>Cacti实战指南-阀值预警.pdf
</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><p>先看这个文档 <a href="http://blog.chinaunix.net/attachment/attach/21/08/97/212108972176206e1112f29600926449bdeedb3970.pdf">http://blog.chinaunix.net/attachment/attach/21/08/97/212108972176206e1112f29600926449bdeedb3970.pdf</a></p></li>
<li><p><a href="http://blog.csdn.net/chen3888015/article/details/8233125">http://blog.csdn.net/chen3888015/article/details/8233125</a></p></li>
<li><p><a href="http://www.cacti.net/downloads/docs/pdf/manual.pdf">http://www.cacti.net/downloads/docs/pdf/manual.pdf</a></p></li>
<li><p><a href="http://wenku.baidu.com/view/57aa69487fd5360cba1adb40.html?re=view">http://wenku.baidu.com/view/57aa69487fd5360cba1adb40.html?re=view</a></p></li>
<li><p><a href="http://wenku.baidu.com/view/b2d1f6c689eb172ded63b7f9.html?re=view">http://wenku.baidu.com/view/b2d1f6c689eb172ded63b7f9.html?re=view</a></p></li>
<li><p><a href="http://www.ehowstuff.com/how-to-install-and-configure-epel-repository-on-centos-6-2/">http://www.ehowstuff.com/how-to-install-and-configure-epel-repository-on-centos-6-2/</a></p></li>
<li><a href="http://www.ehowstuff.com/how-to-install-cacti-on-centos-6-2-using-epel-repository/">http://www.ehowstuff.com/how-to-install-cacti-on-centos-6-2-using-epel-repository/</a></li>
<li><a href="http://www.ehowstuff.com/how-to-setup-and-configure-cacti-on-centos-6-2/">http://www.ehowstuff.com/how-to-setup-and-configure-cacti-on-centos-6-2/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[解决SecureCRT【zmodem Transfer Canceled by Remote Side】问题]]></title>
    <link href="http://winseliu.com/blog/2015/09/21/solve-securecrt-zmodem-transfer-canceled-by-remote-side/"/>
    <updated>2015-09-21T16:02:18+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/21/solve-securecrt-zmodem-transfer-canceled-by-remote-side</id>
    <content type="html"><![CDATA[<p>处理方法很简单，解决后，对于ssh机器之间多次跳转的文件传输操作会方便很多。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>写到/etc/profile以后就可以直接使用了
</span><span class='line'>[root@af042cb4b34c ~]# alias rz="rz -e"
</span><span class='line'>
</span><span class='line'>[root@af042cb4b34c ~]# cd /usr/share/cacti/
</span><span class='line'>[root@af042cb4b34c cacti]# cd plugins
</span><span class='line'>[root@af042cb4b34c plugins]# rz
</span><span class='line'>rz waiting to receive.
</span><span class='line'>Starting zmodem transfer.  Press Ctrl+C to cancel.
</span><span class='line'>Transferring monitor-v1.3-1.tgz...
</span><span class='line'>  100%     231 KB     231 KB/sec    00:00:01       0 Errors  
</span><span class='line'>
</span><span class='line'>[root@af042cb4b34c plugins]# ll
</span><span class='line'>total 236
</span><span class='line'>-rw-r--r-- 1 root root     44 Aug  7  2013 index.php
</span><span class='line'>-rw-r--r-- 1 root root 236682 Sep 21 07:54 monitor-v1.3-1.tgz</span></code></pre></td></tr></table></div></figure>


<p>zmoden还是有比较多的限制。sftp还是不能少啊！！</p>

<h2>参考</h2>

<ul>
<li><a href="http://iamhere1.blog.163.com/blog/static/23612284201372322622902/">http://iamhere1.blog.163.com/blog/static/23612284201372322622902/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【linux 101 Hacks】读后感]]></title>
    <link href="http://winseliu.com/blog/2015/09/13/review-linux-101-hacks/"/>
    <updated>2015-09-13T13:12:53+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/13/review-linux-101-hacks</id>
    <content type="html"><![CDATA[<p>本书讲了linux维护和管理过程中常用的命令。</p>

<p>分12个章节，分别将了目录切换、日期、SSH远程登录、常用linux命令、PS1-4操作提示符、解压缩、命令历史记录、系统管理、容器服务器Apache、脚本环境变量、性能监控等。</p>

<p>介绍的命令有：cd, dirs, pushd, popd, cdpath, <strong>alias</strong>, mkdir, eval, date, hwclock, ssh, grep, find, 输出重定向, join, tr, xargs, sort, uniq, cut, stat, diff, ac, ps1, ps2, ps3, ps4, PROMPT_COMMAND, zip, unzip, tar, gzip, bzip2, HISTTIMEFORMAT, HISTSIZE, HISTIGNORE, fdisk, mke2fsk, mount, tune2fs, useradd, adduser, passwd, groupadd, id, ssh-copy-id, ssh-agent, crontab, apachectl, httpd, <strong>.bash_rc</strong>, .bash_profile, 单引号, 双引号, free, top, ps, df, kill, du, lsof, sar, vmstat, netstat, sysctl, nice, renice等等。</p>

<p>下面结合工作中的一些实践，谈一谈</p>

<h2>技巧一：登录服务器</h2>

<p>不管是正式环境还是云端的测试环境，一般提供给我们访问的只有一个入口（也就是常说的跳板机），登录跳板机后然后才能连接其他服务器。常用的工具有【SecureCRT】和【Xshell】，它们的使用方式基本相同。</p>

<p>最佳实践：连接跳板机的同时，建立自己机器和内网机器之间的隧道，即可以方便浏览器的访问，同时也可以使用sftp直接传输文件到内网机器。</p>

<p><img src="http://winseliu.com/images/blogs/linux-101-hacks-review-securecrt-config.png" alt="" /></p>

<p><img src="http://winseliu.com/images/blogs/linux-101-hacks-review-securecrt-web.png" alt="" /></p>

<h2>技巧二：ssh-copy-id【hack 72】</h2>

<p>想不通，现在的教程都使用【复制-添加-修改权限】公钥的方式来进行无密钥登录配置。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 ~]$ scp .ssh/id_rsa.pub 172.17.0.3:~/master_id_rsa.pub
</span><span class='line'>hadoop@172.17.0.3's password: 
</span><span class='line'>id_rsa.pub                                                                                                                     100%  403     0.4KB/s   00:00    
</span><span class='line'>[hadoop@hadoop-master1 ~]$ ssh 172.17.0.3
</span><span class='line'>hadoop@172.17.0.3's password: 
</span><span class='line'>Last login: Sun Sep 13 11:41:17 2015 from 172.17.0.2
</span><span class='line'>[hadoop@hadoop-slaver1 ~]$ cat master_id_rsa.pub &gt;&gt; .ssh/authorized_keys 
</span><span class='line'>[hadoop@hadoop-slaver1 ~]$ ll -d .ssh
</span><span class='line'>drwx------. 2 hadoop hadoop 4096 Mar 10  2015 .ssh
</span><span class='line'>[hadoop@hadoop-slaver1 ~]$ ll .ssh/authorized_keys 
</span><span class='line'>-rw-------. 1 hadoop hadoop 403 Sep 13 11:58 .ssh/authorized_keys</span></code></pre></td></tr></table></div></figure>


<p>处理一个ssh无密钥登录搞N多的步骤，还不一定能成功！其实使用ssh-copy-id的命令就行了，不知道各类书籍上面都使用老旧的方法，都是抄来的吗？！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver1 ~]$ ssh-copy-id -i .ssh/id_rsa.pub 172.17.0.2
</span><span class='line'>The authenticity of host '172.17.0.2 (172.17.0.2)' can't be established.
</span><span class='line'>RSA key fingerprint is aa:41:79:6d:9d:c2:ec:f1:29:71:43:24:39:09:58:b6.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added '172.17.0.2' (RSA) to the list of known hosts.
</span><span class='line'>hadoop@172.17.0.2's password: 
</span><span class='line'>Now try logging into the machine, with "ssh '172.17.0.2'", and check in:
</span><span class='line'>
</span><span class='line'>  .ssh/authorized_keys
</span><span class='line'>
</span><span class='line'>to make sure we haven't added extra keys that you weren't expecting.</span></code></pre></td></tr></table></div></figure>


<h2>技巧三：查看机器</h2>

<p>碰到不认识的人，我们都会上下打量。机器也一样，首先要了解机器，才能充分的发挥自己的性能。存储不够要么删点，要么加磁盘等等。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>uname -a
</span><span class='line'>cat /etc/redhat-release 
</span><span class='line'>ifconfig
</span><span class='line'>
</span><span class='line'>date
</span><span class='line'>
</span><span class='line'>df -h , df -Tha
</span><span class='line'>free -m 
</span><span class='line'>uptime
</span><span class='line'>top
</span><span class='line'>ps aux , ps auxf
</span><span class='line'>netstat -atp
</span><span class='line'>du -h --max-depth=1
</span><span class='line'>lsof -i:[PORT]
</span><span class='line'>
</span><span class='line'>cat /etc/hosts</span></code></pre></td></tr></table></div></figure>


<h2>技巧四：管道</h2>

<p>一个命令的结果直接输出给另一个命令。就像水从一个结头通过管子直接流向下一个结头一样。中间不需要落地，直接立即用于下一个命令，直到结果输出。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat /etc/hosts | grep 'hadoop' | awk '{print $2}' | while read h ; do echo $h ; done</span></code></pre></td></tr></table></div></figure>


<p>shell的命令那么多，简单功能的材料都准备好了，就像堆积木一样，叠加后总能实现你想得到的效果。</p>

<p>在进行一次性文件拷贝时，如果文件数量过多，可以先打包然后传到远程机器再解压：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zc nginx | ssh bigdata1 'tar zx'</span></code></pre></td></tr></table></div></figure>


<h2>技巧N：查看帮助</h2>

<p>写java的没看过开源项目不要说自己会java，写shell没用过man不要说自己会shell！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>man [CMD]
</span><span class='line'>info [CMD]
</span><span class='line'>[CMD] help
</span><span class='line'>[CMD] -h
</span><span class='line'>[CMD] -help</span></code></pre></td></tr></table></div></figure>


<p>总有一款适合你，带着实践和问题的目的去学/写，能更好的把握它。（shell的命令太多，不要寄希望于看一个宝典就能写好！实践出真知，真正用到的才是实用的）</p>

<h2>技巧N-1：调试Shell脚本</h2>

<p>Shell脚本/命令在执行前会对变量进行解析、处理。查看最终执行的命令，能让我们了解到脚本不正确的地方，然后及时进行更正。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set命令的参数说明
</span><span class='line'>-v      Print shell input lines as they are read.
</span><span class='line'>-x      After  expanding each simple command, for command, case command, select command, or arithmetic for command, 
</span><span class='line'>display the expanded value of PS4, followed by the command and its expanded arguments or associated word list.
</span><span class='line'>    
</span><span class='line'>[hadoop@hadoop-master2 ~]$ set -x
</span><span class='line'>[hadoop@hadoop-master2 1]$ cmd=*
</span><span class='line'>+ cmd='*'
</span><span class='line'>[hadoop@hadoop-master2 1]$ echo $cmd
</span><span class='line'>+ echo 2 file
</span><span class='line'>2 file
</span><span class='line'>[hadoop@hadoop-master2 1]$ echo "$cmd" # 双引号
</span><span class='line'>+ echo '*'
</span><span class='line'>*
</span><span class='line'>[hadoop@hadoop-master2 1]# echo '$cmd' # 单引号
</span><span class='line'>+ echo '$cmd'
</span><span class='line'>$cmd</span></code></pre></td></tr></table></div></figure>


<p>调试脚本</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 1]# vi run.sh
</span><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'>bin=$(dir=`dirname $0`; cd $dir ; pwd)
</span><span class='line'>
</span><span class='line'>cd $bin
</span><span class='line'>ls -l
</span><span class='line'>
</span><span class='line'>[root@hadoop-master2 1]# sh -x run.sh 
</span><span class='line'>+++ dirname run.sh
</span><span class='line'>++ dir=.
</span><span class='line'>++ cd .
</span><span class='line'>++ pwd
</span><span class='line'>+ bin=/tmp/1
</span><span class='line'>+ cd /tmp/1
</span><span class='line'>+ ls -l
</span><span class='line'>total 8
</span><span class='line'>drwxrwxr-x 2 hadoop hadoop 4096 Sep 13 20:33 2
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop    0 Sep 13 20:33 file
</span><span class='line'>-rw-r--r-- 1 root   root     66 Sep 13 21:12 run.sh</span></code></pre></td></tr></table></div></figure>


<h2>技巧N-2：历史history</h2>

<p>历史如足迹。如果你要学习前辈的经验，理着他的足迹，一步步的走！</p>

<p>很多书上说的，<code>CTRL+R, !!, !-1, CTRL+P, ![CMD], !!:$, !^, ![CMD]:2, ![CMD]:$</code>用于获取以后执行的命令或者参数，多半好看不实用。会写的也就前面1-2个命令重复用一下，上下方向键就可以了，不会写的用history查看全部慢慢学更实际点。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>历史记录执行时间
</span><span class='line'>export HISTTIMEFORMAT='%F %T '
</span><span class='line'>输出最近10条历史
</span><span class='line'>alias hist10='history 10'
</span><span class='line'>
</span><span class='line'>持久化保存的历史记录数
</span><span class='line'>vi ~/.bash_profile
</span><span class='line'>HISTSIZE=450
</span><span class='line'>HISTFILESIZE=450
</span><span class='line'># HISTFILE
</span><span class='line'>
</span><span class='line'>忽略连续重复的命令
</span><span class='line'>export HISTCONTROL=ignoredups
</span><span class='line'>
</span><span class='line'>忽略重复的命令
</span><span class='line'>export HISTCONTROL=erasedups
</span><span class='line'>
</span><span class='line'>忽略指定的命令
</span><span class='line'>export HISTIGNORE='pwd:ls'</span></code></pre></td></tr></table></div></figure>


<h2>技巧N-3：shell之grep awk sed vi</h2>

<p>这些就不是看看man就能上手的，细嚼慢咽找几本书翻翻！！</p>

<p>推荐两本书： [sed与awk(第二版)], [Shell脚本学习指南]</p>

<h2>技巧N-4：批量处理之神：expect/for/while</h2>

<p>传入用户（与ssh的用户一致）密码，进行SSH无密钥认证：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 1]# vi ssh-copy-id.expect
</span><span class='line'>#!/usr/bin/expect  
</span><span class='line'>
</span><span class='line'>## Usage $0 [user@]host password
</span><span class='line'>
</span><span class='line'>set host [lrange $argv 0 0];
</span><span class='line'>set password [lrange $argv 1 1] ;
</span><span class='line'>
</span><span class='line'>set timeout 30;
</span><span class='line'>
</span><span class='line'>spawn ssh-copy-id $host ;
</span><span class='line'>
</span><span class='line'>expect {
</span><span class='line'>  "(yes/no)?" { send yes\n; exp_continue; }
</span><span class='line'>  "password:" { send $password\n; exp_continue; }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>exec sleep 1;
</span><span class='line'>
</span><span class='line'>批量处理
</span><span class='line'>[root@hadoop-master2 1]# for h in `cat /etc/hosts | grep hadoop | awk '{print $2}' ` ; do ./ssh-copy-id.expect $h root-password ; done</span></code></pre></td></tr></table></div></figure>


<p>传入新用户名称和密码，新建用户：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@hadoop-master2 1]# vi passwd.expect
</span><span class='line'>#!/usr/bin/expect  
</span><span class='line'>
</span><span class='line'>## Usage $0 host username password
</span><span class='line'>
</span><span class='line'>set host [lrange $argv 0 0];
</span><span class='line'>set username [lrange $argv 1 1];
</span><span class='line'>set password [lrange $argv 2 2] ;
</span><span class='line'>
</span><span class='line'>set timeout 30;
</span><span class='line'>
</span><span class='line'>##
</span><span class='line'>
</span><span class='line'>spawn ssh $host useradd $username ;
</span><span class='line'>
</span><span class='line'>exec sleep 1;
</span><span class='line'>
</span><span class='line'>##
</span><span class='line'>
</span><span class='line'>spawn ssh $host passwd $username ;
</span><span class='line'>
</span><span class='line'>## password and repasswd all use this
</span><span class='line'>expect {
</span><span class='line'>  "password:" { send $password\n; exp_continue; }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>exec sleep 1;
</span><span class='line'>
</span><span class='line'>批量处理
</span><span class='line'>[root@hadoop-master2 1]# for h in `cat /etc/hosts | grep hadoop | awk '{print $2}' ` ; do ./passwd.expect $h hadoop hadoop-password ; done</span></code></pre></td></tr></table></div></figure>


<h2>最后</h2>

<p>当然还有很多命令，xargs, if等需要在实践中慢慢积累，shell博大精深继续码字！cdpath眼前一亮，alias还可以这么用！！</p>

<p>在linux把xml转成properties键值对形式的命令，觉得也挺有意思的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ vi format.xslt
</span><span class='line'>&lt;xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform"&gt;
</span><span class='line'>&lt;xsl:output method="text" encoding="iso-8859-1"/&gt;
</span><span class='line'>
</span><span class='line'>&lt;xsl:strip-space elements="*" /&gt;
</span><span class='line'>
</span><span class='line'>&lt;xsl:template match="/*/child::*"&gt;
</span><span class='line'>&lt;xsl:for-each select="child::*"&gt;
</span><span class='line'>&lt;xsl:if test="position() != last()"&gt;&lt;xsl:value-of select="normalize-space(.)"/&gt;=&lt;/xsl:if&gt;
</span><span class='line'>&lt;xsl:if test="position() = last()"&gt;&lt;xsl:value-of select="normalize-space(.)"/&gt; &lt;xsl:text&gt;&#xa;&lt;/xsl:text&gt; &lt;/xsl:if&gt;
</span><span class='line'>&lt;/xsl:for-each&gt;
</span><span class='line'>&lt;/xsl:template&gt;
</span><span class='line'>
</span><span class='line'>&lt;/xsl:stylesheet&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 ~]$ xsltproc format.xslt ~/hadoop-2.2.0/etc/hadoop/yarn-site.xml
</span><span class='line'>yarn.nodemanager.aux-services=mapreduce_shuffle
</span><span class='line'>yarn.nodemanager.aux-services.mapreduce.shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
</span><span class='line'>yarn.resourcemanager.address=hadoop-master1:8032
</span><span class='line'>yarn.resourcemanager.scheduler.address=hadoop-master1:8030
</span><span class='line'>yarn.resourcemanager.resource-tracker.address=hadoop-master1:8031
</span><span class='line'>yarn.resourcemanager.admin.address=hadoop-master1:8033
</span><span class='line'>yarn.resourcemanager.webapp.address=hadoop-master1:8088
</span><span class='line'>yarn.nodemanager.resource.memory-mb=51200=yarn-default.xml
</span><span class='line'>yarn.scheduler.minimum-allocation-mb=1024=yarn-default.xml</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Oozie Start Guide]]></title>
    <link href="http://winseliu.com/blog/2015/09/08/oozie-start-guide/"/>
    <updated>2015-09-08T11:15:14+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/08/oozie-start-guide</id>
    <content type="html"><![CDATA[<h2>步骤记录</h2>

<p>说明：cu2就是hadoop-master2</p>

<ol>
<li>打包</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi bin/mkdistro.sh 
</span><span class='line'>MVN_OPTS="-Dbuild.time=${DATETIME} -Dvc.revision=${VC_REV} -Dvc.url=${VC_URL} "
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/mkdistro.sh -DskipTests -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<ol>
<li>依赖</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>打包后，文件的位置
</span><span class='line'>[hadoop@cu2 ~]$ tar zxvf sources/oozie-4.2.0/distro/target/oozie-4.2.0-distro.tar.gz
</span><span class='line'>
</span><span class='line'>下载 &lt;http://dev.sencha.com/deploy/ext-2.2.zip&gt;
</span><span class='line'>
</span><span class='line'>yum install zip
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ mkdir libext
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ cd libext/
</span><span class='line'>[hadoop@cu2 libext]$ ll
</span><span class='line'>total 7584
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop 6800612 Sep  7 16:00 ext-2.2.zip
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop  960372 Feb 28  2015 mysql-connector-java-5.1.34.jar</span></code></pre></td></tr></table></div></figure>


<ol>
<li>安装</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/oozie-setup.sh prepare-war
</span><span class='line'>
</span><span class='line'>setup后，生成的war的位置：/home/hadoop/oozie-4.2.0/oozie-server/webapps/oozie.war</span></code></pre></td></tr></table></div></figure>


<ol>
<li>初始化数据库</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>创建数据库用户
</span><span class='line'>
</span><span class='line'>CREATE DATABASE oozie;
</span><span class='line'>GRANT ALL ON oozie.* TO 'oozie'@'%' IDENTIFIED BY 'oozie';
</span><span class='line'>FLUSH PRIVILEGES;
</span><span class='line'>GRANT ALL ON oozie.* TO 'oozie'@'localhost'  IDENTIFIED BY 'oozie';
</span><span class='line'>FLUSH PRIVILEGES;
</span><span class='line'>
</span><span class='line'>show grants for oozie;
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi conf/oozie-site.xml 
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;oozie.service.JPAService.jdbc.driver&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;oozie.service.JPAService.jdbc.url&lt;/name&gt;&lt;value&gt;jdbc:mysql://localhost:3306/oozie&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;oozie.service.JPAService.jdbc.username&lt;/name&gt;&lt;value&gt;oozie&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;oozie.service.JPAService.jdbc.password&lt;/name&gt;&lt;value&gt;oozie&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>这里直接把hadoop的jar添加到脚本中，不拷贝到libext下面
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi bin/ooziedb.sh
</span><span class='line'>OOZIECPPATH=""
</span><span class='line'>if [ ! -z ${HADOOP_HOME} ] ; then
</span><span class='line'>  OOZIECPPATH="${OOZIECPPATH}:$($HADOOP_HOME/bin/hadoop classpath)"
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>照着写就行了，不必考虑sql文件的存在与否
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/ooziedb.sh create -sqlfile oozie.sql -run
</span><span class='line'>  setting CATALINA_OPTS="$CATALINA_OPTS -Xmx1024m"
</span><span class='line'>
</span><span class='line'>Validate DB Connection
</span><span class='line'>DONE
</span><span class='line'>DB schema does not exist
</span><span class='line'>Check OOZIE_SYS table does not exist
</span><span class='line'>DONE
</span><span class='line'>Create SQL schema
</span><span class='line'>DONE
</span><span class='line'>Create OOZIE_SYS table
</span><span class='line'>DONE
</span><span class='line'>
</span><span class='line'>Oozie DB has been created for Oozie version '4.2.0'
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>The SQL commands have been written to: oozie.sql</span></code></pre></td></tr></table></div></figure>


<ol>
<li>启动服务</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>由于war中没有hadoop的jar，所以这里也需要把它们添加到tomcat
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ $HADOOP_HOME/bin/hadoop classpath | sed 's/:/,/g'
</span><span class='line'>/home/hadoop/hadoop-2.7.1/etc/hadoop,/home/hadoop/hadoop-2.7.1/share/hadoop/common/lib/*,/home/hadoop/hadoop-2.7.1/share/hadoop/common/*,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs/lib/*,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs/*,/home/hadoop/hadoop-2.7.1/share/hadoop/yarn/lib/*,/home/hadoop/hadoop-2.7.1/share/hadoop/yarn/*,/home/hadoop/hadoop-2.7.1/share/hadoop/mapreduce/lib/*,/home/hadoop/hadoop-2.7.1/share/hadoop/mapreduce/*,/home/hadoop/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
</span><span class='line'>
</span><span class='line'>处理下把*改成*.jar
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi oozie-server/conf/catalina.properties 
</span><span class='line'>common.loader=${catalina.base}/lib,${catalina.base}/lib/*.jar,${catalina.home}/lib,${catalina.home}/lib/*.jar,/home/hadoop/hadoop-2.7.1/etc/hadoop,/home/hadoop/hadoop-2.7.1/share/hadoop/common/lib/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/common/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs/lib/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/hdfs/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/yarn/lib/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/yarn/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/mapreduce/lib/*.jar,/home/hadoop/hadoop-2.7.1/share/hadoop/mapreduce/*.jar,/home/hadoop/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
</span><span class='line'>
</span><span class='line'># 前台运行 bin/oozied.sh run
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/oozied.sh start
</span><span class='line'>
</span><span class='line'>http://localhost:11000/</span></code></pre></td></tr></table></div></figure>


<ol>
<li>测试</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi bin/oozie
</span><span class='line'>OOZIECPPATH=""
</span><span class='line'>if [ ! -z ${HADOOP_HOME} ] ; then
</span><span class='line'>  OOZIECPPATH="${OOZIECPPATH}:$($HADOOP_HOME/bin/hadoop classpath)"
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/oozie admin -oozie http://localhost:11000/oozie -status
</span><span class='line'>System mode: NORMAL</span></code></pre></td></tr></table></div></figure>


<ol>
<li>跑个helloworld</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 oozie-4.2.0]$ tar zxvf oozie-sharelib-4.2.0.tar.gz 
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ ~/hadoop-2.7.1/bin/hadoop fs -rmr share
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ ~/hadoop-2.7.1/bin/hadoop fs -put share share
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ tar zxvf oozie-examples.tar.gz 
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ ~/hadoop-2.7.1/bin/hadoop fs -put examples examples
</span><span class='line'>
</span><span class='line'>修改share后重启下oozie，sharelib在应用中会缓冲，中间上传程序不能识别，会报`Could not locate Oozie sharelib`的错。
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ vi examples/apps/map-reduce/job.properties 
</span><span class='line'>nameNode=hdfs://hadoop-master2:9000
</span><span class='line'>jobTracker=hadoop-master2:8032
</span><span class='line'>queueName=default
</span><span class='line'>examplesRoot=examples
</span><span class='line'>
</span><span class='line'>oozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/apps/map-reduce/workflow.xml
</span><span class='line'>outputDir=map-reduce
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/oozie job -oozie http://localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run
</span><span class='line'>Error: E0501 : E0501: Could not perform authorization operation, User: hadoop is not allowed to impersonate hadoop
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.7.1]$ vi etc/hadoop/core-site.xml 
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 ~]$ for h in `cat /etc/hosts | grep slaver | awk '{print $2}' ` ; do rsync -vaz hadoop-2.7.1 $h:~/ --exclude=logs ; done
</span><span class='line'>
</span><span class='line'>同步重启集群
</span><span class='line'>
</span><span class='line'>注：增加以上配置后，无需重启集群，可以直接用hadoop管理员账号重新加载这两个属性值，命令为：
</span><span class='line'>    hdfs dfsadmin -refreshSuperUserGroupsConfiguration
</span><span class='line'>    yarn rmadmin -refreshSuperUserGroupsConfiguration
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 oozie-4.2.0]$ bin/oozie job -oozie http://localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run
</span><span class='line'>job: 0000000-150908082015741-oozie-hado-W
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.7.1]$ bin/hadoop fs -cat /user/hadoop/examples/output-data/map-reduce/part-00000
</span><span class='line'>
</span><span class='line'>尽管能看到结果了，但是不算任务执行成功。任务是有报错的`JA006: Call From cu2/192.168.0.214 to hadoop-master2:10020 failed on connection exception`
</span><span class='line'>
</span><span class='line'>[hadoop@cu2 hadoop-2.7.1]$ sbin/mr-jobhistory-daemon.sh start historyserver
</span><span class='line'>
</span><span class='line'>在运行一次就ok了。</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="https://oozie.apache.org/docs/4.2.0/DG_QuickStart.html">https://oozie.apache.org/docs/4.2.0/DG_QuickStart.html</a></li>
<li><a href="http://ju.outofmemory.cn/entry/65688">http://ju.outofmemory.cn/entry/65688</a></li>
<li><a href="http://stackoverflow.com/questions/30926357/oozie-on-yarn-oozie-is-not-allowed-to-impersonate-hadoop">http://stackoverflow.com/questions/30926357/oozie-on-yarn-oozie-is-not-allowed-to-impersonate-hadoop</a></li>
<li><a href="http://oozie.apache.org/docs/4.0.0/DG_QuickStart.html#Oozie_Share_Lib_Installation">http://oozie.apache.org/docs/4.0.0/DG_QuickStart.html#Oozie_Share_Lib_Installation</a></li>
<li><a href="https://oozie.apache.org/docs/4.2.0/DG_Examples.html">https://oozie.apache.org/docs/4.2.0/DG_Examples.html</a></li>
<li><p><a href="http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/ClusterSetup.html">http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/ClusterSetup.html</a></p></li>
<li><p><a href="http://blog.csdn.net/wngn123/article/details/41380013">http://blog.csdn.net/wngn123/article/details/41380013</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装http代理服务器squid]]></title>
    <link href="http://winseliu.com/blog/2015/09/06/squid-http-proxy-server-install/"/>
    <updated>2015-09-06T23:22:50+08:00</updated>
    <id>http://winseliu.com/blog/2015/09/06/squid-http-proxy-server-install</id>
    <content type="html"><![CDATA[<h2>环境说明</h2>

<ul>
<li>squid-3.3.14.tar.gz</li>
<li>centos6.6</li>
</ul>


<h2>安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install gcc gcc-c++
</span><span class='line'>cd squid-3.3.14
</span><span class='line'>./configure
</span><span class='line'>make
</span><span class='line'>make install
</span><span class='line'>
</span><span class='line'>cd /usr/local/squid
</span><span class='line'>#不修改会有权限的问题
</span><span class='line'>chmod 777 var/logs
</span><span class='line'>sbin/squid 
</span><span class='line'>sbin/squid -k shutdown</span></code></pre></td></tr></table></div></figure>


<h2>使用</h2>

<p>在浏览器中设置Http代理。端口为3128</p>

<h2>参考</h2>

<ul>
<li><a href="ftp://ftp.cuhk.edu.hk/pub/packages/info-systems/www/squid/">ftp://ftp.cuhk.edu.hk/pub/packages/info-systems/www/squid/</a></li>
<li><a href="http://www.educity.cn/linux/517165.html">http://www.educity.cn/linux/517165.html</a></li>
<li><a href="http://www.ajaxstu.com/Proxyfuwuqi/283731.html">http://www.ajaxstu.com/Proxyfuwuqi/283731.html</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_537b9caa010185xo.html">http://blog.sina.com.cn/s/blog_537b9caa010185xo.html</a></li>
<li><a href="http://blog.163.com/sword_111/blog/static/6658941620114163458435/">http://blog.163.com/sword_111/blog/static/6658941620114163458435/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[手动安装supervisor]]></title>
    <link href="http://winseliu.com/blog/2015/08/24/manual-install-supervisor/"/>
    <updated>2015-08-24T16:24:25+08:00</updated>
    <id>http://winseliu.com/blog/2015/08/24/manual-install-supervisor</id>
    <content type="html"><![CDATA[<h2>下载依赖以及安装包，并安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>python -V
</span><span class='line'>
</span><span class='line'>tar zxvf setuptools-18.2.tar.gz 
</span><span class='line'>cd setuptools-18.2
</span><span class='line'>python setup.py install
</span><span class='line'>
</span><span class='line'>tar zxvf meld3-0.6.5.tar.gz 
</span><span class='line'>cd meld3-0.6.5
</span><span class='line'>python setup.py install
</span><span class='line'>
</span><span class='line'>tar zxvf elementtree-1.2.6-20050316.tar.gz 
</span><span class='line'>cd elementtree-1.2.6-20050316
</span><span class='line'>python setup.py install
</span><span class='line'>
</span><span class='line'>tar zxvf supervisor-3.1.3.tar.gz 
</span><span class='line'>cd supervisor-3.1.3
</span><span class='line'>python setup.py  install</span></code></pre></td></tr></table></div></figure>


<h2>配置启动</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@master supervisor-3.1.3]# echo_supervisord_conf &gt;/etc/supervisord.conf
</span><span class='line'>[root@master supervisor-3.1.3]# supervisord
</span><span class='line'>
</span><span class='line'>[root@master supervisor-3.1.3]# ps aux | grep supervisor | grep -v grep
</span><span class='line'>root      6123  0.0  0.7 207292 13296 ?        Ss   08:15   0:00 /usr/bin/python /usr/bin/supervisord
</span><span class='line'>
</span><span class='line'>[root@master supervisor-3.1.3]# supervisorctl shutdown
</span><span class='line'>Shut down
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://supervisord.org/installing.html#installing-to-a-system-without-internet-access">http://supervisord.org/installing.html#installing-to-a-system-without-internet-access</a></li>
<li><a href="http://supervisord.org/configuration.html#programx-section">http://supervisord.org/configuration.html#programx-section</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logstash Elasticsearch Kibana日志采集查询系统搭建]]></title>
    <link href="http://winseliu.com/blog/2015/08/21/logstash-elasticsearch-kibana-startguide/"/>
    <updated>2015-08-21T14:42:30+08:00</updated>
    <id>http://winseliu.com/blog/2015/08/21/logstash-elasticsearch-kibana-startguide</id>
    <content type="html"><![CDATA[<h2>软件版本</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@master opt]# ll
</span><span class='line'>total 20
</span><span class='line'>drwxr-xr-x 7 root root 4096 Aug 21 01:23 elasticsearch-1.7.1
</span><span class='line'>drwxr-xr-x 8 uucp  143 4096 Mar 18  2014 jdk1.8.0_05
</span><span class='line'>drwxrwxr-x 7 1000 1000 4096 Aug 21 01:09 kibana-4.1.1-linux-x64
</span><span class='line'>drwxr-xr-x 5 root root 4096 Aug 21 05:58 logstash-1.5.3
</span><span class='line'>drwxrwxr-x 6 root root 4096 Aug 21 06:44 redis-3.0.3</span></code></pre></td></tr></table></div></figure>


<h2>安装运行脚本</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># java
</span><span class='line'>vi /etc/profile
</span><span class='line'>source /etc/profile
</span><span class='line'>
</span><span class='line'>cd /opt/elasticsearch-1.7.1
</span><span class='line'>bin/elasticsearch -p elasticsearch.pid -d
</span><span class='line'>
</span><span class='line'>curl localhost:9200/_cluster/nodes/172.17.0.4
</span><span class='line'>
</span><span class='line'>cd /opt/kibana-4.1.1-linux-x64/
</span><span class='line'>bin/kibana 
</span><span class='line'># http://master:5601
</span><span class='line'>
</span><span class='line'>cd /opt/redis-3.0.3
</span><span class='line'>yum install gcc
</span><span class='line'>yum install bzip2
</span><span class='line'>make MALLOC=jemalloc
</span><span class='line'>
</span><span class='line'># 也可以修改配置的daemon属性
</span><span class='line'>nohup src/redis-server & 
</span><span class='line'>
</span><span class='line'>cd /opt/logstash-1.5.3/
</span><span class='line'>bin/logstash -e 'input { stdin { } } output { stdout {} }'
</span><span class='line'>
</span><span class='line'>vi index.conf
</span><span class='line'>vi agent.conf
</span><span class='line'>
</span><span class='line'># agent可不加
</span><span class='line'>bin/logstash agent -f agent.conf &
</span><span class='line'>bin/logstash agent -f index.conf &</span></code></pre></td></tr></table></div></figure>


<h2>logstash配置</h2>

<p>由于程序都运行在一台机器(localhost)，redis、elasticsearch和kibana都使用默认配置。下面贴的是logstash的采集和过滤的配置：</p>

<p>(kibaba的配置config/kibana.yml, elasticsearch的配置config/elasticsearch.yml)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@master logstash-1.5.3]# cat agent.conf 
</span><span class='line'>input {
</span><span class='line'>  file {
</span><span class='line'>    path =&gt; "/var/log/yum.log"
</span><span class='line'>    start_position =&gt; beginning
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  redis {
</span><span class='line'>    key =&gt; "logstash.redis"
</span><span class='line'>    data_type =&gt; list
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  # 便于查看调试
</span><span class='line'>  stdout { }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@master logstash-1.5.3]# cat index.conf 
</span><span class='line'>input {
</span><span class='line'>  redis {
</span><span class='line'>    data_type =&gt; list
</span><span class='line'>    key =&gt; "logstash.redis"
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>  elasticsearch {
</span><span class='line'>    host =&gt; "localhost"
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>注意要改动下被采集的原始文件！！然后启动相应的程序，打开浏览器<a href="http://master:5601">http://master:5601</a>配置一下索引项，就可以查看了。</p>

<p>至于input/output/filter(map,reduce)怎么配置，查看官方文档<a href="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html">filter-plugins</a></p>

<h2>filter</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu1 logstash-1.5.3]$ bin/logstash -e "
</span><span class='line'>input {
</span><span class='line'>stdin {}
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>filter {
</span><span class='line'>grok { 
</span><span class='line'>match =&gt; {\"message\" =&gt; \"%{WORD:content}\"}
</span><span class='line'>add_field =&gt; { \"foo_%{content}\" =&gt; \"helloworld\" }
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>output {
</span><span class='line'>stdout { codec =&gt; json }
</span><span class='line'>}
</span><span class='line'>"
</span><span class='line'>
</span><span class='line'>abc
</span><span class='line'>{"message":"abc","@version":"1","@timestamp":"2015-09-10T08:02:52.024Z","host":"cu1","content":"abc","foo_abc":"helloworld"}</span></code></pre></td></tr></table></div></figure>


<p>grok-pattern文件的位置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu2 logstash-1.5.3]$ less ./vendor/bundle/jruby/1.9/gems/logstash-patterns-core-0.1.10/patterns/grok-patterns 
</span><span class='line'>
</span><span class='line'>2015-09-06 15:23:53,027 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
</span><span class='line'>%{TIMESTAMP_ISO8601:time} %{LOGLEVEL:loglevel} %{GREEDYDATA:content}
</span><span class='line'>
</span><span class='line'>[2015-09-10 08:00:46,539][INFO ][cluster.metadata         ] [Jumbo Carnation] [logstash-2015.09.10] update_mapping [hbase-logs] (dynamic)
</span><span class='line'>\[%{TIMESTAMP_ISO8601:time}\]\[%{LOGLEVEL:loglevel}%{SPACE}\]%{GREEDYDATA:content}</span></code></pre></td></tr></table></div></figure>


<h2>学习</h2>

<p>过滤DEBUG/INFO日志</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@cu1 logstash-1.5.3]$ bin/logstash -e "
</span><span class='line'> input {
</span><span class='line'> stdin {}
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> filter {
</span><span class='line'> grok {
</span><span class='line'> match =&gt; { \"message\" =&gt; \"%{TIMESTAMP_ISO8601:time} %{LOGLEVEL:loglevel} %{GREEDYDATA:content}\" }
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> if [loglevel] == \"INFO\" { drop {} }
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> output {
</span><span class='line'> stdout {}
</span><span class='line'> }
</span><span class='line'> 
</span><span class='line'> "</span></code></pre></td></tr></table></div></figure>


<p>用shell先预处理</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>input {
</span><span class='line'>    stdin {
</span><span class='line'>        type =&gt; "nginx"
</span><span class='line'>        format =&gt; "json_event"
</span><span class='line'>    }
</span><span class='line'>} 
</span><span class='line'>output {
</span><span class='line'>    amqp {
</span><span class='line'>        type =&gt; "nginx"
</span><span class='line'>        host =&gt; "10.10.10.10"
</span><span class='line'>        key  =&gt; "cdn"
</span><span class='line'>        name =&gt; "logstash"
</span><span class='line'>        exchange_type =&gt; "direct"
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>#!/bin/sh
</span><span class='line'>      tail -F /data/nginx/logs/access.json \
</span><span class='line'>    | sed 's/upstreamtime":-/upstreamtime":0/' \
</span><span class='line'>    | /usr/local/logstash/bin/logstash -f /usr/local/logstash/etc/agent.conf &</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.cnblogs.com/buzzlight/p/logstash_elasticsearch_kibana_log.html">http://www.cnblogs.com/buzzlight/p/logstash_elasticsearch_kibana_log.html</a></li>
<li><a href="http://www.cnblogs.com/ibook360/archive/2013/03/15/2961428.html">http://www.cnblogs.com/ibook360/archive/2013/03/15/2961428.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/index.html">https://www.elastic.co/guide/en/logstash/current/index.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/first-event.html">https://www.elastic.co/guide/en/logstash/current/first-event.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/working-with-plugins.html">https://www.elastic.co/guide/en/logstash/current/working-with-plugins.html</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html">https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html</a></li>
<li><p><a href="https://www.elastic.co/guide/en/logstash/current/codec-plugins.html">https://www.elastic.co/guide/en/logstash/current/codec-plugins.html</a></p></li>
<li><p><a href="http://blog.csdn.net/yeasy/article/details/45332493">http://blog.csdn.net/yeasy/article/details/45332493</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop不同版本yarn和hdfs混搭，spark-yarn环境配置]]></title>
    <link href="http://winseliu.com/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs/"/>
    <updated>2015-06-10T18:48:19+08:00</updated>
    <id>http://winseliu.com/blog/2015/06/10/hadoop-deploy-spark-diff-version-yarn-and-hdfs</id>
    <content type="html"><![CDATA[<p>hadoop分为存储和计算两个主要的功能，hdfs步入hadoop2后不论稳定性还是HA等等功能都比hadoop1要更吸引人。hadoop-2.2.0的hdfs已经比较稳定，但是yarn高版本有更加丰富的功能。本文主要关注spark-yarn下日志的查看，以及spark-yarn-dynamic的配置。</p>

<p>hadoop-2.2.0的hdfs原本已经在使用的环境，在这基础上搭建运行yarn-2.6.0，以及spark-1.3.0-bin-2.2.0。</p>

<ul>
<li>编译</li>
</ul>


<p>我是在虚拟机里面编译，共享了host主机的maven库。参考【VMware共享目录】，【VMware-Centos6 Build hadoop-2.6】注意<strong>cmake_symlink_library的异常，由于共享的windows目录下不能创建linux的软链接</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxvf ~/hadoop-2.6.0-src.tar.gz 
</span><span class='line'>cd hadoop-2.6.0-src/
</span><span class='line'>mvn package -Pdist,native -DskipTests -Dtar -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'># 由于hadoop-hdfs还是2.2的，这里编译spark需要用2.2版本！
</span><span class='line'># 如果用2.6会遇到[UnsatisfiedLinkError:org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray ](http://blog.csdn.net/zeng_84_long/article/details/44340441)
</span><span class='line'>cd spark-1.3.0
</span><span class='line'>export MAVEN_OPTS="-Xmx3g -XX:MaxPermSize=1g -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>mvn clean package -Phadoop-2.2 -Pyarn -Phive -Phive-thriftserver -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests
</span><span class='line'>
</span><span class='line'>vi make-distribution.sh #注释掉BUILD_COMMAND那一行，不重复执行package！
</span><span class='line'>./make-distribution.sh  --mvn `which mvn` --tgz  --skip-java-test -Phadoop-2.6 -Pyarn -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -DskipTests</span></code></pre></td></tr></table></div></figure>


<ul>
<li><p>配置注意点</p></li>
<li><p>core-site不要全部拷贝原来的，只要一些主要的配置即可。</p></li>
<li>yarn-site的<code>yarn.resourcemanager.webapp.address</code>需要填写具体的地址，不能写<code>0.0.0.0</code>。</li>
<li>yarn-site的<code>yarn.nodemanager.aux-services</code>添加spark_shuffle服务。<a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation</a></li>
<li>把hive-site的文件拷贝/链接到spark的conf目录下。</li>
<li>spark-yarn-dynamic配置: <a href="https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation">https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ cat conf/spark-defaults.conf 
</span><span class='line'># spark.master                     spark://bigdatamgr1:7077,bigdata8:7077
</span><span class='line'># spark.eventLog.enabled           true
</span><span class='line'># spark.eventLog.dir               hdfs://namenode:8021/directory
</span><span class='line'># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
</span><span class='line'># spark.executor.extraJavaOptions       -Xmx16g -Xms16g -Xmn256m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:ParallelGCThreads=10
</span><span class='line'>spark.driver.memory              48g
</span><span class='line'>spark.executor.memory            48g
</span><span class='line'>spark.sql.shuffle.partitions     200
</span><span class='line'>
</span><span class='line'>#spark.scheduler.mode FAIR
</span><span class='line'>spark.serializer  org.apache.spark.serializer.KryoSerializer
</span><span class='line'>spark.driver.maxResultSize 8g
</span><span class='line'>#spark.kryoserializer.buffer.max.mb 2048
</span><span class='line'>
</span><span class='line'>spark.dynamicAllocation.enabled true
</span><span class='line'>spark.dynamicAllocation.minExecutors 4
</span><span class='line'>spark.shuffle.service.enabled true
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 conf]$ cat spark-env.sh 
</span><span class='line'>#!/usr/bin/env bash
</span><span class='line'>
</span><span class='line'>JAVA_HOME=/home/eshore/jdk1.7.0_60
</span><span class='line'>
</span><span class='line'># log4j
</span><span class='line'>
</span><span class='line'>__add_to_classpath() {
</span><span class='line'>
</span><span class='line'>  root=$1
</span><span class='line'>
</span><span class='line'>  if [ -d "$root" ] ; then
</span><span class='line'>    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
</span><span class='line'>      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
</span><span class='line'>      else
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$f
</span><span class='line'>      fi
</span><span class='line'>    done
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'># this add tail of SPARK_CLASSPATH
</span><span class='line'>__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"
</span><span class='line'>
</span><span class='line'>#export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
</span><span class='line'>export HADOOP_CONF_DIR=/home/eshore/hadoop-2.6.0/etc/hadoop
</span><span class='line'>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'># HA
</span><span class='line'>SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark" 
</span><span class='line'>
</span><span class='line'>SPARK_PID_DIR=${SPARK_HOME}/pids
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>同步</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for h in `cat slaves ` ; do rsync -vaz hadoop-2.6.0 $h:~/ --delete --exclude=work --exclude=logs --exclude=metastore_db --exclude=data --exclude=pids ; done</span></code></pre></td></tr></table></div></figure>


<ul>
<li>启动spark-hive-thrift</li>
</ul>


<p>./sbin/start-thriftserver.sh &ndash;executor-memory 29g &ndash;master yarn-client</p>

<p>对于多任务的集群来说，配置自动动态分配（类似资源池）更有利于资源的使用。可以通过【All Applications】-【ApplicationMaster】-【Executors】来观察执行进程的变化。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tachyon剖析]]></title>
    <link href="http://winseliu.com/blog/2015/04/18/tachyon-deep-source/"/>
    <updated>2015-04-18T23:13:01+08:00</updated>
    <id>http://winseliu.com/blog/2015/04/18/tachyon-deep-source</id>
    <content type="html"><![CDATA[<p>要了解一个框架，一般都是从框架提供/开放的接口入手。先从最直接的方式下手，可以通过<code>tachyon tfs</code>和浏览器19999端口获取集群以及文件的相关信息。</p>

<ul>
<li>要了解tachyon首先就是其文件系统，可以从两个功能开始：命令行tachyon.command.TFsShell和web-servlet。</li>
<li>然后会深入tachyon.client包，了解<strong>TachyonFS</strong>和TachyonFile处理io的方式。以及tachyon.hadoop的包。</li>
<li>io处理：

<ul>
<li>写：BlockOutStream（#getLocalBlockTemporaryPath； MappedByteBuffer）、FileOutStream</li>
<li>读：RemoteBlockInStream、LocalBlockInStream</li>
</ul>
</li>
<li>了解thrift：MasterClient、MasterServiceHandler、WorkerClient、WorkerServiceHandler、ClientBlockInfo、ClientFileInfo。</li>
<li>看tachyon.example，巩固</li>
</ul>


<p>注：MappedByteBuffer在windows存在资源占用的bug！参见<a href="http://www.th7.cn/Program/java/2012/01/31/57401.shtml">http://www.th7.cn/Program/java/2012/01/31/57401.shtml</a>，</p>

<p>把整个io流理清之后，然后需要了解tachyon是怎么维护这些信息的：</p>

<ul>
<li>配置：WorkerConf、MasterConf、UserConf</li>
<li>了解thrift：MasterClient、MasterServiceHandler、ClientWorkerInfo、MasterInfo</li>
<li>TachyonMaster和TachyonWorker的启动</li>
<li>Checkpoint、Image、Journal</li>
<li>内存淘汰策略</li>
<li>DataServer在哪里用到（nio/netty）：TachyonFile#readRemoteByteBuffer、RemoteBlockInStream#read(byte[], int, int)</li>
<li>HA</li>
<li>Dependency（不知道怎么用）</li>
</ul>


<p>远程调试Worker以及tfs：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ cat conf/tachyon-env.sh 
</span><span class='line'>export TACHYON_WORKER_JAVA_OPTS="$TACHYON_JAVA_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070"
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ export TACHYON_JAVA_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8070"
</span><span class='line'>[hadoop@hadoop-master2 tachyon-0.6.1]$ bin/tachyon tfs lsr /</span></code></pre></td></tr></table></div></figure>


<h2>IO/client</h2>

<ul>
<li>TachyonFS是client与Master/Worker的纽带，请求<strong>文件系统和文件</strong>的元数据CRUD操作。其中的WorkerClient仅用于写（保存）文件。</li>
<li>TachyonFile是文件的抽象处理集合，可以获取文件的基本属性（元数据），同时提供了IO的接口用于文件内容的读写。</li>
<li>InStream读、获取文件内容

<ul>
<li>EmptyBlockInStream(文件包括的块为0）</li>
<li>BlockInStream(文件包括的块为1）

<ul>
<li>LocalBlockInStream 仅当是localworker且该块在本机时，通过MappedByteBuffer获取数据（数据在ramdisk也就是内存盘上哦）。</li>
<li>RemoteBlockInStream （通过nio从远程的worker#DataServer机器获取数据#retrieveByteBufferFromRemoteMachine，如果readtype设置为cache同时把数据缓冲到本地worker）</li>
</ul>
</li>
<li>FileInStream(文件包括的块为1+，可以理解为BlocksInStream。通过mCurrentPosition / mBlockCapacity来获取当前的blockindex最终还是调用BlockInStream）</li>
</ul>
</li>
<li>FileOutStream写，写数据入口就是只有FileOutStream

<ul>
<li>BlockOutStream（WriteType设置了需要缓冲，会写到本地localworker。<strong>由于需要进行分块，会复杂些#appendCurrentBuffer</strong>）</li>
<li>UnderFileSystem（如果WriteType设置了Through，则把数据写一份到underfs文件系统）</li>
</ul>
</li>
</ul>


<h2>Master</h2>

<p>TachyonMaster是master的启动类，所有的服务都是在这个类里面初始化启动的。</p>

<ul>
<li>HA：LeaderSelectorClient</li>
<li>EditLog：EditLogProcessor、Journal。

<ul>
<li>如果是HA模式，leader调用setup方法把EditLogProcessor停掉。也就是说在HA模式下，standby才会运行EditLogProcessor实时处理editlog。</li>
<li>leader和非HA master则仅在启动时通过调用MasterInfo#init处理editlog一次。</li>
</ul>
</li>
<li>Thrift: TServer、MasterServiceHandler；与MasterClient对应的服务端。</li>
<li>Web: UIWebServer，使用jetty的内嵌服务器。</li>
<li>MasterInfo</li>
</ul>


<h2>Worker</h2>

<h2>Thrift</h2>

<h2>HA</h2>

<p>当配置<code>tachyon.usezookeeper</code>设置为true时，启动master时会初始化LeaderSelectorClient对象。使用curator连接到zookeeper服务器进行leader的选举。</p>

<p><strong>LeaderSelectorClient</strong>实现了LeaderSelectorListener接口，创建LeaderSelector并传入当前实例作为监听实例，当选举完成后，被选leader触发takeLeadership事件。</p>

<blockquote><p>public void takeLeadership(CuratorFramework client) throws Exception
Called when your instance has been granted leadership. This method should not return until you wish to release leadership</p></blockquote>

<p>takeLeadership方法中把<code>mIsLeader</code>设置为true（master自己判断）、创建<code>mLeaderFolder + mName</code>路径（客户端获取master leader）；然后隔5s的死循环（运行在LeaderSelector单独的线程池）。</p>

<h2>Checkpoint</h2>

<h2>Journal</h2>

<hr />

<h2>问题</h2>

<ul>
<li>程序没有返回内容，没有响应</li>
</ul>


<p>tfs 默认是CACHE_THROUGH，会缓冲同时写ufs。如果改成must则只写cache，然后清理内存，再获取数据，一直没有内容返回，不知道为什么？</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ export TACHYON_JAVA_OPTS="-Dtachyon.user.file.writetype.default=MUST_CACHE "
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs copyFromLocal LICENSE /LICENSE2
</span><span class='line'>Copied LICENSE to /LICENSE2
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs free /LICENSE2
</span><span class='line'>/LICENSE2 was successfully freed from memory.
</span><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs cat /LICENSE2</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tachyon入门指南]]></title>
    <link href="http://winseliu.com/blog/2015/04/15/tachyon-quickstart/"/>
    <updated>2015-04-15T22:56:09+08:00</updated>
    <id>http://winseliu.com/blog/2015/04/15/tachyon-quickstart</id>
    <content type="html"><![CDATA[<p>tachyon程序是在HDFS与程序之间缓冲，相当于CPU与磁盘设备之间内存的功能。tachyon提供了TachyonFS、TachyonFile等API使操作起来更像一个文件系统；同时实现了HDFS的FileSystem接口，方便原有程序的迁移，只要把url的模式（schema）hdfs改成tachyon。</p>

<p>tachyon和HDFS一样也是master-slaver（worker）结构：master保存元数据，worker节点使用内存盘缓冲数据。</p>

<h2>部署集群</h2>

<p>下载tachyon的编译文件后，按下面的步骤部署：</p>

<ul>
<li>解压</li>
<li>修改conf/tachyon-env.sh（JAVA_HOME，TACHYON_UNDERFS_ADDRESS，TACHYON_MASTER_ADDRESS）</li>
<li>修改conf/worker</li>
<li>同步代码到workers子节点</li>
<li>格式化tachyon（建立master和worker所需的各种目录）</li>
<li>挂载内存盘</li>
<li>启动集群</li>
<li>通过19999端口访问</li>
</ul>


<p>如果hadoop集群的版本不是最新的2.6.0，需要手工编译源码：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mvn clean package assembly:single -Dhadoop.version=2.2.0 -DskipTests -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<p>同步程序的脚本如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do  rsync -vaz tachyon-0.6.1 $h:~/ --exclude=logs --exclude=underfs --exclude=journal ; done</span></code></pre></td></tr></table></div></figure>


<p>用tachyon用户格式化：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/tachyon format</span></code></pre></td></tr></table></div></figure>


<p>使用root挂载内存盘：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/tachyon-mount.sh Mount workers
</span><span class='line'>for h in `cat slaves ` ; do  ssh $h "chmod 777 /mnt/ramdisk; chmod 777 /mnt/tachyon_default_home"  ; done</span></code></pre></td></tr></table></div></figure>


<p>确认下worker节点是否有underfs/tmp/tachyon/data，如果没有手动创建下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do ssh $h mkdir -p ~/tachyon-0.6.1/underfs/tmp/tachyon/data ; done</span></code></pre></td></tr></table></div></figure>


<p>启动集群：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 tachyon-0.6.1]$ bin/tachyon-start.sh all NoMount</span></code></pre></td></tr></table></div></figure>


<p>上传文件到tachyon：（注意，这里是在worker节点！）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 tachyon-0.6.1]$ bin/tachyon tfs copyFromLocal README.md /
</span><span class='line'>Copied README.md to /</span></code></pre></td></tr></table></div></figure>


<h2>集成到Spark</h2>

<p>注意，这里是在worker节点，使用local本地集群的方式（spark集群资源全部被spark-sql占用了，导致提交的任务分配不到资源！）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdata1 spark-1.3.0-bin-2.2.0]$ export SPARK_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar 
</span><span class='line'>[eshore@bigdata1 spark-1.3.0-bin-2.2.0]$ bin/spark-shell --master local[1] -Dspark.ui.port=4041
</span><span class='line'>scala&gt; val s = sc.textFile("tachyon://bigdatamgr1:19998/README.md")
</span><span class='line'>s: org.apache.spark.rdd.RDD[String] = tachyon://bigdatamgr1:19998/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:21
</span><span class='line'>
</span><span class='line'>scala&gt; s.count()
</span><span class='line'>15/04/03 11:13:09 WARN : tachyon.home is not set. Using /mnt/tachyon_default_home as the default value.
</span><span class='line'>res0: Long = 45
</span><span class='line'>
</span><span class='line'>scala&gt; val wordCounts = s.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
</span><span class='line'>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:23
</span><span class='line'>
</span><span class='line'>scala&gt; wordCounts.saveAsTextFile("tachyon://bigdatamgr1:19998/wordcount-README")
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 tachyon-0.6.1]$ bin/tachyon tfs ls /wordcount-README/
</span><span class='line'>1407.00 B 04-03-2015 11:16:05:483  In Memory      /wordcount-README/part-00000
</span><span class='line'>0.00 B    04-03-2015 11:16:05:787  In Memory      /wordcount-README/_SUCCESS</span></code></pre></td></tr></table></div></figure>


<p>为啥要在worker节点运行呢？不能在master节点运行？运行肯定是可以的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ export SPARK_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ bin/spark-shell --master local[1] --jars /home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>
</span><span class='line'>scala&gt; val s = sc.textFile("tachyon://bigdatamgr1:19998/NOTICE")
</span><span class='line'>s: org.apache.spark.rdd.RDD[String] = tachyon://bigdatamgr1:19998/NOTICE MapPartitionsRDD[1] at textFile at &lt;console&gt;:15
</span><span class='line'>
</span><span class='line'>scala&gt; s.count()
</span><span class='line'>15/04/13 16:05:45 WARN BlockReaderLocal: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
</span><span class='line'>15/04/13 16:05:45 WARN : tachyon.home is not set. Using /mnt/tachyon_default_home as the default value.
</span><span class='line'>java.io.IOException: The machine does not have any local worker.
</span><span class='line'>        at tachyon.client.BlockOutStream.&lt;init&gt;(BlockOutStream.java:94)
</span><span class='line'>        at tachyon.client.BlockOutStream.&lt;init&gt;(BlockOutStream.java:65)
</span><span class='line'>        at tachyon.client.RemoteBlockInStream.read(RemoteBlockInStream.java:204)
</span><span class='line'>        at tachyon.hadoop.HdfsFileInputStream.read(HdfsFileInputStream.java:142)
</span><span class='line'>        at java.io.DataInputStream.read(DataInputStream.java:100)
</span><span class='line'>        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)
</span><span class='line'>        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
</span><span class='line'>        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)
</span><span class='line'>        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)
</span><span class='line'>        at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:245)
</span><span class='line'>        at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:212)
</span><span class='line'>        at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
</span><span class='line'>        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
</span><span class='line'>        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
</span><span class='line'>        at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1466)
</span><span class='line'>        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
</span><span class='line'>        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
</span><span class='line'>        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
</span><span class='line'>        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
</span><span class='line'>        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
</span><span class='line'>        at org.apache.spark.scheduler.Task.run(Task.scala:64)
</span><span class='line'>        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
</span><span class='line'>        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</span><span class='line'>        at java.lang.Thread.run(Thread.java:745)
</span><span class='line'>res0: Long = 2</span></code></pre></td></tr></table></div></figure>


<p>两个点：</p>

<ul>
<li>这里是运行的spark local集群；</li>
<li>运行当然没有问题，但是会打印不和谐的<strong>The machine does not have any local worker</strong>警告日志。这与FileSystem的获取输入流<code>ReadType.CACHE</code>实现有关（见源码HdfsFileInputStream）。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mTachyonFileInputStream = mTachyonFile.getInStream(ReadType.CACHE);</span></code></pre></td></tr></table></div></figure>


<p>如果master为spark集群，spark-driver不管运行在哪台集群都没有问题。因为，此时运行任务的spark-worker就是tachyon-worker节点啊，当然就有local worker了。</p>

<p>为了更深入的了解，还可以试验一下<code>ReadType.CACHE</code>的作用：原本不在内存的数据，计算后就会被载入到缓冲（内存）！！</p>

<p>可以再试一次，先从内存中删掉（此处underfs配置存储在HDFS）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs free /NOTICE
</span><span class='line'>/NOTICE was successfully freed from memory.
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs fileinfo /NOTICE
</span><span class='line'>/NOTICE with file id 2 has the following blocks: 
</span><span class='line'>ClientBlockInfo(blockId:2147483648, offset:0, length:62, locations:[NetAddress(mHost:bigdata8, mPort:-1, mSecondaryPort:-1), NetAddress(bigdata6, mPort:-1, mSecondaryPort:-1), NetAddress(mHost:bigdata5, mPort:-1, mSecondaryPort:-1)])</span></code></pre></td></tr></table></div></figure>


<p>再次运行count：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; s.count()
</span><span class='line'>res1: Long = 2</span></code></pre></td></tr></table></div></figure>


<p>再次查看文件状态：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 spark-1.3.0-bin-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs fileinfo /NOTICE
</span><span class='line'>/NOTICE with file id 2 has the following blocks: 
</span><span class='line'>ClientBlockInfo(blockId:2147483648, offset:0, length:62, locations:[NetAddress(mHost:bigdata1, mPort:29998, mSecondaryPort:29999)])</span></code></pre></td></tr></table></div></figure>


<p>此时文件对应的block所在机器变成了bigdata1，也就是spark-worker运行的节点（这里用local，worker和driver都在bigdata1上）。</p>

<p>参考</p>

<ul>
<li><a href="http://tachyon-project.org/Running-Tachyon-on-a-Cluster.html">http://tachyon-project.org/Running-Tachyon-on-a-Cluster.html</a></li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html">http://spark.apache.org/docs/latest/configuration.html</a></li>
<li><a href="http://tachyon-project.org/Running-Spark-on-Tachyon.html">http://tachyon-project.org/Running-Spark-on-Tachyon.html</a></li>
</ul>


<h2>集成到Hadoop集群</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ export HADOOP_CLASSPATH=/home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount -libjars /home/eshore/tachyon-0.6.1/core/target/tachyon-0.6.1-jar-with-dependencies.jar tachyon://bigdatamgr1:19998/NOTICE tachyon://bigdatamgr1:19998/NOTICE-wordcount
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ ~/tachyon-0.6.1/bin/tachyon tfs cat /NOTICE-wordcount/part-r-00000
</span><span class='line'>2012-2014       1
</span><span class='line'>Berkeley        1
</span><span class='line'>California,     1
</span><span class='line'>Copyright       1
</span><span class='line'>Tachyon 1
</span><span class='line'>University      1
</span><span class='line'>of      1</span></code></pre></td></tr></table></div></figure>


<h2>后记</h2>

<p>当前apache开源大部分集群的部署都是同一种模式，源码也基本都是用maven来进行构建。部署其实没有什么难度，如果是应用到spark、hadoop这样的平台，其实只要部署，然后用FileSystem的接口就一切ok了。但是要了解其原理，官网的文档也不是很全，那得需要深入源码。</p>

<p>入门写到这里，差不多了，下一篇从TachyonFS角度解析tachyon。</p>

<h2>附录</h2>

<ul>
<li>spark-env.sh</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>JAVA_HOME=/home/eshore/jdk1.7.0_60
</span><span class='line'>
</span><span class='line'># log4j
</span><span class='line'>
</span><span class='line'>__add_to_classpath() {
</span><span class='line'>
</span><span class='line'>  root=$1
</span><span class='line'>
</span><span class='line'>  if [ -d "$root" ] ; then
</span><span class='line'>    for f in `ls $root/*.jar | grep -v -E '/hive.*.jar'`  ; do
</span><span class='line'>      if [ -n "$SPARK_DIST_CLASSPATH" ] ; then
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:$f
</span><span class='line'>      else
</span><span class='line'>        export SPARK_DIST_CLASSPATH=$f
</span><span class='line'>      fi
</span><span class='line'>    done
</span><span class='line'>  fi
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>__add_to_classpath "/home/eshore/tez-0.4.0-incubating"
</span><span class='line'>__add_to_classpath "/home/eshore/tez-0.4.0-incubating/lib"
</span><span class='line'>__add_to_classpath "/home/eshore/apache-hive-0.13.1/lib"
</span><span class='line'>
</span><span class='line'>export HADOOP_CONF_DIR=/data/opt/ibm/biginsights/hadoop-2.2.0/etc/hadoop
</span><span class='line'>export SPARK_CLASSPATH=$SPARK_CLASSPATH:/home/eshore/spark-1.3.0-bin-2.2.0/conf:$HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'># HA
</span><span class='line'>SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bi-00-01.bi.domain.com:2181 -Dspark.deploy.zookeeper.dir=/spark"
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ for h in `cat slaves ` ; do rsync -vaz spark-1.3.0-bin-2.2.0 $h:~/ --exclude=logs --exclude=metastore_db --exclude=work --delete ; done</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用RamDisk来优化系统]]></title>
    <link href="http://winseliu.com/blog/2015/04/12/optimize-system-ramdisk/"/>
    <updated>2015-04-12T16:56:09+08:00</updated>
    <id>http://winseliu.com/blog/2015/04/12/optimize-system-ramdisk</id>
    <content type="html"><![CDATA[<p>最近加了一条8G的内存，总共16G。暂时来说很难全部用起来。如果能够实现类似linux的shm分区的话，那就完美了，把临时的数据全部放到这个内存分区中。最好是免费的，通过一阵折腾搜索，整理如下：</p>

<p>去到官网<a href="http://www.ltr-data.se/opencode.html/#ImDisk">http://www.ltr-data.se/opencode.html/#ImDisk</a>直接下载<code>ImDisk Toolkit</code><a href="http://reboot.pro/files/file/284-imdisk-toolkit/">http://reboot.pro/files/file/284-imdisk-toolkit/</a>，toolkit里面已经集成了ImDisk软件。（新版本的toolkit可以节省很多事情，参考最后的两个链接看看即可）</p>

<p>配置：填写大小<code>5</code>、盘符<code>S</code>、磁盘格式<code>NTFS</code>，然后点击【确定】格式化磁盘，然后就可以使用了。</p>

<p><img src="http://winseliu.com/images/blogs/ramdisk-config.png" alt="" /></p>

<p>把临时的文件目录指定到ramdisk，重启系统。</p>

<p><img src="http://winseliu.com/images/blogs/ramdisk-temp.png" alt="" /></p>

<p>上面仅仅是把用户和系统的临时目录移到<strong>内存盘</strong>中。由于rar，java一些软件都是用用户的临时目录，已经可以体验到加速的快感了！！直接拖拽解压rar情况下速度明显快了很多。</p>

<p>还有一个问题，重启后，内存盘的数据会被全部清掉。默认情况下只建立了Temp目录，没有我们指定的Cache目录。Chrome启动的时刻如果发现Cache目录为不可用状态会重建该目录。</p>

<p>在Advanced页签，<strong>Load Content from Image File or Folder</strong>选项可以选择初始化加载的内容。我们只要先把目录结构建立后，然后在初始化后加载该路径一切都解决了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\local\home\RamDiskInit&gt;find .
</span><span class='line'>.
</span><span class='line'>./Temp
</span><span class='line'>./Temp/Chrome
</span><span class='line'>./Temp/Chrome/Cache</span></code></pre></td></tr></table></div></figure>


<p>然后在<code>RamDisk Config</code>的Advanced页签选择<strong>E:\local\home\RamDiskInit</strong>作为<strong>Load Content</strong>即可。</p>

<h2>参考</h2>

<ul>
<li><a href="http://zohead.com/archives/rsync-performance-linux-cygwin-msys/">http://zohead.com/archives/rsync-performance-linux-cygwin-msys/</a> 从这里看到ramdisk-imdisk</li>
<li><a href="http://www.appinn.com/imdisk/">http://www.appinn.com/imdisk/</a> 安装简单使用，以及两篇核心文章的链接</li>
<li><a href="http://www.ltr-data.se/opencode.html/#ImDisk">http://www.ltr-data.se/opencode.html/#ImDisk</a></li>
<li><a href="http://www.kenming.idv.tw/super_lighweight_ramdisk_imdisk_setup#more-1995">超小巧效能强悍的穷人版 Ramdisk－ImDisk (设定篇) </a></li>
<li><a href="http://www.mobile01.com/topicdetail.php?f=300&amp;t=2200352">Win7 x64 下使用 ImDisk 当作RamDisk的小小心得与改良方法</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[已有HDFS上部署yarn]]></title>
    <link href="http://winseliu.com/blog/2015/03/25/deploy-separated-yarn-on-exists-hdfs-cluster/"/>
    <updated>2015-03-25T21:22:59+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/25/deploy-separated-yarn-on-exists-hdfs-cluster</id>
    <content type="html"><![CDATA[<h2>原有环境</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[biadmin@bigdatamgr1 IHC]$ pwd
</span><span class='line'>/data/opt/ibm/biginsights/IHC
</span><span class='line'>
</span><span class='line'>[biadmin@bigdatamgr1 biginsights]$ ll conf/ hadoop-conf
</span><span class='line'>conf/:
</span><span class='line'>total 64
</span><span class='line'>-rwxr-xr-x 1 biadmin biadmin  2886 Jan 30 15:09 biginsights-env.sh
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>hadoop-conf:
</span><span class='line'>total 108
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  7698 Mar 12 17:57 capacity-scheduler.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   535 Mar 12 17:57 configuration.xsl
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   872 Mar 12 17:57 console-site.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  3744 Mar 24 16:51 core-site.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   569 Mar 12 17:57 fair-scheduler.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   410 Mar 12 17:57 flex-scheduler.xml
</span><span class='line'>-rwxrwxr-x 1 biadmin biadmin  5027 Mar 12 17:57 hadoop-env.sh
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  1859 Mar 12 17:57 hadoop-metrics2.properties
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  4886 Mar 12 17:57 hadoop-policy.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  3836 Mar 12 17:57 hdfs-site.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  2678 Mar 12 17:57 ibm-hadoop.properties
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   207 Mar 12 17:57 includes
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin 10902 Mar 12 17:57 log4j.properties
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   610 Mar 12 17:57 mapred-queue-acls.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  6951 Mar 23 17:24 mapred-site.xml
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin    44 Mar 12 17:57 masters
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   207 Mar 12 17:57 slaves
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  1243 Mar 12 17:57 ssl-client.xml.example
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin  1195 Mar 12 17:57 ssl-server.xml.example
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   301 Mar 12 17:57 taskcontroller.cfg
</span><span class='line'>-rw-rw-r-- 1 biadmin biadmin   172 Mar 12 17:57 zk-jaas.conf
</span><span class='line'>
</span><span class='line'>[root@bigdatamgr1 ~]# cat /etc/profile
</span><span class='line'>...
</span><span class='line'>for i in /etc/profile.d/*.sh ; do
</span><span class='line'>    if [ -r "$i" ]; then
</span><span class='line'>        if [ "${-#*i}" != "$-" ]; then
</span><span class='line'>            . "$i"
</span><span class='line'>        else
</span><span class='line'>            . "$i" &gt;/dev/null 2&gt;&1
</span><span class='line'>        fi
</span><span class='line'>    fi
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[root@bigdatamgr1 ~]# ll /etc/profile.d/
</span><span class='line'>total 60
</span><span class='line'>lrwxrwxrwx  1 root root   49 Jan 30 15:10 biginsights-env.sh -&gt; /data/opt/ibm/biginsights/conf/biginsights-env.sh
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[biadmin@bigdatamgr1 biginsights]$ cat hadoop-conf/hadoop-env.sh
</span><span class='line'>...
</span><span class='line'># include biginsights-env.sh
</span><span class='line'>if [ -r "/data/opt/ibm/biginsights/hdm/../conf/biginsights-env.sh" ]; then
</span><span class='line'>        source "/data/opt/ibm/biginsights/hdm/../conf/biginsights-env.sh"
</span><span class='line'>fi
</span><span class='line'>...
</span><span class='line'>export HADOOP_LOG_DIR=/data/var/ibm/biginsights/hadoop/logs
</span><span class='line'>...
</span><span class='line'>export HADOOP_PID_DIR=/data/var/ibm/biginsights/hadoop/pids
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>hdfs用的是2.x的，但是mr是1.x。真心坑爹！！</p>

<h2>单独部署新的yarn</h2>

<p>由于biginsights整了一套的环境变量，在加载profile的时刻就会进行初始化。所以需要搞一个<strong>新的用户</strong>在加载用户的环境变量的时刻把这些值清理掉。同时也为了与原来的有所区分。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 ~]$ cat .bash_profile 
</span><span class='line'>...
</span><span class='line'>for i in ~/conf/*.sh ; do
</span><span class='line'>  if [ -r "$i" ] ; then
</span><span class='line'>    . "$i"
</span><span class='line'>  fi
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 ~]$ ll conf/
</span><span class='line'>total 4
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 292 Mar 24 20:48 reset-biginsights-env.sh</span></code></pre></td></tr></table></div></figure>


<p>使用biadmin停掉原来的jobtracker-tasktracker。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[biadmin@bigdatamgr1 IHC]$ ssh `hdfs getconf -confKey mapreduce.jobtracker.address | sed 's/:.*//' ` "sudo -u mapred /data/opt/ibm/biginsights/IHC/sbin/hadoop-daemon.sh  stop jobtracker"
</span><span class='line'>
</span><span class='line'>[biadmin@bigdatamgr1 biginsights]$ for h in `cat hadoop-conf/slaves ` ; do ssh $h "sudo -u mapred /data/opt/ibm/biginsights/IHC/sbin/hadoop-daemon.sh  stop tasktracker" ; done
</span></code></pre></td></tr></table></div></figure>


<p>这里使用while不行，不知道为啥!?</p>

<p>部署新的hadoop-2.2.0。使用超级管理员新建目录给eshore用户：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>usermod -g biadmin eshore
</span><span class='line'>mkdir /data/opt/ibm/biginsights/hadoop-2.2.0
</span><span class='line'>chown eshore:biadmin hadoop-2.2.0</span></code></pre></td></tr></table></div></figure>


<p>使用超级管理员同步到各个slaver节点：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@bigdatamgr1 biginsights]# for line in `cat hadoop-conf/slaves` ; do ssh $line "usermod -g biadmin eshore" ; done
</span><span class='line'>
</span><span class='line'>[root@bigdatamgr1 biginsights]# cat hadoop-conf/slaves | while read line ; do rsync -vazXog hadoop-2.2.0 $line:/data/opt/ibm/biginsights/ ; done
</span><span class='line'>
</span><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ cd etc/hadoop/
</span><span class='line'>[eshore@bigdatamgr1 hadoop]$ ll
</span><span class='line'>total 116
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 3560 Feb 15  2014 capacity-scheduler.xml
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 1335 Feb 15  2014 configuration.xsl
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  318 Feb 15  2014 container-executor.cfg
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  713 Mar 24 23:31 core-site.xml
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 3614 Mar 24 22:45 hadoop-env.sh
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 1774 Feb 15  2014 hadoop-metrics2.properties
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 2490 Feb 15  2014 hadoop-metrics.properties
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 9257 Feb 15  2014 hadoop-policy.xml
</span><span class='line'>lrwxrwxrwx 1 eshore biadmin   51 Mar 24 21:33 hdfs-site.xml -&gt; /data/opt/ibm/biginsights/hadoop-conf/hdfs-site.xml
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 1180 Feb 15  2014 httpfs-env.sh
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 1657 Feb 15  2014 httpfs-log4j.properties
</span><span class='line'>-rw-r--r-- 1 eshore biadmin   21 Feb 15  2014 httpfs-signature.secret
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  620 Feb 15  2014 httpfs-site.xml
</span><span class='line'>-rw-rw-r-- 1 eshore biadmin   75 Feb 15  2014 journalnodes
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 9116 Feb 15  2014 log4j.properties
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 1383 Feb 15  2014 mapred-env.sh
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 4113 Feb 15  2014 mapred-queues.xml.template
</span><span class='line'>-rw-rw-r-- 1 eshore biadmin 1508 Mar 24 21:42 mapred-site.xml
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  758 Feb 15  2014 mapred-site.xml.template
</span><span class='line'>lrwxrwxrwx 1 eshore biadmin   44 Mar 24 21:34 slaves -&gt; /data/opt/ibm/biginsights/hadoop-conf/slaves
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 2316 Feb 15  2014 ssl-client.xml.example
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 2251 Feb 15  2014 ssl-server.xml.example
</span><span class='line'>lrwxrwxrwx 1 eshore biadmin   16 Mar 25 16:10 tez-site.xml -&gt; tez-site.xml-0.4
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  282 Mar 25 15:37 tez-site.xml-0.4
</span><span class='line'>-rw-r--r-- 1 eshore biadmin  347 Mar 25 15:49 tez-site.xml-0.6
</span><span class='line'>-rwxr-xr-x 1 eshore biadmin 4039 Mar 24 22:26 yarn-env.sh
</span><span class='line'>-rw-r--r-- 1 eshore biadmin 1826 Mar 24 21:42 yarn-site.xml</span></code></pre></td></tr></table></div></figure>


<p>把属性配置好（hdfs，slaves<strong>可以用原来</strong>的就建立一个软链即可），然后用sbin/start-yarn.sh启动即可。</p>

<h2>其他命令</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[eshore@bigdatamgr1 hadoop-2.2.0]$ for line in `cat etc/hadoop/slaves` ; do echo "================$line" ; ssh $line "top -u eshore -n 1 -b | grep java | xargs -I{}  kill {} "   ; done</span></code></pre></td></tr></table></div></figure>


<h2>部署值得鉴戒学习的IBM bigsql套件：</h2>

<ul>
<li>一个管理用户部署，各个引用使用各自的用户</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@bigdatamgr1 ~]# cat /etc/sudoers
</span><span class='line'>biadmin ALL=(ALL)   NOPASSWD: ALL
</span><span class='line'>
</span><span class='line'>[root@bigdatamgr1 ~]# cat /etc/passwd
</span><span class='line'>biadmin:x:200:501::/home/biadmin:/bin/bash
</span><span class='line'>avahi-autoipd:x:170:170:Avahi IPv4LL Stack:/var/lib/avahi-autoipd:/sbin/nologin
</span><span class='line'>hive:x:205:501::/home/hive:/bin/bash
</span><span class='line'>oozie:x:206:501::/home/oozie:/bin/bash
</span><span class='line'>monitoring:x:220:501::/home/monitoring:/bin/bash
</span><span class='line'>alert:x:225:501::/home/alert:/bin/bash
</span><span class='line'>catalog:x:224:501::/home/catalog:/bin/bash
</span><span class='line'>hdfs:x:201:501::/home/hdfs:/bin/bash
</span><span class='line'>httpfs:x:221:501::/home/httpfs:/bin/bash
</span><span class='line'>bigsql:x:222:501::/home/bigsql:/bin/bash
</span><span class='line'>console:x:223:501::/home/console:/bin/bash
</span><span class='line'>mapred:x:202:501::/home/mapred:/bin/bash
</span><span class='line'>orchestrator:x:226:501::/home/orchestrator:/bin/bash
</span><span class='line'>hbase:x:204:501::/home/hbase:/bin/bash
</span><span class='line'>zookeeper:x:203:501::/home/zookeeper:/bin/bash</span></code></pre></td></tr></table></div></figure>


<p>启用时管理员用户使用<code>sudo -u XXX COMMAND</code>操作。</p>

<ul>
<li>所有应用部署/启动管理</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[biadmin@bigdatamgr1 biginsights]$ bin/start.sh -h
</span><span class='line'>Usage: start.sh &lt;component&gt;...
</span><span class='line'>    Start one or more BigInsights components. Start all components if 'all' is
</span><span class='line'>    specified. If a component is already started, this command does nothing to it.
</span><span class='line'>    
</span><span class='line'>    For example:
</span><span class='line'>        start.sh all
</span><span class='line'>          - Starts all components.
</span><span class='line'>        start.sh hadoop zookeeper
</span><span class='line'>          - Starts hadoop and zookeeper daemons.
</span><span class='line'>
</span><span class='line'>OPTIONS:
</span><span class='line'>    -ex=&lt;component&gt;
</span><span class='line'>        Exclude a component, often used together with 'all'. I.e. 
</span><span class='line'>        `stop.sh all -ex=console` stops all components but the mgmt console.
</span><span class='line'>
</span><span class='line'>    -h, --help
</span><span class='line'>        Get help information.</span></code></pre></td></tr></table></div></figure>


<ul>
<li>反复依赖的包，通过软链来管理</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[biadmin@bigdatamgr1 lib]$ ll
</span><span class='line'>total 50336
</span><span class='line'>-rw-r--r-- 1 biadmin biadmin   303042 Jan 30 15:22 avro-1.7.4.jar
</span><span class='line'>lrwxrwxrwx 1 biadmin biadmin       60 Jan 30 15:22 biginsights-gpfs-2.2.0.jar -&gt; /data/opt/ibm/biginsights/IHC/lib/biginsights-gpfs-2.2.0.jar
</span><span class='line'>-rw-r--r-- 1 biadmin biadmin    15322 Jan 30 15:22 findbugs-annotations-1.3.9-1.jar
</span><span class='line'>lrwxrwxrwx 1 biadmin biadmin       48 Jan 30 15:22 guardium-proxy.jar -&gt; /data/opt/ibm/biginsights/lib/guardium-proxy.jar
</span><span class='line'>-rw-r--r-- 1 biadmin biadmin  1795932 Jan 30 15:22 guava-12.0.1.jar
</span><span class='line'>-rw-r--r-- 1 biadmin biadmin   710492 Jan 30 15:22 guice-3.0.jar
</span><span class='line'>-rw-r--r-- 1 biadmin biadmin    65012 Jan 30 15:22 guice-servlet-3.0.jar
</span><span class='line'>lrwxrwxrwx 1 biadmin biadmin       45 Jan 30 15:22 hadoop-core.jar -&gt; /data/opt/ibm/biginsights/IHC/hadoop-core.jar
</span><span class='line'>lrwxrwxrwx 1 biadmin biadmin       76 Jan 30 15:22 hadoop-distcp-2.2.0.jar -&gt; /data/opt/ibm/biginsights/IHC/share/hadoop/tools/lib/hadoop-distcp-2.2.0.jar</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Distcp]]></title>
    <link href="http://winseliu.com/blog/2015/03/13/hadoop-distcp/"/>
    <updated>2015-03-13T20:38:23+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/13/hadoop-distcp</id>
    <content type="html"><![CDATA[<p>HDFS提供的CP是单线程的，对于大数据量的拷贝操作希望能并行的复制。Hadoop Tools提供了DistCp工具，通过调用MapRed来实现并行的拷贝。</p>

<h2>先来了解下hdfs cp的功能：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage: hdfs dfs -cp [-f] [-p | -p[topax]] URI [URI ...] &lt;dest&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-not-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -mkdir /cp-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-not-exists2/
</span><span class='line'>cp: `/cp-not-exists2/': No such file or directory
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -ls -R /
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 19:55 /cp
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 19:55 /cp/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 19:54 /cp/README.txt
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-exists
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-exists/cp
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-exists/cp/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-exists/cp/README.txt
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-not-exists
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-not-exists/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-not-exists/README.txt</span></code></pre></td></tr></table></div></figure>


<h2>DistCp(distributed copy)分布式拷贝简单使用方式：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ bin/hadoop distcp /cp /cp-distcp</span></code></pre></td></tr></table></div></figure>


<p>用到分布式一般就说明规模不少，且数据量大，操作时间长。DistCp提供了一些参数来控制程序：</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> DistCpOptionSwitch选项    </th>
<th style="text-align:center;"> 命令行参数                      </th>
<th style="text-align:left;"> 描述                                        </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> LOG_PATH                  </td>
<td style="text-align:center;"> <code>-log &lt;logdir&gt;               </code> </td>
<td style="text-align:left;"> map结果输出的目录。默认为<code>JobStagingDir/_logs</code>，在DistCp#configureOutputFormat把该路径设置给CopyOutputFormat#setOutputPath。</td>
</tr>
<tr>
<td style="text-align:left;"> SOURCE_FILE_LISTING       </td>
<td style="text-align:center;"> <code>-f &lt;urilist_uri&gt;            </code> </td>
<td style="text-align:left;"> 需要拷贝的source-path&hellip;从改文件获取。</td>
</tr>
<tr>
<td style="text-align:left;"> MAX_MAPS                  </td>
<td style="text-align:center;"> <code>-m &lt;num_maps&gt;               </code> </td>
<td style="text-align:left;"> 默认20个，创建job时通过<code>JobContext.NUM_MAPS</code>添加到配置。</td>
</tr>
<tr>
<td style="text-align:left;"> ATOMIC_COMMIT             </td>
<td style="text-align:center;"> <code>-atomic                     </code> </td>
<td style="text-align:left;"> 原子操作。要么全部拷贝成功，那么失败。与<code>SYNC_FOLDERS</code> &amp; <code>DELETE_MISSING</code>选项不兼容。</td>
</tr>
<tr>
<td style="text-align:left;"> WORK_PATH                 </td>
<td style="text-align:center;"> <code>-tmp &lt;tmp_dir&gt;              </code> </td>
<td style="text-align:left;"> 与atomic一起使用，中间过程存储数据目录。成功后在CopyCommitter一次性移动到target-path下。</td>
</tr>
<tr>
<td style="text-align:left;"> SYNC_FOLDERS              </td>
<td style="text-align:center;"> <code>-update                     </code> </td>
<td style="text-align:left;"> 新建或更新文件。当文件大小和blockSize（以及crc）一样忽略。</td>
</tr>
<tr>
<td style="text-align:left;"> DELETE_MISSING            </td>
<td style="text-align:center;"> <code>-delete                     </code> </td>
<td style="text-align:left;"> 针对target-path目录，清理source-paths目录下没有的文件。常和<code>SYNC_FOLDERS</code>选项一起使用。</td>
</tr>
<tr>
<td style="text-align:left;"> BLOCKING                  </td>
<td style="text-align:center;"> <code>-async                      </code> </td>
<td style="text-align:left;"> 异步运行。其实就是job提交后，不打印日志了没有调用<code>job.waitForCompletion(true)</code>罢了。</td>
</tr>
<tr>
<td style="text-align:left;"> BANDWIDTH                 </td>
<td style="text-align:center;"> <code>-bandwidth num(M)           </code> </td>
<td style="text-align:left;"> 获取数据的最大速度。结合ThrottledInputStream来进行控制，在RetriableFileCopyCommand中初始化。</td>
</tr>
<tr>
<td style="text-align:left;"> COPY_STRATEGY             </td>
<td style="text-align:center;"> <code>-strategy dynamic/uniformsize</code> </td>
<td style="text-align:left;"> 复制的时刻分组策略，即每个Map到底处理写什么数据。后面会讲到，分为静态和动态。</td>
</tr>
</tbody>
</table>


<p>还有新增的两个属性skipcrccheck（SKIP_CRC），append（APPEND）。保留Preserve 属性和ssl选项由于暂时没用到，这里不表，以后用到再补充。</p>

<h2>DistCp的源码</h2>

<p>放在<code>hadoop-2.6.0-src\hadoop-tools\hadoop-distcp</code>目录下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn eclipse:eclipse </span></code></pre></td></tr></table></div></figure>


<p>网络没问题的话，一般都能成功生成.classpath和.project两个Eclipse需要的项目文件。然后把项目导入eclipse即可。包括4个目录。</p>

<p>还是先说说整个distcp的实现流程，看看distcp怎么跑的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ export HADOOP_CLIENT_OPTS="-Dhadoop.root.logger=debug,console -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8071"
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop distcp /cp /cp-distcp
</span><span class='line'>Listening for transport dt_socket at address: 8071</span></code></pre></td></tr></table></div></figure>


<p>运行eclipse远程调试，连接服务器的8071端口，在DistCp的run方法打个断点，就可以调试了解其运行方式。修改log4j为debug，可以查看更详细的日志，了解执行的流程。</p>

<p>服务器的jdk版本和本地eclipse的jdk版本最好一致，这样调试的时刻比较顺畅。</p>

<h3>Driver</h3>

<p>首先进到DistCp(Driver)的main方法，DistCp继承Configured实现了Tool接口，</p>

<p>第一步解析参数</p>

<ol>
<li>使用<code>ToolRunner.run</code>运行会调用GenericOptionsParser解析<code>-D</code>的属性到Configuration实例；</li>
<li>进到run方法后，通过<code>OptionsParser.parse</code>来解析配置为DistCpOptions实例；功能比较单一，主要涉及到DistCpOptionSwitch和DistCpOptions两个类。</li>
</ol>


<p>第二步准备MapRed的Job实例</p>

<ol>
<li>创建metaFolderPath（后面的 待拷贝文件seq文件存取的位置：StagingDir/_distcp[RAND]），对应<code>CONF_LABEL_META_FOLDER</code>属性；</li>
<li>创建Job，设置名称、InputFormat(UniformSizeInputFormat|DynamicInputFormat)、Map类CopyMapper、Map个数（默认20个）、Reduce个数（0个）、OutputKey|ValueClass、MAP_SPECULATIVE（使用RetriableCommand代替）、CopyOutputFormat</li>
<li>把命令行的配置写入Configuration。</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>metaFolderPath /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp-1344594636</span></code></pre></td></tr></table></div></figure>


<p>此处有话题，设置InputFormat时通过<code>DistCpUtils#getStrategy</code>获取，代码中并没有<code>strategy.impl</code>的键加入到configuration。why？此处也是我们可以学习的，这个设置项在distcp-default.xml配置文件中，这种方式可以实现代码的解耦。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public static Class&lt;? extends InputFormat&gt; getStrategy(Configuration conf,
</span><span class='line'>                                                                 DistCpOptions options) {
</span><span class='line'>    String confLabel = "distcp." +
</span><span class='line'>        options.getCopyStrategy().toLowerCase(Locale.getDefault()) + ".strategy.impl";
</span><span class='line'>    return conf.getClass(confLabel, UniformSizeInputFormat.class, InputFormat.class);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>// 配置
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>        &lt;name&gt;distcp.dynamic.strategy.impl&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;org.apache.hadoop.tools.mapred.lib.DynamicInputFormat&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;Implementation of dynamic input format&lt;/description&gt;
</span><span class='line'>    &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>        &lt;name&gt;distcp.static.strategy.impl&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;org.apache.hadoop.tools.mapred.UniformSizeInputFormat&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;Implementation of static input format&lt;/description&gt;
</span><span class='line'>    &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>配置CopyOutputFormat时，设置了三个路径：</p>

<ul>
<li>WorkingDirectory（中间临时存储目录，atomic选项时为tmp路径，否则为target-path路径）；</li>
<li>CommitDirectory（文件拷贝最终目录，即target-path）；</li>
<li>OutputPath（map write记录输出路径）。</li>
</ul>


<p>关于命令行选项有一个疑问，用eclipse查看<code>Call Hierachy</code>调用关系的时刻，并没有发现调用<code>DistCpOptions#getXXX</code>的方法，那么是通过什么方式把这些配置项设置到Configuration的呢？ 在DistCpOptionSwitch的枚举类中定义了每个选项的confLabel，在<code>DistCpOptions#appendToConf</code>方法中一起把这些属性填充到Configuration中。 [统一配置] ！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public void appendToConf(Configuration conf) {
</span><span class='line'>    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.ATOMIC_COMMIT,
</span><span class='line'>        String.valueOf(atomicCommit));
</span><span class='line'>    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.IGNORE_FAILURES,
</span><span class='line'>        String.valueOf(ignoreFailures));
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>第三步整理需要拷贝的文件列表</p>

<p>这个真tmd的独到，提前把要做的事情规划好。需要拷贝的列表数据最终写入<code>[metaFolder]/fileList.seq</code>（key：与source-path的相对路径，value：该文件的CopyListingFileStatus），对应<code>CONF_LABEL_LISTING_FILE_PATH</code>，也就是map的输入（在自定义的InputFormat中处理）。</p>

<p>涉及CopyList的三个实现FileBasedCopyListing（-f）、GlobbedCopyListing、SimpleCopyListing。最终都调用SimpleCopyListing把文件和空目录列表写入到fileList.seq；最后校验否有重复的文件名，如果存在会抛出DuplicateFileException。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/tmp/hadoop-yarn/staging/hadoop/.staging/_distcp179796572/fileList.seq</span></code></pre></td></tr></table></div></figure>


<p>同时计算需要拷贝的个数和大小（Byte），对应<code>CONF_LABEL_TOTAL_BYTES_TO_BE_COPIED</code>和<code>CONF_LABEL_TOTAL_NUMBER_OF_RECORDS</code>。</p>

<p>第四步提交任务，等待等待无尽的等待。</p>

<p>也可以设置async选项，提交成功后直接完成Driver。</p>

<h3>Mapper</h3>

<p>首先，setup从Configuration中获取配置属性：sync(update)/忽略错误(i)/校验码/overWrite/workPath/finalPath</p>

<p>然后，从CONF_LABEL_LISTING_FILE_PATH路径获取准备好的sourcepath->CopyListingFileStatus键值对作为map的输入。</p>

<p>其实CopyListingFileStatus这个对象真正用到的就是原始Path的路径，真心不知道搞这么多属性干嘛！获取原始路径后又重新实例CopyListingFileStatus为sourceCurrStatus。</p>

<ul>
<li>如果源路径为文件夹，调用createTargetDirsWithRetry（RetriableDirectoryCreateCommand）创建路径，COPY计数加1，return。</li>
<li>如果源路径为文件，但是checkUpdate（文件大小和块大小一致）为skip，SKIP计数加1，BYTESSKIPPED计数加上sourceCurrStatus的长度，把改条记录写入map输出，return。</li>
<li>如果源路径为文件，且检查后不是skip则调用copyFileWithRetry（RetriableFileCopyCommand）拷贝文件，BYTESEXPECTED计数加上sourceCurrStatus的长度，BYTESCOPIED计数加上拷贝文件的大小，COPY计数加1，再return。</li>
<li>如果配置有保留文件/文件夹属性，对目标进行属性修改。</li>
</ul>


<p>从CopyListing获取数据，调用FileSystem-IO接口进行数据的拷贝（在原有IO的基础上封装了ThrottledInputStream来进行限流处理）。于此同时会涉及到source路径是文件夹但是target不是文件夹等的检查；更新还是覆盖；文件属性的保留和Map计数值的更新操作。</p>

<h3>InputFormat</h3>

<p>自定义了InputFormat来UniformSizeInputFormat进行拆分构造FileSplit，对CONF_LABEL_LISTING_FILE_PATH文件的每个键值的文件大小平均分成Map num
个数小块，根据键值的位置构造Map num个FileSplit对象。执行map时，RecordReader根据FileSplit来获取键值对，然后传递给map。</p>

<p>新版本的增加了DynamicInputFormat，实现能者多难的功能。先通过实际的日志，看看运行效果：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ export HADOOP_CLIENT_OPTS="-Dhadoop.root.logger=debug,console -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8071"
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop distcp "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090" -strategy dynamic -m 2 /cp /cp-distcp-dynamic
</span><span class='line'>
</span><span class='line'># 创建的chunk
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        198 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00000
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        224 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00001
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        220 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00002
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>
</span><span class='line'># 分配后的chunk
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        220 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00002
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        198 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/task_1426773672048_0006_m_000000
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        224 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/task_1426773672048_0006_m_000001
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>
</span><span class='line'># map获取后
</span><span class='line'>[hadoop@hadoop-master2 ~]$  ssh -g -L 8090:hadoop-slaver1:8090 hadoop-slaver1
</span><span class='line'># 每拷贝完一个chunk/最后map结束，会把上一个跑完的chunk文件删除
</span><span class='line'># job跑完后，临时目录的数据就被清楚了
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>ls: `/tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446': No such file or directory</span></code></pre></td></tr></table></div></figure>


<p>由于设置的map num为2，还有一个chunk没有分配出去，等到真正执行的时刻再进行分配。体现了策略的动态性。这个<strong>chunkm_000000分配给map0(其他类似)</strong>，其他没有分配出去的chunk让给map去<strong>抢</strong>。</p>

<p>首先InputFormat创建FileSplit，在此过程中把原来的<code>CONF_LABEL_LISTING_FILE_PATH</code>中的需要处理的文件根据个数等份成chunk。（具体实现看源码，其中numEntriesPerChunk计算一个chunk几个文件比较复杂点）</p>

<p>chunk中的也是sourcepath->CopyListingFileStatus键值对，以seq格式的存储文件中。<code>DynamicInputChunk#acquire(TaskAttemptContext)</code>读取数据的时刻比较有意思，在Driver阶段分配的chunk处理完后，就会动态的取处理余下的chunk，能者多劳。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public static DynamicInputChunk acquire(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
</span><span class='line'>    if (!areInvariantsInitialized())
</span><span class='line'>        initializeChunkInvariants(taskAttemptContext.getConfiguration());
</span><span class='line'>
</span><span class='line'>    String taskId = taskAttemptContext.getTaskAttemptID().getTaskID().toString();
</span><span class='line'>    Path acquiredFilePath = new Path(chunkRootPath, taskId);
</span><span class='line'>
</span><span class='line'>    if (fs.exists(acquiredFilePath)) {
</span><span class='line'>      LOG.info("Acquiring pre-assigned chunk: " + acquiredFilePath);
</span><span class='line'>      return new DynamicInputChunk(acquiredFilePath, taskAttemptContext);
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    for (FileStatus chunkFile : getListOfChunkFiles()) {
</span><span class='line'>      if (fs.rename(chunkFile.getPath(), acquiredFilePath)) {
</span><span class='line'>        LOG.info(taskId + " acquired " + chunkFile.getPath());
</span><span class='line'>        return new DynamicInputChunk(acquiredFilePath, taskAttemptContext);
</span><span class='line'>      }
</span><span class='line'>      else
</span><span class='line'>        LOG.warn(taskId + " could not acquire " + chunkFile.getPath());
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    return null;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<h3>OutputFormat &amp; Committer</h3>

<p>自定义的CopyOutputFormat包括了working/commit/output路径的get/set方法，同时指定了自定义的OutputCommitter：CopyCommitter。</p>

<p>正常情况为app-master调用CopyCommitter#commitJob处理善后的事情：保留文件属性的情况下更新文件的属性，atomic情况下把working转到commit路径，delete情况下删除target目录多余的文件。最后清理临时目录。</p>

<p>看完DistCp然后再去看DistCpV1，尽管说功能上类似，但是要和新版本对上仍然要去看distcp的代码。好的代码就是这样吧，让人很自然轻松的理解，而不必反复来回的折腾，甚至于为了免得来回折腾而记住该代码块。（类太大，方法太长，变量定义和使用的位置相隔很远！一个变量作用域太长赋值变更次数太多）</p>

<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/FileSystemShell.html#cp">FileSystemShell cp</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistCp.html">DistCp官方文档</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Windows Build hadoop-2.6]]></title>
    <link href="http://winseliu.com/blog/2015/03/09/windows-build-hadoop-2-dot-6/"/>
    <updated>2015-03-09T12:01:55+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/09/windows-build-hadoop-2-dot-6</id>
    <content type="html"><![CDATA[<h2>环境</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>C:\Users\winse&gt;java -version
</span><span class='line'>java version "1.7.0_02"
</span><span class='line'>Java(TM) SE Runtime Environment (build 1.7.0_02-b13)
</span><span class='line'>Java HotSpot(TM) Client VM (build 22.0-b10, mixed mode, sharing)
</span><span class='line'>
</span><span class='line'>C:\Users\winse&gt;protoc --version
</span><span class='line'>libprotoc 2.5.0
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ cygcheck -c cygwin
</span><span class='line'>Cygwin Package Information
</span><span class='line'>Package              Version        Status
</span><span class='line'>cygwin               1.7.33-1       OK</span></code></pre></td></tr></table></div></figure>


<h2>具体步骤</h2>

<p>在windows下，hadoop-2.6还不能直接编译java-x86的dll。需要自己处理/打patch<a href="https://issues.apache.org/jira/browse/HADOOP-9922">HADOOP-9922</a>，但是官网jira-patch给出来的和2.6.0-src对不上。自己动手丰衣足食，把x64的全部改成Win32即可，附编译成功的patch<a href="http://yunpan.cn/cJaZzSu6DIibg">下载hadoop-2.6.0-common-native-win32-diff.patch（提取码：08fd）</a>。</p>

<ul>
<li>用visual studio2010的x86命令行进入：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Visual Studio 命令提示(2010)
</span><span class='line'>
</span><span class='line'>Setting environment for using Microsoft Visual Studio 2010 x86 tools.</span></code></pre></td></tr></table></div></figure>


<ul>
<li>切换到hadoop源码目录，打补丁和编译。同时protobuf目录和cygwin\bin目录加入PATH：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd hadoop-2.6.0-src
</span><span class='line'>cd hadoop-common-project\hadoop-common
</span><span class='line'>patch -p0 &lt; hadoop-2.6.0-common-native-win32-diff.patch
</span><span class='line'>
</span><span class='line'>set PATH=%PATH%;E:\local\home\Administrator\bin;c:\cygwin\bin
</span><span class='line'>
</span><span class='line'>mvn package -Pdist,native-win -DskipTests -Dtar -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<ul>
<li>编译完成后，直接把<code>hadoop-common\target\bin</code>目录下的内容拷贝到程序的bin目录下。</li>
</ul>


<p>在windows下，执行java程序java.library.path默认到PATH路径找。这也是需要定义<strong>环境变量HADOOP_HOME</strong>，以及把<code>%HADOOP_HOME%\bin</code>加入到PATH的原因！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>HADOOP_HOME=E:\local\libs\big\hadoop-2.2.0 
</span><span class='line'>PATH=%HADOOP_HOME%\bin;%PATH%</span></code></pre></td></tr></table></div></figure>


<ul>
<li>配置坑：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0
</span><span class='line'>$ find . -name "*-default.xml" | xargs -I{} grep "hadoop.tmp.dir" {}
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/local&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/system&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/staging&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/temp&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/history/recoverystore&lt;/value&gt;
</span><span class='line'>  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/io/local&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/s3&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/s3a&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/name&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/data&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/namesecondary&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn/system/rmstore&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/nm-local-dir&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn-nm-recovery&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn/timeline&lt;/value&gt;</span></code></pre></td></tr></table></div></figure>


<p>就dfs的在前面加了<code>file://</code>前缀！</p>

<p>所以，在windows下如果你只配置hadoop.tmp.dir（<code>file:///e:/tmp/hadoop</code>）的话还得同时配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/name&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/data&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/namesecondary&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>接下来格式化，启动都和同时一样。</p>

<h2>其他</h2>

<p>调试，下载maven源码等</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set HADOOP_NAMENODE_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"
</span><span class='line'>
</span><span class='line'>mvn dependency:resolve -Dclassifier=sources
</span><span class='line'>
</span><span class='line'>mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs 
</span><span class='line'>
</span><span class='line'>mvn dependency:sources 
</span><span class='line'>mvn dependency:resolve -Dclassifier=javadoc
</span><span class='line'>
</span><span class='line'>/* 操作HDFS */
</span><span class='line'>set HADOOP_ROOT_LOGGER=DEBUG,console</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VMware-Centos6 Build hadoop-2.6]]></title>
    <link href="http://winseliu.com/blog/2015/03/08/vmware-build-hadoop2-dot-6/"/>
    <updated>2015-03-08T08:22:14+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/08/vmware-build-hadoop2-dot-6</id>
    <content type="html"><![CDATA[<p>每次编译hadoop（-common）都是惊心动魄，没一次顺顺当当的！由于作者的偷懒，引发的有一起血案~~~</p>

<h2>环境说明</h2>

<ul>
<li>操作系统</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# uname -a
</span><span class='line'>Linux localhost.localdomain 2.6.32-431.el6.x86_64 #1 SMP Fri Nov 22 03:15:09 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
</span><span class='line'>[root@localhost ~]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.5 (Final)</span></code></pre></td></tr></table></div></figure>


<ul>
<li>使用VMware的<strong>Shared Folders</strong>建立了maven和hadoop-2.6.0-src到宿主机器的映射：(不要直接在源码映射的目录下编译，先拷贝到linux的硬盘下！！)</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# ll -a hadoop-2.6.0-src maven
</span><span class='line'>lrwxrwxrwx. 1 root root 26 Mar  7 22:47 hadoop-2.6.0-src -&gt; /mnt/hgfs/hadoop-2.6.0-src
</span><span class='line'>lrwxrwxrwx. 1 root root 15 Mar  7 22:47 maven -&gt; /mnt/hgfs/maven</span></code></pre></td></tr></table></div></figure>


<h2>具体操作</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 安装maven，jdk
</span><span class='line'>cat apache-maven-3.2.3-bin.tar.gz | ssh root@192.168.154.130 "cat - | tar zxv "
</span><span class='line'>
</span><span class='line'>tar zxvf jdk-7u60-linux-x64.gz -C ~/
</span><span class='line'>vi .bash_profile 
</span><span class='line'>
</span><span class='line'># 开发环境
</span><span class='line'>yum install gcc glibc-headers gcc-c++ zlib-devel
</span><span class='line'>yum install openssl-devel
</span><span class='line'>
</span><span class='line'># 安装protobuf
</span><span class='line'>tar zxvf protobuf-2.5.0.tar.gz 
</span><span class='line'>cd protobuf-2.5.0
</span><span class='line'>./configure 
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>## 编译hadoop-common
</span><span class='line'># 从映射文件中拷贝hadoop-common到linux文件系统，然后在编译hadoop-common
</span><span class='line'>cd hadoop-2.6.0-src/hadoop-common-project/hadoop-common/
</span><span class='line'>cd ..
</span><span class='line'>cp -r  hadoop-common ~/  #Q:为啥要拷贝一份，【遇到的问题】中有进行解析
</span><span class='line'>cd ~/hadoop-common
</span><span class='line'>mvn install
</span><span class='line'>mvn -X package -Pdist,native -Dmaven.test.skip=true -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'>## 编译全部，耗时比较久，可以先去吃个饭^v^
</span><span class='line'>cp -r /mnt/hgfs/hadoop-2.6.0-src ~/
</span><span class='line'>mvn package -Pdist,native -DskipTests -Dmaven.javadoc.skip=true #Q:这里为啥不能用maven.test.skip?</span></code></pre></td></tr></table></div></figure>


<h2>遇到的问题</h2>

<ul>
<li><p>第一个问题肯定是没有<strong>c</strong>的编译环境，安装gcc即可。</p></li>
<li><p><code>configure: error: C++ preprocessor "/lib/cpp" fails sanity check</code>，安装c++。</p></li>
</ul>


<p>-> <a href="http://www.cnblogs.com/niocai/archive/2011/11/04/2236458.html">configure: error: C++ preprocessor &ldquo;/lib/cpp&rdquo; fails sanity check</a></p>

<ul>
<li><code>Unknown lifecycle phase "c"</code>，点击错误提示最后的链接查看解决方法，即执行<code>mvn install</code>。</li>
</ul>


<p>-> <a href="http://blog.csdn.net/kamemo/article/details/6523992">执行第一maven用例出错：Unknown lifecycle phase &ldquo;complile&rdquo;.</a>
-> <a href="https://cwiki.apache.org/confluence/display/MAVEN/LifecyclePhaseNotFoundException">LifecyclePhaseNotFoundException</a></p>

<ul>
<li><code>CMake Error at /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:108 (message): Could NOT find ZLIB (missing: ZLIB_INCLUDE_DIR)</code>， 缺少zlib-devel。</li>
</ul>


<p>-> <a href="http://ask.csdn.net/questions/62307">Cmake时报错：Could NOT find ImageMagick</a></p>

<ul>
<li><code>cmake_symlink_library: System Error: Operation not supported</code>， 共享的windows目录下不能创建linux的软链接。</li>
</ul>


<p>-> <a href="http://bbs.chinaunix.net/forum.php?mod=viewthread&amp;tid=3595245&amp;fromuid=26971268">参见9楼回复</a></p>

<blockquote><p>创建链接不成功，要确认当前帐户下是否有权限在编译的目录中有创建链接的权限</p>

<p>比如，你如果是在一个WINDOWS机器上的共享目录中编译，就没法创建链接，就会失败。把源码复制到本地的目录中再编译就不会有这问题。</p></blockquote>

<ul>
<li>全部编译时需使用skipTests。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>main:
</span><span class='line'>     [echo] Running test_libhdfs_threaded
</span><span class='line'>     [exec] nmdCreate: NativeMiniDfsCluster#Builder#Builder error:
</span><span class='line'>     [exec] java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/MiniDFSCluster$Builder
</span><span class='line'>     [exec] Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdfs.MiniDFSCluster$Builder
</span><span class='line'>     [exec]     at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
</span><span class='line'>     [exec]     at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
</span><span class='line'>     [exec]     at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>     [exec]     at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
</span><span class='line'>     [exec]     at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
</span><span class='line'>     [exec]     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
</span><span class='line'>     [exec]     at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</span><span class='line'>     [exec] TEST_ERROR: failed on /root/hadoop-2.6.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c:326 (errno: 2): got NULL from tlhCluster</span></code></pre></td></tr></table></div></figure>


<ul>
<li><code>Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the</code>，安装openssl-devel。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>main:
</span><span class='line'>    [mkdir] Created dir: /root/hadoop-2.6.0-src/hadoop-tools/hadoop-pipes/target/native
</span><span class='line'>     [exec] -- The C compiler identification is GNU 4.4.7
</span><span class='line'>     [exec] -- The CXX compiler identification is GNU 4.4.7
</span><span class='line'>     [exec] -- Check for working C compiler: /usr/bin/cc
</span><span class='line'>     [exec] -- Check for working C compiler: /usr/bin/cc -- works
</span><span class='line'>     [exec] -- Detecting C compiler ABI info
</span><span class='line'>     [exec] -- Detecting C compiler ABI info - done
</span><span class='line'>     [exec] -- Check for working CXX compiler: /usr/bin/c++
</span><span class='line'>     [exec] -- Check for working CXX compiler: /usr/bin/c++ -- works
</span><span class='line'>     [exec] -- Detecting CXX compiler ABI info
</span><span class='line'>     [exec] -- Detecting CXX compiler ABI info - done
</span><span class='line'>     [exec] -- Configuring incomplete, errors occurred!
</span><span class='line'>     [exec] See also "/root/hadoop-2.6.0-src/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeOutput.log".
</span><span class='line'>     [exec] CMake Error at /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:108 (message):
</span><span class='line'>     [exec]   Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the
</span><span class='line'>     [exec]   system variable OPENSSL_ROOT_DIR (missing: OPENSSL_LIBRARIES
</span><span class='line'>     [exec]   OPENSSL_INCLUDE_DIR)
</span><span class='line'>     [exec] Call Stack (most recent call first):
</span><span class='line'>     [exec]   /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:315 (_FPHSA_FAILURE_MESSAGE)
</span><span class='line'>     [exec]   /usr/share/cmake/Modules/FindOpenSSL.cmake:313 (find_package_handle_standard_args)
</span><span class='line'>     [exec]   CMakeLists.txt:20 (find_package)
</span><span class='line'>     [exec] 
</span><span class='line'>     [exec] </span></code></pre></td></tr></table></div></figure>


<h2>成功</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[INFO] Executed tasks
</span><span class='line'>[INFO] 
</span><span class='line'>[INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---
</span><span class='line'>[INFO] Skipping javadoc generation
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Reactor Summary:
</span><span class='line'>[INFO] 
</span><span class='line'>[INFO] Apache Hadoop Main ................................. SUCCESS [ 43.005 s]
</span><span class='line'>[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 25.511 s]
</span><span class='line'>[INFO] Apache Hadoop Annotations .......................... SUCCESS [ 21.177 s]
</span><span class='line'>[INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 11.728 s]
</span><span class='line'>[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 51.274 s]
</span><span class='line'>[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 35.625 s]
</span><span class='line'>[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 21.936 s]
</span><span class='line'>[INFO] Apache Hadoop Auth ................................. SUCCESS [ 24.665 s]
</span><span class='line'>[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 17.058 s]
</span><span class='line'>[INFO] Apache Hadoop Common ............................... SUCCESS [06:07 min]
</span><span class='line'>[INFO] Apache Hadoop NFS .................................. SUCCESS [ 41.279 s]
</span><span class='line'>[INFO] Apache Hadoop KMS .................................. SUCCESS [ 59.186 s]
</span><span class='line'>[INFO] Apache Hadoop Common Project ....................... SUCCESS [  7.216 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS ................................. SUCCESS [04:29 min]
</span><span class='line'>[INFO] Apache Hadoop HttpFS ............................... SUCCESS [ 52.883 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.972 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 24.901 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  7.486 s]
</span><span class='line'>[INFO] hadoop-yarn ........................................ SUCCESS [  7.466 s]
</span><span class='line'>[INFO] hadoop-yarn-api .................................... SUCCESS [ 32.970 s]
</span><span class='line'>[INFO] hadoop-yarn-common ................................. SUCCESS [ 25.549 s]
</span><span class='line'>[INFO] hadoop-yarn-server ................................. SUCCESS [  6.709 s]
</span><span class='line'>[INFO] hadoop-yarn-server-common .......................... SUCCESS [ 25.292 s]
</span><span class='line'>[INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 29.555 s]
</span><span class='line'>[INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [ 12.800 s]
</span><span class='line'>[INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 14.025 s]
</span><span class='line'>[INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 21.121 s]
</span><span class='line'>[INFO] hadoop-yarn-server-tests ........................... SUCCESS [ 24.019 s]
</span><span class='line'>[INFO] hadoop-yarn-client ................................. SUCCESS [ 18.949 s]
</span><span class='line'>[INFO] hadoop-yarn-applications ........................... SUCCESS [  7.586 s]
</span><span class='line'>[INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  8.428 s]
</span><span class='line'>[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [ 12.671 s]
</span><span class='line'>[INFO] hadoop-yarn-site ................................... SUCCESS [  7.518 s]
</span><span class='line'>[INFO] hadoop-yarn-registry ............................... SUCCESS [ 18.518 s]
</span><span class='line'>[INFO] hadoop-yarn-project ................................ SUCCESS [ 38.781 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client ............................ SUCCESS [ 13.133 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 23.772 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 22.815 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [ 16.810 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.404 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [ 18.157 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 14.637 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  9.190 s]
</span><span class='line'>[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  9.037 s]
</span><span class='line'>[INFO] hadoop-mapreduce ................................... SUCCESS [ 59.280 s]
</span><span class='line'>[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 26.724 s]
</span><span class='line'>[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 31.503 s]
</span><span class='line'>[INFO] Apache Hadoop Archives ............................. SUCCESS [ 19.867 s]
</span><span class='line'>[INFO] Apache Hadoop Rumen ................................ SUCCESS [ 27.401 s]
</span><span class='line'>[INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 20.102 s]
</span><span class='line'>[INFO] Apache Hadoop Data Join ............................ SUCCESS [ 20.382 s]
</span><span class='line'>[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [ 12.207 s]
</span><span class='line'>[INFO] Apache Hadoop Extras ............................... SUCCESS [ 24.069 s]
</span><span class='line'>[INFO] Apache Hadoop Pipes ................................ SUCCESS [ 31.975 s]
</span><span class='line'>[INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 32.225 s]
</span><span class='line'>[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [02:45 min]
</span><span class='line'>[INFO] Apache Hadoop Client ............................... SUCCESS [01:38 min]
</span><span class='line'>[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 15.450 s]
</span><span class='line'>[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 46.489 s]
</span><span class='line'>[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [01:31 min]
</span><span class='line'>[INFO] Apache Hadoop Tools ................................ SUCCESS [  7.603 s]
</span><span class='line'>[INFO] Apache Hadoop Distribution ......................... SUCCESS [ 32.967 s]
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] BUILD SUCCESS
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Total time: 39:30 min
</span><span class='line'>[INFO] Finished at: 2015-03-08T10:55:47+08:00
</span><span class='line'>[INFO] Final Memory: 102M/340M
</span><span class='line'>[INFO] ------------------------------------------------------------------------</span></code></pre></td></tr></table></div></figure>


<p>把src编译出来的native下面的文件拷贝到hadoop集群程序目录下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 lib]$ scp -r root@172.17.42.1:~/hadoop-2.6.0-src/hadoop-dist/target/hadoop-2.6.0/lib/native ./
</span><span class='line'>[hadoop@hadoop-master1 lib]$ cd native/
</span><span class='line'>[hadoop@hadoop-master1 native]$ ll
</span><span class='line'>total 4356
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop 1119518 Mar  8 03:11 libhadoop.a
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop 1486964 Mar  8 03:11 libhadooppipes.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop      18 Mar  3 21:08 libhadoop.so -&gt; libhadoop.so.1.0.0
</span><span class='line'>-rwxr-xr-x. 1 hadoop hadoop  671237 Mar  8 03:11 libhadoop.so.1.0.0
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop  581944 Mar  8 03:11 libhadooputils.a
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop  359490 Mar  8 03:11 libhdfs.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop      16 Mar  3 21:08 libhdfs.so -&gt; libhdfs.so.0.0.0
</span><span class='line'>-rwxr-xr-x. 1 hadoop hadoop  228451 Mar  8 03:11 libhdfs.so.0.0.0</span></code></pre></td></tr></table></div></figure>


<p>添加编译的native包前后对比：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.0]$ hadoop fs -ls /
</span><span class='line'>15/03/08 03:09:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Found 3 items
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-06 16:49 /README.txt
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-06 16:54 /tmp
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-06 16:54 /user
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.0]$ hadoop fs -ls /
</span><span class='line'>Found 3 items
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-06 16:49 /README.txt
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-06 16:54 /tmp
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-06 16:54 /user</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
</feed>
