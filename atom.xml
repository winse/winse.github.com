<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Winse Blog]]></title>
  <link href="http://winseliu.com/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2015-03-23T00:23:45+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop Distcp]]></title>
    <link href="http://winseliu.com/blog/2015/03/13/hadoop-distcp/"/>
    <updated>2015-03-13T20:38:23+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/13/hadoop-distcp</id>
    <content type="html"><![CDATA[<p>HDFS提供的CP是单线程的，对于大数据量的拷贝操作希望能并行的复制。Hadoop Tools提供了DistCp工具，通过调用MapRed来实现并行的拷贝。</p>

<h2>先来了解下hdfs cp的功能：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage: hdfs dfs -cp [-f] [-p | -p[topax]] URI [URI ...] &lt;dest&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-not-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -mkdir /cp-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-exists
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -cp /cp /cp-not-exists2/
</span><span class='line'>cp: `/cp-not-exists2/': No such file or directory
</span><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ hadoop fs -ls -R /
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 19:55 /cp
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 19:55 /cp/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 19:54 /cp/README.txt
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-exists
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-exists/cp
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-exists/cp/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-exists/cp/README.txt
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-14 20:17 /cp-not-exists
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-not-exists/README.1.txt
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-14 20:17 /cp-not-exists/README.txt</span></code></pre></td></tr></table></div></figure>


<h2>DistCp(distributed copy)分布式拷贝简单使用方式：</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 hadoop-2.6.0]$ bin/hadoop distcp /cp /cp-distcp</span></code></pre></td></tr></table></div></figure>


<p>用到分布式一般就说明规模不少，且数据量大，操作时间长。DistCp提供了一些参数来控制程序：</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> DistCpOptionSwitch选项    </th>
<th style="text-align:center;"> 命令行参数                      </th>
<th style="text-align:left;"> 描述                                        </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> LOG_PATH                  </td>
<td style="text-align:center;"> <code>-log &lt;logdir&gt;               </code> </td>
<td style="text-align:left;"> map结果输出的目录。默认为<code>JobStagingDir/_logs</code>，在DistCp#configureOutputFormat把该路径设置给CopyOutputFormat#setOutputPath。</td>
</tr>
<tr>
<td style="text-align:left;"> SOURCE_FILE_LISTING       </td>
<td style="text-align:center;"> <code>-f &lt;urilist_uri&gt;            </code> </td>
<td style="text-align:left;"> 需要拷贝的source-path&hellip;从改文件获取。</td>
</tr>
<tr>
<td style="text-align:left;"> MAX_MAPS                  </td>
<td style="text-align:center;"> <code>-m &lt;num_maps&gt;               </code> </td>
<td style="text-align:left;"> 默认20个，创建job时通过<code>JobContext.NUM_MAPS</code>添加到配置。</td>
</tr>
<tr>
<td style="text-align:left;"> ATOMIC_COMMIT             </td>
<td style="text-align:center;"> <code>-atomic                     </code> </td>
<td style="text-align:left;"> 原子操作。要么全部拷贝成功，那么失败。与<code>SYNC_FOLDERS</code> &amp; <code>DELETE_MISSING</code>选项不兼容。</td>
</tr>
<tr>
<td style="text-align:left;"> WORK_PATH                 </td>
<td style="text-align:center;"> <code>-tmp &lt;tmp_dir&gt;              </code> </td>
<td style="text-align:left;"> 与atomic一起使用，中间过程存储数据目录。成功后在CopyCommitter一次性移动到target-path下。</td>
</tr>
<tr>
<td style="text-align:left;"> SYNC_FOLDERS              </td>
<td style="text-align:center;"> <code>-update                     </code> </td>
<td style="text-align:left;"> 新建或更新文件。当文件大小和blockSize（以及crc）一样忽略。</td>
</tr>
<tr>
<td style="text-align:left;"> DELETE_MISSING            </td>
<td style="text-align:center;"> <code>-delete                     </code> </td>
<td style="text-align:left;"> 针对target-path目录，清理source-paths目录下没有的文件。常和<code>SYNC_FOLDERS</code>选项一起使用。</td>
</tr>
<tr>
<td style="text-align:left;"> BLOCKING                  </td>
<td style="text-align:center;"> <code>-async                      </code> </td>
<td style="text-align:left;"> 异步运行。其实就是job提交后，不打印日志了没有调用<code>job.waitForCompletion(true)</code>罢了。</td>
</tr>
<tr>
<td style="text-align:left;"> BANDWIDTH                 </td>
<td style="text-align:center;"> <code>-bandwidth num(M)           </code> </td>
<td style="text-align:left;"> 获取数据的最大速度。结合ThrottledInputStream来进行控制，在RetriableFileCopyCommand中初始化。</td>
</tr>
<tr>
<td style="text-align:left;"> COPY_STRATEGY             </td>
<td style="text-align:center;"> <code>-strategy dynamic/uniformsize</code> </td>
<td style="text-align:left;"> 复制的时刻分组策略，即每个Map到底处理写什么数据。后面会讲到，分为静态和动态。</td>
</tr>
</tbody>
</table>


<p>还有新增的两个属性skipcrccheck（SKIP_CRC），append（APPEND）。保留Preserve 属性和ssl选项由于暂时没用到，这里不表，以后用到再补充。</p>

<h2>DistCp的源码</h2>

<p>放在<code>hadoop-2.6.0-src\hadoop-tools\hadoop-distcp</code>目录下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn eclipse:eclipse </span></code></pre></td></tr></table></div></figure>


<p>网络没问题的话，一般都能成功生成.classpath和.project两个Eclipse需要的项目文件。然后把项目导入eclipse即可。包括4个目录。</p>

<p>还是先说说整个distcp的实现流程，看看distcp怎么跑的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ export HADOOP_CLIENT_OPTS="-Dhadoop.root.logger=debug,console -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8071"
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop distcp /cp /cp-distcp
</span><span class='line'>Listening for transport dt_socket at address: 8071</span></code></pre></td></tr></table></div></figure>


<p>运行eclipse远程调试，连接服务器的8071端口，在DistCp的run方法打个断点，就可以调试了解其运行方式。修改log4j为debug，可以查看更详细的日志，了解执行的流程。</p>

<p>服务器的jdk版本和本地eclipse的jdk版本最好一致，这样调试的时刻比较顺畅。</p>

<h3>Driver</h3>

<p>首先进到DistCp(Driver)的main方法，DistCp继承Configured实现了Tool接口，</p>

<p>第一步解析参数</p>

<ol>
<li>使用<code>ToolRunner.run</code>运行会调用GenericOptionsParser解析<code>-D</code>的属性到Configuration实例；</li>
<li>进到run方法后，通过<code>OptionsParser.parse</code>来解析配置为DistCpOptions实例；功能比较单一，主要涉及到DistCpOptionSwitch和DistCpOptions两个类。</li>
</ol>


<p>第二步准备MapRed的Job实例</p>

<ol>
<li>创建metaFolderPath（后面的 待拷贝文件seq文件存取的位置：StagingDir/_distcp[RAND]），对应<code>CONF_LABEL_META_FOLDER</code>属性；</li>
<li>创建Job，设置名称、InputFormat(UniformSizeInputFormat|DynamicInputFormat)、Map类CopyMapper、Map个数（默认20个）、Reduce个数（0个）、OutputKey|ValueClass、MAP_SPECULATIVE（使用RetriableCommand代替）、CopyOutputFormat</li>
<li>把命令行的配置写入Configuration。</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>metaFolderPath /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp-1344594636</span></code></pre></td></tr></table></div></figure>


<p>此处有话题，设置InputFormat时通过<code>DistCpUtils#getStrategy</code>获取，代码中并没有<code>strategy.impl</code>的键加入到configuration。why？此处也是我们可以学习的，这个设置项在distcp-default.xml配置文件中，这种方式可以实现代码的解耦。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public static Class&lt;? extends InputFormat&gt; getStrategy(Configuration conf,
</span><span class='line'>                                                                 DistCpOptions options) {
</span><span class='line'>    String confLabel = "distcp." +
</span><span class='line'>        options.getCopyStrategy().toLowerCase(Locale.getDefault()) + ".strategy.impl";
</span><span class='line'>    return conf.getClass(confLabel, UniformSizeInputFormat.class, InputFormat.class);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>// 配置
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>        &lt;name&gt;distcp.dynamic.strategy.impl&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;org.apache.hadoop.tools.mapred.lib.DynamicInputFormat&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;Implementation of dynamic input format&lt;/description&gt;
</span><span class='line'>    &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>        &lt;name&gt;distcp.static.strategy.impl&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;org.apache.hadoop.tools.mapred.UniformSizeInputFormat&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;Implementation of static input format&lt;/description&gt;
</span><span class='line'>    &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>配置CopyOutputFormat时，设置了三个路径：</p>

<ul>
<li>WorkingDirectory（中间临时存储目录，atomic选项时为tmp路径，否则为target-path路径）；</li>
<li>CommitDirectory（文件拷贝最终目录，即target-path）；</li>
<li>OutputPath（map write记录输出路径）。</li>
</ul>


<p>关于命令行选项有一个疑问，用eclipse查看<code>Call Hierachy</code>调用关系的时刻，并没有发现调用<code>DistCpOptions#getXXX</code>的方法，那么是通过什么方式把这些配置项设置到Configuration的呢？ 在DistCpOptionSwitch的枚举类中定义了每个选项的confLabel，在<code>DistCpOptions#appendToConf</code>方法中一起把这些属性填充到Configuration中。 [统一配置] ！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public void appendToConf(Configuration conf) {
</span><span class='line'>    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.ATOMIC_COMMIT,
</span><span class='line'>        String.valueOf(atomicCommit));
</span><span class='line'>    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.IGNORE_FAILURES,
</span><span class='line'>        String.valueOf(ignoreFailures));
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>第三步整理需要拷贝的文件列表</p>

<p>这个真tmd的独到，提前把要做的事情规划好。需要拷贝的列表数据最终写入<code>[metaFolder]/fileList.seq</code>（key：与source-path的相对路径，value：该文件的CopyListingFileStatus），对应<code>CONF_LABEL_LISTING_FILE_PATH</code>，也就是map的输入（在自定义的InputFormat中处理）。</p>

<p>涉及CopyList的三个实现FileBasedCopyListing（-f）、GlobbedCopyListing、SimpleCopyListing。最终都调用SimpleCopyListing把文件和空目录列表写入到fileList.seq；最后校验否有重复的文件名，如果存在会抛出DuplicateFileException。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/tmp/hadoop-yarn/staging/hadoop/.staging/_distcp179796572/fileList.seq</span></code></pre></td></tr></table></div></figure>


<p>同时计算需要拷贝的个数和大小（Byte），对应<code>CONF_LABEL_TOTAL_BYTES_TO_BE_COPIED</code>和<code>CONF_LABEL_TOTAL_NUMBER_OF_RECORDS</code>。</p>

<p>第四步提交任务，等待等待无尽的等待。</p>

<p>也可以设置async选项，提交成功后直接完成Driver。</p>

<h3>Mapper</h3>

<p>首先，setup从Configuration中获取配置属性：sync(update)/忽略错误(i)/校验码/overWrite/workPath/finalPath</p>

<p>然后，从CONF_LABEL_LISTING_FILE_PATH路径获取准备好的sourcepath->CopyListingFileStatus键值对作为map的输入。</p>

<p>其实CopyListingFileStatus这个对象真正用到的就是原始Path的路径，真心不知道搞这么多属性干嘛！获取原始路径后又重新实例CopyListingFileStatus为sourceCurrStatus。</p>

<ul>
<li>如果源路径为文件夹，调用createTargetDirsWithRetry（RetriableDirectoryCreateCommand）创建路径，COPY计数加1，return。</li>
<li>如果源路径为文件，但是checkUpdate（文件大小和块大小一致）为skip，SKIP计数加1，BYTESSKIPPED计数加上sourceCurrStatus的长度，把改条记录写入map输出，return。</li>
<li>如果源路径为文件，且检查后不是skip则调用copyFileWithRetry（RetriableFileCopyCommand）拷贝文件，BYTESEXPECTED计数加上sourceCurrStatus的长度，BYTESCOPIED计数加上拷贝文件的大小，COPY计数加1，再return。</li>
<li>如果配置有保留文件/文件夹属性，对目标进行属性修改。</li>
</ul>


<p>从CopyListing获取数据，调用FileSystem-IO接口进行数据的拷贝（在原有IO的基础上封装了ThrottledInputStream来进行限流处理）。于此同时会涉及到source路径是文件夹但是target不是文件夹等的检查；更新还是覆盖；文件属性的保留和Map计数值的更新操作。</p>

<h3>InputFormat</h3>

<p>自定义了InputFormat来UniformSizeInputFormat进行拆分构造FileSplit，对CONF_LABEL_LISTING_FILE_PATH文件的每个键值的文件大小平均分成Map num
个数小块，根据键值的位置构造Map num个FileSplit对象。执行map时，RecordReader根据FileSplit来获取键值对，然后传递给map。</p>

<p>新版本的增加了DynamicInputFormat，实现能者多难的功能。先通过实际的日志，看看运行效果：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master2 ~]$ export HADOOP_CLIENT_OPTS="-Dhadoop.root.logger=debug,console -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8071"
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop distcp "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090" -strategy dynamic -m 2 /cp /cp-distcp-dynamic
</span><span class='line'>
</span><span class='line'># 创建的chunk
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        198 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00000
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        224 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00001
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        220 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00002
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>
</span><span class='line'># 分配后的chunk
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        220 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/fileList.seq.chunk.00002
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        198 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/task_1426773672048_0006_m_000000
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        224 2015-03-20 00:41 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/chunkDir/task_1426773672048_0006_m_000001
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        506 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq
</span><span class='line'>-rw-r--r--   1 hadoop supergroup        446 2015-03-20 00:40 /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446/fileList.seq_sorted
</span><span class='line'>
</span><span class='line'># map获取后
</span><span class='line'>[hadoop@hadoop-master2 ~]$  ssh -g -L 8090:hadoop-slaver1:8090 hadoop-slaver1
</span><span class='line'># 每拷贝完一个chunk/最后map结束，会把上一个跑完的chunk文件删除
</span><span class='line'># job跑完后，临时目录的数据就被清楚了
</span><span class='line'>[hadoop@hadoop-master2 ~]$ hadoop fs -ls -R /tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446
</span><span class='line'>ls: `/tmp/hadoop-yarn/staging/hadoop/.staging/_distcp1568928446': No such file or directory</span></code></pre></td></tr></table></div></figure>


<p>由于设置的map num为2，还有一个chunk没有分配出去，等到真正执行的时刻再进行分配。体现了策略的动态性。这个<strong>chunkm_000000分配给map0(其他类似)</strong>，其他没有分配出去的chunk让给map去<strong>抢</strong>。</p>

<p>首先InputFormat创建FileSplit，在此过程中把原来的<code>CONF_LABEL_LISTING_FILE_PATH</code>中的需要处理的文件根据个数等份成chunk。（具体实现看源码，其中numEntriesPerChunk计算一个chunk几个文件比较复杂点）</p>

<p>chunk中的也是sourcepath->CopyListingFileStatus键值对，以seq格式的存储文件中。<code>DynamicInputChunk#acquire(TaskAttemptContext)</code>读取数据的时刻比较有意思，在Driver阶段分配的chunk处理完后，就会动态的取处理余下的chunk，能者多劳。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public static DynamicInputChunk acquire(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
</span><span class='line'>    if (!areInvariantsInitialized())
</span><span class='line'>        initializeChunkInvariants(taskAttemptContext.getConfiguration());
</span><span class='line'>
</span><span class='line'>    String taskId = taskAttemptContext.getTaskAttemptID().getTaskID().toString();
</span><span class='line'>    Path acquiredFilePath = new Path(chunkRootPath, taskId);
</span><span class='line'>
</span><span class='line'>    if (fs.exists(acquiredFilePath)) {
</span><span class='line'>      LOG.info("Acquiring pre-assigned chunk: " + acquiredFilePath);
</span><span class='line'>      return new DynamicInputChunk(acquiredFilePath, taskAttemptContext);
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    for (FileStatus chunkFile : getListOfChunkFiles()) {
</span><span class='line'>      if (fs.rename(chunkFile.getPath(), acquiredFilePath)) {
</span><span class='line'>        LOG.info(taskId + " acquired " + chunkFile.getPath());
</span><span class='line'>        return new DynamicInputChunk(acquiredFilePath, taskAttemptContext);
</span><span class='line'>      }
</span><span class='line'>      else
</span><span class='line'>        LOG.warn(taskId + " could not acquire " + chunkFile.getPath());
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    return null;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<h3>OutputFormat &amp; Committer</h3>

<p>自定义的CopyOutputFormat包括了working/commit/output路径的get/set方法，同时指定了自定义的OutputCommitter：CopyCommitter。</p>

<p>正常情况为app-master调用CopyCommitter#commitJob处理善后的事情：保留文件属性的情况下更新文件的属性，atomic情况下把working转到commit路径，delete情况下删除target目录多余的文件。最后清理临时目录。</p>

<p>看完DistCp然后再去看DistCpV1，尽管说功能上类似，但是要和新版本对上仍然要去看distcp的代码。好的代码就是这样吧，让人很自然轻松的理解，而不必反复来回的折腾，甚至于为了免得来回折腾而记住该代码块。（类太大，方法太长，变量定义和使用的位置相隔很远！一个变量作用域太长赋值变更次数太多）</p>

<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/FileSystemShell.html#cp">FileSystemShell cp</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistCp.html">DistCp官方文档</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Windows Build hadoop-2.6]]></title>
    <link href="http://winseliu.com/blog/2015/03/09/windows-build-hadoop-2-dot-6/"/>
    <updated>2015-03-09T12:01:55+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/09/windows-build-hadoop-2-dot-6</id>
    <content type="html"><![CDATA[<h2>环境</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>C:\Users\winse&gt;java -version
</span><span class='line'>java version "1.7.0_02"
</span><span class='line'>Java(TM) SE Runtime Environment (build 1.7.0_02-b13)
</span><span class='line'>Java HotSpot(TM) Client VM (build 22.0-b10, mixed mode, sharing)
</span><span class='line'>
</span><span class='line'>C:\Users\winse&gt;protoc --version
</span><span class='line'>libprotoc 2.5.0
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0/bin
</span><span class='line'>$ uname -a
</span><span class='line'>CYGWIN_NT-6.3-WOW64 Lenovo-PC 1.7.33-2(0.280/5/3) 2014-11-13 15:45 i686 Cygwin
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ cygcheck -c cygwin
</span><span class='line'>Cygwin Package Information
</span><span class='line'>Package              Version        Status
</span><span class='line'>cygwin               1.7.33-1       OK</span></code></pre></td></tr></table></div></figure>


<h2>编译具体步骤</h2>

<p>hadoop-2.6还不能直接编译java-x86的dll的。需要自己处理/打patch<a href="https://issues.apache.org/jira/browse/HADOOP-9922">HADOOP-9922</a>，但是官网jira-patch给出来的和2.6.0-src对不上。自己动手丰衣足食，把x64的全部改成Win32即可，附编译成功的patch<a href="http://yunpan.cn/cJaZzSu6DIibg">下载hadoop-2.6.0-common-native-win32-diff.patch（提取码：08fd）</a>。</p>

<ul>
<li>用visual studio2010的x86命令行进入：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Visual Studio 命令提示(2010)
</span><span class='line'>
</span><span class='line'>Setting environment for using Microsoft Visual Studio 2010 x86 tools.</span></code></pre></td></tr></table></div></figure>


<ul>
<li>切换到hadoop源码目录，打补丁和编译。protobuf目录和cygwin-bin目录加入PATH：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd hadoop-2.6.0-src
</span><span class='line'>cd hadoop-common-project\hadoop-common
</span><span class='line'>patch -p0 &lt; hadoop-2.6.0-common-native-win32-diff.patch
</span><span class='line'>
</span><span class='line'>set PATH=%PATH%;E:\local\home\Administrator\bin;c:\cygwin\bin
</span><span class='line'>
</span><span class='line'>mvn package -Pdist,native-win -DskipTests -Dtar -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'>mvn eclipse:eclipse</span></code></pre></td></tr></table></div></figure>


<ul>
<li>编译完成后，直接把<code>hadoop-common\target\bin</code>目录下的内容拷贝到程序的bin目录下。</li>
</ul>


<p>在windows下，执行java程序java.library.path默认到PATH路径找。这也是需要定义环境变量HADOOP_HOME，以及把bin加入到PATH的原因吧！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>HADOOP_HOME=E:\local\opt\bigdata\hadoop-2.6.0
</span><span class='line'>PATH=%HADOOP_HOME%\bin;%PATH%</span></code></pre></td></tr></table></div></figure>


<p>bin目录结构:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0/bin
</span><span class='line'>$ ls -l
</span><span class='line'>total 3372
</span><span class='line'>----------+ 1 winse None  159183 Nov 14 05:20 container-executor
</span><span class='line'>----------+ 1 winse None    5479 Nov 14 05:20 hadoop
</span><span class='line'>----------+ 1 winse None    8298 Nov 14 05:20 hadoop.cmd
</span><span class='line'>----------+ 1 winse None   71680 Mar  9 10:33 hadoop.dll
</span><span class='line'>----------+ 1 winse None   17033 Mar  9 10:33 hadoop.exp
</span><span class='line'>----------+ 1 winse None   28666 Mar  9 10:33 hadoop.lib
</span><span class='line'>----------+ 1 winse None  502784 Mar  9 10:33 hadoop.pdb
</span><span class='line'>----------+ 1 winse None   11142 Nov 14 05:20 hdfs
</span><span class='line'>----------+ 1 winse None    6923 Nov 14 05:20 hdfs.cmd
</span><span class='line'>----------+ 1 winse None 1213602 Mar  9 10:33 libwinutils.lib
</span><span class='line'>----------+ 1 winse None    5205 Nov 14 05:20 mapred
</span><span class='line'>----------+ 1 winse None    5949 Nov 14 05:20 mapred.cmd
</span><span class='line'>----------+ 1 winse None    1776 Nov 14 05:20 rcc
</span><span class='line'>----------+ 1 winse None  201659 Nov 14 05:20 test-container-executor
</span><span class='line'>----------+ 1 winse None   97792 Mar  9 10:33 winutils.exe
</span><span class='line'>----------+ 1 winse None 1059840 Mar  9 10:33 winutils.pdb
</span><span class='line'>----------+ 1 winse None   11380 Nov 14 05:20 yarn
</span><span class='line'>----------+ 1 winse None   10895 Nov 14 05:20 yarn.cmd
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0/bin
</span><span class='line'>$ file winutils.exe hadoop.dll
</span><span class='line'>winutils.exe: PE32 executable (console) Intel 80386, for MS Windows
</span><span class='line'>hadoop.dll:   PE32 executable (DLL) (GUI) Intel 80386, for MS Windows</span></code></pre></td></tr></table></div></figure>


<h2>配置</h2>

<ul>
<li>配置坑：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC /cygdrive/e/local/opt/bigdata/hadoop-2.6.0
</span><span class='line'>$ find . -name "*-default.xml" | xargs -I{} grep "hadoop.tmp.dir" {}
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/local&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/system&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/staging&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/temp&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/mapred/history/recoverystore&lt;/value&gt;
</span><span class='line'>  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/io/local&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/s3&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/s3a&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/name&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/data&lt;/value&gt;
</span><span class='line'>  &lt;value&gt;file://${hadoop.tmp.dir}/dfs/namesecondary&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn/system/rmstore&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/nm-local-dir&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn-nm-recovery&lt;/value&gt;
</span><span class='line'>    &lt;value&gt;${hadoop.tmp.dir}/yarn/timeline&lt;/value&gt;</span></code></pre></td></tr></table></div></figure>


<p>就dfs的配置项前面加了<code>file://</code>前缀！</p>

<p>所以，在windows下如果只配置hadoop.tmp.dir（<code>file:///e:/tmp/hadoop</code>）的话得同时配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/name&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/data&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;${hadoop.tmp.dir}/dfs/namesecondary&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>接下来格式化，启动都和linux操作一样。不多说。</p>

<h2>其他</h2>

<p>调试，下载maven源码等</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set HADOOP_NAMENODE_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"
</span><span class='line'>
</span><span class='line'>mvn dependency:resolve -Dclassifier=sources
</span><span class='line'>
</span><span class='line'>mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs 
</span><span class='line'>
</span><span class='line'>mvn dependency:sources 
</span><span class='line'>mvn dependency:resolve -Dclassifier=javadoc
</span><span class='line'>
</span><span class='line'>/* 操作HDFS */
</span><span class='line'>set HADOOP_ROOT_LOGGER=DEBUG,console</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://wiki.apache.org/hadoop/Hadoop2OnWindows">Build and Install Hadoop 2.x or newer on Windows</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VMware-Centos6 Build hadoop-2.6]]></title>
    <link href="http://winseliu.com/blog/2015/03/08/vmware-build-hadoop2-dot-6/"/>
    <updated>2015-03-08T08:22:14+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/08/vmware-build-hadoop2-dot-6</id>
    <content type="html"><![CDATA[<p>每次编译hadoop（-common）都是惊心动魄，没一次顺顺当当的！由于作者的偷懒，引发的有一起血案~~~</p>

<h2>环境说明</h2>

<ul>
<li>操作系统</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# uname -a
</span><span class='line'>Linux localhost.localdomain 2.6.32-431.el6.x86_64 #1 SMP Fri Nov 22 03:15:09 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
</span><span class='line'>[root@localhost ~]# cat /etc/redhat-release 
</span><span class='line'>CentOS release 6.5 (Final)</span></code></pre></td></tr></table></div></figure>


<ul>
<li>使用VMware的<strong>Shared Folders</strong>建立了maven和hadoop-2.6.0-src到宿主机器的映射：(不要直接在源码映射的目录下编译，先拷贝到linux的硬盘下！！)</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# ll -a hadoop-2.6.0-src maven
</span><span class='line'>lrwxrwxrwx. 1 root root 26 Mar  7 22:47 hadoop-2.6.0-src -&gt; /mnt/hgfs/hadoop-2.6.0-src
</span><span class='line'>lrwxrwxrwx. 1 root root 15 Mar  7 22:47 maven -&gt; /mnt/hgfs/maven</span></code></pre></td></tr></table></div></figure>


<h2>具体操作</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 安装maven，jdk
</span><span class='line'>cat apache-maven-3.2.3-bin.tar.gz | ssh root@192.168.154.130 "cat - | tar zxv "
</span><span class='line'>
</span><span class='line'>tar zxvf jdk-7u60-linux-x64.gz -C ~/
</span><span class='line'>vi .bash_profile 
</span><span class='line'>
</span><span class='line'># 开发环境
</span><span class='line'>yum install gcc glibc-headers gcc-c++ zlib-devel
</span><span class='line'>yum install openssl-devel
</span><span class='line'>
</span><span class='line'># 安装protobuf
</span><span class='line'>tar zxvf protobuf-2.5.0.tar.gz 
</span><span class='line'>cd protobuf-2.5.0
</span><span class='line'>./configure 
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'># 从映射文件中拷贝hadoop-common到linux文件系统，然后在编译hadoop-common
</span><span class='line'>cd hadoop-2.6.0-src/hadoop-common-project/hadoop-common/
</span><span class='line'>cd ..
</span><span class='line'>cp -r  hadoop-common ~/
</span><span class='line'>cd ~/hadoop-common
</span><span class='line'>mvn install
</span><span class='line'>mvn -X package -Pdist,native -Dmaven.test.skip=true -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'>## 编译全部，耗时比较久，可以先去吃个饭^v^
</span><span class='line'>cp -r /mnt/hgfs/hadoop-2.6.0-src ~/
</span><span class='line'>mvn package -Pdist,native -DskipTests -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<h2>遇到的问题</h2>

<ul>
<li><p>第一个问题肯定是没有<strong>c</strong>的编译环境，安装gcc即可。</p></li>
<li><p><code>configure: error: C++ preprocessor "/lib/cpp" fails sanity check</code>，安装c++。</p></li>
</ul>


<p>-> <a href="http://www.cnblogs.com/niocai/archive/2011/11/04/2236458.html">configure: error: C++ preprocessor &ldquo;/lib/cpp&rdquo; fails sanity check</a></p>

<ul>
<li><code>Unknown lifecycle phase "c"</code>，点击错误提示最后的链接查看解决方法，即执行<code>mvn install</code>。</li>
</ul>


<p>-> <a href="http://blog.csdn.net/kamemo/article/details/6523992">执行第一maven用例出错：Unknown lifecycle phase &ldquo;complile&rdquo;.</a>
-> <a href="https://cwiki.apache.org/confluence/display/MAVEN/LifecyclePhaseNotFoundException">LifecyclePhaseNotFoundException</a></p>

<ul>
<li><code>CMake Error at /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:108 (message): Could NOT find ZLIB (missing: ZLIB_INCLUDE_DIR)</code>， 缺少zlib-devel。</li>
</ul>


<p>-> <a href="http://ask.csdn.net/questions/62307">Cmake时报错：Could NOT find ImageMagick</a></p>

<ul>
<li><code>cmake_symlink_library: System Error: Operation not supported</code>， 共享的windows目录下不能创建linux的软链接。</li>
</ul>


<p>-> <a href="http://bbs.chinaunix.net/forum.php?mod=viewthread&amp;tid=3595245&amp;fromuid=26971268">参见9楼回复</a></p>

<blockquote><p>创建链接不成功，要确认当前帐户下是否有权限在编译的目录中有创建链接的权限</p>

<p>比如，你如果是在一个WINDOWS机器上的共享目录中编译，就没法创建链接，就会失败。把源码复制到本地的目录中再编译就不会有这问题。</p></blockquote>

<ul>
<li>全部编译时使用skipTests。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>main:
</span><span class='line'>     [echo] Running test_libhdfs_threaded
</span><span class='line'>     [exec] nmdCreate: NativeMiniDfsCluster#Builder#Builder error:
</span><span class='line'>     [exec] java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/MiniDFSCluster$Builder
</span><span class='line'>     [exec] Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdfs.MiniDFSCluster$Builder
</span><span class='line'>     [exec]     at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
</span><span class='line'>     [exec]     at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
</span><span class='line'>     [exec]     at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>     [exec]     at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
</span><span class='line'>     [exec]     at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
</span><span class='line'>     [exec]     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
</span><span class='line'>     [exec]     at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</span><span class='line'>     [exec] TEST_ERROR: failed on /root/hadoop-2.6.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c:326 (errno: 2): got NULL from tlhCluster</span></code></pre></td></tr></table></div></figure>


<ul>
<li><code>Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the</code>，安装openssl-devel。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>main:
</span><span class='line'>    [mkdir] Created dir: /root/hadoop-2.6.0-src/hadoop-tools/hadoop-pipes/target/native
</span><span class='line'>     [exec] -- The C compiler identification is GNU 4.4.7
</span><span class='line'>     [exec] -- The CXX compiler identification is GNU 4.4.7
</span><span class='line'>     [exec] -- Check for working C compiler: /usr/bin/cc
</span><span class='line'>     [exec] -- Check for working C compiler: /usr/bin/cc -- works
</span><span class='line'>     [exec] -- Detecting C compiler ABI info
</span><span class='line'>     [exec] -- Detecting C compiler ABI info - done
</span><span class='line'>     [exec] -- Check for working CXX compiler: /usr/bin/c++
</span><span class='line'>     [exec] -- Check for working CXX compiler: /usr/bin/c++ -- works
</span><span class='line'>     [exec] -- Detecting CXX compiler ABI info
</span><span class='line'>     [exec] -- Detecting CXX compiler ABI info - done
</span><span class='line'>     [exec] -- Configuring incomplete, errors occurred!
</span><span class='line'>     [exec] See also "/root/hadoop-2.6.0-src/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeOutput.log".
</span><span class='line'>     [exec] CMake Error at /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:108 (message):
</span><span class='line'>     [exec]   Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the
</span><span class='line'>     [exec]   system variable OPENSSL_ROOT_DIR (missing: OPENSSL_LIBRARIES
</span><span class='line'>     [exec]   OPENSSL_INCLUDE_DIR)
</span><span class='line'>     [exec] Call Stack (most recent call first):
</span><span class='line'>     [exec]   /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:315 (_FPHSA_FAILURE_MESSAGE)
</span><span class='line'>     [exec]   /usr/share/cmake/Modules/FindOpenSSL.cmake:313 (find_package_handle_standard_args)
</span><span class='line'>     [exec]   CMakeLists.txt:20 (find_package)
</span><span class='line'>     [exec] 
</span><span class='line'>     [exec] </span></code></pre></td></tr></table></div></figure>


<h2>成功</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[INFO] Executed tasks
</span><span class='line'>[INFO] 
</span><span class='line'>[INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-dist ---
</span><span class='line'>[INFO] Skipping javadoc generation
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Reactor Summary:
</span><span class='line'>[INFO] 
</span><span class='line'>[INFO] Apache Hadoop Main ................................. SUCCESS [ 43.005 s]
</span><span class='line'>[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 25.511 s]
</span><span class='line'>[INFO] Apache Hadoop Annotations .......................... SUCCESS [ 21.177 s]
</span><span class='line'>[INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 11.728 s]
</span><span class='line'>[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 51.274 s]
</span><span class='line'>[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 35.625 s]
</span><span class='line'>[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 21.936 s]
</span><span class='line'>[INFO] Apache Hadoop Auth ................................. SUCCESS [ 24.665 s]
</span><span class='line'>[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 17.058 s]
</span><span class='line'>[INFO] Apache Hadoop Common ............................... SUCCESS [06:07 min]
</span><span class='line'>[INFO] Apache Hadoop NFS .................................. SUCCESS [ 41.279 s]
</span><span class='line'>[INFO] Apache Hadoop KMS .................................. SUCCESS [ 59.186 s]
</span><span class='line'>[INFO] Apache Hadoop Common Project ....................... SUCCESS [  7.216 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS ................................. SUCCESS [04:29 min]
</span><span class='line'>[INFO] Apache Hadoop HttpFS ............................... SUCCESS [ 52.883 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 28.972 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 24.901 s]
</span><span class='line'>[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  7.486 s]
</span><span class='line'>[INFO] hadoop-yarn ........................................ SUCCESS [  7.466 s]
</span><span class='line'>[INFO] hadoop-yarn-api .................................... SUCCESS [ 32.970 s]
</span><span class='line'>[INFO] hadoop-yarn-common ................................. SUCCESS [ 25.549 s]
</span><span class='line'>[INFO] hadoop-yarn-server ................................. SUCCESS [  6.709 s]
</span><span class='line'>[INFO] hadoop-yarn-server-common .......................... SUCCESS [ 25.292 s]
</span><span class='line'>[INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [ 29.555 s]
</span><span class='line'>[INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [ 12.800 s]
</span><span class='line'>[INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [ 14.025 s]
</span><span class='line'>[INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 21.121 s]
</span><span class='line'>[INFO] hadoop-yarn-server-tests ........................... SUCCESS [ 24.019 s]
</span><span class='line'>[INFO] hadoop-yarn-client ................................. SUCCESS [ 18.949 s]
</span><span class='line'>[INFO] hadoop-yarn-applications ........................... SUCCESS [  7.586 s]
</span><span class='line'>[INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  8.428 s]
</span><span class='line'>[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [ 12.671 s]
</span><span class='line'>[INFO] hadoop-yarn-site ................................... SUCCESS [  7.518 s]
</span><span class='line'>[INFO] hadoop-yarn-registry ............................... SUCCESS [ 18.518 s]
</span><span class='line'>[INFO] hadoop-yarn-project ................................ SUCCESS [ 38.781 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client ............................ SUCCESS [ 13.133 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 23.772 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 22.815 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [ 16.810 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 14.404 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [ 18.157 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 14.637 s]
</span><span class='line'>[INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  9.190 s]
</span><span class='line'>[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  9.037 s]
</span><span class='line'>[INFO] hadoop-mapreduce ................................... SUCCESS [ 59.280 s]
</span><span class='line'>[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 26.724 s]
</span><span class='line'>[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 31.503 s]
</span><span class='line'>[INFO] Apache Hadoop Archives ............................. SUCCESS [ 19.867 s]
</span><span class='line'>[INFO] Apache Hadoop Rumen ................................ SUCCESS [ 27.401 s]
</span><span class='line'>[INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 20.102 s]
</span><span class='line'>[INFO] Apache Hadoop Data Join ............................ SUCCESS [ 20.382 s]
</span><span class='line'>[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [ 12.207 s]
</span><span class='line'>[INFO] Apache Hadoop Extras ............................... SUCCESS [ 24.069 s]
</span><span class='line'>[INFO] Apache Hadoop Pipes ................................ SUCCESS [ 31.975 s]
</span><span class='line'>[INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 32.225 s]
</span><span class='line'>[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [02:45 min]
</span><span class='line'>[INFO] Apache Hadoop Client ............................... SUCCESS [01:38 min]
</span><span class='line'>[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 15.450 s]
</span><span class='line'>[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 46.489 s]
</span><span class='line'>[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [01:31 min]
</span><span class='line'>[INFO] Apache Hadoop Tools ................................ SUCCESS [  7.603 s]
</span><span class='line'>[INFO] Apache Hadoop Distribution ......................... SUCCESS [ 32.967 s]
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] BUILD SUCCESS
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Total time: 39:30 min
</span><span class='line'>[INFO] Finished at: 2015-03-08T10:55:47+08:00
</span><span class='line'>[INFO] Final Memory: 102M/340M
</span><span class='line'>[INFO] ------------------------------------------------------------------------</span></code></pre></td></tr></table></div></figure>


<p>把src编译出来的native下面的文件拷贝到hadoop集群程序目录下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 lib]$ scp -r root@172.17.42.1:~/hadoop-2.6.0-src/hadoop-dist/target/hadoop-2.6.0/lib/native ./
</span><span class='line'>[hadoop@hadoop-master1 lib]$ cd native/
</span><span class='line'>[hadoop@hadoop-master1 native]$ ll
</span><span class='line'>total 4356
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop 1119518 Mar  8 03:11 libhadoop.a
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop 1486964 Mar  8 03:11 libhadooppipes.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop      18 Mar  3 21:08 libhadoop.so -&gt; libhadoop.so.1.0.0
</span><span class='line'>-rwxr-xr-x. 1 hadoop hadoop  671237 Mar  8 03:11 libhadoop.so.1.0.0
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop  581944 Mar  8 03:11 libhadooputils.a
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop  359490 Mar  8 03:11 libhdfs.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop      16 Mar  3 21:08 libhdfs.so -&gt; libhdfs.so.0.0.0
</span><span class='line'>-rwxr-xr-x. 1 hadoop hadoop  228451 Mar  8 03:11 libhdfs.so.0.0.0</span></code></pre></td></tr></table></div></figure>


<p>前后对比：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.0]$ hadoop fs -ls /
</span><span class='line'>15/03/08 03:09:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>Found 3 items
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-06 16:49 /README.txt
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-06 16:54 /tmp
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-06 16:54 /user
</span><span class='line'>[hadoop@hadoop-master1 hadoop-2.6.0]$ hadoop fs -ls /
</span><span class='line'>Found 3 items
</span><span class='line'>-rw-r--r--   1 hadoop supergroup       1366 2015-03-06 16:49 /README.txt
</span><span class='line'>drwx------   - hadoop supergroup          0 2015-03-06 16:54 /tmp
</span><span class='line'>drwxr-xr-x   - hadoop supergroup          0 2015-03-06 16:54 /user</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VMware共享目录]]></title>
    <link href="http://winseliu.com/blog/2015/03/07/vmware-sharefolder/"/>
    <updated>2015-03-07T22:25:52+08:00</updated>
    <id>http://winseliu.com/blog/2015/03/07/vmware-sharefolder</id>
    <content type="html"><![CDATA[<p>VMware提供了与主机共享目录的功能，可以在虚拟机访问宿主机器的文件。</p>

<ol>
<li>选择映射目录
 选择[Edit virtual machine settings]，在弹出的对话框中选择[Options]页签，选择[Shared Folders]，点击右边的[Add]按钮添加需要映射(maven)的本地目录。</li>
<li>安装VMware Tools

<ul>
<li>启动linux虚拟机，选择[VM]菜单，再选择[Install VMware Tools&hellip;]菜单。下载完成后，会自动通过cdrom加载到虚拟机。</li>
<li>登录linux虚拟机，执行以下命令：</li>
</ul>
</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /mnt
</span><span class='line'>mkdir cdrom
</span><span class='line'>mount /dev/cdrom cdrom
</span><span class='line'>cd cdrom/
</span><span class='line'>mkdir ~/vmware
</span><span class='line'>tar zxvf VMwareTools-9.2.0-799703.tar.gz -C ~/vmware
</span><span class='line'>
</span><span class='line'>cd ~/vmware
</span><span class='line'>cd vmware-tools-distrib/
</span><span class='line'>./vmware-install.pl 
</span><span class='line'>reboot
</span><span class='line'>
</span><span class='line'>cd /mnt/hgfs/maven</span></code></pre></td></tr></table></div></figure>


<p>当前的maven目录是映射到宿主的机器目录。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost maven]# ll -a
</span><span class='line'>total 3
</span><span class='line'>drwxrwxrwx. 1 root root    0 Dec 28  2012 .
</span><span class='line'>dr-xr-xr-x. 1 root root 4192 Mar  7 22:41 ..
</span><span class='line'>drwxrwxrwx. 1 root root    0 Dec 28  2012 .m2</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[开年2015]]></title>
    <link href="http://winseliu.com/blog/2015/02/19/starting-2015-spring-festival/"/>
    <updated>2015-02-19T01:42:27+08:00</updated>
    <id>http://winseliu.com/blog/2015/02/19/starting-2015-spring-festival</id>
    <content type="html"><![CDATA[<p>红包把整个2015春节捣腾的。。。</p>

<p>到了凌晨，春晚谢幕，寒气开始袭人，陆陆续续大家都开始休息，除夕的红包大战也告一段落。</p>

<p>静下来，方能更好的反思总结。</p>

<p>2014年过的还算顺当，任务上不仅仅是一些繁琐的应付事的工作。提供了足够的空间余地，可以做做自己喜欢的事情，捣腾一阵后，有时间可以给自己思考。</p>

<p>工作4年，到新的环境已然不是新人，时间的积淀和历练让我们更成熟，更有资本的同时，承担更多的责任。在项目组中，可能需要做一些表率。我觉得也是最尴尬的时间，没有傲立群雄的能耐，也不是自甘堕落的无能者，后有追兵前有猛狼。真实的感觉到85后尴尬一代的现实！</p>

<p>工作一段时间后，越来越容易被身边的事情干扰，时不时就会被这样那样的事情打断头绪！在工作之余，学习的时间越来越少，被电视剧和游戏霸占，觉得很是不应该！对于好胜心强的自己来说，接下来还是应该一心一意的去做一件事情！这样自己才能提升的明显，不至于自己觉得碌碌无为！</p>

<p>2014也帮师兄做了些事情，遇到了一些不一样的人和事情，改变了自己原来而一些看法（或者说被现实打败了）。原来总认为别人做了类似的东西，咋用就好了，原来的自己不屑一顾这些东西。还要自己去再造轮子，不希望也觉得浪费时间精力。</p>

<p>其实已有的东西，自己实现一遍后，才是你自己的东西！！不要觉得是在做无用功，当你改进或者添加新的功能时，你会发现自己写的自己实践过的才是自己的，才能驾轻就熟！</p>

<p>好基友都结婚的勾搭的，知名而不认命，什么时刻能踏上点呢！？</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[学习btrace]]></title>
    <link href="http://winseliu.com/blog/2015/02/06/start-btrace/"/>
    <updated>2015-02-06T20:38:33+08:00</updated>
    <id>http://winseliu.com/blog/2015/02/06/start-btrace</id>
    <content type="html"><![CDATA[<p>到官网下载<a href="https://kenai.com/projects/btrace/downloads/directory/releases">btrace</a>，在btrace-bin.zip压缩包中包括了usersguide.html入门教程。源码用hg管理<a href="https://kenai.com/projects/btrace/sources/hg/show">点击下载</a>。</p>

<p>Btrace程序是一个普通的java类，由<strong>BTrace annotations</strong>标注的带有的<code>public static void</code>方法组成。从代码上面来看，类似于spring aop的写法。标注用于指定需要监控的位置（方法，类）。</p>

<p>Btrace程序是只读的，以及只能执行有限制的操作。一般的限制：</p>

<ul>
<li>不能创建新对象（new objects）</li>
<li>不能创建数组（new arrays）</li>
<li>不能抛出异常（throw）</li>
<li>不能捕获异常（catch）</li>
<li>不能调用实例/静态方法。仅仅能调用com.sun.btrace.BTraceUtils的static方法。</li>
<li>不能给被监测的程序的实例或者静态字段复制。但是Btrace程序可以给自己的类的静态字段复制。</li>
<li>不能有实例字段和方法。Btrace类中仅能包括static字段和<code>public static void</code>的方法。</li>
<li>不能有内部类，嵌套类（outer, inner, nested or local classes）</li>
<li>不能有同步块或同步方法</li>
<li>不能包括循环（for，while，do&hellip;while）</li>
<li>不能继承（父类只能是默认的java.lang.Object）</li>
<li>不能实现接口</li>
<li>不能包括assert语句</li>
<li>不能使用类常量</li>
</ul>


<h2>helloworld</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// import all BTrace annotations
</span><span class='line'>import com.sun.btrace.annotations.*;
</span><span class='line'>// import statics from BTraceUtils class
</span><span class='line'>import static com.sun.btrace.BTraceUtils.*;
</span><span class='line'>
</span><span class='line'>// @BTrace annotation tells that this is a BTrace program
</span><span class='line'>@BTrace
</span><span class='line'>public class HelloWorld {
</span><span class='line'> 
</span><span class='line'>    // @OnMethod annotation tells where to probe.
</span><span class='line'>    // In this example, we are interested in entry 
</span><span class='line'>    // into the Thread.start() method. 
</span><span class='line'>    @OnMethod(
</span><span class='line'>        clazz="java.lang.Thread",
</span><span class='line'>        method="start"
</span><span class='line'>    )
</span><span class='line'>    public static void func() {
</span><span class='line'>        // println is defined in BTraceUtils
</span><span class='line'>        // you can only call the static methods of BTraceUtils
</span><span class='line'>        println("about to start a thread!");
</span><span class='line'>    }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>通过命令行脚本<code>btrace &lt;PID&gt; &lt;btrace-script&gt;</code>脚本运行。script可以是java源文件，或者已经编译好的class字节码文件。</p>

<p>btracec提供了类似于javac的功能，额外会对include的文件中定义的变量进行替换。如果你的btrace类就是一个普通功能的java类的话，直接用javac编译及可以了。</p>

<p>编写一个测试类，然后监控这个java程序的线程启动：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>public class HelloTest {
</span><span class='line'>
</span><span class='line'>  @Test
</span><span class='line'>  public void test() throws Exception {
</span><span class='line'>      testNewThread();
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  public void testNewThread() throws InterruptedException {
</span><span class='line'>      Thread.sleep(20 * 1000); // 最佳方式就是使用Scanner，手动输入一个操作后执行后面的操作。scanner.nextLine()
</span><span class='line'>
</span><span class='line'>      for (int i = 0; i &lt; 100; i++) {
</span><span class='line'>          final int index = i;
</span><span class='line'>          new Thread(//
</span><span class='line'>                  new Runnable() {
</span><span class='line'>                      public void run() {
</span><span class='line'>                          System.out.println("my order: " + index);
</span><span class='line'>                      }
</span><span class='line'>                  } //
</span><span class='line'>          ).start();
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>然后，启动btrace程序：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd src\test\script
</span><span class='line'>
</span><span class='line'>#下面的内容是一个批处理文件
</span><span class='line'>set PATH=%PATH%;C:\cygwin\bin;C:\cygwin\usr\local\bin
</span><span class='line'>
</span><span class='line'>set BTRACE_HOME=E:\local\opt\btrace-bin
</span><span class='line'>set CUR_ROOT=%cd%\..\..\..
</span><span class='line'>set SCRIPT=%CUR_ROOT%\src\main\java\com\github\winse\btrace\HelloWorld.java
</span><span class='line'>set SCRIPT=%CUR_ROOT%\target\classes\com\github\winse\btrace\HelloWorld.class
</span><span class='line'>
</span><span class='line'>jps -m  | findstr HelloTest | gawk '{print $1}' | xargs -I {} %BTRACE_HOME%\bin\btrace.bat {} %SCRIPT%</span></code></pre></td></tr></table></div></figure>


<p>上面的主程序启动后sleep了20s，等btrace程序启动。如果是程序一启动就要进行监控记录，可vm的参数添加javaagent：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-javaagent:E:\local\opt\btrace-bin\build\btrace-agent.jar=noServer=true,scriptOutputFile=C:\Temp\test.txt,script=F:\workspaces\cms_hadoop\btrace\target\classes\com\github\winse\btrace\HelloWorld.class</span></code></pre></td></tr></table></div></figure>


<p>添加到eclipse的运行配置（Debug Configurations）参数（Arguments）的VM arguments输入框内。</p>

<p>启动主程序，就可以在C:\Temp\test.txt文件看到btrace程序输出的内容了。</p>

<h2>注解</h2>

<ul>
<li>参数</li>
</ul>


<p>@Self获取this对象
@Return用于获取方法的返回值对象
@TargetInstance和@TargetMethodOrField用来查看被监控的方法内部调用那些实例的方法
@ProbeClassName和@ProbeMethodName用来检测获取当前被监控实例和方法（在OnMethod中使用通配符时，查看到底有那些方法被调用）</p>

<ul>
<li>方法</li>
</ul>


<p>@OnMethod
@OnTimer
@OnError
@OnExist
@OnLowMemory
@OnEvent
@OnProbe</p>

<h2>源码</h2>

<ul>
<li><a href="https://github.com/winse/helloJ/tree/hello/btrace">https://github.com/winse/helloJ/tree/hello/btrace</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Windows Gif]]></title>
    <link href="http://winseliu.com/blog/2015/02/04/windows-gif/"/>
    <updated>2015-02-04T15:18:20+08:00</updated>
    <id>http://winseliu.com/blog/2015/02/04/windows-gif</id>
    <content type="html"><![CDATA[<p>看到linux上各种录制gif的工具：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install byzanz
</span><span class='line'>
</span><span class='line'>byzanz-record -d 10 -x 0 -y 0 -w 1363 -h 758 byzanz-demo.gif</span></code></pre></td></tr></table></div></figure>


<p>还有各种包装的工具：<a href="https://github.com/KeyboardFire/mkcast">mkcast</a></p>

<p>本来想在cygwin中安装byzanz，但是编译需要各种库，最终放弃了。</p>

<p>其实在windows下面，也有很好的gif录制的工具：<a href="http://www.cockos.com/licecap/">LICEcap</a></p>

<p><img src="http://winseliu.com/images/blogs/gif-capture-helloworld.gif" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://v2ex.com/t/139035">windows 下有没有什么录制 gif 截屏质量较好的软件可推荐?</a></li>
<li><a href="http://v5b7.com/other/ubuntu_byzanz.html">Ubuntu录制GIF图</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Build redis-2.8]]></title>
    <link href="http://winseliu.com/blog/2015/01/22/build-redis/"/>
    <updated>2015-01-22T09:59:13+08:00</updated>
    <id>http://winseliu.com/blog/2015/01/22/build-redis</id>
    <content type="html"><![CDATA[<h2>jemalloc</h2>

<p>默认make使用的libc，在内存方面会产生比较多的碎片。可以使用jemalloc要进行内存的分配管理。</p>

<p>如果报<code>make cc Command not found</code>，需要先安装gcc。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxvf redis-2.8.13.bin.tar.gz 
</span><span class='line'>cd redis-2.8.13
</span><span class='line'>cd deps/jemalloc/
</span><span class='line'># 用于产生h头文件
</span><span class='line'>./configure 
</span><span class='line'>
</span><span class='line'>cd redis-2.8.13
</span><span class='line'>make MALLOC=jemalloc
</span><span class='line'>src/redis-server </span></code></pre></td></tr></table></div></figure>


<p>查看jemalloc的include的内容如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@localhost jemalloc]$ cd include/jemalloc/
</span><span class='line'>internal/              jemalloc_defs.h.in     jemalloc_macros.h      jemalloc_mangle.h      jemalloc_mangle.sh     jemalloc_protos.h.in   jemalloc_rename.h      jemalloc.sh
</span><span class='line'>jemalloc_defs.h        jemalloc.h             jemalloc_macros.h.in   jemalloc_mangle_jet.h  jemalloc_protos.h      jemalloc_protos_jet.h  jemalloc_rename.sh  </span></code></pre></td></tr></table></div></figure>


<p>查看内存使用：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@localhost redis-2.8.13]$ src/redis-cli info
</span><span class='line'>...
</span><span class='line'># Memory
</span><span class='line'>used_memory:503576
</span><span class='line'>used_memory_human:491.77K
</span><span class='line'>used_memory_rss:2158592
</span><span class='line'>used_memory_peak:503576
</span><span class='line'>used_memory_peak_human:491.77K
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:4.29
</span><span class='line'>mem_allocator:jemalloc-3.6.0
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>redis在使用过程中，会产生碎片。重启以及libc和jemalloc的对比如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 运行中实例
</span><span class='line'># Memory
</span><span class='line'>used_memory:4623527744
</span><span class='line'>used_memory_human:4.31G
</span><span class='line'>used_memory_rss:48304705536
</span><span class='line'>used_memory_peak:38217543280
</span><span class='line'>used_memory_peak_human:35.59G
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:10.45
</span><span class='line'>mem_allocator:libc
</span><span class='line'>
</span><span class='line'>51616 hadoop    20   0 45.1g  44g 1136 S  0.0 35.7   3410:42 /home/hadoop/redis-2.8.13/src/redis-server *:6371
</span><span class='line'>
</span><span class='line'># 序列化为rdb的文件大小
</span><span class='line'>[hadoop@hadoop-master1 18111]$ ll
</span><span class='line'>总用量 1183116
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1210319541 1月  14 11:28 dump.rdb
</span><span class='line'>
</span><span class='line'># 重启后的实例
</span><span class='line'>[hadoop@hadoop-master1 18111]$  ~/redis-2.8.13/src/redis-server --port 18111
</span><span class='line'>[77484] 14 Jan 14:33:17.910 * DB loaded from disk: 218.337 seconds
</span><span class='line'>
</span><span class='line'># Memory
</span><span class='line'>used_memory:4763158704
</span><span class='line'>used_memory_human:4.44G
</span><span class='line'>used_memory_rss:6217580544
</span><span class='line'>used_memory_peak:4763158704
</span><span class='line'>used_memory_peak_human:4.44G
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:1.31
</span><span class='line'>mem_allocator:libc
</span><span class='line'>
</span><span class='line'>77484 hadoop    20   0 6052m 5.8g 1200 S  0.0  4.6   3:38.39 /home/hadoop/redis-2.8.13/src/redis-server *:18111
</span><span class='line'>
</span><span class='line'># 使用jemalloc替换libc的实例
</span><span class='line'>[hadoop@hadoop-master1 18111]$ ~/redis-jemalloc/redis-2.8.13/src/redis-server --port 18888
</span><span class='line'>[14793] 14 Jan 14:50:11.250 * DB loaded from disk: 209.839 seconds
</span><span class='line'>
</span><span class='line'># Memory
</span><span class='line'>used_memory:4527760088
</span><span class='line'>used_memory_human:4.22G
</span><span class='line'>used_memory_rss:4625887232
</span><span class='line'>used_memory_peak:4527760088
</span><span class='line'>used_memory_peak_human:4.22G
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:1.02
</span><span class='line'>mem_allocator:jemalloc-3.6.0
</span><span class='line'>
</span><span class='line'>14793 hadoop    20   0 4538m 4.3g 1360 S  0.0  3.4   3:28.10 /home/hadoop/redis-jemalloc/redis-2.8.13/src/redis-server *:18888                                                                                                                       </span></code></pre></td></tr></table></div></figure>


<h2>tcmalloc</h2>

<ul>
<li>root安装</li>
</ul>


<p>如果有root用户的话操作比较简单。现在<a href="https://code.google.com/p/gperftools/">gperftools</a>和<a href="http://download.savannah.gnu.org/releases/libunwind/libunwind-0.99-beta.tar.gz">libunwind-0.99-beta</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd libunwind-0.99-beta
</span><span class='line'>./configure 
</span><span class='line'>make && make install
</span><span class='line'>cd /home/hadoop/gperftools-2.4
</span><span class='line'>./configure 
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>cd redis-2.8.13
</span><span class='line'>make MALLOC=tcmalloc</span></code></pre></td></tr></table></div></figure>


<p>如果出现<strong>./libtool: line 1125: g++: command not found</strong>的错误，缺少编译环境；</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost gperftools-2.4]# yum -y install gcc+ gcc-c++</span></code></pre></td></tr></table></div></figure>


<p>编译后，运行报错<strong>src/redis-server: error while loading shared libraries: libtcmalloc.so.4: cannot open shared object file: No such file or directory</strong>，需要配置环境变量：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@localhost redis-2.8.13]$ export LD_LIBRARY_PATH=/usr/local/lib
</span><span class='line'>[hadoop@localhost redis-2.8.13]$ src/redis-server </span></code></pre></td></tr></table></div></figure>


<p>或者按照网上的做法：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>echo "/usr/local/lib" &gt; /etc/ld.so.conf.d/usr_local_lib.conf  
</span><span class='line'>/sbin/ldconfig  </span></code></pre></td></tr></table></div></figure>


<p>检查tcmalloc是否生效<code>lsof -n | grep tcmalloc</code>，出现以下信息说明生效</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>redis-ser 1716    hadoop  mem       REG  253,0  2201976  936349 /usr/local/lib/libtcmalloc.so.4.2.6</span></code></pre></td></tr></table></div></figure>


<p>修改配置文件找到daemonize，将后面的no改为yes，让其可以以服务方式运行。</p>

<ul>
<li>普通用户安装</li>
</ul>


<p>考虑到可以各台机器上面复制，指定编译目录这种方式会比较方便。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd libunwind-0.99-beta
</span><span class='line'>CFLAGS=-fPIC ./configure --prefix=/home/hadoop/redis
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>cd gperftools-2.4
</span><span class='line'>./configure -h
</span><span class='line'>export LDFLAGS="-L/home/hadoop/redis/lib"
</span><span class='line'>export CPPFLAGS="-I/home/hadoop/redis/include"
</span><span class='line'>./configure --prefix=/home/hadoop/redis
</span><span class='line'>make && make install</span></code></pre></td></tr></table></div></figure>


<p>编译好后，把东西redis目录内容移到redis-2.8.13/src下。然后修改src/Makefile：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 redis-2.8.13]$ vi src/Makefile
</span><span class='line'># Include paths to dependencies
</span><span class='line'>FINAL_CFLAGS+= -I../deps/hiredis -I../deps/linenoise -I../deps/lua/src
</span><span class='line'>    
</span><span class='line'>ifeq ($(MALLOC),tcmalloc)
</span><span class='line'>        #FINAL_CFLAGS+= -DUSE_TCMALLOC
</span><span class='line'>        #FINAL_LIBS+= -ltcmalloc
</span><span class='line'>        FINAL_CFLAGS+= -DUSE_TCMALLOC -I./include
</span><span class='line'>        FINAL_LIBS+= -L./lib  -ltcmalloc -ldl
</span><span class='line'>
</span><span class='line'>endif
</span><span class='line'>
</span><span class='line'>ifeq ($(MALLOC),tcmalloc_minimal)
</span><span class='line'>        FINAL_CFLAGS+= -DUSE_TCMALLOC
</span><span class='line'>        FINAL_LIBS+= -ltcmalloc_minimal
</span><span class='line'>endif</span></code></pre></td></tr></table></div></figure>


<p>然后编译：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 redis-2.8.13]$ export LD_LIBRARY_PATH=/home/hadoop/redis-2.8.13/src/lib
</span><span class='line'>[hadoop@master1 redis-2.8.13]$ make MALLOC=tcmalloc
</span><span class='line'>cd src && make all
</span><span class='line'>make[1]: Entering directory `/home/hadoop/redis-2.8.13/src'
</span><span class='line'>    LINK redis-server
</span><span class='line'>    INSTALL redis-sentinel
</span><span class='line'>    CC redis-cli.o
</span><span class='line'>In file included from zmalloc.h:40,
</span><span class='line'>                 from redis-cli.c:50:
</span><span class='line'>./include/google/tcmalloc.h:35:2: warning: #warning is a GCC extension
</span><span class='line'>./include/google/tcmalloc.h:35:2: warning: #warning "google/tcmalloc.h is deprecated. Use gperftools/tcmalloc.h instead"
</span><span class='line'>    LINK redis-cli
</span><span class='line'>    CC redis-benchmark.o
</span><span class='line'>In file included from zmalloc.h:40,
</span><span class='line'>                 from redis-benchmark.c:47:
</span><span class='line'>./include/google/tcmalloc.h:35:2: warning: #warning is a GCC extension
</span><span class='line'>./include/google/tcmalloc.h:35:2: warning: #warning "google/tcmalloc.h is deprecated. Use gperftools/tcmalloc.h instead"
</span><span class='line'>    LINK redis-benchmark
</span><span class='line'>    CC redis-check-dump.o
</span><span class='line'>    LINK redis-check-dump
</span><span class='line'>    CC redis-check-aof.o
</span><span class='line'>    LINK redis-check-aof
</span><span class='line'>
</span><span class='line'>Hint: To run 'make test' is a good idea ;)
</span><span class='line'>
</span><span class='line'>make[1]: Leaving directory `/home/hadoop/redis-2.8.13/src'
</span><span class='line'>[hadoop@master1 redis-2.8.13]$ </span></code></pre></td></tr></table></div></figure>


<h2>redis3集群安装cluster</h2>

<p>编译安装和2.8一样，configuration/make/makeinstall即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 cluster-test]$ cat cluster.conf 
</span><span class='line'>port .
</span><span class='line'>cluster-enabled yes
</span><span class='line'>cluster-config-file nodes.conf
</span><span class='line'>cluster-node-timeout 5000
</span><span class='line'>appendonly yes</span></code></pre></td></tr></table></div></figure>


<p>比较苦逼的是需要安装ruby，服务器不能上网！其实ruby在能访问的机器上面安装就可以了！初始化集群的脚本其实就是客户端连接服务端，初始化集群而已。
还有就是在调用命令的时刻要加上<code>-c</code>，这样才是使用集群模式，不然仅仅连单机，读写其他集群服务会报错！</p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVx5-Ab4jaAAA9_Lg7l-I862.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVyXSAC5iEAABfrrHCfuI114.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVzBSAc3KOAADvQfFIPrs908.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCc-AXZ3EAAHoKZnb1nQ426.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCkWAZYuLAAAWM5VoXJI861.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCuSAAuXxAABB-LpH1nQ340.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/05/5F/wKhkA1PZBrSAPaMTAAAcSnjmhXE093.png" alt="" /></p>

<h2>Cygwin</h2>

<p>开发环境系统都是在windows，想调试一步步的看源码就得编译下redis。由于cygwin环境，模拟的linux，有部分的变量没有定义，需要进行修改。修改如下:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git log -1
</span><span class='line'>commit 0c211a1953afeda3d0d45126653e2d4c38bd88cb
</span><span class='line'>Author: antirez &lt;antirez@gmail.com&gt;
</span><span class='line'>Date:   Fri Dec 5 10:51:09 2014 +010
</span><span class='line'>
</span><span class='line'>$ git branch
</span><span class='line'>* 2.8
</span><span class='line'>
</span><span class='line'>$ git diff
</span><span class='line'>diff --git a/deps/hiredis/net.c b/deps/hiredis/net.c
</span><span class='line'>index bdb84ce..6e95f22 100644
</span><span class='line'>--- a/deps/hiredis/net.c
</span><span class='line'>+++ b/deps/hiredis/net.c
</span><span class='line'>@@ -51,6 +51,13 @@
</span><span class='line'> #include "net.h"
</span><span class='line'> #include "sds.h"
</span><span class='line'>
</span><span class='line'>+/* Cygwin Fix */
</span><span class='line'>+#ifdef __CYGWIN__
</span><span class='line'>+#define TCP_KEEPCNT 8
</span><span class='line'>+#define TCP_KEEPINTVL 150
</span><span class='line'>+#define TCP_KEEPIDLE 14400
</span><span class='line'>+#endif
</span><span class='line'>+
</span><span class='line'> /* Defined in hiredis.c */
</span><span class='line'> void __redisSetError(redisContext *c, int type, const char *str);
</span><span class='line'>
</span><span class='line'>diff --git a/src/Makefile b/src/Makefile
</span><span class='line'>index 8b3e959..a72b2f2 100644
</span><span class='line'>--- a/src/Makefile
</span><span class='line'>+++ b/src/Makefile
</span><span class='line'>@@ -63,6 +63,9 @@ else
</span><span class='line'> ifeq ($(uname_S),Darwin)
</span><span class='line'>        # Darwin (nothing to do)
</span><span class='line'> else
</span><span class='line'>+ifeq ($(uname_S),CYGWIN_NT-6.3-WOW64)
</span><span class='line'>+       # cygwin (nothing to do)
</span><span class='line'>+else
</span><span class='line'> ifeq ($(uname_S),AIX)
</span><span class='line'>         # AIX
</span><span class='line'>         FINAL_LDFLAGS+= -Wl,-bexpall
</span><span class='line'>@@ -75,6 +78,7 @@ else
</span><span class='line'> endif
</span><span class='line'> endif
</span><span class='line'> endif
</span><span class='line'>+endif
</span><span class='line'> # Include paths to dependencies
</span><span class='line'> FINAL_CFLAGS+= -I../deps/hiredis -I../deps/linenoise -I../deps/lua/src
</span></code></pre></td></tr></table></div></figure>


<p>然后编译：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd deps/
</span><span class='line'>make lua hiredis linenoise
</span><span class='line'>
</span><span class='line'>cd ..
</span><span class='line'>make</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>编译成功后，把程序导入eclipse CDT环境进行运行调试。导入后需要重新构建一下，不然调试的时刻会按照/cygwin的路径来查找源码。</p>

<ul>
<li>Import，然后选择C/C++目录下的[Existing Code as Makefile project]</li>
<li>在[Existing Code Location]填入redis程序对应的目录，在[Toolchain for Indexer Settings]选择<strong>Cygwin GCC</strong></li>
<li>导入完成后，右键选择[Build Configuration]->[Build All]</li>
<li>Run然后选择执行redis-server即可。</li>
</ul>


<p>好像也可以远程调试</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@Frankzfz]$gdbserver 10.27.10.48:9000 ./test_seg_fault</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://blog.sina.com.cn/s/blog_71954d8a0100nixe.html">tcp Keepalive</a></li>
<li><a href="http://jiangzhixiang123.blog.163.com/blog/static/2780206220115643822896/">setsockopt之 TCP_KEEPIDLE/TCP_KEEPINTVL/TCP_KEEPCNT - [Linux]</a></li>
<li><a href="http://blog.csdn.net/ce123_zhouwei/article/details/6625486">GDB+GdbServer: ARM程序调试</a></li>
<li><a href="http://my.oschina.net/shelllife/blog/167914">使用gdbserver远程调试</a></li>
<li><p><a href="http://qingfengju.com/article.asp?id=303">用gdb,gdbserver,eclipse+cdt在windows上远程调试linux程序</a></p></li>
<li><p><a href="http://www.cnblogs.com/kernel_hcy/archive/2011/05/15/2046963.html">redis源码分析（1）内存管理</a></p></li>
<li><a href="http://blog.csdn.net/unix21/article/details/12119059">利用TCMalloc替换Nginx和Redis默认glibc库的malloc内存分配</a>)</li>
<li><a href="http://blog.nosqlfan.com/html/3490.html">Redis采用不同内存分配器碎片率对比</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka快速入门]]></title>
    <link href="http://winseliu.com/blog/2015/01/08/kafka-guide/"/>
    <updated>2015-01-08T22:02:21+08:00</updated>
    <id>http://winseliu.com/blog/2015/01/08/kafka-guide</id>
    <content type="html"><![CDATA[<p>年前的时刻就听过kafka的大名，但是一直没有机会亲手尝试。数据写入HDFS然后再MapReduce去处理数据，这样会多出很多中间过程，浪费系统资源。实践下kafka+spark分析是否会更高效。首先了解kafka的基本操作。</p>

<p><a href="http://kafka.apache.org/documentation.html">文档</a>先进行简单的介绍。kafka是一个分布式的、分区的、冗余的日志服务，提供消息系统类似的功能。主要的概念： Topic，Producers，Consumers，Partition，Distribution（replicated）；producers通过TCP发送消息给Kafka集群，然后consumer从Kafka集群获取信息。</p>

<p>Kafka遵循：</p>

<ul>
<li>对于同一个生产者产生的消息有序。</li>
<li>消费者看到的消息顺序和消息存储的顺序一致</li>
<li>一个主题冗余为N的，可以容忍N-1个服务器失败而不会丢失任何消息。</li>
</ul>


<p>下载<a href="http://kafka.apache.org/downloads.html">kafka</a>，当前稳定版本为<a href="https://archive.apache.org/dist/kafka/0.8.1.1/RELEASE_NOTES.html">kafka_2.10-0.8.1.1</a>。下载后解压就可以运行了。</p>

<h2>启动单实例</h2>

<p>由于windows运行的程序放在<code>bin\windows</code>下面。需要对kafka-run-class.bat批处理文件进行稍稍修改：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>rem set BASE_DIR=%CD%\..
</span><span class='line'>set BASE_DIR=%CD%\..\..
</span><span class='line'>
</span><span class='line'>rem for %%i in (%BASE_DIR%\core\lib\*.jar) do (
</span><span class='line'>for %%i in (%BASE_DIR%\libs\*.jar) do (</span></code></pre></td></tr></table></div></figure>


<p>运行程序：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin\windows&gt;zookeeper-server-start.bat ..\..\config\zookeeper.properties
</span><span class='line'>
</span><span class='line'>rem 再打开一个cmd窗口运行
</span><span class='line'>bin\windows&gt;kafka-server-start.bat ..\..\config\server.properties
</span></code></pre></td></tr></table></div></figure>


<p>整合成一个脚本，方便以后使用：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>start zookeeper-server-start.bat ..\..\config\zookeeper.properties
</span><span class='line'>timeout 5
</span><span class='line'>start kafka-server-start.bat ..\..\config\server.properties
</span><span class='line'>exit</span></code></pre></td></tr></table></div></figure>


<h2>Topic</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand --create --zookeeper localhost:2181 --replication 1 --partitions 1 --topic hello
</span><span class='line'>Created topic "hello".
</span><span class='line'>
</span><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand --list --zookeeper localhost:2181
</span><span class='line'>hello
</span><span class='line'>
</span><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand  --describe --zookeeper localhost:2181 --topic hello
</span><span class='line'>Topic:hello     PartitionCount:1        ReplicationFactor:1     Configs:
</span><span class='line'>        Topic: hello    Partition: 0    Leader: 0       Replicas: 0     Isr: 0</span></code></pre></td></tr></table></div></figure>


<p>如果是在linux下，可以运行kafka-topics.sh来创建和查询。如果觉得打印的日志很不爽，可以修改config目录下的log4j.properties（在脚本中通过环境变量log4j.configuration指定为该文件）。</p>

<h2>发送接受消息</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>rem 生产者
</span><span class='line'>bin\windows&gt;kafka-console-producer.bat --broker-list localhost:9092 --topic hello
</span><span class='line'>
</span><span class='line'>rem 消费者（新开一个窗口）
</span><span class='line'>bin\windows&gt;kafka-console-consumer.bat --zookeeper localhost:2181 --topic hello --from-beginning</span></code></pre></td></tr></table></div></figure>


<p>都启动后，在producer的窗口输入信息。同一时刻，consumer也会打印输入的内容。</p>

<p>这个两个命令都有很多参数，直接输入命令不加任何参数可以输出帮助，了解各个参数的含义及其用法。</p>

<h2>Kafka集群</h2>

<p>集群的配置和zookeeper的集群配置方式很类似。只要修改broker.id和数据存储目录即可。</p>

<p>拷贝server.properties，然后修改下面的三个属性：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>broker.id=1
</span><span class='line'>port=9192
</span><span class='line'>log.dir=/tmp/kafka-logs-1</span></code></pre></td></tr></table></div></figure>


<p>然后启动：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set JMX_PORT=19999
</span><span class='line'>start kafka-server-start.bat ..\..\config\server-1.properties
</span><span class='line'>set JMX_PORT=29999
</span><span class='line'>start kafka-server-start.bat ..\..\config\server-2.properties
</span><span class='line'>set JMX_PORT=39999
</span><span class='line'>start kafka-server-start.bat ..\..\config\server-3.properties</span></code></pre></td></tr></table></div></figure>


<p>创建Topic</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic mhello
</span><span class='line'>Created topic "mhello".
</span><span class='line'>
</span><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand --describe --zookeeper localhost:2181 --topic mhello
</span><span class='line'>Topic:mhello    PartitionCount:1        ReplicationFactor:3     Configs:
</span><span class='line'>        Topic: mhello   Partition: 0    Leader: 0       Replicas: 0,3,1 Isr: 0,3,1
</span><span class='line'>
</span><span class='line'>bin\windows&gt;kafka-console-producer.bat --broker-list localhost:9092 --topic mhello
</span><span class='line'>bin\windows&gt;kafka-console-consumer.bat --zookeeper localhost:2181 --topic mhello --from-beginning
</span><span class='line'>      </span></code></pre></td></tr></table></div></figure>


<p>描述命令的第一行是所有分区的概要信息，接下来的每一行是每一个分区的信息。Leader后面的数字表示对应的broker-id的进程为当前分区的主节点，后面的Replicas是数据分布的情况（不管数据存在与否），Isr是当前存活的节点的数据分布情况。</p>

<p>把刚刚启动的1，2，3的节点都停掉，再查描述信息。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin\windows&gt;kafka-run-class.bat kafka.admin.TopicCommand --describe --zookeeper localhost:2181 --topic mhello
</span><span class='line'>Topic:mhello    PartitionCount:1        ReplicationFactor:3     Configs:
</span><span class='line'>        Topic: mhello   Partition: 0    Leader: 0       Replicas: 0,3,1 Isr: 0
</span><span class='line'>
</span><span class='line'>bin\windows&gt;kafka-console-consumer.bat --zookeeper localhost:2181 --topic mhello --from-beginning
</span><span class='line'>hello1
</span><span class='line'>hello2
</span><span class='line'>hello3        </span></code></pre></td></tr></table></div></figure>


<p>只要有一个节点存在，获取数据都没有问题。如果全部停了，就不能提供服务，但是查询describe命令，显示的还是0，囧！！</p>

<p>开启1，2，3节点后，mhello分区的状态：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Topic:mhello    PartitionCount:1        ReplicationFactor:3     Configs:
</span><span class='line'>        Topic: mhello   Partition: 0    Leader: 3       Replicas: 0,3,1 Isr: 3,1</span></code></pre></td></tr></table></div></figure>


<p>问题：当broker-id修改后，原来的数据，并不能透明的过渡。把broker-id为0的节点修改为1000，然后重启。mhello的数据仍然找不到。再次改回0，存活节点才都回来。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    Topic: mhello   Partition: 0    Leader: 3       Replicas: 0,3,1 Isr: 3,1,0</span></code></pre></td></tr></table></div></figure>


<h2>小结</h2>

<p>把基本的功能操作了一遍，都是使用命令行操作，接下来学习下和<a href="https://github.com/linkedin/camus/">hadoop结合</a>，使用java-api来操作Kafka。</p>

<h2>参考</h2>

<ul>
<li><a href="http://kafka.apache.org/documentation.html#gettingStarted">kafka gettingStarted</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redis维护]]></title>
    <link href="http://winseliu.com/blog/2014/12/31/redis-operations/"/>
    <updated>2014-12-31T23:14:57+08:00</updated>
    <id>http://winseliu.com/blog/2014/12/31/redis-operations</id>
    <content type="html"><![CDATA[<p>在使用过程中，接触最多的就是它的commands。除了string/hashmap/set/sortedlist的基本使用方式外，下面总结平时会经常使用的命令：</p>

<h2>启动，客户端连接</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# nohup src/redis-server --port 6370 &
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# src/redis-cli -p 6370
</span><span class='line'>127.0.0.1:6370&gt; </span></code></pre></td></tr></table></div></figure>


<h2>获取redis的整体状态</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; info
</span><span class='line'>...
</span><span class='line'># Memory
</span><span class='line'>used_memory:1415161160
</span><span class='line'>used_memory_human:1.32G
</span><span class='line'>used_memory_rss:0
</span><span class='line'>used_memory_peak:1415161160
</span><span class='line'>used_memory_peak_human:1.32G
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:0.00
</span><span class='line'>mem_allocator:jemalloc-3.6.0
</span><span class='line'>...
</span><span class='line'># CPU
</span><span class='line'>used_cpu_sys:52.47
</span><span class='line'>used_cpu_user:10.07
</span><span class='line'>used_cpu_sys_children:0.00
</span><span class='line'>used_cpu_user_children:0.00
</span><span class='line'>
</span><span class='line'># Keyspace
</span><span class='line'>db0:keys=4253125,expires=0,avg_ttl=0</span></code></pre></td></tr></table></div></figure>


<p>列出的信息，包括了版本、内存/CPU使用、请求数、键值对等信息。通过这些基本了解redis运行情况。</p>

<h2>清空数据库</h2>

<p>对于数据量少的情况下，可以使用flushall来清理记录。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; set abc 1234
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6370&gt; keys *
</span><span class='line'>1) "abc"
</span><span class='line'>127.0.0.1:6370&gt; flushall
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6370&gt; keys *
</span><span class='line'>(empty list or set)</span></code></pre></td></tr></table></div></figure>


<p>数据量大的情况不建议使用flushall，可以直接把rdb数据文件干掉，然后重启redis服务就可以了（找不到数据文件后，就是一个新的库）。</p>

<h2>随机获取一个键</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; mset a 1 b 2 c 3 d 4 e 5 f 6 
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"a"
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"f"
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"e"
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"a"</span></code></pre></td></tr></table></div></figure>


<h2>遍历获取键值</h2>

<p>一般情况下，我们会使用<code>keys PATTERN</code>来查找匹配的键值。但是，如果数据量很大，keys操作会很消耗系统资源，<code>stop the world</code>的事情不是我们想看到的！此时，可以通过scan/hscan/zscan/ssan命令依次获取。</p>

<ul>
<li>获取库中的键值</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; eval "for i=1,100000 do redis.call('set', 'a' .. i, i) end" 0
</span><span class='line'>(nil)
</span><span class='line'>(0.98s)</span></code></pre></td></tr></table></div></figure>


<p>正式环境我们无法预估匹配的键的数量，一根筋的使用keys命令可能并不明智。如果数据量很多，等不到结束应该就会ctrl+c了。这种情况下，可以使用scan命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; scan 0 match ismi:domain:*.upaiyun.com
</span><span class='line'>1) "6553600"
</span><span class='line'>2) 1) "ismi:domain:KunMing:1415646303170928524.test.b0.upaiyun.com"
</span><span class='line'>   2) "ismi:domain:KunMing:1415392926002699280.test.b0.upaiyun.com"
</span><span class='line'>   3) "ismi:domain:KunMing:141489373375899237.test.b0.upaiyun.com"
</span><span class='line'>127.0.0.1:6370&gt; scan 0 match ismi:domain:*.upaiyun.com count 10
</span><span class='line'>1) "6553600"
</span><span class='line'>2) 1) "ismi:domain:KunMing:1415646303170928524.test.b0.upaiyun.com"
</span><span class='line'>   2) "ismi:domain:KunMing:1415392926002699280.test.b0.upaiyun.com"
</span><span class='line'>   3) "ismi:domain:KunMing:141489373375899237.test.b0.upaiyun.com"
</span></code></pre></td></tr></table></div></figure>


<p>Basically with COUNT the user specified the amount of work that should be done at every call in order to retrieve elements from the collection. This is just an hint for the implementation, however generally speaking this is what you could expect most of the times from the implementation.</p>

<p>COUNT数值的意思应该是匹配操作的次数，而不是查询结果的个数。通过和<code>scan 0</code>对比可以得出来。</p>

<p>同理，对于set（smembers）可以使用sscan，sortedlist可以使用zcan等。</p>

<h2>lua脚本</h2>

<p>redis内置的脚本语言，直接使用脚本可以减少客户端和服务端连接（多次请求）的压力。例如要批量删除一些键值：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>src/redis-cli keys 'v2:*' | awk '{print $1}' | while read line; do src/redis-cli del $line ; done</span></code></pre></td></tr></table></div></figure>


<p>先获取匹配的key，然后使用shell再次调用redis的客户端进行删除。表面上看起来没啥问题，如果匹配的key很多，会产生很多的tcp连接，占用redis服务器的端口！最终端口不够用，请求报错。</p>

<p>此时，如果使用lua脚本的方式，就可以轻松处理。无需考虑端口等问题。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 量少时可以使用
</span><span class='line'>eval "local aks=redis.call('keys', 'v2:*'); if #aks &gt;0 then redis.call('del', unpack(aks)) end" 0
</span><span class='line'>
</span><span class='line'># 优美
</span><span class='line'>eval "local aks=redis.call('keys', 'v2:*'); for _,r in ipairs(aks) do redis.call('del', r) end" 0</span></code></pre></td></tr></table></div></figure>


<p>当然，如果键不多，还可以使用一次性全部删除：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>src/redis-cli -p $PORT del `~/redis-2.8.13/src/redis-cli -p $PORT 'keys' 'v2:*' | grep -v 'v2:ci:' | grep -v 'v2:ff' | grep -v "$(date +%Y-%m-%d)" | grep -v "$(date +%Y-%m-%d -d '-1 day')"`</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop查看作业状态Rest接口]]></title>
    <link href="http://winseliu.com/blog/2014/12/07/hadoop-mr-rest-api/"/>
    <updated>2014-12-07T10:09:49+08:00</updated>
    <id>http://winseliu.com/blog/2014/12/07/hadoop-mr-rest-api</id>
    <content type="html"><![CDATA[<p>hadoop yarn提供了web端查看任务状态，同时可以通过rest的方式获取任务的相关信息。rest接口和网页端的每个界面一一对应。</p>

<p><img src="http://file.bmob.cn/M00/D7/C0/oYYBAFSDxeaARA6AAAenwsLMShM027.png" alt="" /></p>

<p>上面的5个图的链接为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>http://hadoop-master1:8088/cluster/apps/RUNNING
</span><span class='line'>http://hadoop-master1:8088/cluster/app/application_1417676507722_1846
</span><span class='line'>http://hadoop-master1:8088/proxy/application_1417676507722_1846/
</span><span class='line'>http://hadoop-master1:8088/proxy/application_1417676507722_1846/mapreduce/job/job_1417676507722_1846
</span><span class='line'>http://hadoop-master1:19888/jobhistory/job/job_1417676507722_1846/mapreduce/job/job_1417676507722_1846</span></code></pre></td></tr></table></div></figure>


<h2>查看正在运行的任务</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://hadoop-master1:8088/ws/v1/cluster/apps?states=RUNNING
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/info
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/counters
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/conf
</span></code></pre></td></tr></table></div></figure>


<p>如果上面的任务是已经完成的，获取对应的信息时返回的值是空的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/counters</span></code></pre></td></tr></table></div></figure>


<h2>查看执行完成的任务</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://hadoop-master1:19888/ws/v1/history
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/info
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs?startedTimeBegin=$(date +%s -d '-1 hour')000
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867/counters
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867/conf
</span><span class='line'>
</span><span class='line'>curl -H "Accept: application/xml" "http://hadoop-master1:8088/ws/v1/cluster/apps?states=FINISHED&limit=1" | xmllint --format - </span></code></pre></td></tr></table></div></figure>


<p>后面的参数和运行任务一致，只是提供服务不同。</p>

<h2>xml转csv</h2>

<p><img src="http://file.bmob.cn/M00/D7/D5/oYYBAFSEHuKAaAgHAACb9_KkMEU331.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -H "Accept: application/xml" "http://hadoop-master1:8088/ws/v1/cluster/apps?startedTimeBegin=$(date +%s -d '-1 hour')000" 2&gt;/dev/null | xsltproc yarn.xslt -  | sort -r
</span><span class='line'>
</span><span class='line'>application_1417676507722_1973,AccessLogOnlyHiveJob,RUNNING,UNDEFINED,1417942144941,0,19416
</span><span class='line'>application_1417676507722_1972,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417942084278,1417942098184,13906
</span><span class='line'>application_1417676507722_1971,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417941603456,1417941617773,14317
</span><span class='line'>application_1417676507722_1970,AccessLogOnlyHiveJob,FINISHED,SUCCEEDED,1417941581080,1417942142287,561207
</span><span class='line'>application_1417676507722_1969,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417941422664,1417941436456,13792</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Hadoop YARN - Introduction to the web services REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">ResourceManager REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/MapredAppMasterRest.html">MapReduce Application Master REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html">History Server REST API&rsquo;s.</a></li>
<li><a href="http://blog.csdn.net/wypblog/article/details/21159795">Hadoop YARN中web服务的REST API介绍</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mysql分区]]></title>
    <link href="http://winseliu.com/blog/2014/11/14/mysql-partition/"/>
    <updated>2014-11-14T15:05:50+08:00</updated>
    <id>http://winseliu.com/blog/2014/11/14/mysql-partition</id>
    <content type="html"><![CDATA[<p>Windows8 Mysql安装后数据默认放在<code>C:\ProgramData\MySQL\MySQL Server 5.6\data</code>下。</p>

<blockquote><p>2、MyISAM数据库表文件：
.MYD文件：即MY Data，表数据文件
.MYI文件：即MY Index，索引文件
.log文件：日志文件</p>

<p>3、InnoDB采用表空间（tablespace）来管理数据，存储表数据和索引，
InnoDB数据库文件（即InnoDB文件集，ib-file set）：
  ibdata1、ibdata2等：系统表空间文件，存储InnoDB系统信息和用户数据库表数据和索引，所有表共用
  .ibd文件：单表表空间文件，每个表使用一个表空间文件（file per table），存放用户数据库表数据和索引
  日志文件： ib_logfile1、ib_logfile2</p></blockquote>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>create database hello;
</span><span class='line'>use hello;
</span><span class='line'>create table abc ( name varchar(1000), age int );
</span><span class='line'>insert into abc values ("1", 1);
</span><span class='line'>
</span><span class='line'>create table abc_myisam ( name varchar(100), age int ) engine=myisam;
</span><span class='line'>insert into abc_myisam values ( '1', 1), ('2',2);
</span><span class='line'>alter table abc_myisam partition by hash(age) partitions 4 ;
</span><span class='line'>
</span><span class='line'>insert into abc_myisam values ( '11', 10), ('2',20), ( '1', 11), ('2',21), ( '1', 21), ('2',22), ( '1', 31), ('2',32), ( '1', 41), ('2',24), ( '1', 15), ('2',23) ;</span></code></pre></td></tr></table></div></figure>


<p>最终库目录如下:</p>

<p><img src="http://file.bmob.cn/M00/D2/16/oYYBAFRlrMaAAAdoAADDsNJhdNs617.png" alt="" /></p>

<p>根据月份来进行分区：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>--最好按照月份分区(date需要为日期类型)
</span><span class='line'>alter table abc_myisam PARTITION BY RANGE (extract(YEAR_MONTH from date)) (  
</span><span class='line'>    PARTITION p410 VALUES LESS THAN (201411),  
</span><span class='line'>    PARTITION p411 VALUES LESS THAN (201412),  
</span><span class='line'>    PARTITION p412 VALUES LESS THAN (201501),  
</span><span class='line'>  PARTITION p501 VALUES LESS THAN (201502), 
</span><span class='line'>  PARTITION p502 VALUES LESS THAN (201503), 
</span><span class='line'>  PARTITION p503 VALUES LESS THAN (201504), 
</span><span class='line'>  PARTITION p504 VALUES LESS THAN (201505), 
</span><span class='line'>  PARTITION p505 VALUES LESS THAN (201506), 
</span><span class='line'>    PARTITION p0 VALUES LESS THAN MAXVALUE  
</span><span class='line'>)</span></code></pre></td></tr></table></div></figure>


<p>根据日期来分区：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>alter table t_dta_activeresources_ip PARTITION BY RANGE (to_days(day)) (  
</span><span class='line'>    PARTITION p0 VALUES LESS THAN (735926),  
</span><span class='line'>PARTITION p141124 VALUES LESS THAN (735927),
</span><span class='line'>PARTITION p141125 VALUES LESS THAN (735928),
</span><span class='line'>PARTITION p141126 VALUES LESS THAN (735929),
</span><span class='line'>PARTITION p88 VALUES LESS THAN MAXVALUE  
</span><span class='line'>)</span></code></pre></td></tr></table></div></figure>


<p>查询时执行计划带上partitions可以查看命中的是那个分区：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mysql&gt; explain select * from t_dta_illegalweb where day='2015-01-04';
</span><span class='line'>+----+-------------+------------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>| id | select_type | table            | type | possible_keys | key  | key_len | ref  | rows    | Extra       |
</span><span class='line'>+----+-------------+------------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>|  1 | SIMPLE      | t_dta_illegalweb | ALL  | NULL          | NULL | NULL    | NULL | 1335432 | Using where |
</span><span class='line'>+----+-------------+------------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>1 row in set
</span><span class='line'>
</span><span class='line'>mysql&gt; explain partitions
</span><span class='line'> select * from t_dta_illegalweb where day='2015-01-04';
</span><span class='line'>+----+-------------+------------------+------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>| id | select_type | table            | partitions | type | possible_keys | key  | key_len | ref  | rows    | Extra       |
</span><span class='line'>+----+-------------+------------------+------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>|  1 | SIMPLE      | t_dta_illegalweb | p150104    | ALL  | NULL          | NULL | NULL    | NULL | 1335432 | Using where |
</span><span class='line'>+----+-------------+------------------+------------+------+---------------+------+---------+------+---------+-------------+
</span><span class='line'>1 row in set</span></code></pre></td></tr></table></div></figure>


<p>如果清理掉分区的数据后，再查看执行计划：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mysql&gt; alter table t_dta_illegalweb truncate partition p150104;
</span><span class='line'>Query OK, 0 rows affected
</span><span class='line'>
</span><span class='line'>mysql&gt; explain partitions select * from t_dta_illegalweb where day='2015-01-04';
</span><span class='line'>+----+-------------+-------+------------+------+---------------+------+---------+------+------+-----------------------------------------------------+
</span><span class='line'>| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | Extra                                               |
</span><span class='line'>+----+-------------+-------+------------+------+---------------+------+---------+------+------+-----------------------------------------------------+
</span><span class='line'>|  1 | SIMPLE      | NULL  | NULL       | NULL | NULL          | NULL | NULL    | NULL | NULL | Impossible WHERE noticed after reading const tables |
</span><span class='line'>+----+-------------+-------+------------+------+---------------+------+---------+------+------+-----------------------------------------------------+
</span><span class='line'>1 row in set</span></code></pre></td></tr></table></div></figure>


<h2>打开日志开关</h2>

<p>默认mysql是没有打开记录一般日志的开关的，可以通过命令行修改参数。对于查看具体执行了那些sql语句，调试很有帮助。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mysql&gt; set global general_log = 1;
</span><span class='line'>Query OK, 0 rows affected
</span><span class='line'>
</span><span class='line'>mysql&gt; SHOW  GLOBAL VARIABLES LIKE '%log%';</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://blog.csdn.net/yaotinging/article/details/6671506">MySQL数据文件介绍及存放位置</a></li>
<li><a href="http://lehsyh.iteye.com/blog/732719">MySQL的表分区</a></li>
<li><a href="http://lobert.iteye.com/blog/1955841">http://lobert.iteye.com/blog/1955841</a></li>
<li><a href="http://blog.csdn.net/jiao_fuyou/article/details/14214213">http://blog.csdn.net/jiao_fuyou/article/details/14214213</a></li>
<li><a href="http://database.51cto.com/art/201002/184392.htm">http://database.51cto.com/art/201002/184392.htm</a></li>
<li><a href="http://dev.mysql.com/doc/refman/5.5/en/error-log.html">mysql不重启清理日志</a></li>
<li><a href="http://pangge.blog.51cto.com/6013757/1319304">http://pangge.blog.51cto.com/6013757/1319304</a></li>
<li><a href="http://bbs.csdn.net/topics/70096519">http://bbs.csdn.net/topics/70096519</a></li>
<li><a href="http://bbs.csdn.net/topics/350138520">http://bbs.csdn.net/topics/350138520</a></li>
<li><a href="http://www.iteye.com/topic/408701">http://www.iteye.com/topic/408701</a></li>
<li><a href="http://www.blogjava.net/allrounder/articles/323591.html">http://www.blogjava.net/allrounder/articles/323591.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx服务配置]]></title>
    <link href="http://winseliu.com/blog/2014/11/13/nginx-serving-static-content/"/>
    <updated>2014-11-13T10:16:17+08:00</updated>
    <id>http://winseliu.com/blog/2014/11/13/nginx-serving-static-content</id>
    <content type="html"><![CDATA[<p>配置nginx作为网页快照的服务，需要理解好配置<code>root</code>的涵义！</p>

<h2>安装、启动</h2>

<p>首先安装，然后修改配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install nginx 
</span><span class='line'>
</span><span class='line'>less /etc/nginx/nginx.conf
</span><span class='line'>less /etc/nginx/conf.d/default.conf 
</span><span class='line'>
</span><span class='line'>service nginx restart</span></code></pre></td></tr></table></div></figure>


<p>实际操作中没有root，只能自己编译了：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>下载nginx，pcre-8.36.zip，zlib-1.2.3.tar.gz解压到src下。
</span><span class='line'>cd nginx-1.7.7
</span><span class='line'>./configure --prefix=/home/omc/tools/nginx --with-pcre=src/pcre --with-zlib=src/zlib
</span><span class='line'>make && make install
</span><span class='line'>
</span><span class='line'>cd /home/omc/tools/nginx
</span><span class='line'>vi conf/nginx.conf # 修改listen的端口，80要root才能起
</span><span class='line'>sbin/nginx
</span><span class='line'>sbin/nginx -s reload</span></code></pre></td></tr></table></div></figure>


<p>如果编译的目录和真正存放程序的路径不一致时，可以使用<code>-p</code>参数来指定。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd nginx
</span><span class='line'>sbin/nginx -p $PWD
</span><span class='line'>sbin/nginx -s reload -p $PWD</span></code></pre></td></tr></table></div></figure>


<h2>静态页面服务配置</h2>

<p>下面具体说说配置的涵义：</p>

<ul>
<li>root（不管在那个配置节点下）位置都对应 请求的根路径。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location /static {
</span><span class='line'>    root  /usr/share/static/html;
</span><span class='line'>    autoindex on;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>location / {
</span><span class='line'>    root   /usr/share/nginx/html;
</span><span class='line'>    index  index.html index.htm;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li>location的<code>/static</code>对应的是访问目录<code>/usr/share/static/html/static</code>下的内容，请求<code>/static/hello.html</code>对应到<code>/usr/share/static/html/static/hello.html</code>。也就是说节点下的root目录 对应 的是 访问地址的<code>/</code>。</li>
<li>autoindex可以用于list列出目录内容。</li>
</ul>


<p><img src="http://file.bmob.cn/M00/05/49/ooYBAFRkHkmAe3wcAACCDsZ0Oc8983.png" alt="" /></p>

<p>配置了两个路径后，问题来了：如果<code>/usr/share/nginx/html/</code>也有目录static，那nginx会访问谁？<strong>nginx来先匹配配置，访问/static定位到<code>/usr/share/static/html</code></strong>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location /static {
</span><span class='line'>    root  /usr/share/static/html;
</span><span class='line'>  try_files $uri /static/404.html;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li>try_files可以设置默认页面，如<code>/usr/share/static/html/static</code>目录下不存在abc.html，那么会内部重定向到<code>/static/404.html</code>。这里路径要<code>/static</code>下面。</li>
</ul>


<p>try_files还可以返回状态值，跳转到对应状态的页面：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location / {
</span><span class='line'>    try_files $uri $uri/ $uri.html =404;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/D1/D0/oYYBAFRkJ6eAc8UiAAEKid3ICHw052.png" alt="" /></p>

<p>如果try_files的所给出的地址不包括<code>$uri</code>时，请求会被重定向配置指向的新代理服务：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location / {
</span><span class='line'>    try_files $uri $uri/ @backend;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>location @backend {
</span><span class='line'>    proxy_pass http://backend.example.com;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>实践</h2>

<p>在实际操作遇到的不能访问的问题，配置本机的其他JavaWeb应用，但是在登录后，点其他链接总是跳转到登陆页。可以查看下真正请求的地址。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location /omc {
</span><span class='line'>      proxy_pass http://REAL-IP:9000/omc;
</span><span class='line'>      #proxy_pass http://localhost:9000/omc;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>填写localhost不能访问，但是填具体的外网IP时是可以访问的。查看后，在页面定义了<code>&lt;base href="${basedir}/&gt;</code>导致请求都跳转到localhost了。在客户端肯定就访问失败了。这个需要特别注意下。</p>

<p><img src="http://file.bmob.cn/M00/05/C3/ooYBAFRpn1qAKNKiAACUae7DmjY717.png" alt="" /></p>

<p>在特定的情况下，文件不一定是html后缀的（如：txt），如果要在浏览器解析html，需要配置content-type标题头。同时访问的url和真实存放的文件的路径有出处时，可以通过rewrite指令来进行适配。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>server {
</span><span class='line'>  ...
</span><span class='line'>    location /snapshot {
</span><span class='line'>        root   /home/ud/html-snapshot;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>        rewrite ^/snapshot/.*/(.*)$  /snapshot/$1   last;
</span><span class='line'>        try_files $uri $uri.html $uri.htm =404;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<h2>主备配置</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>upstream backend {
</span><span class='line'>    server backend1.example.com 
</span><span class='line'>    server backup1.example.com  backup;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>server {
</span><span class='line'>    location / {
</span><span class='line'>        proxy_pass http://backend;
</span><span class='line'>    }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li><a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#upstream">nginx upstream</a></li>
</ul>


<h2>防火墙跳转情况下nginx配置</h2>

<p>如在防火墙做了11111端口映射到9000端口，如果按照的配置，应用的redirect会被nginx转换为9000端口发给用户，而不是原始的用户访问的11111端口。导致不一致甚至不能访问。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  location ~ \.do$ {
</span><span class='line'>    proxy_pass              http://localhost:8080;
</span><span class='line'>    proxy_set_header        X-Real-IP $remote_addr;
</span><span class='line'>    proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
</span><span class='line'>    proxy_set_header        Host $http_host;
</span><span class='line'>  }                                                                                                       
</span><span class='line'>  location ~ \.jsp$ {
</span><span class='line'>    proxy_pass              http://localhost:8080;
</span><span class='line'>    proxy_set_header        X-Real-IP $remote_addr;
</span><span class='line'>    proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
</span><span class='line'>    proxy_set_header        Host $http_host;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<ul>
<li><a href="http://wiki.nginx.org/JavaServers">JavaServers</a></li>
<li><a href="http://wiki.nginx.org/LikeApache">Apache ProxyPassReverse</a></li>
<li><a href="http://wiki.nginx.org/NginxHttpProxyModule#proxy_pass">NginxHttpProxyModule</a></li>
</ul>


<h2>rewrite</h2>

<p>flag有两个last和break参数。last和break最大的不同在于</p>

<ul>
<li>break是终止当前location的rewrite检测,而且不再进行location匹配
– last是终止当前location的rewrite检测,但会继续重试location匹配并处理区块中的rewrite规则</li>
</ul>


<p>结果rewrite的结果重新命中了location /download/ 虽然这次并没有命中rewrite规则的正则表达式,但因为缺少终止rewrite的标志,其仍会不停重试download中rewrite规则直到达到10次上限返回HTTP 500。</p>

<p>配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>    location / {
</span><span class='line'>       root   html;
</span><span class='line'>
</span><span class='line'>rewrite ^/snapshot/[^\/]*/(.*)$  /snapshot/$1 last;
</span><span class='line'>       index  index.html index.htm;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<p>日志：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2015/03/13 11:53:42 [error] 32395#0: *17 rewrite or internal redirection cycle while processing "/snapshot/45/c7/2f/45c72f9a926d2b72b0c705a125d2764a.txt", client: 132.122.237.189, server: localhost, request: "GET //snapshot/1/2/3/4/5/6/7/8/9/10/11/45/c7/2f/45c72f9a926d2b72b0c705a125d2764a.txt HTTP/1.1", host: "umcc97-44:8888"
</span><span class='line'>
</span><span class='line'>2015/03/13 11:54:14 [error] 32395#0: *20 open() "/home/hadoop/nginx/html/snapshot/45c72f9a926d2b72b0c705a125d2764a.txt" failed (2: No such file or directory), client: 132.122.237.189, server: localhost, request: "GET //snapshot/1/2/3/4/5/45c72f9a926d2b72b0c705a125d2764a.txt HTTP/1.1", host: "umcc97-44:8888"</span></code></pre></td></tr></table></div></figure>


<p>10次以上就报错，少于10次是ok的。</p>

<ul>
<li><a href="http://www.cnblogs.com/dami520/archive/2012/08/16/2642967.html">Nginx Rewrite详解</a></li>
</ul>


<h2>参考</h2>

<ul>
<li><a href="http://nginx.com/resources/admin-guide/serving-static-content/">Serving Static Content</a></li>
<li><a href="http://nginx.com/resources/admin-guide/web-server/">NGINX Web Server</a></li>
<li><a href="http://www.cnblogs.com/cgli/archive/2011/05/16/2047920.html">nginx rewrite规则</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为github Pages页面设置自定义域名]]></title>
    <link href="http://winseliu.com/blog/2014/10/24/github-custom-domain/"/>
    <updated>2014-10-24T00:17:19+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/24/github-custom-domain</id>
    <content type="html"><![CDATA[<ol>
<li>注册个域名（net.cn）</li>
<li>添加CNAME文件（github.com）</li>
<li>添加解析记录（net.cn）</li>
</ol>


<p><img src="http://file.bmob.cn/M00/20/C5/wKhkA1RJKuyAf6lWAACIJ28IFe8161.png" alt="" /></p>

<p>如果是使用子域名的话非常简单。在（pages）CNAME文件中填写www.winseliu.com，然后在（net.cn）解析页添加CNAME指向winse.github.io即可。</p>

<p>如果想默认顶级域名也能访问，需要添加的两个ip指向，参见上图。同时（pages）CNAME中使用winseliu.com。</p>

<h2>参考</h2>

<ul>
<li><a href="https://help.github.com/articles/my-custom-domain-isn-t-working/">My custom domain isn&rsquo;t working</a></li>
<li><a href="https://help.github.com/articles/tips-for-configuring-an-a-record-with-your-dns-provider/">Tips for configuring an A record with your DNS provider</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dnsmasq解决docker集群节点互通问题]]></title>
    <link href="http://winseliu.com/blog/2014/10/18/modify-hosts-build-hadoop-cluster-on-docker/"/>
    <updated>2014-10-18T04:19:21+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/18/modify-hosts-build-hadoop-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>上个星期学习了一下docker，写了一个<a href="https://github.com/winse/docker-hadoop/tree/Pseudo-Distributed">伪分布式的Dockerfile</a>。</p>

<p>通过<code>--link</code>的方式master能访问slaver，毕竟slaver的相关信息已经被写入到master的hosts文件里面去了嘛！理所当然认为，直接把master的hosts文件全部复制一份到所有slaver节点问题就解决了。</p>

<p>等真正操作的时刻，发现不是那么回事，docker容器不给修改hosts文件！！</p>

<h2>错误实现</h2>

<p>首先，看下不当的操作：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -d --name slaver1 -h slaver1 hadoop
</span><span class='line'>[root@docker ~]# docker run -d --name slaver2 -h slaver2 hadoop
</span><span class='line'>[root@docker ~]# docker run -d --name master -h master --link slaver1:slaver1 --link slaver2:slaver2 hadoop
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
</span><span class='line'>dafc82678811        hadoop:latest       /bin/sh -c '/usr/sbi   40 seconds ago      Up 40 seconds       22/tcp              master
</span><span class='line'>86d2da5209c5        hadoop:latest       /bin/sh -c '/usr/sbi   49 seconds ago      Up 48 seconds       22/tcp              master/slaver2,slaver2
</span><span class='line'>7b9761fb05a8        hadoop:latest       /bin/sh -c '/usr/sbi   56 seconds ago      Up 55 seconds       22/tcp              master/slaver1,slaver1</span></code></pre></td></tr></table></div></figure>


<p>此时，通过<code>--link</code>连接方式，master的hosts中已经包括了slaver1和slaver2，按照正常的路子，登录master拷贝其hosts到slaver节点，一切就妥妥的了。现实是残酷的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-bash-4.1# scp /etc/hosts slaver1:/etc/
</span><span class='line'>scp: /etc//hosts: Read-only file system</span></code></pre></td></tr></table></div></figure>


<h2>DNS完美解决问题</h2>

<p>首先需要在宿主机器上安装dns服务器，bind不多说比较麻烦。这里参考网上人家解决方式，使用dnsmasq来搭建DNS服务器。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# yum install dnsmasq -y
</span><span class='line'>
</span><span class='line'>[root@docker ~]# cp /etc/resolv.conf /etc/resolv.dnsmasq.conf 
</span><span class='line'>[root@docker ~]# touch /etc/dnsmasq.hosts
</span><span class='line'>
</span><span class='line'>[root@docker ~]# vi /etc/resolv.conf
</span><span class='line'>[root@docker ~]# cat /etc/resolv.conf
</span><span class='line'>; generated by /sbin/dhclient-script
</span><span class='line'>nameserver 127.0.0.1 
</span><span class='line'>
</span><span class='line'>[root@docker ~]# vi /etc/dnsmasq.conf
</span><span class='line'>[root@docker ~]# cat /etc/dnsmasq.conf
</span><span class='line'>...
</span><span class='line'>resolv-file=/etc/resolv.dnsmasq.conf
</span><span class='line'>...
</span><span class='line'>addn-hosts=/etc/dnsmasq.hosts
</span><span class='line'>
</span><span class='line'>[root@docker ~]# service dnsmasq restart
</span><span class='line'>
</span><span class='line'>[root@docker ~]# dig www.baidu.com
</span><span class='line'>...
</span><span class='line'>;; SERVER: 127.0.0.1#53(127.0.0.1)
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>通过dig可以查看当前的DNS服务器你已经修改为localhost了。然后启动docker容器来搭建环境。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d  --dns 172.17.42.1 --name slaver1 -h slaver1 hadoop
</span><span class='line'>[root@docker ~]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver2 hadoop
</span><span class='line'>[root@docker ~]# docker run -d  --dns 172.17.42.1 --name master -h master hadoop
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
</span><span class='line'>f6e63b311e60        hadoop:latest       /bin/sh -c '/usr/sbi   6 seconds ago       Up 5 seconds        22/tcp              master
</span><span class='line'>454ae2c3e435        hadoop:latest       /bin/sh -c '/usr/sbi   13 seconds ago      Up 12 seconds       22/tcp              slaver2
</span><span class='line'>7698230a03fb        hadoop:latest       /bin/sh -c '/usr/sbi   21 seconds ago      Up 20 seconds       22/tcp              slaver1
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps | grep hadoop | awk '{print $1}' | xargs -I{} docker inspect -f '{{.NetworkSettings.IPAddress}} {{.Config.Hostname}}' {} &gt; /etc/dnsmasq.hosts
</span><span class='line'>[root@docker ~]# service dnsmasq restart
</span><span class='line'>
</span><span class='line'>[root@docker ~]# ssh hadoop@master
</span><span class='line'>hadoop@master's password: 
</span><span class='line'>[hadoop@master ~]$ ping slaver1
</span><span class='line'>PING slaver1 (172.17.0.9) 56(84) bytes of data.
</span><span class='line'>64 bytes from slaver1 (172.17.0.9): icmp_seq=1 ttl=64 time=1.79 ms
</span><span class='line'>...
</span><span class='line'>[hadoop@master ~]$ ping slaver2
</span><span class='line'>PING slaver2 (172.17.0.10) 56(84) bytes of data.
</span><span class='line'>64 bytes from slaver2 (172.17.0.10): icmp_seq=1 ttl=64 time=1.96 ms
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'></span></code></pre></td></tr></table></div></figure>


<p>节点互通后，后面的步骤都类似了，ssh无密钥通信，格式化namenode，启动等等。</p>

<h2>遇到的问题</h2>

<ul>
<li>一开始我把配置文件放在/root目录下，dnsmasq总是不起作用。最后放到/etc目录就可以，不知道啥子问题。</li>
<li>配置dns启动docker容器后，如果不起作用看下<code>/etc/resolv.conf</code>。如果互ping不同，去掉resolv的<code>search localhost</code>再试下。</li>
</ul>


<p>DNS可以正常工作的配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-bash-4.1# ping slaver
</span><span class='line'>PING slaver (172.17.0.7) 56(84) bytes of data.
</span><span class='line'>64 bytes from slaver (172.17.0.7): icmp_seq=1 ttl=64 time=0.095 ms
</span><span class='line'>
</span><span class='line'>-bash-4.1# cat /etc/resolv.conf 
</span><span class='line'>nameserver 172.17.42.1
</span><span class='line'>search localdomain
</span><span class='line'>
</span><span class='line'>-bash-4.1# cat /etc/resolv.conf 
</span><span class='line'>nameserver 172.17.42.1</span></code></pre></td></tr></table></div></figure>


<p>如果还是不行的话，关掉防火墙然后重启下docker服务: <code>service iptables stop; service docker restart</code></p>

<h2>参考</h2>

<ul>
<li><a href="http://top.jobbole.com/7904/">DNS和Docker的小技巧</a></li>
<li><a href="http://www.07net01.com/linux/zuixindnsmasqanzhuangbushuxiangjie_centos6__653221_1381214991.html">dnsmasq安装部署详解-centos6</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[编译/搭建Spark环境]]></title>
    <link href="http://winseliu.com/blog/2014/10/16/build-and-configuration-spark/"/>
    <updated>2014-10-16T16:55:39+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/16/build-and-configuration-spark</id>
    <content type="html"><![CDATA[<p>官网提供的hadoop版本没有2.5的。这里我自己下载源码再进行编译。先下载spark-1.1.0.tgz，解压然后执行命令编译：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive -X -DskipTests clean package</span></code></pre></td></tr></table></div></figure>


<p>建议加上maven参数，不然很可能出现OOM。编译的时间也挺长的，可以先去吃个饭。或者取消一些功能的编译（如hive）。</p>

<p>编译完后，在assembly功能下会生成包括所有spark及其依赖的jar文件。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker scala-2.10]# cd spark-1.1.0/assembly/target/scala-2.10/
</span><span class='line'>[root@docker scala-2.10]# ll -h
</span><span class='line'>total 135M
</span><span class='line'>-rw-r--r--. 1 root root 135M Oct 15 21:18 spark-assembly-1.1.0-hadoop2.5.1.jar</span></code></pre></td></tr></table></div></figure>


<h2>打包</h2>

<p>上面我们已经编译好了spark程序，这里对其进行打包集成到一个压缩包。使用程序自带的make-distribution.sh即可。</p>

<p>为了减少重新编译的巨长的等待时间，修改下脚本<code>make-distribution.sh</code>的maven编译参数，去掉maven的clean阶段操作，修改最终结果如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>
</span><span class='line'>#BUILD_COMMAND="mvn clean package -DskipTests $@"
</span><span class='line'>BUILD_COMMAND="mvn package -DskipTests $@"</span></code></pre></td></tr></table></div></figure>


<p>然后执行命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0]# sh -x make-distribution.sh --tgz  --skip-java-test -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive 
</span><span class='line'>[root@docker spark-1.1.0]# ll -h
</span><span class='line'>total 185M
</span><span class='line'>...
</span><span class='line'>-rw-r--r--. 1 root root 185M Oct 16 00:09 spark-1.1.0-bin-2.5.1.tgz</span></code></pre></td></tr></table></div></figure>


<p>最终会在目录行打包生成tgz的文件。</p>

<h2>本地运行local</h2>

<p>把本机ip主机名写入到hosts，方便以后windows本机查看日志</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# echo 192.168.154.128 docker &gt;&gt; /etc/hosts
</span><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# cat /etc/hosts
</span><span class='line'>...
</span><span class='line'>192.168.154.128 docker</span></code></pre></td></tr></table></div></figure>


<h3>运行helloworld：</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# bin/run-example SparkPi 10
</span><span class='line'>Spark assembly has been built with Hive, including Datanucleus jars on classpath
</span><span class='line'>...
</span><span class='line'>14/10/16 00:22:36 INFO SparkContext: Job finished: reduce at SparkPi.scala:35, took 2.848632007 s
</span><span class='line'>Pi is roughly 3.139344
</span><span class='line'>14/10/16 00:22:36 INFO SparkUI: Stopped Spark web UI at http://docker:4040
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<h3>交互式操作：</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# bin/spark-shell --master local[2]
</span><span class='line'>Welcome to
</span><span class='line'>      ____              __
</span><span class='line'>     / __/__  ___ _____/ /__
</span><span class='line'>    _\ \/ _ \/ _ `/ __/  '_/
</span><span class='line'>   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0
</span><span class='line'>      /_/
</span><span class='line'>
</span><span class='line'>Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_60)
</span><span class='line'>...
</span><span class='line'>14/10/16 00:25:57 INFO SparkUI: Started SparkUI at http://docker:4040
</span><span class='line'>14/10/16 00:25:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>14/10/16 00:25:58 INFO Executor: Using REPL class URI: http://192.168.154.128:39385
</span><span class='line'>14/10/16 00:25:58 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@docker:57417/user/HeartbeatReceiver
</span><span class='line'>14/10/16 00:25:58 INFO SparkILoop: Created spark context..
</span><span class='line'>Spark context available as sc.
</span><span class='line'>
</span><span class='line'>scala&gt; 
</span></code></pre></td></tr></table></div></figure>


<p>说明下环境，我使用windows作为开发环境，使用虚拟机中的linux作为测试环境。同时通过ssh连接的隧道来实现windows无缝的访问虚拟机linux操作系统。</p>

<p>启动交互式访问后，就可以通过浏览器访问4040查看spark程序的状态。</p>

<p><img src="http://file.bmob.cn/M00/1E/4B/wKhkA1Q_3NOALefuAAEimqVy6-s418.png" alt="" /></p>

<p>任务已经启动，接下来就可以进行操作：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val textFile=sc.textFile("README.md")
</span><span class='line'>textFile: org.apache.spark.rdd.RDD[String] = README.md MappedRDD[1] at textFile at &lt;console&gt;:12
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.count()
</span><span class='line'>res0: Long = 141
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.first()
</span><span class='line'>res1: String = # Apache Spark
</span><span class='line'>
</span><span class='line'>scala&gt; val linesWithSpark = textFile.filter(line=&gt;line.contains("Spark"))
</span><span class='line'>linesWithSpark: org.apache.spark.rdd.RDD[String] = FilteredRDD[2] at filter at &lt;console&gt;:14
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.filter(line=&gt;line.contains("Spark")).count()
</span><span class='line'>res2: Long = 21
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)
</span><span class='line'>res3: Int = 15
</span><span class='line'>
</span><span class='line'>scala&gt; import java.lang.Math
</span><span class='line'>import java.lang.Math
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b)=&gt;Math.max(a,b))
</span><span class='line'>res4: Int = 15
</span><span class='line'>
</span><span class='line'>scala&gt; val wordCounts = textFile.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_)
</span><span class='line'>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at &lt;console&gt;:15
</span><span class='line'>
</span><span class='line'>scala&gt; wordCounts.collect()
</span><span class='line'>res5: Array[(String, Int)] = Array((means,1), (under,2), (this,4), (Because,1), (Python,2), (agree,1), (cluster.,1), (its,1), (follows.,1), (general,2), (have,2), (YARN,,3), (pre-built,1), (locally.,1), (locally,2), (changed,1), (MRv1,,1), (several,1), (only,1), (sc.parallelize(1,1), (This,2), (learning,,1), (basic,1), (requests,1), (first,1), (Configuration,1), (MapReduce,2), (CLI,1), (graph,1), (without,1), (documentation,1), ("yarn-client",1), ([params]`.,1), (any,2), (setting,2), (application,1), (prefer,1), (SparkPi,2), (engine,1), (version,3), (file,1), (documentation,,1), (&lt;http://spark.apache.org/&gt;,1), (MASTER,1), (entry,1), (example,3), (are,2), (systems.,1), (params,1), (scala&gt;,1), (provides,1), (refer,1), (MLLib,1), (Interactive,2), (artifact,1), (configure,1), (can,8), (&lt;art...
</span></code></pre></td></tr></table></div></figure>


<p>执行了上面一些操作后，通过网页查看状态变化：</p>

<p><img src="http://file.bmob.cn/M00/1E/4C/wKhkA1Q_3w6AM6njAAF-MCCYh2s170.png" alt="" /></p>

<h2>Spark-standalone集群</h2>

<p>部署集群需要用到多个服务器，这里我使用docker来进行部署。</p>

<p>本来应该早早完成本文的实践，但是在搭建docker-hadoop集群时花费了很多的时间。关于搭建集群dnsmasq处理域名问题参见下一篇文章。
最终实现可以参考：<a href="https://github.com/winse/docker-hadoop/tree/spark-yarn">docker-hadoop</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver1 spark-yarn
</span><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver2 spark-yarn
</span><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name master -h master spark-yarn
</span><span class='line'>
</span><span class='line'>[root@docker docker-hadoop]# docker ps | grep spark | awk '{print $1}' | xargs -I{} docker inspect -f ' ' {} &gt; /etc/dnsmasq.hosts
</span><span class='line'>[root@docker docker-hadoop]# cat /etc/dnsmasq.hosts 
</span><span class='line'>172.17.0.29 master
</span><span class='line'>172.17.0.28 slaver2
</span><span class='line'>172.17.0.27 slaver1
</span><span class='line'>[root@docker docker-hadoop]# service dnsmasq restart
</span><span class='line'>[root@docker docker-hadoop]# ssh hadoop@master
</span><span class='line'>
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id master
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id localhost
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id slaver1
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id slaver2
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ sbin/start-all.sh 
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ /opt/jdk1.7.0_67/bin/jps  -m
</span><span class='line'>266 Jps -m
</span><span class='line'>132 Master --ip master --port 7077 --webui-port 8080
</span></code></pre></td></tr></table></div></figure>


<p>通过网页可以查看集群的状态：</p>

<p><img src="http://file.bmob.cn/M00/1E/F8/wKhkA1RClV2AE0biAAEmpXJlzTc914.png" alt="" /></p>

<p>运行任务连接到master：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-shell --master spark://master:7077
</span><span class='line'>...
</span><span class='line'>14/10/17 11:31:08 INFO BlockManagerMasterActor: Registering block manager slaver2:55473 with 265.4 MB RAM
</span><span class='line'>14/10/17 11:31:09 INFO BlockManagerMasterActor: Registering block manager slaver1:33441 with 265.4 MB RAM
</span><span class='line'>
</span><span class='line'>scala&gt; </span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmG-AO--XAAD84ATrCew955.png" alt="" /></p>

<p>从上图可以看到，程序已经正确连接到spark集群，master为driver，任务节点为slaver1和slaver2。下面运行下程序，然后通过网页查看运行的状态。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val textFile=sc.textFile("README.md")
</span><span class='line'>scala&gt; textFile.count()
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmdmAB3M9AAFIzMb4yk0370.png" alt="" /></p>

<p>系统安装好了，启动spark-standalone集群和hadoop-yarn一样。配置ssh、java，然后启动，配合网页8080/4040可以实时的了解任务的指标。</p>

<h2>yarn集群</h2>

<p>如果你是按照前面的步骤来操作的，需要先把spark-standalone的集群停掉。端口8080和yarn web使用端口冲突，会导致yarn启动失败。</p>

<p>修改spark-env.sh，添加HADOOP_CONF_DIR参数。然后提交任务到yarn上执行就行了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ cat conf/spark-env.sh
</span><span class='line'>#!/usr/bin/env bash
</span><span class='line'>
</span><span class='line'>JAVA_HOME=/opt/jdk1.7.0_67 
</span><span class='line'>
</span><span class='line'>HADOOP_CONF_DIR=/opt/hadoop-2.5.1/etc/hadoop
</span><span class='line'>
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.1.0-hadoop2.5.1.jar  10</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/FD/wKhkA1RCszeAALCPAAK1Nzk6faQ330.png" alt="" /></p>

<p>运行的结果输出在driver的slaver2节点，对应输出型来说不是很直观。spark-yarn提供了另一种方式，driver直接本地运行<em>yarn-client</em>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client lib/spark-examples-1.1.0-hadoop2.5.1.jar  10
</span><span class='line'>...
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8248 ms on slaver1 (1/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 231 ms on slaver1 (2/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 158 ms on slaver1 (3/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 284 ms on slaver1 (4/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 175 ms on slaver1 (5/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 301 ms on slaver1 (6/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 175 ms on slaver1 (7/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 143 ms on slaver1 (8/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 164 ms on slaver1 (9/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO cluster.YarnClientSchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@slaver2:51923/user/Executor#1132577949] with ID 1
</span><span class='line'>14/10/17 13:31:04 INFO util.RackResolver: Resolved slaver2 to /default-rack
</span><span class='line'>14/10/17 13:31:04 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 397 ms on slaver1 (10/10)
</span><span class='line'>14/10/17 13:31:04 INFO cluster.YarnClientClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
</span><span class='line'>14/10/17 13:31:04 INFO scheduler.DAGScheduler: Stage 0 (reduce at SparkPi.scala:35) finished in 26.084 s
</span><span class='line'>14/10/17 13:31:04 INFO spark.SparkContext: Job finished: reduce at SparkPi.scala:35, took 28.31400558 s
</span><span class='line'>Pi is roughly 3.140248</span></code></pre></td></tr></table></div></figure>


<h2>总结</h2>

<p>本文主要是搭建spark的环境搭建，本地运行、以及在docker中搭建spark集群、yarn集群三种方式。本地运行最简单方便，但是没有模拟到集群环境；spark提供了yarn框架上的实现，直接提交任务到yarn即可；spark集群相对比较简单和方便，接下来的远程调试主要通过spark伪分布式集群方式来进行。</p>

<h2>参考</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/building-with-maven.html">Building Spark with Maven</a></li>
<li><a href="http://spark.apache.org/docs/latest/quick-start.html">Quick Start</a></li>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html">Spark Configuration</a></li>
<li><a href="http://www.07net01.com/linux/zuixindnsmasqanzhuangbushuxiangjie_centos6__653221_1381214991.html">DNS</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读码] Spark1.1.0前篇--代码统计导入Eclipse]]></title>
    <link href="http://winseliu.com/blog/2014/10/12/read-spark1-source-starter/"/>
    <updated>2014-10-12T13:12:57+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/12/read-spark1-source-starter</id>
    <content type="html"><![CDATA[<p>看过亚太研究院的spark在线教学视频，说spark1.0的源码仅有3w+的代码，蠢蠢欲动。先具体看下源码的量，估算估算；然后搭建eclipse读码环境。</p>

<h2>计算源码行数</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ git branch -v
</span><span class='line'>* (detached from v1.1.0) 2f9b2bd [maven-release-plugin] prepare release v1.1.0-rc4
</span><span class='line'>  master                 4d8ae70 [behind 1246] Cleanup on Connection and ConnectionManager
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ find . -name "*.scala" | grep 'src/main' | xargs sed  -e 's:\/\*.*\*\/::' -e  '/\/\*/, /\*\//{
</span><span class='line'>/\/\*/{
</span><span class='line'> s:\/\*.*::p
</span><span class='line'>}
</span><span class='line'>/\*\//{
</span><span class='line'> s:.*\*\/::p
</span><span class='line'>}
</span><span class='line'>d
</span><span class='line'>}' | sed -e '/^\s*$/d' -e '/^\s*\/\//d' | grep -v '^import' | grep -v '^package' | wc -l
</span><span class='line'>72967
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^scala^java
</span><span class='line'>1749
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^src/main^core/src/main
</span><span class='line'>877
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^java^scala
</span><span class='line'>38526
</span></code></pre></td></tr></table></div></figure>


<p>全部源码的数量（去掉测试）大概在7W左右，仅计算核心代码core下面的代码量在4W。从量上面来说还是比较乐观的，学习scala然后读spark的源码。</p>

<p>spark1.0.0的核心代码量在3w左右。1.1多了大概1w行！！</p>

<h2>Docker</h2>

<p>查看目录结构的时刻，看到spark1下面竟然有docker，不过看Dockerfile的内容只是简单的安装了scala、把本机的spark映射到docker容器、然后运行spark主从集群。</p>

<h2>导入eclipse</h2>

<p>spark使用主要使用scala编写，首先需要下载<a href="http://scala-ide.org/download/sdk.html">scala-ide</a>直接下载2.10的版本（基于eclipse，很多操作都类似）；然后下载<a href="https://github.com/apache/spark.git">spark的源码</a>检出v1.1.0的；然后使用maven生成eclipse工程文件。</p>

<p>(不推荐)使用<a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-Eclipse">sbt生成工程文件</a>。这种方式会缺少一些依赖的jar，处理比较麻烦，还不清楚到底是少了啥！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd sbt/
</span><span class='line'>$ sed -i 's/^M//g' *
</span><span class='line'>$ cd ..
</span><span class='line'>$ sbt/sbt eclipse -mem 512</span></code></pre></td></tr></table></div></figure>


<p>(推荐)使用MVN编译生成，<a href="http://spark.apache.org/docs/latest/building-with-maven.html">使用Maven生成官网文章</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ git clean -x -fd #清理非仓库代码
</span><span class='line'>
</span><span class='line'>$ echo $SCALA_HOME #指定scala-home
</span><span class='line'>/cygdrive/d/scala
</span><span class='line'>
</span><span class='line'># 这里我直接修改默认值，理论上加 -Phadoop-2.2 选项应该也是可以的
</span><span class='line'>$ vi pom.xml # hadoop.version 2.2.0
</span><span class='line'>$ mvn eclipse:eclipse
</span><span class='line'>
</span><span class='line'>$ find . -name ".classpath" | xargs sed -i -e 's/including="\*\*\/\*.java"//' -e 's/excluding="\*\*\/\*.java"//'
</span><span class='line'>
</span><span class='line'>#也可以把添加特性的操作/添加scala源码包操作批量处理掉</span></code></pre></td></tr></table></div></figure>


<p>然后导入到eclipse，然后再针对性的处理报错：</p>

<ul>
<li>先把每个工程都<strong>添加scala特性</strong></li>
<li>把含有python源码包的去掉（手动删除.classpath中classpathentry即可）</li>
<li>确认下并加上<code>src/test/scala</code>的源码包。</li>
</ul>


<p>注意，进行上面的步骤之前，由于scala源文件比较多，编译的时间会比较长，先把Project->Build Automatically去掉，然后一次性把问题处理掉后再手动build！</p>

<ul>
<li>手动使用<code>existing maven projects</code>导入yarn/stable，然后把<strong>yarn/common以链接的形式引入</strong>，并添加到源码包。</li>
</ul>


<p><img src="http://file.bmob.cn/M00/1C/E7/wKhkA1Q7jQ2AMhweAAOC-l-jcz4872.png" alt="" /></p>

<p>还有一个<strong> value q is not a member of StringContext </strong><a href="http://docs.scala-lang.org/overviews/quasiquotes/intro.html">quasiquotes</a>的错误，有些类需要在2.10添加编译组件才能正常编译，修改scala编译首选项。</p>

<p><img src="http://file.bmob.cn/M00/1D/07/wKhkA1Q76GyAFNYPAAEYJfk_ZGw816.png" alt="" /></p>

<p>添加依赖的编译组件后，整个功能就能正常编译通过了。接下来就能调试看源码了。</p>

<p><strong>备注：</strong>clean后发现target目录下并没有重新编译生成class，去掉<code>-Xshow-phases</code>才行。</p>

<blockquote><p> -Xshow-phases                  Print a synopsis of compiler phases.</p></blockquote>

<h2>Maven编译spark</h2>

<p>如果使用的hadoop版本在官网没有集成assembly版本，可以使用maven手动构建。至于打包可以查看下一篇文章。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>$ mvn -Pyarn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean package</span></code></pre></td></tr></table></div></figure>


<p><code>yarn</code>的profile能够编译成可执行的jar文件（包括所有依赖的spark），具体内容下一篇讲。</p>

<h2>小结</h2>

<p>断断续续的写了两天，字数统计弄了大半天，主要在于多行注释的处理。时间最主要都消耗在sbt、maven构建eclipse项目文件（生成、fixed）上。编译scala量上去后确实非常非常的慢，不管是maven还是eclipse都慢！</p>

<p>下一篇将使用docker搭建spark环境，并使用远程调试连接到helloworld程序。</p>

<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/24800129/scala-maven-builder-doesnt-understand-quasiquotes">Scala maven builder doesn&rsquo;t understand quasiquotes</a></li>
<li><a href="http://docs.scala-lang.org/overviews/macros/paradise.html">Macro Paradise</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[思考]]></title>
    <link href="http://winseliu.com/blog/2014/10/07/thinking/"/>
    <updated>2014-10-07T19:07:26+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/07/thinking</id>
    <content type="html"><![CDATA[<p>随着年龄的增大，很多原来不曾想的问题慢慢的都开始环绕在自己四周。开始让自己不得不反思，不得不去改变。</p>

<p>本人是一个性格比较极端，又很内向，所以对自己不关心、无自己原来没有直接联系的东西，很少体现积极主动的一面。时时刻刻展现着保守派的作风。自己又在学习能力方面自我感觉良好，对现状总是很不满，对一样事物的持续坚持的耐久力不足（倒不是不能吃苦、吃不了苦的问题）！</p>

<p>从出生到毕业，一直以来都有亲人朋友让我依靠，有很明确值得挑战和超越的目标（总体水平一般，在我前面的人乌压压一片）。出来工作后一直都很迷失，不知道自己能干啥，可以干啥，师范类专业连教师资格证都没有拿到（不是后悔，自己觉得不应该）！！现在想来其实自己太执拗，像极了不撞南墙死不改的蛮牛！！</p>

<p>年龄增加体力不及，开始思考着应该去锻炼锻炼了，但是一直各种借口无疾而终！觉得身体还行，以后再说。。。
工作资历增加直接辅导指导的大哥不再，开始各种瞎折腾，东一锤西一棒，终究是拣了芝麻丢了西瓜！觉得学习能力强以后都敢都来得及，以后再学呗。。。   <br/>
但是在运动场上，一直坚持运动的同学，打个3、4个小时的羽毛球气不喘一下，这时开始懊悔。
当原来一起协作的同事，开始在领域有所斩获，各种嫉妒羡慕的心里开始作祟。</p>

<p>星期一个个的开始了结束，自己却没有得到该有的锤炼和进度，在大势所趋下，自己却总是那么的慢慢吞吞！阅读一个类的千行源码，竟然断断续续花费了仅2个月！本来年前看tomcat原来的计划最终石沉大海！</p>

<p>总是对自己不够狠；狠下来一次后，总是各种理由，最终不能坚持！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[配置ssh登录docker-centos]]></title>
    <link href="http://winseliu.com/blog/2014/09/30/docker-ssh-on-centos/"/>
    <updated>2014-09-30T00:10:02+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/30/docker-ssh-on-centos</id>
    <content type="html"><![CDATA[<p>上一篇写的是docker的入门知识，并没有进行实战。这些记录下使用ssh登录centos容器。</p>

<p>前文中参考的博客介绍了使用ssh登录tutorial容器（ubuntu），然后进行tomcat的安装，以及通过端口映射在客户机进行访问的例子。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull learn/tutorial
</span><span class='line'>docker run -i -t learn/tutorial /bin/bash
</span><span class='line'>  apt-get update
</span><span class='line'>  apt-get install openssh-server
</span><span class='line'>  which sshd
</span><span class='line'>  /usr/sbin/sshd
</span><span class='line'>  mkdir /var/run/sshd
</span><span class='line'>  passwd #输入用户密码，我这里设置为123456，便于SSH客户端登陆使用
</span><span class='line'>  exit #退出
</span><span class='line'>docker ps -l
</span><span class='line'>docker commit 51774a81beb3 learn/tutorial # 提交后，下次启动就可以基于容器更改的系统
</span><span class='line'>docker run -d -p 49154:22 -p 80:8080 learn/tutorial /usr/sbin/sshd -D
</span><span class='line'>ssh root@127.0.0.1 -p 49154
</span><span class='line'>  # 在ubuntu 12.04上安装oracle jdk 7
</span><span class='line'>  apt-get install python-software-properties
</span><span class='line'>  add-apt-repository ppa:webupd8team/java
</span><span class='line'>  apt-get update
</span><span class='line'>  apt-get install -y wget
</span><span class='line'>  apt-get install oracle-java7-installer
</span><span class='line'>  java -version
</span><span class='line'>  # 下载tomcat 7.0.47
</span><span class='line'>  wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-7/v7.0.47/bin/apache-tomcat-7.0.47.tar.gz
</span><span class='line'>  # 解压，运行
</span><span class='line'>  tar xvf apache-tomcat-7.0.47.tar.gz
</span><span class='line'>  cd apache-tomcat-7.0.47
</span><span class='line'>  bin/startup.sh</span></code></pre></td></tr></table></div></figure>


<p>然而在centos上，运行是不成功的。总结操作如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker pull centos:centos6
</span><span class='line'>[root@docker ~]# docker run -i -t  centos:centos6 /bin/bash
</span><span class='line'>  yum install which openssh-server openssh-clients
</span><span class='line'>
</span><span class='line'>  /usr/sbin/sshd # 这里会报错，需要手动生成key
</span><span class='line'>  ssh-keygen -f /etc/ssh/ssh_host_rsa_key
</span><span class='line'>  ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key
</span><span class='line'>
</span><span class='line'>  vi /etc/pam.d/sshd  # 修改pam_loginuid.so为optional
</span><span class='line'>  # /bin/sed -i 's/.*session.*required.*pam_loginuid.so.*/session optional pam_loginuid.so/g' /etc/pam.d/sshd
</span><span class='line'>  
</span><span class='line'>  passwd # 添加密码</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>[root@docker ~]# docker commit 3a7b6994bb2a winse/hadoop # 保存为自己使用的版本
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d winse/hadoop /usr/sbin/sshd
</span><span class='line'>f5cb57f6ec22dd9d257bf610322e2bd547ea0064262fcad63308b932c0490670
</span><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS                     PORTS               NAMES
</span><span class='line'>f5cb57f6ec22        winse/hadoop:latest   /usr/sbin/sshd      2 seconds ago       Exited (0) 2 seconds ago                       sharp_rosalind      
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d -p 8888:22 winse/hadoop /usr/sbin/sshd -D
</span><span class='line'>f9814253159373e8a8df3261904200a733b41c63f55708db3cb56a7ebf650cef
</span><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS                  NAMES
</span><span class='line'>f98142531593        winse/hadoop:latest   /usr/sbin/sshd -D   5 seconds ago       Up 4 seconds        0.0.0.0:8888-&gt;22/tcp   boring_bell         
</span><span class='line'>[root@docker ~]# ssh localhost -p 8888
</span><span class='line'>The authenticity of host '[localhost]:8888 ([::1]:8888)' can't be established.
</span><span class='line'>RSA key fingerprint is f5:5e:be:ae:ea:b1:ed:e8:49:43:28:9e:80:87:0d:86.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added '[localhost]:8888' (RSA) to the list of known hosts.
</span><span class='line'>root@localhost's password: 
</span><span class='line'>Last login: Mon Sep 29 14:48:23 2014 from localhost
</span><span class='line'>-bash-4.1# </span></code></pre></td></tr></table></div></figure>


<p>参数<code>-D</code>表示sshd运行在前台。这样当前的docker容器就会一直有程序在运行，不至于执行完指定的任务就被关闭掉了。</p>

<p>在centos配置ssh登录需要进行额外参数的设置。这个还是挺折腾人的。关于把<code>/etc/pam.d/sshd</code>中的<code>pam_loginuid.so</code>修改为optional，<a href="(http://stackoverflow.com/questions/21391142/why-is-it-needed-to-set-pam-loginuid-to-its-optional-value-with-docker">stackoverflow</a>)上的回答还是挺中肯的。</p>

<p>连上ssh后，下一步就和你远程操作服务器一样了。其实docker运行一个容器后，就会分配一个ip，你也可以根据这个ip来连接。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -t -i winse/hadoop /bin/bash
</span><span class='line'>bash-4.1# ssh localhost
</span><span class='line'>ssh: connect to host localhost port 22: Connection refused
</span><span class='line'>bash-4.1# service sshd start
</span><span class='line'>Starting sshd:                                             [  OK  ]
</span><span class='line'>bash-4.1# ifconfig
</span><span class='line'>eth0      Link encap:Ethernet  HWaddr 1E:2B:23:16:98:7E  
</span><span class='line'>          inet addr:172.17.0.31  Bcast:0.0.0.0  Mask:255.255.0.0
</span><span class='line'>          inet6 addr: fe80::1c2b:23ff:fe16:987e/64 Scope:Link
</span><span class='line'>
</span><span class='line'># 新开一个终端
</span><span class='line'>[root@docker ~]# ssh 172.17.0.31
</span><span class='line'>The authenticity of host '172.17.0.31 (172.17.0.31)' can't be established.
</span><span class='line'>RSA key fingerprint is f5:5e:be:ae:ea:b1:ed:e8:49:43:28:9e:80:87:0d:86.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added '172.17.0.31' (RSA) to the list of known hosts.
</span><span class='line'>root@172.17.0.31's password: 
</span><span class='line'>Last login: Mon Sep 29 14:48:23 2014 from localhost
</span><span class='line'>-bash-4.1#           </span></code></pre></td></tr></table></div></figure>


<h2>使用Dockerfile脚本安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# mkdir hadoop
</span><span class='line'>[root@docker ~]# cd hadoop/
</span><span class='line'>[root@docker hadoop]# touch Dockerfile
</span><span class='line'>[root@docker hadoop]# vi Dockerfile
</span><span class='line'>  # hadoop2 on docker-centos
</span><span class='line'>  FROM centos:centos6
</span><span class='line'>  MAINTAINER Winse &lt;fuqiuliu2006@qq.com&gt;
</span><span class='line'>  RUN yum install -y which openssh-clients openssh-server #-y表示交互都输入yes
</span><span class='line'>
</span><span class='line'>  RUN ssh-keygen -f /etc/ssh/ssh_host_rsa_key
</span><span class='line'>  RUN ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key
</span><span class='line'>
</span><span class='line'>  RUN echo 'root:hadoop' |chpasswd
</span><span class='line'>
</span><span class='line'>  RUN sed -i '/pam_loginuid.so/c session    optional     pam_loginuid.so'  /etc/pam.d/sshd
</span><span class='line'>
</span><span class='line'>  EXPOSE 22
</span><span class='line'>  CMD /usr/sbin/sshd -D
</span><span class='line'>  
</span><span class='line'>[root@docker hadoop]# docker build -t="winse/hadoop" .
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>winse/hadoop        latest              9d7f115ef0ec        5 minutes ago       289.1 MB
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker run -d --name slaver1 winse/hadoop
</span><span class='line'>[root@docker hadoop]# docker run -d --name slaver2 winse/hadoop
</span><span class='line'>[root@docker hadoop]# docker run -d --name master1 -P --link slaver1:slaver1 --link slaver2:slaver2  winse/hadoop
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker restart slaver1 slaver2 master1
</span><span class='line'>slaver1
</span><span class='line'>slaver2
</span><span class='line'>master1
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker port master1 22
</span><span class='line'>0.0.0.0:49159
</span><span class='line'>[root@docker hadoop]# ssh localhost -p 49159
</span><span class='line'>... 
</span><span class='line'>-bash-4.1# cat /etc/hosts
</span><span class='line'>172.17.0.31     7ef63f98e2d1
</span><span class='line'>127.0.0.1       localhost
</span><span class='line'>::1     localhost ip6-localhost ip6-loopback
</span><span class='line'>fe00::0 ip6-localnet
</span><span class='line'>ff00::0 ip6-mcastprefix
</span><span class='line'>ff02::1 ip6-allnodes
</span><span class='line'>ff02::2 ip6-allrouters
</span><span class='line'>172.17.0.29     slaver1
</span><span class='line'>172.17.0.30     slaver2</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.blogjava.net/yongboy/archive/2013/12/12/407498.html">Docker学习笔记之一，搭建一个JAVA Tomcat运行环境</a></li>
<li><a href="http://www.csdn123.com/html/topnews201408/36/1236.htm">Docker之配置Centos_ssh</a></li>
<li><a href="http://linux.die.net/man/8/pam_loginuid">pam_loginuid(8) - Linux man page</a></li>
<li><a href="http://stackoverflow.com/questions/21391142/why-is-it-needed-to-set-pam-loginuid-to-its-optional-value-with-docker">Why is it needed to set <code>pam_loginuid</code> to its <code>optional</code> value with docker?</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker入门]]></title>
    <link href="http://winseliu.com/blog/2014/09/27/docker-start-guide-on-centos/"/>
    <updated>2014-09-27T20:28:24+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/27/docker-start-guide-on-centos</id>
    <content type="html"><![CDATA[<p>docker进一年来火热，发现挺适合用来做运维系统发布的。如果用来捣鼓hadoop的系统部署感觉还是挺不错的。下面一起来学习下docker吧。</p>

<p>docker中提供了<a href="https://docs.docker.com/installation/windows/">windows的安装文档</a>，但是其实很坑爹啊。尽管<a href="https://github.com/boot2docker/windows-installer/releases">提供exe安装</a>，但是最终还是安装visualbox，然后启动带了docker的linux系统（iso）。</p>

<p>如果你已经安装了vmware，但没有安装linux，可以直接<a href="https://github.com/boot2docker/boot2docker/releases">下载iso</a>，然后通过iso来启动。</p>

<h2>安装</h2>

<p>如果你同时安装了vmware，又已经安装了linux，那下面简单列出安装配置docker中使用的命令。docker需要64位的linux操作系统，我这里使用的是centos6，具体的安装步骤看<a href="https://docs.docker.com/installation/centos/">官网的安装教程</a>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# yum install epel-release
</span><span class='line'>
</span><span class='line'>[root@docker ~]# yum install docker-io
</span><span class='line'>[root@docker ~]# service docker start
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run learn/tutorial /bin/echo hello world
</span><span class='line'>Unable to find image 'learn/tutorial' locally
</span><span class='line'>Pulling repository learn/tutorial
</span><span class='line'>8dbd9e392a96: Pulling fs layer 
</span><span class='line'>8dbd9e392a96: Download complete 
</span><span class='line'>hello world
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>learn/tutorial      latest              8dbd9e392a96        17 months ago       128 MB
</span><span class='line'>[root@docker ~]# docker images learn/tutorial 
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>learn/tutorial      latest              8dbd9e392a96        17 months ago       128 MB</span></code></pre></td></tr></table></div></figure>


<p>docker执行run命令时，如果指定的image本地不存在，会从<a href="https://registry.hub.docker.com/">hub服务器</a>获取。也可以先从服务器获取image，然后在执行。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull centos</span></code></pre></td></tr></table></div></figure>


<h2>简单入门</h2>

<p><a href="https://docs.docker.com/userguide/dockerizing/">HelloWorld教程</a></p>

<h4>单次执行</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run learn/tutorial /bin/echo 'hello world'
</span><span class='line'>hello world</span></code></pre></td></tr></table></div></figure>


<p>命令执行完后，容器就会关闭。</p>

<h4>交互式执行方式</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -t -i learn/tutorial /bin/bash
</span><span class='line'>root@274ede23baad:/# uptime
</span><span class='line'> 12:36:02 up  5:59,  0 users,  load average: 0.00, 0.00, 0.00
</span><span class='line'>root@9db219d2e98b:/# cat /etc/issue
</span><span class='line'>Ubuntu 12.04 LTS \n \l
</span><span class='line'>root@274ede23baad:/# pwd
</span><span class='line'>/
</span><span class='line'>root@274ede23baad:/# ls
</span><span class='line'>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  selinux  srv  sys  tmp  usr  var
</span><span class='line'>root@274ede23baad:/# exit
</span><span class='line'>exit</span></code></pre></td></tr></table></div></figure>


<ul>
<li>-t flag assigns a pseudo-tty or terminal inside our new container。</li>
<li>-i flag allows us to make an interactive connection by grabbing the standard in (STDIN) of the container.</li>
</ul>


<h4>后台任务</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -d learn/tutorial /bin/sh -c "while true; do echo hello world; sleep 1; done" 
</span><span class='line'>17e28b56e0cc4ddb5522736e2bcfd752d849a5b1d0b598478ee66b255801aa7c
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS               NAMES
</span><span class='line'>17e28b56e0cc        learn/tutorial:latest   /bin/sh -c 'while tr   2 minutes ago       Up 2 minutes                            trusting_wozniak    </span></code></pre></td></tr></table></div></figure>


<ul>
<li>-d flag tells Docker to run the container and put it in the background, to daemonize it.</li>
</ul>


<p>执行返回的是containter id(唯一ID)。通过ps可以查看当前的后台任务列表。ps列表中的containter id对应，可以查看相应的信息，最后的字段是一个随机指定的名字（也可以指定，后面再讲）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker logs trusting_wozniak
</span><span class='line'>hello world
</span><span class='line'>hello world
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker stop trusting_wozniak
</span><span class='line'>trusting_wozniak
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span></code></pre></td></tr></table></div></figure>


<p>可以通过logs查看容器的标准输出，通过stop来停止容器。</p>

<h2>深入容器</h2>

<p><a href="https://docs.docker.com/userguide/usingdocker/">Working with Containers</a></p>

<p>可以交互式的方式运行container，也可以后台任务的方式运行。</p>

<p>docker的命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Usage:  [sudo] docker [flags] [command] [arguments] ..
</span><span class='line'># Example:
</span><span class='line'>$ sudo docker run -i -t ubuntu /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>每个命令可以指定跟一系列的开关标识(flags)和参数(arguments)。</p>

<h4>各种参数</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker version
</span><span class='line'>
</span><span class='line'>$ docker run -d -P training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps -l
</span><span class='line'>CONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES
</span><span class='line'>bc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155-&gt;5000/tcp  nostalgic_morse
</span><span class='line'>
</span><span class='line'># docker run -d -p 6379 -v /home/hadoop/redis-2.8.13:/opt/redis-2.8.13 learn/tutorial /opt/redis-2.8.13/src/redis-server 
</span><span class='line'>be0b410f3601ea36070b3e519d9cc7cbe259caa2392f468c2dd2baebef42c4a8
</span><span class='line'>
</span><span class='line'># docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                     NAMES
</span><span class='line'>be0b410f3601        learn/tutorial:latest   /opt/redis-2.8.13/sr   10 seconds ago      Up 10 seconds       0.0.0.0:49153-&gt;6379/tcp   sad_colden          
</span><span class='line'>
</span><span class='line'># /home/hadoop/redis-2.8.13/src/redis-cli -p 49153
</span><span class='line'>127.0.0.1:49153&gt; keys *
</span><span class='line'>(empty list or set)
</span><span class='line'>127.0.0.1:49153&gt; </span></code></pre></td></tr></table></div></figure>


<ul>
<li>-P flag is new and tells Docker to map any required network ports inside our container to our host. This lets us view our web application.</li>
<li>-l tells the docker ps command to return the details of the last container started.</li>
<li>-a the docker ps command only shows information about running containers. If you want to see stopped containers too use the -a flag.</li>
<li>-p Network port bindings are very configurable in Docker. In our last example the -P flag is a shortcut for -p 5000 that maps port 5000 inside the container to a high port (from the range 49153 to 65535) on the local Docker host. We can also bind Docker containers to specific ports using the -p flag。</li>
<li>-v flag you can also mount a directory from your own host into a container.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker run -d -p 6379:6379 -v /home/hadoop/redis-2.8.13:/opt/redis-2.8.13 learn/tutorial /opt/redis-2.8.13/src/redis-server 
</span><span class='line'>2c50850c9437698769e54281a9f4154dc4120da2e113802454f1a23c83ab91fe
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                    NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   29 seconds ago      Up 28 seconds       0.0.0.0:6379-&gt;6379/tcp   naughty_yonath  
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker port naughty_yonath 6379
</span><span class='line'>0.0.0.0:6379
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker logs -f naughty_yonath
</span><span class='line'>...
</span><span class='line'>[1] 27 Sep 13:48:12.192 * The server is now ready to accept connections on port 6379
</span><span class='line'>[1] 27 Sep 13:50:33.228 * DB saved on disk
</span><span class='line'>[1] 27 Sep 13:50:43.730 * DB saved on disk</span></code></pre></td></tr></table></div></figure>


<ul>
<li>-f This time though we&rsquo;ve added a new flag, -f. This causes the docker logs command to act like the tail -f command and watch the container&rsquo;s standard out. We can see here the logs from Flask showing the application running on port 5000 and the access log entries for it.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker top naughty_yonath
</span><span class='line'>UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
</span><span class='line'>root                5015                1433                0                   21:48               ?                   00:00:00            /opt/redis-2.8.13/src/redis-server *:6379
</span><span class='line'>[root@docker redis-2.8.13]# docker inspect naughty_yonath
</span><span class='line'>...
</span><span class='line'>    "Volumes": {
</span><span class='line'>        "/opt/redis-2.8.13": "/home/hadoop/redis-2.8.13"
</span><span class='line'>    },
</span><span class='line'>    "VolumesRW": {
</span><span class='line'>        "/opt/redis-2.8.13": true
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker inspect -f '' naughty_yonath
</span><span class='line'>map[/opt/redis-2.8.13:/home/hadoop/redis-2.8.13]
</span></code></pre></td></tr></table></div></figure>


<h4>重启</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker stop naughty_yonath
</span><span class='line'>naughty_yonath
</span><span class='line'>[root@docker redis-2.8.13]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS                     PORTS               NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   8 minutes ago       Exited (0) 5 seconds ago                       naughty_yonath      
</span><span class='line'>[root@docker redis-2.8.13]# docker start naughty_yonath
</span><span class='line'>naughty_yonath
</span><span class='line'>[root@docker redis-2.8.13]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                    NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   8 minutes ago       Up 1 seconds        0.0.0.0:6379-&gt;6379/tcp   naughty_yonath</span></code></pre></td></tr></table></div></figure>


<h4>删除</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker stop naughty_yonath
</span><span class='line'>docker rm naughty_yonath</span></code></pre></td></tr></table></div></figure>


<h2>Images</h2>

<p><a href="https://docs.docker.com/userguide/dockerimages/">Working with Docker Images</a></p>

<h4>列出本地的images</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker images
</span><span class='line'># REPO[:TAG]
</span><span class='line'>docker run -t -i ubuntu:14.04 /bin/bash
</span><span class='line'>docker run -t -i ubuntu:latest /bin/bash</span></code></pre></td></tr></table></div></figure>


<h4>从Hub获取镜像Image</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull centos
</span><span class='line'>docker run -t -i centos /bin/bash
</span><span class='line'>docker search sinatra 
</span><span class='line'>docker pull training/sinatra</span></code></pre></td></tr></table></div></figure>


<h4>创建自己的images</h4>

<p>直接更新image</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -t -i training/sinatra /bin/bash
</span><span class='line'>root@0b2616b0e5a8:/# gem install json
</span><span class='line'>$ sudo docker commit -m="Added json gem" -a="Kate Smith" \
</span><span class='line'>  0b2616b0e5a8 ouruser/sinatra:v2
</span><span class='line'>$ docker images
</span><span class='line'>$ docker run -t -i ouruser/sinatra:v2 /bin/bash
</span><span class='line'>root@78e82f680994:/#</span></code></pre></td></tr></table></div></figure>


<p>通过DockerFile来添加功能，进行更新。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mkdir sinatra
</span><span class='line'>$ cd sinatra
</span><span class='line'>$ touch Dockerfile
</span><span class='line'>  # This is a comment
</span><span class='line'>  FROM ubuntu:14.04
</span><span class='line'>  MAINTAINER Kate Smith &lt;ksmith@example.com&gt;
</span><span class='line'>  RUN apt-get update && apt-get install -y ruby ruby-dev
</span><span class='line'>  RUN gem install sinatra
</span><span class='line'>
</span><span class='line'>$ docker build -t="ouruser/sinatra:v2" .
</span><span class='line'>$ docker run -t -i ouruser/sinatra:v2 /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>具体的DockerFile中各个指令的含义及其使用方法，参考<a href="https://docs.docker.com/userguide/dockerimages/">Building an image from a Dockerfile</a>和<a href="https://docs.docker.com/articles/dockerfile_best-practices/">Best Practices for Writing Dockerfiles</a>，以及<a href="https://docs.docker.com/reference/builder/">Dockerfile Reference</a>。具体例子<a href="https://github.com/perl/docker-perl/blob/r20140922.0/5.020.001-64bit,threaded/Dockerfile">docker-perl</a></p>

<h4>添加新标签Tag</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker tag 5db5f8471261 ouruser/sinatra:devel
</span><span class='line'>$ docker images ouruser/sinatra
</span><span class='line'>REPOSITORY          TAG     IMAGE ID      CREATED        VIRTUAL SIZE
</span><span class='line'>ouruser/sinatra     latest  5db5f8471261  11 hours ago   446.7 MB
</span><span class='line'>ouruser/sinatra     devel   5db5f8471261  11 hours ago   446.7 MB</span></code></pre></td></tr></table></div></figure>


<h4>上传分享到<a href="https://hub.docker.com/">hub</a></h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker push ouruser/sinatra</span></code></pre></td></tr></table></div></figure>


<h4>从本地删除</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker rmi training/sinatra</span></code></pre></td></tr></table></div></figure>


<h2>多container结合使用</h2>

<p><a href="https://docs.docker.com/userguide/dockerlinks/">Linking Containers Together</a></p>

<h4>端口映射</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d -P training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker ps nostalgic_morse
</span><span class='line'>CONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES
</span><span class='line'>bc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155-&gt;5000/tcp  nostalgic_morse
</span><span class='line'>
</span><span class='line'>docker run -d -p 5000:5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker run -d -p 127.0.0.1:5000:5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker run -d -p 127.0.0.1::5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'># The -p flag can be used multiple times to configure multiple ports.
</span><span class='line'>docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker port nostalgic_morse 5000
</span><span class='line'>127.0.0.1:49155</span></code></pre></td></tr></table></div></figure>


<h4>Container Linking</h4>

<p>docker想的还是很周到的。面临两个container互相访问，一个db，一个web，哪web怎么访问db的数据呢？</p>

<p>指定container的名称：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -d -P --name web training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps -l
</span><span class='line'>CONTAINER ID  IMAGE                  COMMAND        CREATED       STATUS       PORTS                    NAMES
</span><span class='line'>aed84ee21bde  training/webapp:latest python app.py  12 hours ago  Up 2 seconds 0.0.0.0:49154-&gt;5000/tcp  web
</span><span class='line'>
</span><span class='line'>$ docker inspect -f "" aed84ee21bde
</span><span class='line'>/web</span></code></pre></td></tr></table></div></figure>


<p>容器互通：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -d --name db training/postgres
</span><span class='line'>
</span><span class='line'>$ docker rm -f web
</span><span class='line'>$ docker run -d -P --name web --link db:db training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps
</span><span class='line'>CONTAINER ID  IMAGE                     COMMAND               CREATED             STATUS             PORTS                    NAMES
</span><span class='line'>349169744e49  training/postgres:latest  su postgres -c '/usr  About a minute ago  Up About a minute  5432/tcp                 db, web/db
</span><span class='line'>aed84ee21bde  training/webapp:latest    python app.py         16 hours ago        Up 2 minutes       0.0.0.0:49154-&gt;5000/tcp  web</span></code></pre></td></tr></table></div></figure>


<p>链接后，在web容器会添加DB的环境变量，同时把db的ip加入到/etc/hosts中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>$ docker run --rm --name web2 --link db:db training/webapp env
</span><span class='line'>    . . .
</span><span class='line'>    DB_NAME=/web2/db
</span><span class='line'>    DB_PORT=tcp://172.17.0.5:5432
</span><span class='line'>    DB_PORT_5432_TCP=tcp://172.17.0.5:5432
</span><span class='line'>    DB_PORT_5432_TCP_PROTO=tcp
</span><span class='line'>    DB_PORT_5432_TCP_PORT=5432
</span><span class='line'>    DB_PORT_5432_TCP_ADDR=172.17.0.5
</span><span class='line'>
</span><span class='line'>$ docker run -t -i --rm --link db:db training/webapp /bin/bash
</span><span class='line'>root@aed84ee21bde:/opt/webapp# cat /etc/hosts
</span><span class='line'>172.17.0.7  aed84ee21bde
</span><span class='line'>. . .
</span><span class='line'>172.17.0.5  db    </span></code></pre></td></tr></table></div></figure>


<p>You can see that Docker has created a series of environment variables with useful information about the source db container. Each variable is prefixed with <code>DB_</code>, which is populated from the alias you specified above. If the alias were db1, the variables would be prefixed with <code>DB1_</code>.</p>

<h2>存储</h2>

<p><a href="https://docs.docker.com/userguide/dockervolumes/">Managing Data in Containers</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Adding a data volume
</span><span class='line'>docker run -d -P --name web -v /webapp training/webapp python app.py
</span><span class='line'>
</span><span class='line'># Mount a Host Directory as a Data Volume
</span><span class='line'>docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py
</span><span class='line'># 只读
</span><span class='line'>docker run -d -P --name web -v /src/webapp:/opt/webapp:ro training/webapp python app.py
</span><span class='line'>
</span><span class='line'># Mount a Host File as a Data Volume
</span><span class='line'>docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash
</span><span class='line'>
</span><span class='line'># Creating and mounting a Data Volume Container
</span><span class='line'>docker run -d -v /dbdata --name dbdata training/postgres echo Data-only container for postgres
</span><span class='line'>docker run -d --volumes-from dbdata --name db1 training/postgres
</span><span class='line'>docker run -d --volumes-from dbdata --name db2 training/postgres
</span><span class='line'>docker run -d --name db3 --volumes-from db1 training/postgres
</span><span class='line'>
</span><span class='line'># Backup, restore, or migrate data volumes
</span><span class='line'>docker run --volumes-from dbdata -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata
</span><span class='line'>docker run -v /dbdata --name dbdata2 ubuntu /bin/bash
</span><span class='line'>docker run --volumes-from dbdata2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar
</span></code></pre></td></tr></table></div></figure>


<h2>回顾</h2>

<p>管理docker主要使用其提供的各种命令、以及参数来进行。</p>

<ul>
<li>本地的镜像管理: docker images / docker rmi [image identify]</li>
<li>容器管理： docker ps -a|-l / docker start|stop|rm|restart [image identify]</li>
<li>运行容器：docker run [images] [command]

<ul>
<li>-d 后台运行</li>
<li>-ti tty交互式运行</li>
<li>-P 把容器expose的端口映射到宿主机器端口。可以通过<code>docker port [container-name]</code>来查看端口映射关系。</li>
<li>-p [host-machine-port:container-machine-port]手动指定端口映射关系</li>
<li>-h [hostname] 实例操作系统的hostname</li>
<li>&ndash;name [name] 容器实例标识</li>
<li>-v [path] 建立目录</li>
<li>-v [host-machine-path:container-machine-path] 把宿主的文件路径映射到容器操作系统的指定目录</li>
<li>&ndash;link [container-name:name] 多容器之间互相访问。</li>
</ul>
</li>
</ul>


<p>还有很多辅助命令如：<code>top</code>, <code>logs</code>, <code>port</code>, <code>inspect</code>。以及进行版本管理的<code>pull</code>, <code>push</code>, <code>commit</code>, <code>tag</code>等等。</p>

<h2>更新</h2>

<ul>
<li>2015年3月3日00:29:44</li>
</ul>


<p>docker官网连不上，巨坑！从原来的docker导出</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker ps -a
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS                      PORTS               NAMES
</span><span class='line'>4a1ba5605868        learn/tutorial:latest   /bin/bash              15 seconds ago      Exited (0) 11 seconds ago                       loving_wilson        
</span><span class='line'>6e8a77ff8c26        centos:centos6          /bin/bash              10 minutes ago      Exited (0) 10 minutes ago                       determined_almeida  
</span><span class='line'>[root@docker ~]# docker export loving_wilson &gt; learn_tutorial.tar
</span><span class='line'>
</span><span class='line'>#===
</span><span class='line'>
</span><span class='line'>[root@localhost ~]# cat centos6.tar | docker import - centos:centos6
</span><span class='line'>876f82e7032a2ed567421298c6dd12a74ac7b37fc28ef4fd062ebb4678bd6821
</span><span class='line'>[root@localhost ~]# cat learn_tutorial.tar | docker import - learn/tutorial
</span><span class='line'>dc574b587de3479ecc3622c7b4f12227d894aa1461737612130122092a72bdb4
</span><span class='line'>[root@localhost ~]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED              VIRTUAL SIZE
</span><span class='line'>learn/tutorial      latest              dc574b587de3        23 seconds ago       128.2 MB
</span><span class='line'>centos              centos6             876f82e7032a        About a minute ago   212.7 MB</span></code></pre></td></tr></table></div></figure>


<ul>
<li>2015年3月2日16:13:12</li>
</ul>


<p>再在centos6.5上安装最新的，启动后报错：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]# docker -d
</span><span class='line'>INFO[0000] +job serveapi(unix:///var/run/docker.sock)   
</span><span class='line'>INFO[0000] WARNING: You are running linux kernel version 2.6.32-431.el6.x86_64, which might be unstable running docker. Please upgrade your kernel to 3.8.0. 
</span><span class='line'>INFO[0000] Listening for HTTP on unix (/var/run/docker.sock) 
</span><span class='line'>docker: relocation error: docker: symbol dm_task_get_info_with_deferred_remove, version Base not defined in file libdevmapper.so.1.02 with link time reference</span></code></pre></td></tr></table></div></figure>


<p>需要再安装新的依赖（囧，md，用yum安装还要自己安装其他依赖！！）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@localhost ~]#  yum install device-mapper-event-libs</span></code></pre></td></tr></table></div></figure>


<ul>
<li>docker本地存储的路径</li>
</ul>


<pre><code></code></pre>

<h2>参考</h2>

<ul>
<li><a href="http://www.blogjava.net/yongboy/archive/2013/12/12/407498.html">Docker学习笔记之一，搭建一个JAVA Tomcat运行环境</a></li>
<li><a href="http://www.inspires.cn/note/36">You are running linux kernel version 2.6.32-431.el6.x86_64(centos 6.5)</a></li>
<li><a href="http://blog.thoward37.me/articles/where-are-docker-images-stored/">Where are Docker images stored?(老版本，也值得一看)</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
