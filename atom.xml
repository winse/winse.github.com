<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Winse Blog]]></title>
  <link href="http://winseliu.com/atom.xml" rel="self"/>
  <link href="http://winseliu.com/"/>
  <updated>2015-01-01T01:22:26+08:00</updated>
  <id>http://winseliu.com/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Redis维护]]></title>
    <link href="http://winseliu.com/blog/2014/12/31/redis-operations/"/>
    <updated>2014-12-31T23:14:57+08:00</updated>
    <id>http://winseliu.com/blog/2014/12/31/redis-operations</id>
    <content type="html"><![CDATA[<p>在使用过程中，接触最多的就是它的commands。除了string/hashmap/set/sortedlist的基本使用方式外，下面总结平时会经常使用的命令：</p>

<h2>启动，客户端连接</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# nohup src/redis-server --port 6370 &
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# src/redis-cli -p 6370
</span><span class='line'>127.0.0.1:6370&gt; </span></code></pre></td></tr></table></div></figure>


<h2>获取redis的整体状态</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; info
</span><span class='line'>...
</span><span class='line'># Memory
</span><span class='line'>used_memory:1415161160
</span><span class='line'>used_memory_human:1.32G
</span><span class='line'>used_memory_rss:0
</span><span class='line'>used_memory_peak:1415161160
</span><span class='line'>used_memory_peak_human:1.32G
</span><span class='line'>used_memory_lua:33792
</span><span class='line'>mem_fragmentation_ratio:0.00
</span><span class='line'>mem_allocator:jemalloc-3.6.0
</span><span class='line'>...
</span><span class='line'># CPU
</span><span class='line'>used_cpu_sys:52.47
</span><span class='line'>used_cpu_user:10.07
</span><span class='line'>used_cpu_sys_children:0.00
</span><span class='line'>used_cpu_user_children:0.00
</span><span class='line'>
</span><span class='line'># Keyspace
</span><span class='line'>db0:keys=4253125,expires=0,avg_ttl=0</span></code></pre></td></tr></table></div></figure>


<p>列出的信息，包括了版本、内存/CPU使用、请求数、键值对等信息。通过这些基本了解redis运行情况。</p>

<h2>清空数据库</h2>

<p>对于数据量少的情况下，可以使用flushall来清理记录。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; set abc 1234
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6370&gt; keys *
</span><span class='line'>1) "abc"
</span><span class='line'>127.0.0.1:6370&gt; flushall
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6370&gt; keys *
</span><span class='line'>(empty list or set)</span></code></pre></td></tr></table></div></figure>


<p>数据量大的情况不建议使用flushall，可以直接把rdb数据文件干掉，然后重启redis服务就可以了（找不到数据文件后，就是一个新的库）。</p>

<h2>随机获取一个键</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; mset a 1 b 2 c 3 d 4 e 5 f 6 
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"a"
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"f"
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"e"
</span><span class='line'>127.0.0.1:6370&gt; RANDOMKEY
</span><span class='line'>"a"</span></code></pre></td></tr></table></div></figure>


<h2>遍历获取键值</h2>

<p>一般情况下，我们会使用<code>keys PATTERN</code>来查找匹配的键值。但是，如果数据量很大，keys操作会很消耗系统资源，<code>stop the world</code>的事情不是我们想看到的！此时，可以通过scan/hscan/zscan/ssan命令依次获取。</p>

<ul>
<li>获取库中的键值</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; eval "for i=1,100000 do redis.call('set', 'a' .. i, i) end" 0
</span><span class='line'>(nil)
</span><span class='line'>(0.98s)</span></code></pre></td></tr></table></div></figure>


<p>正式环境我们无法预估匹配的键的数量，一根筋的使用keys命令可能并不明智。如果数据量很多，等不到结束应该就会ctrl+c了。这种情况下，可以使用scan命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6370&gt; scan 0 match ismi:domain:*.upaiyun.com
</span><span class='line'>1) "6553600"
</span><span class='line'>2) 1) "ismi:domain:KunMing:1415646303170928524.test.b0.upaiyun.com"
</span><span class='line'>   2) "ismi:domain:KunMing:1415392926002699280.test.b0.upaiyun.com"
</span><span class='line'>   3) "ismi:domain:KunMing:141489373375899237.test.b0.upaiyun.com"
</span><span class='line'>127.0.0.1:6370&gt; scan 0 match ismi:domain:*.upaiyun.com count 10
</span><span class='line'>1) "6553600"
</span><span class='line'>2) 1) "ismi:domain:KunMing:1415646303170928524.test.b0.upaiyun.com"
</span><span class='line'>   2) "ismi:domain:KunMing:1415392926002699280.test.b0.upaiyun.com"
</span><span class='line'>   3) "ismi:domain:KunMing:141489373375899237.test.b0.upaiyun.com"
</span></code></pre></td></tr></table></div></figure>


<p>Basically with COUNT the user specified the amount of work that should be done at every call in order to retrieve elements from the collection. This is just an hint for the implementation, however generally speaking this is what you could expect most of the times from the implementation.</p>

<p>COUNT数值的意思应该是匹配操作的次数，而不是查询结果的个数。通过和<code>scan 0</code>对比可以得出来。</p>

<p>同理，对于set（smembers）可以使用sscan，sortedlist可以使用zcan等。</p>

<h2>lua脚本</h2>

<p>redis内置的脚本语言，直接使用脚本可以减少客户端和服务端连接（多次请求）的压力。例如要批量删除一些键值：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>src/redis-cli keys 'v2:*' | awk '{print $1}' | while read line; do src/redis-cli del $line ; done</span></code></pre></td></tr></table></div></figure>


<p>先获取匹配的key，然后使用shell再次调用redis的客户端进行删除。表面上看起来没啥问题，如果匹配的key很多，会产生很多的tcp连接，占用redis服务器的端口！最终端口不够用，请求报错。</p>

<p>此时，如果使用lua脚本的方式，就可以轻松处理。无需考虑端口等问题。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 量少时可以使用
</span><span class='line'>eval "local aks=redis.call('keys', 'v2:*'); if #aks &gt;0 then redis.call('del', unpack(aks)) end" 0
</span><span class='line'>
</span><span class='line'># 优美
</span><span class='line'>eval "local aks=redis.call('keys', 'v2:*'); for _,r in ipairs(aks) do redis.call('del', r) end" 0</span></code></pre></td></tr></table></div></figure>


<p>当然，如果键不多，还可以使用一次性全部删除：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>src/redis-cli -p $PORT del `~/redis-2.8.13/src/redis-cli -p $PORT 'keys' 'v2:*' | grep -v 'v2:ci:' | grep -v 'v2:ff' | grep -v "$(date +%Y-%m-%d)" | grep -v "$(date +%Y-%m-%d -d '-1 day')"`</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop查看作业状态Rest接口]]></title>
    <link href="http://winseliu.com/blog/2014/12/07/hadoop-mr-rest-api/"/>
    <updated>2014-12-07T10:09:49+08:00</updated>
    <id>http://winseliu.com/blog/2014/12/07/hadoop-mr-rest-api</id>
    <content type="html"><![CDATA[<p>hadoop yarn提供了web端查看任务状态，同时可以通过rest的方式获取任务的相关信息。rest接口和网页端的每个界面一一对应。</p>

<p><img src="http://file.bmob.cn/M00/D7/C0/oYYBAFSDxeaARA6AAAenwsLMShM027.png" alt="" /></p>

<p>上面的5个图的链接为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>http://hadoop-master1:8088/cluster/apps/RUNNING
</span><span class='line'>http://hadoop-master1:8088/cluster/app/application_1417676507722_1846
</span><span class='line'>http://hadoop-master1:8088/proxy/application_1417676507722_1846/
</span><span class='line'>http://hadoop-master1:8088/proxy/application_1417676507722_1846/mapreduce/job/job_1417676507722_1846
</span><span class='line'>http://hadoop-master1:19888/jobhistory/job/job_1417676507722_1846/mapreduce/job/job_1417676507722_1846</span></code></pre></td></tr></table></div></figure>


<h2>查看正在运行的任务</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://hadoop-master1:8088/ws/v1/cluster/apps?states=RUNNING
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/info
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/counters
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/conf
</span></code></pre></td></tr></table></div></figure>


<p>如果上面的任务是已经完成的，获取对应的信息时返回的值是空的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://hadoop-master1:8088/proxy/application_1417676507722_1867/ws/v1/mapreduce/jobs/job_1417676507722_1867/counters</span></code></pre></td></tr></table></div></figure>


<h2>查看执行完成的任务</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://hadoop-master1:19888/ws/v1/history
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/info
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs?startedTimeBegin=$(date +%s -d '-1 hour')000
</span><span class='line'>...
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867/counters
</span><span class='line'>curl http://hadoop-master1:19888/ws/v1/history/mapreduce/jobs/job_1417676507722_1867/conf
</span><span class='line'>
</span><span class='line'>curl -H "Accept: application/xml" "http://hadoop-master1:8088/ws/v1/cluster/apps?states=FINISHED&limit=1" | xmllint --format - </span></code></pre></td></tr></table></div></figure>


<p>后面的参数和运行任务一致，只是提供服务不同。</p>

<h2>xml转csv</h2>

<p><img src="http://file.bmob.cn/M00/D7/D5/oYYBAFSEHuKAaAgHAACb9_KkMEU331.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -H "Accept: application/xml" "http://hadoop-master1:8088/ws/v1/cluster/apps?startedTimeBegin=$(date +%s -d '-1 hour')000" 2&gt;/dev/null | xsltproc yarn.xslt -  | sort -r
</span><span class='line'>
</span><span class='line'>application_1417676507722_1973,AccessLogOnlyHiveJob,RUNNING,UNDEFINED,1417942144941,0,19416
</span><span class='line'>application_1417676507722_1972,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417942084278,1417942098184,13906
</span><span class='line'>application_1417676507722_1971,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417941603456,1417941617773,14317
</span><span class='line'>application_1417676507722_1970,AccessLogOnlyHiveJob,FINISHED,SUCCEEDED,1417941581080,1417942142287,561207
</span><span class='line'>application_1417676507722_1969,InfoSecurityLogJob,FINISHED,SUCCEEDED,1417941422664,1417941436456,13792</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Hadoop YARN - Introduction to the web services REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">ResourceManager REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/MapredAppMasterRest.html">MapReduce Application Master REST API&rsquo;s.</a></li>
<li><a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html">History Server REST API&rsquo;s.</a></li>
<li><a href="http://blog.csdn.net/wypblog/article/details/21159795">Hadoop YARN中web服务的REST API介绍</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mysql分区]]></title>
    <link href="http://winseliu.com/blog/2014/11/14/mysql-partition/"/>
    <updated>2014-11-14T15:05:50+08:00</updated>
    <id>http://winseliu.com/blog/2014/11/14/mysql-partition</id>
    <content type="html"><![CDATA[<p>Windows8 Mysql安装后数据默认放在<code>C:\ProgramData\MySQL\MySQL Server 5.6\data</code>下。</p>

<blockquote><p>2、MyISAM数据库表文件：
.MYD文件：即MY Data，表数据文件
.MYI文件：即MY Index，索引文件
.log文件：日志文件</p>

<p>3、InnoDB采用表空间（tablespace）来管理数据，存储表数据和索引，
InnoDB数据库文件（即InnoDB文件集，ib-file set）：
  ibdata1、ibdata2等：系统表空间文件，存储InnoDB系统信息和用户数据库表数据和索引，所有表共用
  .ibd文件：单表表空间文件，每个表使用一个表空间文件（file per table），存放用户数据库表数据和索引
  日志文件： ib_logfile1、ib_logfile2</p></blockquote>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>create database hello;
</span><span class='line'>use hello;
</span><span class='line'>create table abc ( name varchar(1000), age int );
</span><span class='line'>insert into abc values ("1", 1);
</span><span class='line'>
</span><span class='line'>create table abc_myisam ( name varchar(100), age int ) engine=myisam;
</span><span class='line'>insert into abc_myisam values ( '1', 1), ('2',2);
</span><span class='line'>alter table abc_myisam partition by hash(age) partitions 4 ;
</span><span class='line'>
</span><span class='line'>insert into abc_myisam values ( '11', 10), ('2',20), ( '1', 11), ('2',21), ( '1', 21), ('2',22), ( '1', 31), ('2',32), ( '1', 41), ('2',24), ( '1', 15), ('2',23) ;</span></code></pre></td></tr></table></div></figure>


<p>最终库目录如下:</p>

<p><img src="http://file.bmob.cn/M00/D2/16/oYYBAFRlrMaAAAdoAADDsNJhdNs617.png" alt="" /></p>

<p>根据月份来进行分区：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>--最好按照月份分区(date需要为日期类型)
</span><span class='line'>alter table abc_myisam PARTITION BY RANGE (extract(YEAR_MONTH from date)) (  
</span><span class='line'>    PARTITION p410 VALUES LESS THAN (201411),  
</span><span class='line'>    PARTITION p411 VALUES LESS THAN (201412),  
</span><span class='line'>    PARTITION p412 VALUES LESS THAN (201501),  
</span><span class='line'>  PARTITION p501 VALUES LESS THAN (201502), 
</span><span class='line'>  PARTITION p502 VALUES LESS THAN (201503), 
</span><span class='line'>  PARTITION p503 VALUES LESS THAN (201504), 
</span><span class='line'>  PARTITION p504 VALUES LESS THAN (201505), 
</span><span class='line'>  PARTITION p505 VALUES LESS THAN (201506), 
</span><span class='line'>    PARTITION p0 VALUES LESS THAN MAXVALUE  
</span><span class='line'>)</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://blog.csdn.net/yaotinging/article/details/6671506">MySQL数据文件介绍及存放位置</a></li>
<li><a href="http://lehsyh.iteye.com/blog/732719">MySQL的表分区</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nginx配置静态站点服务]]></title>
    <link href="http://winseliu.com/blog/2014/11/13/nginx-serving-static-content/"/>
    <updated>2014-11-13T10:16:17+08:00</updated>
    <id>http://winseliu.com/blog/2014/11/13/nginx-serving-static-content</id>
    <content type="html"><![CDATA[<p>配置nginx作为网页快照的服务，需要理解好配置<code>root</code>的涵义！</p>

<p>首先安装，然后修改配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install nginx 
</span><span class='line'>
</span><span class='line'>less /etc/nginx/nginx.conf
</span><span class='line'>less /etc/nginx/conf.d/default.conf 
</span><span class='line'>
</span><span class='line'>service nginx restart</span></code></pre></td></tr></table></div></figure>


<p>实际操作中没有root，只能自己编译了：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>下载nginx，pcre-8.36.zip，zlib-1.2.3.tar.gz解压到src下。
</span><span class='line'>cd nginx-1.7.7
</span><span class='line'>./configure --prefix=/home/omc/tools/nginx --with-pcre=src/pcre --with-zlib=src/zlib
</span><span class='line'>
</span><span class='line'>cd /home/omc/tools/nginx
</span><span class='line'>vi conf/nginx.conf # 修改listen的端口，80要root才能起
</span><span class='line'>sbin/nginx
</span><span class='line'>sbin/nginx -s reload</span></code></pre></td></tr></table></div></figure>


<p>如果编译的目录和真正存放程序的路径不一致时，可以使用<code>-p</code>参数来指定。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd nginx
</span><span class='line'>sbin/nginx -p $PWD
</span><span class='line'>sbin/nginx -s reload -p $PWD</span></code></pre></td></tr></table></div></figure>


<p>下面具体说说配置的涵义：</p>

<ul>
<li>root（不管在那个配置节点下）位置都对应 请求的根路径。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location /static {
</span><span class='line'>    root  /usr/share/static/html;
</span><span class='line'>    autoindex on;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>location / {
</span><span class='line'>    root   /usr/share/nginx/html;
</span><span class='line'>    index  index.html index.htm;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li>location的<code>/static</code>对应的是访问目录<code>/usr/share/static/html/static</code>下的内容，请求<code>/static/hello.html</code>对应到<code>/usr/share/static/html/static/hello.html</code>。也就是说节点下的root目录 对应 的是 访问地址的<code>/</code>。</li>
<li>autoindex可以用于list列出目录内容。</li>
</ul>


<p><img src="http://file.bmob.cn/M00/05/49/ooYBAFRkHkmAe3wcAACCDsZ0Oc8983.png" alt="" /></p>

<p>配置了两个路径后，问题来了：如果<code>/usr/share/nginx/html/</code>也有目录static，那nginx会访问谁？<strong>nginx来先匹配配置，访问/static定位到<code>/usr/share/static/html</code></strong>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location /static {
</span><span class='line'>    root  /usr/share/static/html;
</span><span class='line'>  try_files $uri /static/404.html;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<ul>
<li>try_files可以设置默认页面，如<code>/usr/share/static/html/static</code>目录下不存在abc.html，那么会内部重定向到<code>/static/404.html</code>。这里路径要<code>/static</code>下面。</li>
</ul>


<p>try_files还可以返回状态值，跳转到对应状态的页面：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location / {
</span><span class='line'>    try_files $uri $uri/ $uri.html =404;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/D1/D0/oYYBAFRkJ6eAc8UiAAEKid3ICHw052.png" alt="" /></p>

<p>如果try_files的所给出的地址不包括<code>$uri</code>时，请求会被重定向配置指向的新代理服务：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location / {
</span><span class='line'>    try_files $uri $uri/ @backend;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>location @backend {
</span><span class='line'>    proxy_pass http://backend.example.com;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>实践</h2>

<p>在实际操作遇到的不能访问的问题，配置本机的其他JavaWeb应用，但是在登录后，点其他链接总是跳转到登陆页。可以查看下真正请求的地址。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>location /omc {
</span><span class='line'>      proxy_pass http://REAL-IP:9000/omc;
</span><span class='line'>      #proxy_pass http://localhost:9000/omc;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>填写localhost不能访问，但是填具体的外网IP时是可以访问的。查看后，在页面定义了<code>&lt;base href="${basedir}/&gt;</code>导致请求都跳转到localhost了。在客户端肯定就访问失败了。这个需要特别注意下。</p>

<p><img src="http://file.bmob.cn/M00/05/C3/ooYBAFRpn1qAKNKiAACUae7DmjY717.png" alt="" /></p>

<p>在特定的情况下，文件不一定是html后缀的（如：txt），如果要在浏览器解析html，需要配置content-type标题头。同时访问的url和真实存放的文件的路径有出处时，可以通过rewrite指令来进行适配。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>server {
</span><span class='line'>  ...
</span><span class='line'>    location /snapshot {
</span><span class='line'>        root   /home/ud/html-snapshot;
</span><span class='line'>        add_header content-type "text/html";
</span><span class='line'>        rewrite ^/snapshot/.*/(.*)$  /snapshot/$1   last;
</span><span class='line'>        try_files $uri $uri.html $uri.htm =404;
</span><span class='line'>    }</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://nginx.com/resources/admin-guide/serving-static-content/">Serving Static Content</a></li>
<li><a href="http://nginx.com/resources/admin-guide/web-server/">NGINX Web Server</a></li>
<li><a href="http://www.cnblogs.com/cgli/archive/2011/05/16/2047920.html">nginx rewrite规则</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为github Pages页面设置自定义域名]]></title>
    <link href="http://winseliu.com/blog/2014/10/24/github-custom-domain/"/>
    <updated>2014-10-24T00:17:19+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/24/github-custom-domain</id>
    <content type="html"><![CDATA[<ol>
<li>注册个域名（net.cn）</li>
<li>添加CNAME文件（github.com）</li>
<li>添加解析记录（net.cn）</li>
</ol>


<p><img src="http://file.bmob.cn/M00/20/C5/wKhkA1RJKuyAf6lWAACIJ28IFe8161.png" alt="" /></p>

<p>如果是使用子域名的话非常简单。在（pages）CNAME文件中填写www.winseliu.com，然后在（net.cn）解析页添加CNAME指向winse.github.io即可。</p>

<p>如果想默认顶级域名也能访问，需要添加的两个ip指向，参见上图。同时（pages）CNAME中使用winseliu.com。</p>

<h2>参考</h2>

<ul>
<li><a href="https://help.github.com/articles/my-custom-domain-isn-t-working/">My custom domain isn&rsquo;t working</a></li>
<li><a href="https://help.github.com/articles/tips-for-configuring-an-a-record-with-your-dns-provider/">Tips for configuring an A record with your DNS provider</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dnsmasq解决docker集群节点互通问题]]></title>
    <link href="http://winseliu.com/blog/2014/10/18/modify-hosts-build-hadoop-cluster-on-docker/"/>
    <updated>2014-10-18T04:19:21+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/18/modify-hosts-build-hadoop-cluster-on-docker</id>
    <content type="html"><![CDATA[<p>上个星期学习了一下docker，写了一个<a href="https://github.com/winse/docker-hadoop/tree/Pseudo-Distributed">伪分布式的Dockerfile</a>。</p>

<p>通过<code>--link</code>的方式master能访问slaver，毕竟slaver的相关信息已经被写入到master的hosts文件里面去了嘛！理所当然认为，直接把master的hosts文件全部复制一份到所有slaver节点问题就解决了。</p>

<p>等真正操作的时刻，发现不是那么回事，docker容器不给修改hosts文件！！</p>

<h2>错误实现</h2>

<p>首先，看下不当的操作：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -d --name slaver1 -h slaver1 hadoop
</span><span class='line'>[root@docker ~]# docker run -d --name slaver2 -h slaver2 hadoop
</span><span class='line'>[root@docker ~]# docker run -d --name master -h master --link slaver1:slaver1 --link slaver2:slaver2 hadoop
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
</span><span class='line'>dafc82678811        hadoop:latest       /bin/sh -c '/usr/sbi   40 seconds ago      Up 40 seconds       22/tcp              master
</span><span class='line'>86d2da5209c5        hadoop:latest       /bin/sh -c '/usr/sbi   49 seconds ago      Up 48 seconds       22/tcp              master/slaver2,slaver2
</span><span class='line'>7b9761fb05a8        hadoop:latest       /bin/sh -c '/usr/sbi   56 seconds ago      Up 55 seconds       22/tcp              master/slaver1,slaver1</span></code></pre></td></tr></table></div></figure>


<p>此时，通过<code>--link</code>连接方式，master的hosts中已经包括了slaver1和slaver2，按照正常的路子，登录master拷贝其hosts到slaver节点，一切就妥妥的了。现实是残酷的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-bash-4.1# scp /etc/hosts slaver1:/etc/
</span><span class='line'>scp: /etc//hosts: Read-only file system</span></code></pre></td></tr></table></div></figure>


<h2>DNS完美解决问题</h2>

<p>首先需要在宿主机器上安装dns服务器，bind不多说比较麻烦。这里参考网上人家解决方式，使用dnsmasq来搭建DNS服务器。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# yum install dnsmasq -y
</span><span class='line'>
</span><span class='line'>[root@docker ~]# cp /etc/resolv.conf /etc/resolv.dnsmasq.conf 
</span><span class='line'>[root@docker ~]# touch /etc/dnsmasq.hosts
</span><span class='line'>
</span><span class='line'>[root@docker ~]# vi /etc/resolv.conf
</span><span class='line'>[root@docker ~]# cat /etc/resolv.conf
</span><span class='line'>; generated by /sbin/dhclient-script
</span><span class='line'>nameserver 127.0.0.1 
</span><span class='line'>
</span><span class='line'>[root@docker ~]# vi /etc/dnsmasq.conf
</span><span class='line'>[root@docker ~]# cat /etc/dnsmasq.conf
</span><span class='line'>...
</span><span class='line'>resolv-file=/etc/resolv.dnsmasq.conf
</span><span class='line'>...
</span><span class='line'>addn-hosts=/etc/dnsmasq.hosts
</span><span class='line'>
</span><span class='line'>[root@docker ~]# service dnsmasq restart
</span><span class='line'>
</span><span class='line'>[root@docker ~]# dig www.baidu.com
</span><span class='line'>...
</span><span class='line'>;; SERVER: 127.0.0.1#53(127.0.0.1)
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>通过dig可以查看当前的DNS服务器你已经修改为localhost了。然后启动docker容器来搭建环境。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d  --dns 172.17.42.1 --name slaver1 -h slaver1 hadoop
</span><span class='line'>[root@docker ~]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver2 hadoop
</span><span class='line'>[root@docker ~]# docker run -d  --dns 172.17.42.1 --name master -h master hadoop
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES
</span><span class='line'>f6e63b311e60        hadoop:latest       /bin/sh -c '/usr/sbi   6 seconds ago       Up 5 seconds        22/tcp              master
</span><span class='line'>454ae2c3e435        hadoop:latest       /bin/sh -c '/usr/sbi   13 seconds ago      Up 12 seconds       22/tcp              slaver2
</span><span class='line'>7698230a03fb        hadoop:latest       /bin/sh -c '/usr/sbi   21 seconds ago      Up 20 seconds       22/tcp              slaver1
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps | grep hadoop | awk '{print $1}' | xargs -I{} docker inspect -f '{{.NetworkSettings.IPAddress}} {{.Config.Hostname}}' {} &gt; /etc/dnsmasq.hosts
</span><span class='line'>[root@docker ~]# service dnsmasq restart
</span><span class='line'>
</span><span class='line'>[root@docker ~]# ssh hadoop@master
</span><span class='line'>hadoop@master's password: 
</span><span class='line'>[hadoop@master ~]$ ping slaver1
</span><span class='line'>PING slaver1 (172.17.0.9) 56(84) bytes of data.
</span><span class='line'>64 bytes from slaver1 (172.17.0.9): icmp_seq=1 ttl=64 time=1.79 ms
</span><span class='line'>...
</span><span class='line'>[hadoop@master ~]$ ping slaver2
</span><span class='line'>PING slaver2 (172.17.0.10) 56(84) bytes of data.
</span><span class='line'>64 bytes from slaver2 (172.17.0.10): icmp_seq=1 ttl=64 time=1.96 ms
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'></span></code></pre></td></tr></table></div></figure>


<p>节点互通后，后面的步骤都类似了，ssh无密钥通信，格式化namenode，启动等等。</p>

<h2>遇到的问题</h2>

<ul>
<li>一开始我把配置文件放在/root目录下，dnsmasq总是不起作用。最后放到/etc目录就可以，不知道啥子问题。</li>
<li>配置dns启动docker容器后，如果不起作用看下<code>/etc/resolv.conf</code>确保仅有nameserver一行数据。</li>
</ul>


<p>DNS可以正常工作的配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-bash-4.1# ping slaver
</span><span class='line'>PING slaver (172.17.0.7) 56(84) bytes of data.
</span><span class='line'>64 bytes from slaver (172.17.0.7): icmp_seq=1 ttl=64 time=0.095 ms
</span><span class='line'>
</span><span class='line'>-bash-4.1# cat /etc/resolv.conf 
</span><span class='line'>nameserver 172.17.42.1</span></code></pre></td></tr></table></div></figure>


<p>DNS配置存在问题的情况：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-bash-4.1# cat /etc/resolv.conf 
</span><span class='line'>nameserver 172.17.42.1
</span><span class='line'>search localdomain</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://top.jobbole.com/7904/">DNS和Docker的小技巧</a></li>
<li><a href="http://www.07net01.com/linux/zuixindnsmasqanzhuangbushuxiangjie_centos6__653221_1381214991.html">dnsmasq安装部署详解-centos6</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[编译/搭建Spark环境]]></title>
    <link href="http://winseliu.com/blog/2014/10/16/build-and-configuration-spark/"/>
    <updated>2014-10-16T16:55:39+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/16/build-and-configuration-spark</id>
    <content type="html"><![CDATA[<p>官网提供的hadoop版本没有2.5的。这里我自己下载源码再进行编译。先下载spark-1.1.0.tgz，解压然后执行命令编译：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive -X -DskipTests clean package</span></code></pre></td></tr></table></div></figure>


<p>建议加上maven参数，不然很可能出现OOM。编译的时间也挺长的，可以先去吃个饭。或者取消一些功能的编译（如hive）。</p>

<p>编译完后，在assembly功能下会生成包括所有spark及其依赖的jar文件。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker scala-2.10]# cd spark-1.1.0/assembly/target/scala-2.10/
</span><span class='line'>[root@docker scala-2.10]# ll -h
</span><span class='line'>total 135M
</span><span class='line'>-rw-r--r--. 1 root root 135M Oct 15 21:18 spark-assembly-1.1.0-hadoop2.5.1.jar</span></code></pre></td></tr></table></div></figure>


<h2>打包</h2>

<p>上面我们已经编译好了spark程序，这里对其进行打包集成到一个压缩包。使用程序自带的make-distribution.sh即可。</p>

<p>为了减少重新编译的巨长的等待时间，修改下脚本<code>make-distribution.sh</code>的maven编译参数，去掉maven的clean阶段操作，修改最终结果如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>
</span><span class='line'>#BUILD_COMMAND="mvn clean package -DskipTests $@"
</span><span class='line'>BUILD_COMMAND="mvn package -DskipTests $@"</span></code></pre></td></tr></table></div></figure>


<p>然后执行命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0]# sh -x make-distribution.sh --tgz  --skip-java-test -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.1 -Phive 
</span><span class='line'>[root@docker spark-1.1.0]# ll -h
</span><span class='line'>total 185M
</span><span class='line'>...
</span><span class='line'>-rw-r--r--. 1 root root 185M Oct 16 00:09 spark-1.1.0-bin-2.5.1.tgz</span></code></pre></td></tr></table></div></figure>


<p>最终会在目录行打包生成tgz的文件。</p>

<h2>本地运行local</h2>

<p>把本机ip主机名写入到hosts，方便以后windows本机查看日志</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# echo 192.168.154.128 docker &gt;&gt; /etc/hosts
</span><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# cat /etc/hosts
</span><span class='line'>...
</span><span class='line'>192.168.154.128 docker</span></code></pre></td></tr></table></div></figure>


<h3>运行helloworld：</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# bin/run-example SparkPi 10
</span><span class='line'>Spark assembly has been built with Hive, including Datanucleus jars on classpath
</span><span class='line'>...
</span><span class='line'>14/10/16 00:22:36 INFO SparkContext: Job finished: reduce at SparkPi.scala:35, took 2.848632007 s
</span><span class='line'>Pi is roughly 3.139344
</span><span class='line'>14/10/16 00:22:36 INFO SparkUI: Stopped Spark web UI at http://docker:4040
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<h3>交互式操作：</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker spark-1.1.0-bin-2.5.1]# bin/spark-shell --master local[2]
</span><span class='line'>Welcome to
</span><span class='line'>      ____              __
</span><span class='line'>     / __/__  ___ _____/ /__
</span><span class='line'>    _\ \/ _ \/ _ `/ __/  '_/
</span><span class='line'>   /___/ .__/\_,_/_/ /_/\_\   version 1.1.0
</span><span class='line'>      /_/
</span><span class='line'>
</span><span class='line'>Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_60)
</span><span class='line'>...
</span><span class='line'>14/10/16 00:25:57 INFO SparkUI: Started SparkUI at http://docker:4040
</span><span class='line'>14/10/16 00:25:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>14/10/16 00:25:58 INFO Executor: Using REPL class URI: http://192.168.154.128:39385
</span><span class='line'>14/10/16 00:25:58 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@docker:57417/user/HeartbeatReceiver
</span><span class='line'>14/10/16 00:25:58 INFO SparkILoop: Created spark context..
</span><span class='line'>Spark context available as sc.
</span><span class='line'>
</span><span class='line'>scala&gt; 
</span></code></pre></td></tr></table></div></figure>


<p>说明下环境，我使用windows作为开发环境，使用虚拟机中的linux作为测试环境。同时通过ssh连接的隧道来实现windows无缝的访问虚拟机linux操作系统。</p>

<p>启动交互式访问后，就可以通过浏览器访问4040查看spark程序的状态。</p>

<p><img src="http://file.bmob.cn/M00/1E/4B/wKhkA1Q_3NOALefuAAEimqVy6-s418.png" alt="" /></p>

<p>任务已经启动，接下来就可以进行操作：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val textFile=sc.textFile("README.md")
</span><span class='line'>textFile: org.apache.spark.rdd.RDD[String] = README.md MappedRDD[1] at textFile at &lt;console&gt;:12
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.count()
</span><span class='line'>res0: Long = 141
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.first()
</span><span class='line'>res1: String = # Apache Spark
</span><span class='line'>
</span><span class='line'>scala&gt; val linesWithSpark = textFile.filter(line=&gt;line.contains("Spark"))
</span><span class='line'>linesWithSpark: org.apache.spark.rdd.RDD[String] = FilteredRDD[2] at filter at &lt;console&gt;:14
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.filter(line=&gt;line.contains("Spark")).count()
</span><span class='line'>res2: Long = 21
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)
</span><span class='line'>res3: Int = 15
</span><span class='line'>
</span><span class='line'>scala&gt; import java.lang.Math
</span><span class='line'>import java.lang.Math
</span><span class='line'>
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b)=&gt;Math.max(a,b))
</span><span class='line'>res4: Int = 15
</span><span class='line'>
</span><span class='line'>scala&gt; val wordCounts = textFile.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_)
</span><span class='line'>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at &lt;console&gt;:15
</span><span class='line'>
</span><span class='line'>scala&gt; wordCounts.collect()
</span><span class='line'>res5: Array[(String, Int)] = Array((means,1), (under,2), (this,4), (Because,1), (Python,2), (agree,1), (cluster.,1), (its,1), (follows.,1), (general,2), (have,2), (YARN,,3), (pre-built,1), (locally.,1), (locally,2), (changed,1), (MRv1,,1), (several,1), (only,1), (sc.parallelize(1,1), (This,2), (learning,,1), (basic,1), (requests,1), (first,1), (Configuration,1), (MapReduce,2), (CLI,1), (graph,1), (without,1), (documentation,1), ("yarn-client",1), ([params]`.,1), (any,2), (setting,2), (application,1), (prefer,1), (SparkPi,2), (engine,1), (version,3), (file,1), (documentation,,1), (&lt;http://spark.apache.org/&gt;,1), (MASTER,1), (entry,1), (example,3), (are,2), (systems.,1), (params,1), (scala&gt;,1), (provides,1), (refer,1), (MLLib,1), (Interactive,2), (artifact,1), (configure,1), (can,8), (&lt;art...
</span></code></pre></td></tr></table></div></figure>


<p>执行了上面一些操作后，通过网页查看状态变化：</p>

<p><img src="http://file.bmob.cn/M00/1E/4C/wKhkA1Q_3w6AM6njAAF-MCCYh2s170.png" alt="" /></p>

<h2>Spark-standalone集群</h2>

<p>部署集群需要用到多个服务器，这里我使用docker来进行部署。</p>

<p>本来应该早早完成本文的实践，但是在搭建docker-hadoop集群时花费了很多的时间。关于搭建集群dnsmasq处理域名问题参见下一篇文章。
最终实现可以参考：<a href="https://github.com/winse/docker-hadoop/tree/spark-yarn">docker-hadoop</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver1 spark-yarn
</span><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name slaver2 -h slaver2 spark-yarn
</span><span class='line'>[root@docker docker-hadoop]# docker run -d  --dns 172.17.42.1 --name master -h master spark-yarn
</span><span class='line'>
</span><span class='line'>[root@docker docker-hadoop]# docker ps | grep spark | awk '{print $1}' | xargs -I{} docker inspect -f ' ' {} &gt; /etc/dnsmasq.hosts
</span><span class='line'>[root@docker docker-hadoop]# cat /etc/dnsmasq.hosts 
</span><span class='line'>172.17.0.29 master
</span><span class='line'>172.17.0.28 slaver2
</span><span class='line'>172.17.0.27 slaver1
</span><span class='line'>[root@docker docker-hadoop]# service dnsmasq restart
</span><span class='line'>[root@docker docker-hadoop]# ssh hadoop@master
</span><span class='line'>
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id master
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id localhost
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id slaver1
</span><span class='line'>[hadoop@master ~]$ ssh-copy-id slaver2
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ sbin/start-all.sh 
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ /opt/jdk1.7.0_67/bin/jps  -m
</span><span class='line'>266 Jps -m
</span><span class='line'>132 Master --ip master --port 7077 --webui-port 8080
</span></code></pre></td></tr></table></div></figure>


<p>通过网页可以查看集群的状态：</p>

<p><img src="http://file.bmob.cn/M00/1E/F8/wKhkA1RClV2AE0biAAEmpXJlzTc914.png" alt="" /></p>

<p>运行任务连接到master：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-shell --master spark://master:7077
</span><span class='line'>...
</span><span class='line'>14/10/17 11:31:08 INFO BlockManagerMasterActor: Registering block manager slaver2:55473 with 265.4 MB RAM
</span><span class='line'>14/10/17 11:31:09 INFO BlockManagerMasterActor: Registering block manager slaver1:33441 with 265.4 MB RAM
</span><span class='line'>
</span><span class='line'>scala&gt; </span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmG-AO--XAAD84ATrCew955.png" alt="" /></p>

<p>从上图可以看到，程序已经正确连接到spark集群，master为driver，任务节点为slaver1和slaver2。下面运行下程序，然后通过网页查看运行的状态。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val textFile=sc.textFile("README.md")
</span><span class='line'>scala&gt; textFile.count()
</span><span class='line'>scala&gt; textFile.map(_.split(" ").size).reduce((a,b) =&gt; if(a&gt;b) a else b)</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/F9/wKhkA1RCmdmAB3M9AAFIzMb4yk0370.png" alt="" /></p>

<p>系统安装好了，启动spark-standalone集群和hadoop-yarn一样。配置ssh、java，然后启动，配合网页8080/4040可以实时的了解任务的指标。</p>

<h2>yarn集群</h2>

<p>如果你是按照前面的步骤来操作的，需要先把spark-standalone的集群停掉。端口8080和yarn web使用端口冲突，会导致yarn启动失败。</p>

<p>修改spark-env.sh，添加HADOOP_CONF_DIR参数。然后提交任务到yarn上执行就行了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ cat conf/spark-env.sh
</span><span class='line'>#!/usr/bin/env bash
</span><span class='line'>
</span><span class='line'>JAVA_HOME=/opt/jdk1.7.0_67 
</span><span class='line'>
</span><span class='line'>HADOOP_CONF_DIR=/opt/hadoop-2.5.1/etc/hadoop
</span><span class='line'>
</span><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster lib/spark-examples-1.1.0-hadoop2.5.1.jar  10</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/1E/FD/wKhkA1RCszeAALCPAAK1Nzk6faQ330.png" alt="" /></p>

<p>运行的结果输出在driver的slaver2节点，对应输出型来说不是很直观。spark-yarn提供了另一种方式，driver直接本地运行<em>yarn-client</em>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master spark-1.1.0-bin-2.5.1]$ bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client lib/spark-examples-1.1.0-hadoop2.5.1.jar  10
</span><span class='line'>...
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8248 ms on slaver1 (1/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 231 ms on slaver1 (2/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 158 ms on slaver1 (3/10)
</span><span class='line'>14/10/17 13:31:02 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 284 ms on slaver1 (4/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 175 ms on slaver1 (5/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 301 ms on slaver1 (6/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 175 ms on slaver1 (7/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 143 ms on slaver1 (8/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 164 ms on slaver1 (9/10)
</span><span class='line'>14/10/17 13:31:03 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, slaver1, PROCESS_LOCAL, 1228 bytes)
</span><span class='line'>14/10/17 13:31:03 INFO cluster.YarnClientSchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@slaver2:51923/user/Executor#1132577949] with ID 1
</span><span class='line'>14/10/17 13:31:04 INFO util.RackResolver: Resolved slaver2 to /default-rack
</span><span class='line'>14/10/17 13:31:04 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 397 ms on slaver1 (10/10)
</span><span class='line'>14/10/17 13:31:04 INFO cluster.YarnClientClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
</span><span class='line'>14/10/17 13:31:04 INFO scheduler.DAGScheduler: Stage 0 (reduce at SparkPi.scala:35) finished in 26.084 s
</span><span class='line'>14/10/17 13:31:04 INFO spark.SparkContext: Job finished: reduce at SparkPi.scala:35, took 28.31400558 s
</span><span class='line'>Pi is roughly 3.140248</span></code></pre></td></tr></table></div></figure>


<h2>总结</h2>

<p>本文主要是搭建spark的环境搭建，本地运行、以及在docker中搭建spark集群、yarn集群三种方式。本地运行最简单方便，但是没有模拟到集群环境；spark提供了yarn框架上的实现，直接提交任务到yarn即可；spark集群相对比较简单和方便，接下来的远程调试主要通过spark伪分布式集群方式来进行。</p>

<h2>参考</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/building-with-maven.html">Building Spark with Maven</a></li>
<li><a href="http://spark.apache.org/docs/latest/quick-start.html">Quick Start</a></li>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html">Spark Standalone Mode</a></li>
<li><a href="http://spark.apache.org/docs/latest/configuration.html">Spark Configuration</a></li>
<li><a href="http://www.07net01.com/linux/zuixindnsmasqanzhuangbushuxiangjie_centos6__653221_1381214991.html">DNS</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读码] Spark1.1.0前篇--代码统计导入Eclipse]]></title>
    <link href="http://winseliu.com/blog/2014/10/12/read-spark1-source-starter/"/>
    <updated>2014-10-12T13:12:57+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/12/read-spark1-source-starter</id>
    <content type="html"><![CDATA[<p>看过亚太研究院的spark在线教学视频，说spark1.0的源码仅有3w+的代码，蠢蠢欲动。先具体看下源码的量，估算估算；然后搭建eclipse读码环境。</p>

<h2>计算源码行数</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ git branch -v
</span><span class='line'>* (detached from v1.1.0) 2f9b2bd [maven-release-plugin] prepare release v1.1.0-rc4
</span><span class='line'>  master                 4d8ae70 [behind 1246] Cleanup on Connection and ConnectionManager
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ find . -name "*.scala" | grep 'src/main' | xargs sed  -e 's:\/\*.*\*\/::' -e  '/\/\*/, /\*\//{
</span><span class='line'>/\/\*/{
</span><span class='line'> s:\/\*.*::p
</span><span class='line'>}
</span><span class='line'>/\*\//{
</span><span class='line'> s:.*\*\/::p
</span><span class='line'>}
</span><span class='line'>d
</span><span class='line'>}' | sed -e '/^\s*$/d' -e '/^\s*\/\//d' | grep -v '^import' | grep -v '^package' | wc -l
</span><span class='line'>72967
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^scala^java
</span><span class='line'>1749
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^src/main^core/src/main
</span><span class='line'>877
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^java^scala
</span><span class='line'>38526
</span></code></pre></td></tr></table></div></figure>


<p>全部源码的数量（去掉测试）大概在7W左右，仅计算核心代码core下面的代码量在4W。从量上面来说还是比较乐观的，学习scala然后读spark的源码。</p>

<p>spark1.0.0的核心代码量在3w左右。1.1多了大概1w行！！</p>

<h2>Docker</h2>

<p>查看目录结构的时刻，看到spark1下面竟然有docker，不过看Dockerfile的内容只是简单的安装了scala、把本机的spark映射到docker容器、然后运行spark主从集群。</p>

<h2>导入eclipse</h2>

<p>spark使用主要使用scala编写，首先需要下载<a href="http://scala-ide.org/download/sdk.html">scala-ide</a>直接下载2.10的版本（基于eclipse，很多操作都类似）；然后下载<a href="https://github.com/apache/spark.git">spark的源码</a>检出v1.1.0的；然后使用maven生成eclipse工程文件。</p>

<p>(不推荐)使用<a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-Eclipse">sbt生成工程文件</a>。这种方式会缺少一些依赖的jar，处理比较麻烦，还不清楚到底是少了啥！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd sbt/
</span><span class='line'>$ sed -i 's/^M//g' *
</span><span class='line'>$ cd ..
</span><span class='line'>$ sbt/sbt eclipse -mem 512</span></code></pre></td></tr></table></div></figure>


<p>(推荐)使用MVN编译生成，<a href="http://spark.apache.org/docs/latest/building-with-maven.html">使用Maven生成官网文章</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ git clean -x -fd #清理非仓库代码
</span><span class='line'>
</span><span class='line'>$ echo $SCALA_HOME #指定scala-home
</span><span class='line'>/cygdrive/d/scala
</span><span class='line'>
</span><span class='line'># 这里我直接修改默认值，理论上加 -Phadoop-2.2 选项应该也是可以的
</span><span class='line'>$ vi pom.xml # hadoop.version 2.2.0
</span><span class='line'>$ mvn eclipse:eclipse
</span><span class='line'>
</span><span class='line'>$ find . -name ".classpath" | xargs sed -i -e 's/including="\*\*\/\*.java"//' -e 's/excluding="\*\*\/\*.java"//'
</span><span class='line'>
</span><span class='line'>#也可以把添加特性的操作/添加scala源码包操作批量处理掉</span></code></pre></td></tr></table></div></figure>


<p>然后导入到eclipse，然后再针对性的处理报错：</p>

<ul>
<li>先把每个工程都<strong>添加scala特性</strong></li>
<li>把含有python源码包的去掉（手动删除.classpath中classpathentry即可）</li>
<li>确认下并加上<code>src/test/scala</code>的源码包。</li>
</ul>


<p>注意，进行上面的步骤之前，由于scala源文件比较多，编译的时间会比较长，先把Project->Build Automatically去掉，然后一次性把问题处理掉后再手动build！</p>

<ul>
<li>手动使用<code>existing maven projects</code>导入yarn/stable，然后把<strong>yarn/common以链接的形式引入</strong>，并添加到源码包。</li>
</ul>


<p><img src="http://file.bmob.cn/M00/1C/E7/wKhkA1Q7jQ2AMhweAAOC-l-jcz4872.png" alt="" /></p>

<p>还有一个<strong> value q is not a member of StringContext </strong><a href="http://docs.scala-lang.org/overviews/quasiquotes/intro.html">quasiquotes</a>的错误，有些类需要在2.10添加编译组件才能正常编译，修改scala编译首选项。</p>

<p><img src="http://file.bmob.cn/M00/1D/07/wKhkA1Q76GyAFNYPAAEYJfk_ZGw816.png" alt="" /></p>

<p>添加依赖的编译组件后，整个功能就能正常编译通过了。接下来就能调试看源码了。</p>

<p><strong>备注：</strong>clean后发现target目录下并没有重新编译生成class，去掉<code>-Xshow-phases</code>才行。</p>

<blockquote><p> -Xshow-phases                  Print a synopsis of compiler phases.</p></blockquote>

<h2>Maven编译spark</h2>

<p>如果使用的hadoop版本在官网没有集成assembly版本，可以使用maven手动构建。至于打包可以查看下一篇文章。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>$ mvn -Pyarn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean package</span></code></pre></td></tr></table></div></figure>


<p><code>yarn</code>的profile能够编译成可执行的jar文件（包括所有依赖的spark），具体内容下一篇讲。</p>

<h2>小结</h2>

<p>断断续续的写了两天，字数统计弄了大半天，主要在于多行注释的处理。时间最主要都消耗在sbt、maven构建eclipse项目文件（生成、fixed）上。编译scala量上去后确实非常非常的慢，不管是maven还是eclipse都慢！</p>

<p>下一篇将使用docker搭建spark环境，并使用远程调试连接到helloworld程序。</p>

<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/24800129/scala-maven-builder-doesnt-understand-quasiquotes">Scala maven builder doesn&rsquo;t understand quasiquotes</a></li>
<li><a href="http://docs.scala-lang.org/overviews/macros/paradise.html">Macro Paradise</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[思考]]></title>
    <link href="http://winseliu.com/blog/2014/10/07/thinking/"/>
    <updated>2014-10-07T19:07:26+08:00</updated>
    <id>http://winseliu.com/blog/2014/10/07/thinking</id>
    <content type="html"><![CDATA[<p>随着年龄的增大，很多原来不曾想的问题慢慢的都开始环绕在自己四周。开始让自己不得不反思，不得不去改变。</p>

<p>本人是一个性格比较极端，又很内向，所以对自己不关心、无自己原来没有直接联系的东西，很少体现积极主动的一面。时时刻刻展现着保守派的作风。自己又在学习能力方面自我感觉良好，对现状总是很不满，对一样事物的持续坚持的耐久力不足（倒不是不能吃苦、吃不了苦的问题）！</p>

<p>从出生到毕业，一直以来都有亲人朋友让我依靠，有很明确值得挑战和超越的目标（总体水平一般，在我前面的人乌压压一片）。出来工作后一直都很迷失，不知道自己能干啥，可以干啥，师范类专业连教师资格证都没有拿到（不是后悔，自己觉得不应该）！！现在想来其实自己太执拗，像极了不撞南墙死不改的蛮牛！！</p>

<p>年龄增加体力不及，开始思考着应该去锻炼锻炼了，但是一直各种借口无疾而终！觉得身体还行，以后再说。。。
工作资历增加直接辅导指导的大哥不再，开始各种瞎折腾，东一锤西一棒，终究是拣了芝麻丢了西瓜！觉得学习能力强以后都敢都来得及，以后再学呗。。。   <br/>
但是在运动场上，一直坚持运动的同学，打个3、4个小时的羽毛球气不喘一下，这时开始懊悔。
当原来一起协作的同事，开始在领域有所斩获，各种嫉妒羡慕的心里开始作祟。</p>

<p>星期一个个的开始了结束，自己却没有得到该有的锤炼和进度，在大势所趋下，自己却总是那么的慢慢吞吞！阅读一个类的千行源码，竟然断断续续花费了仅2个月！本来年前看tomcat原来的计划最终石沉大海！</p>

<p>总是对自己不够狠；狠下来一次后，总是各种理由，最终不能坚持！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[配置ssh登录docker-centos]]></title>
    <link href="http://winseliu.com/blog/2014/09/30/docker-ssh-on-centos/"/>
    <updated>2014-09-30T00:10:02+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/30/docker-ssh-on-centos</id>
    <content type="html"><![CDATA[<p>上一篇写的是docker的入门知识，并没有进行实战。这些记录下使用ssh登录centos容器。</p>

<p>前文中参考的博客介绍了使用ssh登录tutorial容器（ubuntu），然后进行tomcat的安装，以及通过端口映射在客户机进行访问的例子。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull learn/tutorial
</span><span class='line'>docker run -i -t learn/tutorial /bin/bash
</span><span class='line'>  apt-get update
</span><span class='line'>  apt-get install openssh-server
</span><span class='line'>  which sshd
</span><span class='line'>  /usr/sbin/sshd
</span><span class='line'>  mkdir /var/run/sshd
</span><span class='line'>  passwd #输入用户密码，我这里设置为123456，便于SSH客户端登陆使用
</span><span class='line'>  exit #退出
</span><span class='line'>docker ps -l
</span><span class='line'>docker commit 51774a81beb3 learn/tutorial # 提交后，下次启动就可以基于容器更改的系统
</span><span class='line'>docker run -d -p 49154:22 -p 80:8080 learn/tutorial /usr/sbin/sshd -D
</span><span class='line'>ssh root@127.0.0.1 -p 49154
</span><span class='line'>  # 在ubuntu 12.04上安装oracle jdk 7
</span><span class='line'>  apt-get install python-software-properties
</span><span class='line'>  add-apt-repository ppa:webupd8team/java
</span><span class='line'>  apt-get update
</span><span class='line'>  apt-get install -y wget
</span><span class='line'>  apt-get install oracle-java7-installer
</span><span class='line'>  java -version
</span><span class='line'>  # 下载tomcat 7.0.47
</span><span class='line'>  wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-7/v7.0.47/bin/apache-tomcat-7.0.47.tar.gz
</span><span class='line'>  # 解压，运行
</span><span class='line'>  tar xvf apache-tomcat-7.0.47.tar.gz
</span><span class='line'>  cd apache-tomcat-7.0.47
</span><span class='line'>  bin/startup.sh</span></code></pre></td></tr></table></div></figure>


<p>然而在centos上，运行是不成功的。总结操作如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker pull centos:centos6
</span><span class='line'>[root@docker ~]# docker run -i -t  centos:centos6 /bin/bash
</span><span class='line'>  yum install which openssh-server openssh-clients
</span><span class='line'>
</span><span class='line'>  /usr/sbin/sshd # 这里会报错，需要手动生成key
</span><span class='line'>  ssh-keygen -f /etc/ssh/ssh_host_rsa_key
</span><span class='line'>  ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key
</span><span class='line'>
</span><span class='line'>  vi /etc/pam.d/sshd  # 修改pam_loginuid.so为optional
</span><span class='line'>  # /bin/sed -i 's/.*session.*required.*pam_loginuid.so.*/session optional pam_loginuid.so/g' /etc/pam.d/sshd
</span><span class='line'>  
</span><span class='line'>  passwd # 添加密码</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>[root@docker ~]# docker commit 3a7b6994bb2a winse/hadoop # 保存为自己使用的版本
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d winse/hadoop /usr/sbin/sshd
</span><span class='line'>f5cb57f6ec22dd9d257bf610322e2bd547ea0064262fcad63308b932c0490670
</span><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS                     PORTS               NAMES
</span><span class='line'>f5cb57f6ec22        winse/hadoop:latest   /usr/sbin/sshd      2 seconds ago       Exited (0) 2 seconds ago                       sharp_rosalind      
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d -p 8888:22 winse/hadoop /usr/sbin/sshd -D
</span><span class='line'>f9814253159373e8a8df3261904200a733b41c63f55708db3cb56a7ebf650cef
</span><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS                  NAMES
</span><span class='line'>f98142531593        winse/hadoop:latest   /usr/sbin/sshd -D   5 seconds ago       Up 4 seconds        0.0.0.0:8888-&gt;22/tcp   boring_bell         
</span><span class='line'>[root@docker ~]# ssh localhost -p 8888
</span><span class='line'>The authenticity of host '[localhost]:8888 ([::1]:8888)' can't be established.
</span><span class='line'>RSA key fingerprint is f5:5e:be:ae:ea:b1:ed:e8:49:43:28:9e:80:87:0d:86.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added '[localhost]:8888' (RSA) to the list of known hosts.
</span><span class='line'>root@localhost's password: 
</span><span class='line'>Last login: Mon Sep 29 14:48:23 2014 from localhost
</span><span class='line'>-bash-4.1# </span></code></pre></td></tr></table></div></figure>


<p>参数<code>-D</code>表示sshd运行在前台。这样当前的docker容器就会一直有程序在运行，不至于执行完指定的任务就被关闭掉了。</p>

<p>在centos配置ssh登录需要进行额外参数的设置。这个还是挺折腾人的。关于把<code>/etc/pam.d/sshd</code>中的<code>pam_loginuid.so</code>修改为optional，<a href="(http://stackoverflow.com/questions/21391142/why-is-it-needed-to-set-pam-loginuid-to-its-optional-value-with-docker">stackoverflow</a>)上的回答还是挺中肯的。</p>

<p>连上ssh后，下一步就和你远程操作服务器一样了。其实docker运行一个容器后，就会分配一个ip，你也可以根据这个ip来连接。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -t -i winse/hadoop /bin/bash
</span><span class='line'>bash-4.1# ssh localhost
</span><span class='line'>ssh: connect to host localhost port 22: Connection refused
</span><span class='line'>bash-4.1# service sshd start
</span><span class='line'>Starting sshd:                                             [  OK  ]
</span><span class='line'>bash-4.1# ifconfig
</span><span class='line'>eth0      Link encap:Ethernet  HWaddr 1E:2B:23:16:98:7E  
</span><span class='line'>          inet addr:172.17.0.31  Bcast:0.0.0.0  Mask:255.255.0.0
</span><span class='line'>          inet6 addr: fe80::1c2b:23ff:fe16:987e/64 Scope:Link
</span><span class='line'>
</span><span class='line'># 新开一个终端
</span><span class='line'>[root@docker ~]# ssh 172.17.0.31
</span><span class='line'>The authenticity of host '172.17.0.31 (172.17.0.31)' can't be established.
</span><span class='line'>RSA key fingerprint is f5:5e:be:ae:ea:b1:ed:e8:49:43:28:9e:80:87:0d:86.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added '172.17.0.31' (RSA) to the list of known hosts.
</span><span class='line'>root@172.17.0.31's password: 
</span><span class='line'>Last login: Mon Sep 29 14:48:23 2014 from localhost
</span><span class='line'>-bash-4.1#           </span></code></pre></td></tr></table></div></figure>


<h2>使用Dockerfile脚本安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# mkdir hadoop
</span><span class='line'>[root@docker ~]# cd hadoop/
</span><span class='line'>[root@docker hadoop]# touch Dockerfile
</span><span class='line'>[root@docker hadoop]# vi Dockerfile
</span><span class='line'>  # hadoop2 on docker-centos
</span><span class='line'>  FROM centos:centos6
</span><span class='line'>  MAINTAINER Winse &lt;fuqiuliu2006@qq.com&gt;
</span><span class='line'>  RUN yum install -y which openssh-clients openssh-server #-y表示交互都输入yes
</span><span class='line'>
</span><span class='line'>  RUN ssh-keygen -f /etc/ssh/ssh_host_rsa_key
</span><span class='line'>  RUN ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key
</span><span class='line'>
</span><span class='line'>  RUN echo 'root:hadoop' |chpasswd
</span><span class='line'>
</span><span class='line'>  RUN sed -i '/pam_loginuid.so/c session    optional     pam_loginuid.so'  /etc/pam.d/sshd
</span><span class='line'>
</span><span class='line'>  EXPOSE 22
</span><span class='line'>  CMD /usr/sbin/sshd -D
</span><span class='line'>  
</span><span class='line'>[root@docker hadoop]# docker build -t="winse/hadoop" .
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>winse/hadoop        latest              9d7f115ef0ec        5 minutes ago       289.1 MB
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker run -d --name slaver1 winse/hadoop
</span><span class='line'>[root@docker hadoop]# docker run -d --name slaver2 winse/hadoop
</span><span class='line'>[root@docker hadoop]# docker run -d --name master1 -P --link slaver1:slaver1 --link slaver2:slaver2  winse/hadoop
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker restart slaver1 slaver2 master1
</span><span class='line'>slaver1
</span><span class='line'>slaver2
</span><span class='line'>master1
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker port master1 22
</span><span class='line'>0.0.0.0:49159
</span><span class='line'>[root@docker hadoop]# ssh localhost -p 49159
</span><span class='line'>... 
</span><span class='line'>-bash-4.1# cat /etc/hosts
</span><span class='line'>172.17.0.31     7ef63f98e2d1
</span><span class='line'>127.0.0.1       localhost
</span><span class='line'>::1     localhost ip6-localhost ip6-loopback
</span><span class='line'>fe00::0 ip6-localnet
</span><span class='line'>ff00::0 ip6-mcastprefix
</span><span class='line'>ff02::1 ip6-allnodes
</span><span class='line'>ff02::2 ip6-allrouters
</span><span class='line'>172.17.0.29     slaver1
</span><span class='line'>172.17.0.30     slaver2</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.blogjava.net/yongboy/archive/2013/12/12/407498.html">Docker学习笔记之一，搭建一个JAVA Tomcat运行环境</a></li>
<li><a href="http://www.csdn123.com/html/topnews201408/36/1236.htm">Docker之配置Centos_ssh</a></li>
<li><a href="http://linux.die.net/man/8/pam_loginuid">pam_loginuid(8) - Linux man page</a></li>
<li><a href="http://stackoverflow.com/questions/21391142/why-is-it-needed-to-set-pam-loginuid-to-its-optional-value-with-docker">Why is it needed to set <code>pam_loginuid</code> to its <code>optional</code> value with docker?</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker入门]]></title>
    <link href="http://winseliu.com/blog/2014/09/27/docker-start-guide-on-centos/"/>
    <updated>2014-09-27T20:28:24+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/27/docker-start-guide-on-centos</id>
    <content type="html"><![CDATA[<p>docker进一年来火热，发现挺适合用来做运维系统发布的。如果用来捣鼓hadoop的系统部署感觉还是挺不错的。下面一起来学习下docker吧。</p>

<p>docker中提供了<a href="https://docs.docker.com/installation/windows/">windows的安装文档</a>，但是其实很坑爹啊。尽管<a href="https://github.com/boot2docker/windows-installer/releases">提供exe安装</a>，但是最终还是安装visualbox，然后启动带了docker的linux系统（iso）。</p>

<p>如果你已经安装了vmware，但没有安装linux，可以直接<a href="https://github.com/boot2docker/boot2docker/releases">下载iso</a>，然后通过iso来启动。</p>

<h2>安装</h2>

<p>如果你同时安装了vmware，又已经安装了linux，那下面简单列出安装配置docker中使用的命令。docker需要64位的linux操作系统，我这里使用的是centos6，具体的安装步骤看<a href="https://docs.docker.com/installation/centos/">官网的安装教程</a>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# yum install epel-release
</span><span class='line'>
</span><span class='line'>[root@docker ~]# yum install docker-io
</span><span class='line'>[root@docker ~]# service docker start
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run learn/tutorial /bin/echo hello world
</span><span class='line'>Unable to find image 'learn/tutorial' locally
</span><span class='line'>Pulling repository learn/tutorial
</span><span class='line'>8dbd9e392a96: Pulling fs layer 
</span><span class='line'>8dbd9e392a96: Download complete 
</span><span class='line'>hello world
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>learn/tutorial      latest              8dbd9e392a96        17 months ago       128 MB
</span><span class='line'>[root@docker ~]# docker images learn/tutorial 
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>learn/tutorial      latest              8dbd9e392a96        17 months ago       128 MB</span></code></pre></td></tr></table></div></figure>


<p>docker执行run命令时，如果指定的image本地不存在，会从<a href="https://registry.hub.docker.com/">hub服务器</a>获取。也可以先从服务器获取image，然后在执行。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull centos</span></code></pre></td></tr></table></div></figure>


<h2>简单入门</h2>

<p><a href="https://docs.docker.com/userguide/dockerizing/">HelloWorld教程</a></p>

<h4>单次执行</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run learn/tutorial /bin/echo 'hello world'
</span><span class='line'>hello world</span></code></pre></td></tr></table></div></figure>


<p>命令执行完后，容器就会关闭。</p>

<h4>交互式执行方式</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -t -i learn/tutorial /bin/bash
</span><span class='line'>root@274ede23baad:/# uptime
</span><span class='line'> 12:36:02 up  5:59,  0 users,  load average: 0.00, 0.00, 0.00
</span><span class='line'>root@9db219d2e98b:/# cat /etc/issue
</span><span class='line'>Ubuntu 12.04 LTS \n \l
</span><span class='line'>root@274ede23baad:/# pwd
</span><span class='line'>/
</span><span class='line'>root@274ede23baad:/# ls
</span><span class='line'>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  selinux  srv  sys  tmp  usr  var
</span><span class='line'>root@274ede23baad:/# exit
</span><span class='line'>exit</span></code></pre></td></tr></table></div></figure>


<ul>
<li>-t flag assigns a pseudo-tty or terminal inside our new container。</li>
<li>-i flag allows us to make an interactive connection by grabbing the standard in (STDIN) of the container.</li>
</ul>


<h4>后台任务</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -d learn/tutorial /bin/sh -c "while true; do echo hello world; sleep 1; done" 
</span><span class='line'>17e28b56e0cc4ddb5522736e2bcfd752d849a5b1d0b598478ee66b255801aa7c
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS               NAMES
</span><span class='line'>17e28b56e0cc        learn/tutorial:latest   /bin/sh -c 'while tr   2 minutes ago       Up 2 minutes                            trusting_wozniak    </span></code></pre></td></tr></table></div></figure>


<ul>
<li>-d flag tells Docker to run the container and put it in the background, to daemonize it.</li>
</ul>


<p>执行返回的是containter id(唯一ID)。通过ps可以查看当前的后台任务列表。ps列表中的containter id对应，可以查看相应的信息，最后的字段是一个随机指定的名字（也可以指定，后面再讲）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker logs trusting_wozniak
</span><span class='line'>hello world
</span><span class='line'>hello world
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker stop trusting_wozniak
</span><span class='line'>trusting_wozniak
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span></code></pre></td></tr></table></div></figure>


<p>可以通过logs查看容器的标准输出，通过stop来停止容器。</p>

<h2>深入容器</h2>

<p><a href="https://docs.docker.com/userguide/usingdocker/">Working with Containers</a></p>

<p>可以交互式的方式运行container，也可以后台任务的方式运行。</p>

<p>docker的命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Usage:  [sudo] docker [flags] [command] [arguments] ..
</span><span class='line'># Example:
</span><span class='line'>$ sudo docker run -i -t ubuntu /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>每个命令可以指定跟一系列的开关标识(flags)和参数(arguments)。</p>

<h4>各种参数</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker version
</span><span class='line'>
</span><span class='line'>$ docker run -d -P training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps -l
</span><span class='line'>CONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES
</span><span class='line'>bc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155-&gt;5000/tcp  nostalgic_morse
</span><span class='line'>
</span><span class='line'># docker run -d -p 6379 -v /home/hadoop/redis-2.8.13:/opt/redis-2.8.13 learn/tutorial /opt/redis-2.8.13/src/redis-server 
</span><span class='line'>be0b410f3601ea36070b3e519d9cc7cbe259caa2392f468c2dd2baebef42c4a8
</span><span class='line'>
</span><span class='line'># docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                     NAMES
</span><span class='line'>be0b410f3601        learn/tutorial:latest   /opt/redis-2.8.13/sr   10 seconds ago      Up 10 seconds       0.0.0.0:49153-&gt;6379/tcp   sad_colden          
</span><span class='line'>
</span><span class='line'># /home/hadoop/redis-2.8.13/src/redis-cli -p 49153
</span><span class='line'>127.0.0.1:49153&gt; keys *
</span><span class='line'>(empty list or set)
</span><span class='line'>127.0.0.1:49153&gt; </span></code></pre></td></tr></table></div></figure>


<ul>
<li>-P flag is new and tells Docker to map any required network ports inside our container to our host. This lets us view our web application.</li>
<li>-l tells the docker ps command to return the details of the last container started.</li>
<li>-a the docker ps command only shows information about running containers. If you want to see stopped containers too use the -a flag.</li>
<li>-p Network port bindings are very configurable in Docker. In our last example the -P flag is a shortcut for -p 5000 that maps port 5000 inside the container to a high port (from the range 49153 to 65535) on the local Docker host. We can also bind Docker containers to specific ports using the -p flag。</li>
<li>-v flag you can also mount a directory from your own host into a container.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker run -d -p 6379:6379 -v /home/hadoop/redis-2.8.13:/opt/redis-2.8.13 learn/tutorial /opt/redis-2.8.13/src/redis-server 
</span><span class='line'>2c50850c9437698769e54281a9f4154dc4120da2e113802454f1a23c83ab91fe
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                    NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   29 seconds ago      Up 28 seconds       0.0.0.0:6379-&gt;6379/tcp   naughty_yonath  
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker port naughty_yonath 6379
</span><span class='line'>0.0.0.0:6379
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker logs -f naughty_yonath
</span><span class='line'>...
</span><span class='line'>[1] 27 Sep 13:48:12.192 * The server is now ready to accept connections on port 6379
</span><span class='line'>[1] 27 Sep 13:50:33.228 * DB saved on disk
</span><span class='line'>[1] 27 Sep 13:50:43.730 * DB saved on disk</span></code></pre></td></tr></table></div></figure>


<ul>
<li>-f This time though we&rsquo;ve added a new flag, -f. This causes the docker logs command to act like the tail -f command and watch the container&rsquo;s standard out. We can see here the logs from Flask showing the application running on port 5000 and the access log entries for it.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker top naughty_yonath
</span><span class='line'>UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
</span><span class='line'>root                5015                1433                0                   21:48               ?                   00:00:00            /opt/redis-2.8.13/src/redis-server *:6379
</span><span class='line'>[root@docker redis-2.8.13]# docker inspect naughty_yonath
</span><span class='line'>...
</span><span class='line'>    "Volumes": {
</span><span class='line'>        "/opt/redis-2.8.13": "/home/hadoop/redis-2.8.13"
</span><span class='line'>    },
</span><span class='line'>    "VolumesRW": {
</span><span class='line'>        "/opt/redis-2.8.13": true
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker inspect -f '' naughty_yonath
</span><span class='line'>map[/opt/redis-2.8.13:/home/hadoop/redis-2.8.13]
</span></code></pre></td></tr></table></div></figure>


<h4>重启</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker stop naughty_yonath
</span><span class='line'>naughty_yonath
</span><span class='line'>[root@docker redis-2.8.13]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS                     PORTS               NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   8 minutes ago       Exited (0) 5 seconds ago                       naughty_yonath      
</span><span class='line'>[root@docker redis-2.8.13]# docker start naughty_yonath
</span><span class='line'>naughty_yonath
</span><span class='line'>[root@docker redis-2.8.13]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                    NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   8 minutes ago       Up 1 seconds        0.0.0.0:6379-&gt;6379/tcp   naughty_yonath</span></code></pre></td></tr></table></div></figure>


<h4>删除</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker stop naughty_yonath
</span><span class='line'>docker rm naughty_yonath</span></code></pre></td></tr></table></div></figure>


<h2>Images</h2>

<p><a href="https://docs.docker.com/userguide/dockerimages/">Working with Docker Images</a></p>

<h4>列出本地的images</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker images
</span><span class='line'># REPO[:TAG]
</span><span class='line'>docker run -t -i ubuntu:14.04 /bin/bash
</span><span class='line'>docker run -t -i ubuntu:latest /bin/bash</span></code></pre></td></tr></table></div></figure>


<h4>从Hub获取镜像Image</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull centos
</span><span class='line'>docker run -t -i centos /bin/bash
</span><span class='line'>docker search sinatra 
</span><span class='line'>docker pull training/sinatra</span></code></pre></td></tr></table></div></figure>


<h4>创建自己的images</h4>

<p>直接更新image</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -t -i training/sinatra /bin/bash
</span><span class='line'>root@0b2616b0e5a8:/# gem install json
</span><span class='line'>$ sudo docker commit -m="Added json gem" -a="Kate Smith" \
</span><span class='line'>  0b2616b0e5a8 ouruser/sinatra:v2
</span><span class='line'>$ docker images
</span><span class='line'>$ docker run -t -i ouruser/sinatra:v2 /bin/bash
</span><span class='line'>root@78e82f680994:/#</span></code></pre></td></tr></table></div></figure>


<p>通过DockerFile来添加功能，进行更新。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mkdir sinatra
</span><span class='line'>$ cd sinatra
</span><span class='line'>$ touch Dockerfile
</span><span class='line'>  # This is a comment
</span><span class='line'>  FROM ubuntu:14.04
</span><span class='line'>  MAINTAINER Kate Smith &lt;ksmith@example.com&gt;
</span><span class='line'>  RUN apt-get update && apt-get install -y ruby ruby-dev
</span><span class='line'>  RUN gem install sinatra
</span><span class='line'>
</span><span class='line'>$ docker build -t="ouruser/sinatra:v2" .
</span><span class='line'>$ docker run -t -i ouruser/sinatra:v2 /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>具体的DockerFile中各个指令的含义及其使用方法，参考<a href="https://docs.docker.com/userguide/dockerimages/">Building an image from a Dockerfile</a>和<a href="https://docs.docker.com/articles/dockerfile_best-practices/">Best Practices for Writing Dockerfiles</a>，以及<a href="https://docs.docker.com/reference/builder/">Dockerfile Reference</a>。具体例子<a href="https://github.com/perl/docker-perl/blob/r20140922.0/5.020.001-64bit,threaded/Dockerfile">docker-perl</a></p>

<h4>添加新标签Tag</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker tag 5db5f8471261 ouruser/sinatra:devel
</span><span class='line'>$ docker images ouruser/sinatra
</span><span class='line'>REPOSITORY          TAG     IMAGE ID      CREATED        VIRTUAL SIZE
</span><span class='line'>ouruser/sinatra     latest  5db5f8471261  11 hours ago   446.7 MB
</span><span class='line'>ouruser/sinatra     devel   5db5f8471261  11 hours ago   446.7 MB</span></code></pre></td></tr></table></div></figure>


<h4>上传分享到<a href="https://hub.docker.com/">hub</a></h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker push ouruser/sinatra</span></code></pre></td></tr></table></div></figure>


<h4>从本地删除</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker rmi training/sinatra</span></code></pre></td></tr></table></div></figure>


<h2>多container结合使用</h2>

<p><a href="https://docs.docker.com/userguide/dockerlinks/">Linking Containers Together</a></p>

<h4>端口映射</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d -P training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker ps nostalgic_morse
</span><span class='line'>CONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES
</span><span class='line'>bc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155-&gt;5000/tcp  nostalgic_morse
</span><span class='line'>
</span><span class='line'>docker run -d -p 5000:5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker run -d -p 127.0.0.1:5000:5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker run -d -p 127.0.0.1::5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'># The -p flag can be used multiple times to configure multiple ports.
</span><span class='line'>docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker port nostalgic_morse 5000
</span><span class='line'>127.0.0.1:49155</span></code></pre></td></tr></table></div></figure>


<h4>Container Linking</h4>

<p>docker想的还是很周到的。面临两个container互相访问，一个db，一个web，哪web怎么访问db的数据呢？</p>

<p>指定container的名称：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -d -P --name web training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps -l
</span><span class='line'>CONTAINER ID  IMAGE                  COMMAND        CREATED       STATUS       PORTS                    NAMES
</span><span class='line'>aed84ee21bde  training/webapp:latest python app.py  12 hours ago  Up 2 seconds 0.0.0.0:49154-&gt;5000/tcp  web
</span><span class='line'>
</span><span class='line'>$ docker inspect -f "" aed84ee21bde
</span><span class='line'>/web</span></code></pre></td></tr></table></div></figure>


<p>容器互通：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -d --name db training/postgres
</span><span class='line'>
</span><span class='line'>$ docker rm -f web
</span><span class='line'>$ docker run -d -P --name web --link db:db training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps
</span><span class='line'>CONTAINER ID  IMAGE                     COMMAND               CREATED             STATUS             PORTS                    NAMES
</span><span class='line'>349169744e49  training/postgres:latest  su postgres -c '/usr  About a minute ago  Up About a minute  5432/tcp                 db, web/db
</span><span class='line'>aed84ee21bde  training/webapp:latest    python app.py         16 hours ago        Up 2 minutes       0.0.0.0:49154-&gt;5000/tcp  web</span></code></pre></td></tr></table></div></figure>


<p>链接后，在web容器会添加DB的环境变量，同时把db的ip加入到/etc/hosts中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>$ docker run --rm --name web2 --link db:db training/webapp env
</span><span class='line'>    . . .
</span><span class='line'>    DB_NAME=/web2/db
</span><span class='line'>    DB_PORT=tcp://172.17.0.5:5432
</span><span class='line'>    DB_PORT_5432_TCP=tcp://172.17.0.5:5432
</span><span class='line'>    DB_PORT_5432_TCP_PROTO=tcp
</span><span class='line'>    DB_PORT_5432_TCP_PORT=5432
</span><span class='line'>    DB_PORT_5432_TCP_ADDR=172.17.0.5
</span><span class='line'>
</span><span class='line'>$ docker run -t -i --rm --link db:db training/webapp /bin/bash
</span><span class='line'>root@aed84ee21bde:/opt/webapp# cat /etc/hosts
</span><span class='line'>172.17.0.7  aed84ee21bde
</span><span class='line'>. . .
</span><span class='line'>172.17.0.5  db    </span></code></pre></td></tr></table></div></figure>


<p>You can see that Docker has created a series of environment variables with useful information about the source db container. Each variable is prefixed with <code>DB_</code>, which is populated from the alias you specified above. If the alias were db1, the variables would be prefixed with <code>DB1_</code>.</p>

<h2>存储</h2>

<p><a href="https://docs.docker.com/userguide/dockervolumes/">Managing Data in Containers</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Adding a data volume
</span><span class='line'>docker run -d -P --name web -v /webapp training/webapp python app.py
</span><span class='line'>
</span><span class='line'># Mount a Host Directory as a Data Volume
</span><span class='line'>docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py
</span><span class='line'># 只读
</span><span class='line'>docker run -d -P --name web -v /src/webapp:/opt/webapp:ro training/webapp python app.py
</span><span class='line'>
</span><span class='line'># Mount a Host File as a Data Volume
</span><span class='line'>docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash
</span><span class='line'>
</span><span class='line'># Creating and mounting a Data Volume Container
</span><span class='line'>docker run -d -v /dbdata --name dbdata training/postgres echo Data-only container for postgres
</span><span class='line'>docker run -d --volumes-from dbdata --name db1 training/postgres
</span><span class='line'>docker run -d --volumes-from dbdata --name db2 training/postgres
</span><span class='line'>docker run -d --name db3 --volumes-from db1 training/postgres
</span><span class='line'>
</span><span class='line'># Backup, restore, or migrate data volumes
</span><span class='line'>docker run --volumes-from dbdata -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata
</span><span class='line'>docker run -v /dbdata --name dbdata2 ubuntu /bin/bash
</span><span class='line'>docker run --volumes-from dbdata2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar
</span></code></pre></td></tr></table></div></figure>


<h2>回顾</h2>

<p>管理docker主要使用其提供的各种命令、以及参数来进行。</p>

<ul>
<li>本地的镜像管理: docker images / docker rmi [image identify]</li>
<li>容器管理： docker ps -a|-l / docker start|stop|rm|restart [image identify]</li>
<li>运行容器：docker run [images] [command]

<ul>
<li>-d 后台运行</li>
<li>-ti tty交互式运行</li>
<li>-P 把容器expose的端口映射到宿主机器端口。可以通过<code>docker port [container-name]</code>来查看端口映射关系。</li>
<li>-p [host-machine-port:container-machine-port]手动指定端口映射关系</li>
<li>-h [hostname] 实例操作系统的hostname</li>
<li>&ndash;name [name] 容器实例标识</li>
<li>-v [path] 建立目录</li>
<li>-v [host-machine-path:container-machine-path] 把宿主的文件路径映射到容器操作系统的指定目录</li>
<li>&ndash;link [container-name:name] 多容器之间互相访问。</li>
</ul>
</li>
</ul>


<p>还有很多辅助命令如：<code>top</code>, <code>logs</code>, <code>port</code>, <code>inspect</code>。以及进行版本管理的<code>pull</code>, <code>push</code>, <code>commit</code>, <code>tag</code>等等。</p>

<h2>参考</h2>

<ul>
<li><a href="http://www.blogjava.net/yongboy/archive/2013/12/12/407498.html">Docker学习笔记之一，搭建一个JAVA Tomcat运行环境</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在windows开发测试mapreduce几种方式]]></title>
    <link href="http://winseliu.com/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/"/>
    <updated>2014-09-17T12:55:38+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature</id>
    <content type="html"><![CDATA[<blockquote><p>备注： 文后面的maven打包、以及执行的shell脚本还是极好的&hellip;</p></blockquote>

<p>hadoop提供的两大组件HDFS、MapReduce。其中HDFS提供了丰富的API，最重要的有类似shell的脚本进行操作。而编写程序，要很方便的调试测试，其实是一件比较麻烦和繁琐的事情。</p>

<p>首先可能针对拆分的功能进行<strong>单独的方法</strong>级别的单元测试，然后到map/reduce的一个<strong>完整的处理过程</strong>的测试，再就是针对<strong>整个MR</strong>的测试，前面说的都是在IDE中完成后，最后需要到<strong>测试环境</strong>对其进行验证。</p>

<ul>
<li>单独的方法这里就不必多讲，直接使用eclipse自带的junit即可完成。</li>
<li>mrunit，针对map/reduce的测试，以至于整个MR流程的测试，但是mrunit的输入是针对小数据量的。</li>
<li>本地模式运行程序，模拟正式的环境来进行测试，数据直接从hdfs获取。</li>
<li>测试环境远程调试，尽管经过前面的步骤可能还会遇到各种问题，此时可结合<code>remote debug</code>来定位问题。</li>
</ul>


<h3>mrunit测试map/reduce</h3>

<p>首先去到<a href="http://mrunit.apache.org/">官网下载</a>，把对应的jar加入到你项目的依赖。懒得去手工下载的话直接使用maven。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;dependency&gt;
</span><span class='line'>  &lt;groupId&gt;org.apache.mrunit&lt;/groupId&gt;
</span><span class='line'>  &lt;artifactId&gt;mrunit&lt;/artifactId&gt;
</span><span class='line'>  &lt;version&gt;1.1.0&lt;/version&gt;
</span><span class='line'>  &lt;classifier&gt;hadoop2&lt;/classifier&gt;
</span><span class='line'>  &lt;scope&gt;test&lt;/scope&gt;
</span><span class='line'>&lt;/dependency&gt;</span></code></pre></td></tr></table></div></figure>


<p>可以对mapreduce的各种情况（map/reduce/map-reduce/map-combine-reduce）进行简单的测试，验证逻辑上是否存在问题。<a href="https://cwiki.apache.org/confluence/display/MRUNIT/MRUnit+Tutorial">官方文档的例子</a>已经很具体详细了。</p>

<p>先新建初始化driver（MapDriver/ReduceDriver/MapReduceDriver)，然后添加配置配置信息（configuration），再指定withInput来进行输入数据，和withOutput对应的输出数据。运行调用runTest方法就会模拟mr的整个运行机制来对单条的记录进行处理。因为都是在一个jvm中执行，调试是很方便的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>private MapReduceDriver&lt;LongWritable, Text, KeyWrapper, ValueWrapper, Text, Text&gt; mrDriver;
</span><span class='line'>
</span><span class='line'>@Before
</span><span class='line'>public void setUp() {
</span><span class='line'>  AccessLogMapper mapper = new AccessLogMapper();
</span><span class='line'>  AccessLogReducer reducer = new AccessLogReducer();
</span><span class='line'>  // AccessLogCombiner combiner = new AccessLogCombiner();
</span><span class='line'>
</span><span class='line'>  mrDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer);
</span><span class='line'>
</span><span class='line'>  // mDriver = MapDriver.newMapDriver(mapper);
</span><span class='line'>  // mcrDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer, combiner);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>private String[] datas;
</span><span class='line'>
</span><span class='line'>@After
</span><span class='line'>public void run() throws IOException {
</span><span class='line'>  if (datas != null) {
</span><span class='line'>      // 配置
</span><span class='line'>      ...
</span><span class='line'>      mrDriver.setConfiguration(config);
</span><span class='line'>      // mrDriver.getConfiguration().addResource("job_1399189058775_0627_conf.xml");
</span><span class='line'>
</span><span class='line'>    // 输入输出
</span><span class='line'>      Text input = new Text();
</span><span class='line'>      int i = 0;
</span><span class='line'>      for (String data : datas) {
</span><span class='line'>          input.set(data);
</span><span class='line'>          mrDriver.withInput(new LongWritable(++i), new Text(data));
</span><span class='line'>      }
</span><span class='line'>      mrDriver.withOutputFormat(MultipleFileOutputFormat.class, TextInputFormat.class);
</span><span class='line'>      mrDriver.runTest();
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>// / datas
</span><span class='line'>
</span><span class='line'>private String[] datas() {
</span><span class='line'>  return ...;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testOne() throws IOException {
</span><span class='line'>  datas = new String[] { datas()[0] };
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>local方式进行本地测试</h2>

<p>mapreduce默认提供了两种任务框架： local和yarn。YARN环境需要把程序发布到nodemanager上去运行，对于开发测试来讲，还是太繁琐了。</p>

<p>使用local的方式，既不用打包同时拥有IDE本地调试的便利，同时数据直接从HDFS中获取，也就是说，除了任务框架不同，其他都一样，程序的输入输出，任务代码的业务逻辑。为全面开发调试/测试提供了极其重要的方式。</p>

<p>只需要指定服务为local的服务框架，再加上输入输出即可。如果本地用户和hdfs的用户不同，设置下环境变量<code>HADOOP_USER_NAME</code>。同样map、reduce通过线程来模拟，都运行的同一个JVM中，断点调试也很方便。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>public class WordCountTest {
</span><span class='line'>  
</span><span class='line'>  static {
</span><span class='line'>      System.setProperty("HADOOP_USER_NAME", "hadoop");
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  private static final String HDFS_SERVER = "hdfs://umcc97-44:9000";
</span><span class='line'>
</span><span class='line'>  @Test
</span><span class='line'>  public void test() throws Exception {
</span><span class='line'>      WordCount.main(new String[]{
</span><span class='line'>              "-Dmapreduce.framework.name=local", 
</span><span class='line'>              "-Dfs.defaultFS=" + HDFS_SERVER, 
</span><span class='line'>              HDFS_SERVER + "/user/hadoop/dta/001.tar.gz", 
</span><span class='line'>              HDFS_SERVER + "/user/hadoop/output/"});
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>}
</span></code></pre></td></tr></table></div></figure>


<h3>测试环境打包测试</h3>

<p>放到测试环境后，appmanager、map、reduce都是运行在不同的jvm；还有就是需要对程序进行打包，挺啰嗦而且麻烦的事情，依赖包多的话，包还挺大，每次job都需要传递这么大一个文件，也挺浪费的。</p>

<p>提供两种打包方式，一种是直接jar运行的，一种是所有的jar压缩包tar.gz方式。可以结合distributecache减少每次执行程序需要传递给nodemanager的数据量，以及结合mapreduce运行时配置参数可以进行远程调试。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>调试appmanager
</span><span class='line'>-Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090" 
</span><span class='line'>调试map
</span><span class='line'>-Dmapreduce.map.java.opts
</span><span class='line'>调试reduce
</span><span class='line'>-Dmapreduce.reduce.java.opts</span></code></pre></td></tr></table></div></figure>


<h3>小结</h3>

<p>通过以上3中方式基本上能处理工作终于到的大部分问题了。大部分的功能使用mrunit测试就可以了，还可以单独的测试map，或者reduce挺不错的。</p>

<h3>附录：maven打包</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;profile&gt;
</span><span class='line'>  &lt;id&gt;jar&lt;/id&gt;
</span><span class='line'>  &lt;build&gt;
</span><span class='line'>      &lt;plugins&gt;
</span><span class='line'>          &lt;plugin&gt;
</span><span class='line'>              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
</span><span class='line'>              &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
</span><span class='line'>              &lt;executions&gt;
</span><span class='line'>                  &lt;execution&gt;
</span><span class='line'>                      &lt;id&gt;make-assembly&lt;/id&gt;
</span><span class='line'>                      &lt;phase&gt;package&lt;/phase&gt;
</span><span class='line'>                      &lt;goals&gt;
</span><span class='line'>                          &lt;goal&gt;single&lt;/goal&gt;
</span><span class='line'>                      &lt;/goals&gt;
</span><span class='line'>                  &lt;/execution&gt;
</span><span class='line'>              &lt;/executions&gt;
</span><span class='line'>              &lt;configuration&gt;
</span><span class='line'>                  &lt;descriptorRefs&gt;
</span><span class='line'>                      &lt;descriptorRef&gt;
</span><span class='line'>                          jar-with-dependencies
</span><span class='line'>                      &lt;/descriptorRef&gt;
</span><span class='line'>                  &lt;/descriptorRefs&gt;
</span><span class='line'>              &lt;/configuration&gt;
</span><span class='line'>          &lt;/plugin&gt;
</span><span class='line'>
</span><span class='line'>      &lt;/plugins&gt;
</span><span class='line'>  &lt;/build&gt;
</span><span class='line'>&lt;/profile&gt;
</span><span class='line'>
</span><span class='line'>&lt;profile&gt;
</span><span class='line'>  &lt;id&gt;tar&lt;/id&gt;
</span><span class='line'>  &lt;build&gt;
</span><span class='line'>      &lt;plugins&gt;
</span><span class='line'>          &lt;plugin&gt;
</span><span class='line'>              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
</span><span class='line'>              &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
</span><span class='line'>              &lt;executions&gt;
</span><span class='line'>                  &lt;execution&gt;
</span><span class='line'>                      &lt;id&gt;make-assembly&lt;/id&gt;
</span><span class='line'>                      &lt;phase&gt;package&lt;/phase&gt;
</span><span class='line'>                      &lt;goals&gt;
</span><span class='line'>                          &lt;goal&gt;single&lt;/goal&gt;
</span><span class='line'>                      &lt;/goals&gt;
</span><span class='line'>                  &lt;/execution&gt;
</span><span class='line'>              &lt;/executions&gt;
</span><span class='line'>              &lt;configuration&gt;
</span><span class='line'>                  &lt;appendAssemblyId&gt;true&lt;/appendAssemblyId&gt;
</span><span class='line'>                  &lt;descriptors&gt;
</span><span class='line'>                      &lt;descriptor&gt;${basedir}/../assemblies/application.xml&lt;/descriptor&gt;
</span><span class='line'>                  &lt;/descriptors&gt;
</span><span class='line'>              &lt;/configuration&gt;
</span><span class='line'>          &lt;/plugin&gt;
</span><span class='line'>      &lt;/plugins&gt;
</span><span class='line'>  &lt;/build&gt;
</span><span class='line'>&lt;/profile&gt;</span></code></pre></td></tr></table></div></figure>


<p>打包成tar.gz的描述文件：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;assembly&gt;
</span><span class='line'>  &lt;id&gt;dist-${env}&lt;/id&gt;
</span><span class='line'>  &lt;formats&gt;
</span><span class='line'>      &lt;format&gt;tar.gz&lt;/format&gt;
</span><span class='line'>  &lt;/formats&gt;
</span><span class='line'>  &lt;includeBaseDirectory&gt;true&lt;/includeBaseDirectory&gt;
</span><span class='line'>  &lt;fileSets&gt;
</span><span class='line'>      &lt;fileSet&gt;
</span><span class='line'>          &lt;directory&gt;${basedir}/src/main/scripts&lt;/directory&gt;
</span><span class='line'>          &lt;outputDirectory&gt;/bin&lt;/outputDirectory&gt;
</span><span class='line'>          &lt;includes&gt;
</span><span class='line'>              &lt;include&gt;*.sh&lt;/include&gt;
</span><span class='line'>          &lt;/includes&gt;
</span><span class='line'>          &lt;fileMode&gt;0755&lt;/fileMode&gt;
</span><span class='line'>          &lt;lineEnding&gt;unix&lt;/lineEnding&gt;
</span><span class='line'>      &lt;/fileSet&gt;
</span><span class='line'>      &lt;fileSet&gt;
</span><span class='line'>          &lt;directory&gt;${basedir}/target/classes&lt;/directory&gt;
</span><span class='line'>          &lt;outputDirectory&gt;/conf&lt;/outputDirectory&gt;
</span><span class='line'>          &lt;includes&gt;
</span><span class='line'>              &lt;include&gt;*.xml&lt;/include&gt;
</span><span class='line'>              &lt;include&gt;*.properties&lt;/include&gt;
</span><span class='line'>          &lt;/includes&gt;
</span><span class='line'>      &lt;/fileSet&gt;
</span><span class='line'>      &lt;fileSet&gt;
</span><span class='line'>          &lt;directory&gt;${basedir}/target&lt;/directory&gt;
</span><span class='line'>          &lt;outputDirectory&gt;/lib/core&lt;/outputDirectory&gt;
</span><span class='line'>          &lt;includes&gt;
</span><span class='line'>              &lt;include&gt;${project.artifactId}-${project.version}.jar
</span><span class='line'>              &lt;/include&gt;
</span><span class='line'>          &lt;/includes&gt;
</span><span class='line'>      &lt;/fileSet&gt;
</span><span class='line'>  &lt;/fileSets&gt;
</span><span class='line'>  &lt;dependencySets&gt;
</span><span class='line'>      &lt;dependencySet&gt;
</span><span class='line'>          &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt;
</span><span class='line'>          &lt;outputDirectory&gt;/lib/common&lt;/outputDirectory&gt;
</span><span class='line'>          &lt;scope&gt;runtime&lt;/scope&gt;
</span><span class='line'>      &lt;/dependencySet&gt;
</span><span class='line'>  &lt;/dependencySets&gt;
</span><span class='line'>&lt;/assembly&gt;</span></code></pre></td></tr></table></div></figure>


<p>运行整个程序的shell脚本</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'>bin=`which $0`
</span><span class='line'>bin=`dirname ${bin}`
</span><span class='line'>bin=`cd "$bin"; pwd`
</span><span class='line'>
</span><span class='line'>export ANAYSER_HOME=`dirname "$bin"`
</span><span class='line'>
</span><span class='line'>export ANAYSER_LOG_DIR=$ANAYSER_HOME/logs
</span><span class='line'>
</span><span class='line'>export ANAYSER_OPTS="-Dproc_dta_analyser -server -Xms1024M -Xmx2048M -Danalyser.log.dir=${ANAYSER_LOG_DIR}"
</span><span class='line'>
</span><span class='line'>export HADOOP_HOME=${HADOOP_HOME:-/home/hadoop/hadoop-2.2.0}
</span><span class='line'>export ANAYSER_CLASSPATH=$ANAYSER_HOME/conf
</span><span class='line'>export ANAYSER_CLASSPATH=$ANAYSER_CLASSPATH:$HADOOP_HOME/etc/hadoop
</span><span class='line'>
</span><span class='line'>for f in $ANAYSER_HOME/lib/core/*.jar ; do
</span><span class='line'>  export ANAYSER_CLASSPATH+=:$f
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>for f in $ANAYSER_HOME/lib/common/*.jar ; do
</span><span class='line'>  export ANAYSER_CLASSPATH+=:$f
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>if [ ! -d $ANAYSER_LOG_DIR ] ; then
</span><span class='line'>  mkdir -p $ANAYSER_LOG_DIR
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>[ -w "$ANAYSER_PID_DIR" ] ||  mkdir -p "$ANAYSER_PID_DIR"
</span><span class='line'>
</span><span class='line'>nohup ${JAVA_HOME}/bin/java $ANAYSER_OPTS -cp $ANAYSER_CLASSPATH com.analyser.AnalyserStarter &gt;$ANAYSER_LOG_DIR/stdout 2&gt;$ANAYSER_LOG_DIR/stderr &
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala Wordcount on Hadoop2]]></title>
    <link href="http://winseliu.com/blog/2014/09/12/scala-wordcount-on-hadoop/"/>
    <updated>2014-09-12T07:52:01+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/12/scala-wordcount-on-hadoop</id>
    <content type="html"><![CDATA[<p>从了解scala，到spark再次遇见scala，准备好好学学这门语言。函数式编程大势所趋，简洁的语法，更抽象好用的集合操作。土生土长的JVM的语言，以及凭借其与java的互操作性，发展前景一片光明。在云计算以及手机（android）开发都有其大展拳脚的地方。</p>

<p>工作中大部分时间写mapreduce，项目空白期实践了一下把scala搬上hadoop。整体来说用scala写个helloworld是比较简单的，就一些细节的东西比较繁琐。尽管用了几年的eclipse了，但是<a href="http://scala-ide.org/">scala-ide</a>还是需要再适应适应！scala-idea也没有大家说的那么好，和webstorm比差远了。</p>

<div><script src='https://gist.github.com/5df39f77e8bd59348a7a.js'></script>
<noscript><pre><code>package com.github.winse.hadoop

import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.Reducer
import org.apache.hadoop.io.Text
import org.apache.hadoop.io.IntWritable
import org.apache.hadoop.io.LongWritable
import org.apache.hadoop.mapreduce.Mapper
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
import org.apache.hadoop.fs.Path
import scala.Array.canBuildFrom
import org.apache.hadoop.conf.Configured
import org.apache.hadoop.util.Tool
import org.apache.hadoop.util.ToolRunner

class ScalaMapper extends Mapper[LongWritable, Text, Text, IntWritable] {

  val one = new IntWritable(1);

  override def map(key: LongWritable, value: Text, context: Mapper[LongWritable, Text, Text, IntWritable]#Context) {
    value.toString().split(&quot;\\s+&quot;).map(word =&gt; context.write(new Text(word), one))
  }

}

class ScalaReducer extends Reducer[Text, IntWritable, Text, IntWritable] {

  override def reduce(key: Text, values: java.lang.Iterable[IntWritable], context: Reducer[Text, IntWritable, Text, IntWritable]#Context) {
    var sum: Int = 0

    val itr = values.iterator()
    while (itr.hasNext()) {
      sum += itr.next().get()
    }
    context.write(key, new IntWritable(sum))
  }

}

object HelloScalaMapRed extends Configured with Tool {

  override def run(args: Array[String]): Int = {

    val job = Job.getInstance(getConf(), &quot;WordCount Scala.&quot;)
    job.setJarByClass(getClass())

    job.setOutputKeyClass(classOf[Text])
    job.setOutputValueClass(classOf[IntWritable])

    job.setMapperClass(classOf[ScalaMapper])
    job.setCombinerClass(classOf[ScalaReducer])
    job.setReducerClass(classOf[ScalaReducer])

    FileInputFormat.addInputPath(job, new Path(&quot;/scala/in/&quot;));
    FileOutputFormat.setOutputPath(job, new Path(&quot;/scala/out/&quot;));

    job.waitForCompletion(true) match {
      case true =&gt; 0
      case false =&gt; 1
    }

  }

  def main(args: Array[String]) {
    val res: Int = ToolRunner.run(new Configuration(), this, args)
    System.exit(res);
  }

}</code></pre></noscript></div>


<p>使用scala主要原因：</p>

<ul>
<li>写JavaBean更简单方便</li>
<li>多返回值无需定义Result实体类</li>
<li>集合更抽象的方法真的很好用</li>
<li>trait可以更便捷的进行操作层面的聚合，也就是可以把操作分离出来，进行组合就可以实现新的功能。这不就是decorate模式嘛！java的decorate多麻烦的！加点东西太麻烦了！！！</li>
</ul>


<p>上面的scala代码和java的比较类似，主要在集合操作上不同而已，变量定义简单化。</p>

<p>编写好代码后就是运行调试。</p>

<p>前面其他的文章已经说过了，默认<code>mapreduce.framework.name</code>的配置是本地<code>local</code>，所以直接运行就像运行一个普通的本地java程序。这就不多将了。
这里主要讲讲怎么把代码打包放到真实的集群环境运行，相比java的版本要添加那些步骤。</p>

<p>从项目的maven pom中可以发现，其实就是多了scala-lang的新依赖而已，其他都是hadoop自带的公共包。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHV6AAJoCAABANktCWmk664.png" alt="" /></p>

<p>所以运行程序只需要指定把scala-lang.jar添加到运行环境的classpath中即可。使用maven打包后的项目结构如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ cd lib/
</span><span class='line'>[hadoop@master1 lib]$ ls -l
</span><span class='line'>total 8
</span><span class='line'>drwxrwxr-x. 2 hadoop hadoop 4096 Sep 11 23:10 common
</span><span class='line'>drwxrwxr-x. 2 hadoop hadoop 4096 Sep 11 23:56 core
</span><span class='line'>[hadoop@master1 lib]$ ll core/
</span><span class='line'>total 12
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop 11903 Sep 11 23:55 scalamapred-1.0.5.jar
</span><span class='line'>[hadoop@master1 lib]$ ls common/
</span><span class='line'>activation-1.1.jar                commons-lang-2.6.jar            hadoop-hdfs-2.2.0.jar                     jaxb-api-2.2.2.jar                      log4j-1.2.17.jar
</span><span class='line'>aopalliance-1.0.jar               commons-logging-1.1.1.jar       hadoop-mapreduce-client-common-2.2.0.jar  jaxb-impl-2.2.3-1.jar                   management-api-3.0.0-b012.jar
</span><span class='line'>asm-3.1.jar                       commons-math-2.1.jar            hadoop-mapreduce-client-core-2.2.0.jar    jersey-client-1.9.jar                   netty-3.6.2.Final.jar
</span><span class='line'>avro-1.7.4.jar                    commons-net-3.1.jar             hadoop-yarn-api-2.2.0.jar                 jersey-core-1.9.jar                     paranamer-2.3.jar
</span><span class='line'>commons-beanutils-1.7.0.jar       gmbal-api-only-3.0.0-b023.jar   hadoop-yarn-client-2.2.0.jar              jersey-grizzly2-1.9.jar                 protobuf-java-2.5.0.jar
</span><span class='line'>commons-beanutils-core-1.8.0.jar  grizzly-framework-2.1.2.jar     hadoop-yarn-common-2.2.0.jar              jersey-guice-1.9.jar                    scala-library-2.10.4.jar
</span><span class='line'>commons-cli-1.2.jar               grizzly-http-2.1.2.jar          hadoop-yarn-server-common-2.2.0.jar       jersey-json-1.9.jar                     servlet-api-2.5.jar
</span><span class='line'>commons-codec-1.4.jar             grizzly-http-server-2.1.2.jar   jackson-core-asl-1.8.8.jar                jersey-server-1.9.jar                   slf4j-api-1.7.1.jar
</span><span class='line'>commons-collections-3.2.1.jar     grizzly-http-servlet-2.1.2.jar  jackson-jaxrs-1.8.3.jar                   jersey-test-framework-core-1.9.jar      slf4j-log4j12-1.7.1.jar
</span><span class='line'>commons-compress-1.4.1.jar        grizzly-rcm-2.1.2.jar           jackson-mapper-asl-1.8.8.jar              jersey-test-framework-grizzly2-1.9.jar  snappy-java-1.0.4.1.jar
</span><span class='line'>commons-configuration-1.6.jar     guava-17.0.jar                  jackson-xc-1.8.3.jar                      jets3t-0.6.1.jar                        stax-api-1.0.1.jar
</span><span class='line'>commons-daemon-1.0.13.jar         guice-3.0.jar                   jasper-compiler-5.5.23.jar                jettison-1.1.jar                        xmlenc-0.52.jar
</span><span class='line'>commons-digester-1.8.jar          guice-servlet-3.0.jar           jasper-runtime-5.5.23.jar                 jetty-6.1.26.jar                        xz-1.0.jar
</span><span class='line'>commons-el-1.0.jar                hadoop-annotations-2.2.0.jar    javax.inject-1.jar                        jetty-util-6.1.26.jar                   zookeeper-3.4.5.jar
</span><span class='line'>commons-httpclient-3.1.jar        hadoop-auth-2.2.0.jar           javax.servlet-3.1.jar                     jsch-0.1.42.jar
</span><span class='line'>commons-io-2.1.jar                hadoop-common-2.2.0.jar         javax.servlet-api-3.0.1.jar               jsp-api-2.1.jar
</span><span class='line'>[hadoop@master1 lib]$ </span></code></pre></td></tr></table></div></figure>


<p>在lib文件夹下面包括common和core两放置jar的文件夹，common是项目的依赖包，core下面的是项目的源码jar。</p>

<p>接下来运行程序，通过libjar把<strong>scala-library的包加入到mapreduce的运行时classpath</strong>。当然也可以把scala-library加入到<code>mapreduce.application.classpath</code>（默认值为<code>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</code>）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ for j in `find . -name "*.jar"` ; do export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$j ; done
</span><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ 
</span><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ export HADOOP_CLASSPATH=
</span><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ export HADOOP_CLASSPATH=/home/hadoop/scalamapred-1.0.5/lib/core/*:/home/hadoop/scalamapred-1.0.5/lib/common/*
</span><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed -libjars lib/common/scala-library-2.10.4.jar </span></code></pre></td></tr></table></div></figure>


<h2>问题攻略</h2>

<p>上面如果不加libjar的话，会在nodemanager的代码中抛出异常。本来认为不加依赖包也就不能执行mapreduce里面的代码而已。问题的根源在哪里呢？</p>

<p>给代码添加远程调试的配置，然后运行一步步的查找问题（一次找不到就多运行调试几次）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed  -Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090"
</span><span class='line'>
</span><span class='line'>// 我这里slaver就一台，取到机器上查看运行的程序
</span><span class='line'>
</span><span class='line'>[hadoop@slaver1 nmPrivate]$ ps axu|grep java
</span><span class='line'>hadoop    1427  0.6 10.5 1562760 106344 ?      Sl   Sep11   0:45 /opt/jdk1.7.0_60//bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=hadoop-hadoop-datanode-slaver1.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode
</span><span class='line'>hadoop    2874  2.5 11.7 1599312 118980 ?      Sl   00:08   0:57 /opt/jdk1.7.0_60//bin/java -Dproc_nodemanager -Xmx1000m -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.home.dir= -Dyarn.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dyarn.policy.file=hadoop-policy.xml -server -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -classpath /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/etc/hadoop/nm-config/log4j.properties org.apache.hadoop.yarn.server.nodemanager.NodeManager
</span><span class='line'>hadoop    3750  0.0  0.1 106104  1200 ?        Ss   00:43   0:00 /bin/bash -c /opt/jdk1.7.0_60//bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stderr 
</span><span class='line'>hadoop    3759  0.1  1.8 737648 18232 ?        Sl   00:43   0:00 /opt/jdk1.7.0_60//bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster
</span><span class='line'>hadoop    3778  0.0  0.0 103256   832 pts/0    S+   00:45   0:00 grep java
</span><span class='line'>
</span><span class='line'>// 取到对应的目录下查看launcher.sh的脚本
</span><span class='line'>// appmaster launcher
</span><span class='line'>
</span><span class='line'>[hadoop@slaver1 nm-local-dir]$ cd nmPrivate/application_1410453720744_0007/
</span><span class='line'>[hadoop@slaver1 application_1410453720744_0007]$ ll
</span><span class='line'>total 4
</span><span class='line'>drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:43 container_1410453720744_0007_01_000001
</span><span class='line'>[hadoop@slaver1 application_1410453720744_0007]$ less container_1410453720744_0007_01_000001/
</span><span class='line'>container_1410453720744_0007_01_000001.tokens       launch_container.sh                                 
</span><span class='line'>.container_1410453720744_0007_01_000001.tokens.crc  .launch_container.sh.crc                            
</span><span class='line'>[hadoop@slaver1 application_1410453720744_0007]$ less container_1410453720744_0007_01_000001/launch_container.sh 
</span><span class='line'>#!/bin/bash
</span><span class='line'>
</span><span class='line'>export NM_HTTP_PORT="8042"
</span><span class='line'>export LOCAL_DIRS="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007"
</span><span class='line'>export HADOOP_COMMON_HOME="/home/hadoop/hadoop-2.2.0"
</span><span class='line'>export JAVA_HOME="/opt/jdk1.7.0_60/"
</span><span class='line'>export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
</span><span class='line'>"
</span><span class='line'>export HADOOP_YARN_HOME="/home/hadoop/hadoop-2.2.0"
</span><span class='line'>export CLASSPATH="$PWD:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/share/hadoop/common/*:$HADOOP_COMMON_HOME/share/hadoop/common/lib/*:$HADOOP_HDFS_HOME/share/hadoop/hdfs/*:$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*:$HADOOP_YARN_HOME/share/hadoop/yarn/*:$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*:job.jar/job.jar:job.jar/classes/:job.jar/lib/*:$PWD/*"
</span><span class='line'>export HADOOP_TOKEN_FILE_LOCATION="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001/container_tokens"
</span><span class='line'>export NM_HOST="slaver1"
</span><span class='line'>export APPLICATION_WEB_PROXY_BASE="/proxy/application_1410453720744_0007"
</span><span class='line'>export JVM_PID="$$"
</span><span class='line'>export USER="hadoop"
</span><span class='line'>export HADOOP_HDFS_HOME="/home/hadoop/hadoop-2.2.0"
</span><span class='line'>export PWD="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001"
</span><span class='line'>export CONTAINER_ID="container_1410453720744_0007_01_000001"
</span><span class='line'>export HOME="/home/"
</span><span class='line'>export NM_PORT="40888"
</span><span class='line'>export LOGNAME="hadoop"
</span><span class='line'>export APP_SUBMIT_TIME_ENV="1410455811401"
</span><span class='line'>export MAX_APP_ATTEMPTS="2"
</span><span class='line'>export HADOOP_CONF_DIR="/home/hadoop/hadoop-2.2.0/etc/hadoop"
</span><span class='line'>export MALLOC_ARENA_MAX="4"
</span><span class='line'>export LOG_DIRS="/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001"
</span><span class='line'>ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/10/job.jar" "job.jar"
</span><span class='line'>ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/13/job.xml" "job.xml"
</span><span class='line'>mkdir -p jobSubmitDir
</span><span class='line'>ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/11/job.splitmetainfo" "jobSubmitDir/job.splitmetainfo"
</span><span class='line'>mkdir -p jobSubmitDir
</span><span class='line'>ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/12/job.split" "jobSubmitDir/job.split"
</span><span class='line'>exec /bin/bash -c "$JAVA_HOME/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stderr "
</span><span class='line'>
</span><span class='line'>// 去到TMP对应的目录下，查看整个运行的根目录
</span><span class='line'>
</span><span class='line'>[hadoop@slaver1 ~]$ cd /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001
</span><span class='line'>[hadoop@slaver1 container_1410453720744_0007_01_000001]$ ll
</span><span class='line'>total 28
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop   95 Sep 12 00:43 container_tokens
</span><span class='line'>-rwx------. 1 hadoop hadoop  468 Sep 12 00:43 default_container_executor.sh
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:43 job.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/10/job.jar
</span><span class='line'>drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:43 jobSubmitDir
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:43 job.xml -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/13/job.xml
</span><span class='line'>-rwx------. 1 hadoop hadoop 3005 Sep 12 00:43 launch_container.sh
</span><span class='line'>drwx--x---. 2 hadoop hadoop 4096 Sep 12 00:43 tmp
</span><span class='line'>[hadoop@slaver1 container_1410453720744_0007_01_000001]$ 
</span></code></pre></td></tr></table></div></figure>


<p>为了对应，我这里列出来在添加了libjar的TMP目录的列表：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed  -Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090" -libjars lib/common/scala-library-2.10.4.jar 
</span><span class='line'>
</span><span class='line'>[hadoop@slaver1 container_1410453720744_0007_01_000001]$ cd /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/container_1410453720744_0008_01_000001
</span><span class='line'>[hadoop@slaver1 container_1410453720744_0008_01_000001]$ ll
</span><span class='line'>total 32
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop   95 Sep 12 00:49 container_tokens
</span><span class='line'>-rwx------. 1 hadoop hadoop  468 Sep 12 00:49 default_container_executor.sh
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:49 job.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/filecache/10/job.jar
</span><span class='line'>drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:49 jobSubmitDir
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:49 job.xml -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/filecache/13/job.xml
</span><span class='line'>-rwx------. 1 hadoop hadoop 3127 Sep 12 00:49 launch_container.sh
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop   85 Sep 12 00:49 scala-library-2.10.4.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/filecache/10/scala-library-2.10.4.jar
</span><span class='line'>drwx--x---. 2 hadoop hadoop 4096 Sep 12 00:49 tmp
</span><span class='line'>[hadoop@slaver1 container_1410453720744_0008_01_000001]$ </span></code></pre></td></tr></table></div></figure>


<p>windows本地使用eclipse和进行跟踪调试代码。</p>

<p><img src="http://file.bmob.cn/M00/0E/A1/wKhkA1QUG0aARyPVAAMnUXGDgbY378.png" alt="" /></p>

<p>此时可以通过8088的网页查看状态，当前有一个mrappmaster在执行，如果第一个失败，会尝试执行第二次。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHDGAe0anAAEfiNTmB1k734.png" alt="" /></p>

<p>运行调试多次后，<strong>最终确定问题</strong>所在。在master中会检查是否为链式mr，而加载该class的时刻，同时要加载父类的class，即scala的类，所以在这里会抛出异常。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHFOAWJulAAPOawkAbgo349.png" alt="" /></p>

<p>去到查看程序运行的日志，可以看到程序抛出的异常<strong>NoClassDefFoundError</strong>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@slaver1 ~]$ less /home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410448728371_0003/*/syslog
</span><span class='line'>2014-09-11 22:55:12,616 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1410448728371_0003_000001
</span><span class='line'>...
</span><span class='line'>2014-09-11 22:55:18,677 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1410448728371_0003 to jobTokenSecretManager
</span><span class='line'>2014-09-11 22:55:19,119 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster
</span><span class='line'>java.lang.NoClassDefFoundError: scala/Function1
</span><span class='line'>        at java.lang.Class.forName0(Native Method)
</span><span class='line'>        at java.lang.Class.forName(Class.java:190)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1277)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1217)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:135)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1420)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1358)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:972)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:134)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1227)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1035)
</span><span class='line'>        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1445)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at javax.security.auth.Subject.doAs(Subject.java:415)
</span><span class='line'>        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1441)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1374)
</span><span class='line'>Caused by: java.lang.ClassNotFoundException: scala.Function1
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
</span><span class='line'>        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</span><span class='line'>        ... 22 more
</span><span class='line'>2014-09-11 22:55:19,130 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster received a signal. Signaling RMCommunicator and JobHistoryEventHandler.</span></code></pre></td></tr></table></div></figure>


<h2>意外收获</h2>

<ul>
<li>推测执行初始化代码</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHHCATFHtAAMeDcCHWzU166.png" alt="" /></p>

<ul>
<li>OutputFormat的获取Committer代码</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHImAJAq1AALGEfA-F9k811.png" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://digifesto.com/2013/04/15/hadoop-with-scala-hacking-notes/">Hadoop with Scala: hacking notes</a></li>
<li><a href="https://github.com/derrickcheng/ScalaOnHadoop/blob/master/src/main/scala/WordCount.scala">ScalaOnHadoop WordCount.scala</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【笔记】Beginning Scala（1）]]></title>
    <link href="http://winseliu.com/blog/2014/09/08/note-beginning-scala-part1/"/>
    <updated>2014-09-08T07:36:57+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/08/note-beginning-scala-part1</id>
    <content type="html"><![CDATA[<p>Scala借鉴了python、ruby等函数式语言。从java转过来还是需要一个适应阶段，与groovy比似乎困难多了不少。一年前好奇接触过，看了一些官网的入门教程，觉得这就是一个异类，后面就放下了。</p>

<p>直到再次弄hadoop，接触spark。经过一个时间的过渡期后，发现Scala确实能处理java的一些繁琐问题，为我们的双手减负，写出更简洁更优雅的代码，或者说更”易懂“。</p>

<p><strong>这篇是第一章（About Scala and How to Install It）和第二章（Scala Syntax, Scripts, and Your First Scala Programs）的笔记。</strong></p>

<p>作者寄语：</p>

<blockquote><p>My Path was hard, and I hope yours will be easier.</p></blockquote>

<h2>历史与安装</h2>

<p>随着HotSpot对JVM的改进，JDK1.3的程序与C++写的程序一样快。Java程序可以运行几个星期、几个月、甚至一年都不用重启。</p>

<p>好的Java代码与C/C++的代码一样快，甚至更快。在同样功能下，经过深度调优的C/C++程序会比Java程序更高效，与C/C++相比Java程序需要更多的内存，但对于一个适度复杂的项目（非系统内核级别），JVM程序将比C/C++表现的更优异。</p>

<p>这么多年来，Java在语言级别还不成熟。Java语法停滞不前，Java上的web框架越来越笨重。处理XML，或者其他一些简单概念的实现，如字段生成前台的HTML表单，需要越来越多的代码。对Java越来越失望。
Java5增加了枚举和泛型，对语言而言这是一个可喜的消息，但编码方面我们不得不使用IDE来完成Java代码编写。</p>

<p>“写Scala”的Martin Odersky曾编写了java编译器和泛型功能。Scala(2001, first version in 2003)，语法表达能力如ruby，但同时有Java的强类型和高性能。</p>

<p>Scala即快又简洁，同时类型安全。Scala运行效率也很高，最终编译成Java字节码跑在JVM上，又能与Java代码互相调用。</p>

<blockquote><p>But most importantly, Scala taught me  to program and reason about programming differently. I stopped thinking in terms of allocating buffers, structs, and objects, and of changing those pieces of memory. Instead, I learned to think about most of my programs as transforming input to output. This change in thinking has lead to lower defect rates, more modular code, and more testable code. Scala has also given me the tools to write smaller, more modular units of code and asse mble them together into a whole that is maintainable, yet far more complex than anything that I could write in Java or Ruby for that matter.</p></blockquote>

<p>下载安装JDK6+配置PATH, <a href="http://scala-lang.org/download/2.10.4.html">Scala 2.10+</a>下载zip版本的，然后解压就行了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC /cygdrive/d/scala/bin
</span><span class='line'>$ ls -1
</span><span class='line'>fsc
</span><span class='line'>fsc.bat
</span><span class='line'>scala
</span><span class='line'>scala.bat
</span><span class='line'>scalac
</span><span class='line'>scalac.bat
</span><span class='line'>scaladoc
</span><span class='line'>scaladoc.bat
</span><span class='line'>scalap
</span><span class='line'>scalap.bat
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC /cygdrive/d/scala/bin
</span><span class='line'>$ scala
</span><span class='line'>Welcome to Scala version 2.10.4 (Java HotSpot(TM) Client VM, Java 1.7.0_02).
</span><span class='line'>Type in expressions to have them evaluated.
</span><span class='line'>Type :help for more information.
</span><span class='line'>
</span><span class='line'>scala&gt; def fact(n:Int)=1 to n reduceLeft(_*_) // n!
</span><span class='line'>fact: (n: Int)Int
</span><span class='line'>
</span><span class='line'>scala&gt; fact(5)
</span><span class='line'>res0: Int = 120</span></code></pre></td></tr></table></div></figure>


<h2>语法结构，第一个Scala程序</h2>

<p>运行程序的三种方式：</p>

<ul>
<li>命令行交互式的REPL（read-eval-print loop)</li>
<li>shell/cmd脚本</li>
<li>编译打包成jar后运行，跟Java一样</li>
</ul>


<h3>REPL</h3>

<p>进入到Scala的bin目录下，双击scala.bat打开。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; 1+1
</span><span class='line'>res0: Int = 2
</span><span class='line'>
</span><span class='line'>scala&gt; res0*8
</span><span class='line'>res1: Int = 16
</span><span class='line'>
</span><span class='line'>scala&gt; val x="hello world"
</span><span class='line'>x: String = hello world
</span><span class='line'>
</span><span class='line'>scala&gt; var xl=x.length
</span><span class='line'>xl: Int = 11
</span><span class='line'>
</span><span class='line'>scala&gt; import java.util._
</span><span class='line'>import java.util._
</span><span class='line'>
</span><span class='line'>scala&gt; val d = new Date
</span><span class='line'>d: java.util.Date = Mon Sep 08 09:17:08 CST 2014</span></code></pre></td></tr></table></div></figure>


<h3>脚本</h3>

<p>脚本中无需显示的定义main方法，当你运行脚本时，Scala把整个文件的内容添加到类的main方法中，编译代码，然后运行生成的main方法。你只需在脚本文件中编写scala代码即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ scala hello.scala
</span><span class='line'>hello world
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ cat hello.scala
</span><span class='line'>println("hello world")</span></code></pre></td></tr></table></div></figure>


<h3>编译后运行</h3>

<p>运行方式和javac类似，会生成对应类的字节码class文件。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/scala-hello
</span><span class='line'>$ scalac hello.scala
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/scala-hello
</span><span class='line'>$ ll
</span><span class='line'>total 13
</span><span class='line'>-rwxr-xr-x  1 winse None 2067 Sep  8 09:27 hello$.class
</span><span class='line'>-rwxr-xr-x  1 winse None  704 Sep  8 09:27 hello$delayedInit$body.class
</span><span class='line'>-rwxr-xr-x  1 winse None  921 Sep  8 09:27 hello.class
</span><span class='line'>-rw-r--r--+ 1 winse None   58 Sep  8 09:26 hello.scala
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/scala-hello
</span><span class='line'>$ cat hello.scala
</span><span class='line'>object hello extends App {
</span><span class='line'>
</span><span class='line'>  println("hello world")
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/scala-hello
</span><span class='line'>$ scala hello
</span><span class='line'>hello world
</span></code></pre></td></tr></table></div></figure>


<p>编译器的启动是很耗时的操作，你可以使用fsc（fast Scala Compiler），fsc是单独运行在后台的编译进程。</p>

<p>如果你原有的项目中使用Ant或Maven，scala有对应的插件，可以很容易把Scala集成到项目中。</p>

<h3>First Scala Programs</h3>

<p>在Scala，你可以编写像ruby和python脚本语言代码。如输出“hello world”的println方法，封装了System.out.println()。因为太常用了，println被定义在Scala的Predef（预定义成员）中，每个程序都会自动加载，就像java.lang会自动引入到每个java程序一样。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>println("hello world")
</span><span class='line'>
</span><span class='line'>for {i&lt;- 1 to 10}
</span><span class='line'>  println(i)
</span><span class='line'>
</span><span class='line'>for {i&lt;- 1 to 10
</span><span class='line'>     j&lt;- 1 to 10}
</span><span class='line'>  println(i*j)
</span></code></pre></td></tr></table></div></figure>


<p>99乘法表：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; for(i&lt;- 1 to 9){
</span><span class='line'>     | for(j&lt;- 1 to i)
</span><span class='line'>     | printf("%s*%s=%2s\t",j,i,i*j);
</span><span class='line'>     |
</span><span class='line'>     | println()
</span><span class='line'>     | }
</span><span class='line'>1*1= 1
</span><span class='line'>1*2= 2  2*2= 4
</span><span class='line'>1*3= 3  2*3= 6  3*3= 9
</span><span class='line'>1*4= 4  2*4= 8  3*4=12  4*4=16
</span><span class='line'>1*5= 5  2*5=10  3*5=15  4*5=20  5*5=25
</span><span class='line'>1*6= 6  2*6=12  3*6=18  4*6=24  5*6=30  6*6=36
</span><span class='line'>1*7= 7  2*7=14  3*7=21  4*7=28  5*7=35  6*7=42  7*7=49
</span><span class='line'>1*8= 8  2*8=16  3*8=24  4*8=32  5*8=40  6*8=48  7*8=56  8*8=64
</span><span class='line'>1*9= 9  2*9=18  3*9=27  4*9=36  5*9=45  6*9=54  7*9=63  8*9=72  9*9=81</span></code></pre></td></tr></table></div></figure>


<p>编写复杂点的程序，可以使用<a href="http://scala-ide.org/">Scala-IDE</a>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import scala.io._   // like java import scala.io.*
</span><span class='line'>
</span><span class='line'>def toInt(in: String): Option[Int] =
</span><span class='line'>  try {
</span><span class='line'>    Some(Integer.parseInt(in.trim))
</span><span class='line'>  } catch {
</span><span class='line'>    case e: NumberFormatException =&gt; None
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>def sum(in: Seq[String]) = {
</span><span class='line'>  val ints = in.flatMap(s =&gt; toInt(s))
</span><span class='line'>  ints.foldLeft(0)((a, b) =&gt; a + b)
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>println("Enter some numbers and press CTRL+C")
</span><span class='line'>
</span><span class='line'>val input = Source.fromInputStream(System.in)
</span><span class='line'>val lines = input.getLines.toSeq
</span><span class='line'>
</span><span class='line'>println("Sum " + sum(lines))
</span></code></pre></td></tr></table></div></figure>


<p>Option是包含一个或零个对象的容器。如果不包含元素，返回的是单例的None。如果包括一个元素，就是新的Some(theElement)的实例。Option是Scala中避免空指针异常（null pointer）和显示进行null检查的处理一种方式。如果是None，一个业务逻辑将应用到0个元素，是Some就应用到一个元素上。</p>

<p>方法没有显示的return语句，默认就是方法“最后”（逻辑上最后执行的）一个语句的返回值。</p>

<p>sum方法的参数Seq是一个trait（类似java interface），是Array，List以机构其他顺序集合的父trait。trait拥有java interface的所有特性，同时traits可以包括方法的实现。你可以混合很多的traits成一个类。Traits除了不能定义有参构造函数外，其他和类一样。trait使得“多重继承”简单化，无需担忧<strong> the diamond problem</strong>（有点类似近亲结婚 ^ v ^）。</p>

<p>如：当BC都实现了M方法，D不知道用谁的M，会有歧义！！</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Diamond_inheritance.svg/220px-Diamond_inheritance.svg.png" alt="" /></p>

<p>在Scala中，定义参数分为val和var，val类似于java final，var类似于java的变量定义。对于不变化的变量，定义为val可以减少代码错误几率，进行防御性的编程。</p>

<p>接下来运行程序， 输入一些数字后按CTRL+C结束，就会输出计算的和。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\local\home\Administrator\scala-hello&gt;scala Sum.scala
</span><span class='line'>Enter some numbers and press CTRL+C
</span><span class='line'>12
</span><span class='line'>23
</span><span class='line'>34
</span><span class='line'>45
</span><span class='line'>Sum 114
</span><span class='line'>终止批处理操作吗(Y/N)?</span></code></pre></td></tr></table></div></figure>


<h3>基本的语法Basic Syntax</h3>

<p>Scala的全部语法和语言的定义可以查看<a href="http://www.scala-lang.org/docu/files/ScalaReference.pdf">Scala Language Specification</a></p>

<h4>数字、字符串和XML常量</h4>

<ul>
<li><p>; 行结束符可以忽略</p></li>
<li><p>和Java一样的常量定义</p>

<blockquote><p>Integer: 1882, -1
Boolean: true, false
Double: 1.0, 1d, 1e3
Long: 42L
Float: 78.9f
Characters: &lsquo;4&rsquo;, &lsquo;?&rsquo;, &lsquo;z&rsquo;
Strings: &ldquo;Hello World&rdquo;</p></blockquote></li>
<li><p>Scala支持多行的字符串</p>

<blockquote><p>&ldquo;&rdquo;&ldquo;Hello
Multiline
World&rdquo;&ldquo;&rdquo;</p></blockquote></li>
<li><p>Scala支持XML常量，包括内嵌的Scala代码</p>

<blockquote><p><b>Foll</b>
<ul>{(1 to 3).map(i => <li>{i}</li>)}</ul></p></blockquote></li>
</ul>


<h4>包package和import</h4>

<p>package定义在源代码非注释的第一行。和java一样。</p>

<p>import则比java的更加灵活。基本的用法使用：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import scala.xml._</span></code></pre></td></tr></table></div></figure>


<p>scala中的import可以基于前面的imports语句。如再导入<code>scala.xml.transform</code>：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import transform._</span></code></pre></td></tr></table></div></figure>


<p>也可以导入一个具体的class和object：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import scala.collection.mutable.HashMap</span></code></pre></td></tr></table></div></figure>


<p>一次性倒入一个package下的几个class或object：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import scala.collection.immutable.{TreeMap, TreeSet}</span></code></pre></td></tr></table></div></figure>


<p>甚至可以给原有的class或object定义一个别名。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import scala.util.parsing.json.{JSON =&gt; JsonParser}</span></code></pre></td></tr></table></div></figure>


<p>import可以定义在任何代码块中，并且只会在当前作用域内有效。还可以引入objects的method，相当于java的import static。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>class Frog {
</span><span class='line'>  import scala.xml._
</span><span class='line'>  def n: NodeSeq = NodeSeq.Empty
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>object Moose {
</span><span class='line'>  def bark = "woof"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>import Moose._
</span><span class='line'>bark</span></code></pre></td></tr></table></div></figure>


<h4>Class, Trait和Object</h4>

<p>Scala的对象语法和规则比Java的更加复杂。</p>

<p>Scala去掉了一个文件中只能定义一个public类的限制。你想在一个文件里面放n个类都可以，同时文件的名称也没有限制（Java文件名需要和public的类同名）。</p>

<p>Scala中默认访问级别是public的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// scala
</span><span class='line'>class Foo
</span><span class='line'>
</span><span class='line'>// java
</span><span class='line'>public class Foo {
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>如果构造函数、方法没有参数，可以省略参数列表（即不需要输入括号）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>new Foo
</span><span class='line'>
</span><span class='line'>new Foo()
</span><span class='line'>
</span><span class='line'>class Bar(name: String)
</span><span class='line'>
</span><span class='line'>new Bar("Working...")
</span><span class='line'>
</span><span class='line'>class Baz(name: String) {
</span><span class='line'>  // constructor code is inline
</span><span class='line'>  if(name == null) throw new Exception("Name is null")
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Scala的trait，和java中的interface类似。同时trait可以包括具体实现的方法，这是一个非常方便的特性，你不必在定义复杂的类继承关系来实现代码的重用，在Scala中，把代码写在trait中即可。Scala traits类似于Ruby mixins</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>trait Dog
</span><span class='line'>
</span><span class='line'>class Fizz2(name: String) extends Bar(name) with Dog
</span><span class='line'>
</span><span class='line'>trait Cat {
</span><span class='line'>  def meow(): String
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>trait FuzzyCat extends Cat {
</span><span class='line'>  override def meow(): String = "Meeeeeeeeeow"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>trait OtherThing {
</span><span class='line'>  def hello() = 4
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>class Yep extends FuzzyCat with OtherThing
</span><span class='line'>
</span><span class='line'>(new Yep).meow()
</span><span class='line'>(new Yep).hello()</span></code></pre></td></tr></table></div></figure>


<p>Scala中不支持static关键字，可以使用<code>object</code>单例对象来实现类似的功能。当object对象第一次访问才会被初始化，在对应的访问域内仅有一个该实例。Scala object还有一个优势，由于是类的实例，所以可以作为方法参数进行传递。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>object Simple
</span><span class='line'>
</span><span class='line'>object OneMethod {
</span><span class='line'>  def myMethod() = "Only One"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>object Dude extends Yep
</span><span class='line'>
</span><span class='line'>object Dude2 extends Yep {
</span><span class='line'>  override def meow() = "Dude looks like a cat"
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>object OtherDude extends Yep {
</span><span class='line'>  def twoMeows(otherparam: Yep) = meow + ", " + otherparam.meow
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>OtherDude.meow // Meeeeeeeeeow
</span><span class='line'>OtherDude.twoMeows(Dude) // Meeeeeeeeeow, Meeeeeeeeeow
</span><span class='line'>OtherDude.twoMeows(Dude2) // Meeeeeeeeeow, Dude looks like a cat
</span></code></pre></td></tr></table></div></figure>


<p>如果object嵌套定义在class, trait, object内部的时刻，在其作用域下每个<strong>实例</strong>会创建一个object的单例。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>class HasYep {
</span><span class='line'>  object myYep extends Yep {
</span><span class='line'>    override def meow = "Moof"
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>(new HasYep).myYep.meow // 每个HasYep实例会有一个单独的myYep</span></code></pre></td></tr></table></div></figure>


<p>同样Classes，Objects，traits也可以嵌套在classes，objects，traits。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>class HasClass {
</span><span class='line'>  private class MyDude extends FuzzyCat
</span><span class='line'>  def makeOne(): FuzzyCat = new MyDude
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h4>类继承Class Hierarchy</h4>

<p>除了方法（method），其他一切都是对象(an instance of a class)。Java的primitives类型在Scala也被当做对象，如int(Int)。当两个Ints相加时，Scala编译器会对字节码进行优化最终和java的两个ints相加时一样的。如果使用了Int的方法hashCode和toString，当primitive类型被用于需要引用类型时(expects an Any)，Scala编译器会对其进行装箱，如把Int值加入到HashMap。</p>

<p>为了保持命名的规范化，即所有类的第一个单词都是大写的。在Scala中的原始类型对应为Int,Long,Double,Float,Boolean,Char,Short,Byte，他们都是AnyVal的子类。java的void对应Unit， 同样是AnyVal的子类。你也可以使用<code>()</code>来显示的返回Unit类型实例。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val v = ()
</span><span class='line'>
</span><span class='line'>List(v) // List[Unit] = List(())</span></code></pre></td></tr></table></div></figure>


<p><code>Nothing</code>是很酷，任何方法返回Nothing，表示它不是正常返回，肯定是抛出了异常。<code>None</code>是一个<code>Option[Nothing]</code>的实例，它的get方法会返回<code>Nothing</code>，也就是说get方法会抛出异常，而不是返回底层的值类型null。</p>

<p>Any是Scala中所有类的基类，想Object在Java中的地位。但是，Nothing/primitives等等，所以需要在Object下面定义Scala的根基类。</p>

<p>AnyVal是Scala中primitives对象的包装类的基类。
AnyRef与Java中的Object类似。<code>eq</code>,<code>ne</code>,<code>==</code>,<code>!=</code>这些方法的含义不同。<code>==</code>编译后最终调用java的equals方法，如果需要进行对象引用的比较，使用<code>eq</code>进行处理。</p>

<h4>方法声明</h4>

<p>类型推测很强大也很有用，但是需要小心使用，当类型返回类型不明确时，需要显示进行声明。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def myMethod(): String = "Moof"
</span><span class='line'>
</span><span class='line'>def myOtherMethod() = "Moof" // not have to explicity declare the return type
</span><span class='line'>
</span><span class='line'>def foo(a: Int): String = a.toString
</span><span class='line'>
</span><span class='line'>def f2(a: Int, b: Boolean): String = if(b) b.toString else "false"
</span><span class='line'>
</span><span class='line'>def list[T](p: T): List[T] = p :: Nil
</span><span class='line'>
</span><span class='line'>list(1)
</span><span class='line'>list("Hello")
</span><span class='line'>
</span><span class='line'>// 可变参数， Seq[Int]
</span><span class='line'>def largest(as: Int*): Int = as.reduceLeft((a,b) =&gt; a max b)
</span><span class='line'>
</span><span class='line'>largest(1)
</span><span class='line'>largest(2, 3, 99)
</span><span class='line'>largest(33, 22, 33, 22)
</span><span class='line'>
</span><span class='line'>def mkString[T](as: T*): String = as.foldLeft("")(_ + _.toString)
</span><span class='line'>
</span><span class='line'>def sum[T &lt;: Number](as: T*): Double = as.foldLeft(0d)(_ + _.doubleValue)
</span></code></pre></td></tr></table></div></figure>


<p>方法可以定义在<strong>任何方法块</strong>中，除了最外层即classes，traits，objects定义的地方。方法中可以使用当前作用域类的所有的成员。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def readLines(br: BufferedReader) = {
</span><span class='line'>  var ret: List[String] = Nil
</span><span class='line'>
</span><span class='line'>  def readAll(): Unit = br.readLine match { 
</span><span class='line'>      case null =&gt;
</span><span class='line'>      case s =&gt; ret ::= s; readAll()
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  readAll()
</span><span class='line'>  ret.reverse
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>方法重写和java的不一样，被重写的方法必须带上override的修饰符。重写抽象的方法可以不带override的修饰符。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>abstract class Base {
</span><span class='line'>  def thing: String
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>class One extends Base {
</span><span class='line'>  def thing = "Moof"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>不带参数的方法和变量可以使用<strong>相同的方式访问</strong>，重写父类方法时可以使用val代替def。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>class Two extends One {
</span><span class='line'>  override val thing = (new java.util.Date).toString
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>class Three extends One {
</span><span class='line'>  override lazy val thing = super.thing + (new java.util.Date).toString
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h4>变量声明</h4>

<p>和声明方法类似，不过关键字使用val, var, lazy val。var
可以在设置值以后再次进行修改，类似于java中的变量。val在运行到该作用域时就初始化。lazy val仅在访问的时刻进行计算一次。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var y: String = "Moof"
</span><span class='line'>val x: String = "Moof"
</span><span class='line'>lazy val lz: String = someLongQuery()</span></code></pre></td></tr></table></div></figure>


<p>在编程时，不推荐使用var变量除非一定要用变量。Given that mutability leads to unexpected defects, minimizing mutability in code minimizes mutability-related defects.</p>

<p>Scala类型推测对变量一样有效，在参数类型明确的情况下，定义参数时可以不用指定类型。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var y2 = "Moof"
</span><span class='line'>val x2 = "Moof"</span></code></pre></td></tr></table></div></figure>


<p>Scala支持同时接受多个参数值。 If a code block or method returns a Tuple, the Tuple can be assigned to a val variable.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val (i1: Int, s1: String) = Pair(33, "Moof")
</span><span class='line'>val (i2, s2) = Pair(43, "Moof")</span></code></pre></td></tr></table></div></figure>


<p>运行的效果如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; val (i2,s2)=Pair(43,"W")
</span><span class='line'>i2: Int = 43
</span><span class='line'>s2: String = W
</span><span class='line'>
</span><span class='line'>scala&gt; i2
</span><span class='line'>res0: Int = 43</span></code></pre></td></tr></table></div></figure>


<h4>代码块</h4>

<p>方法和参数定义都可以定义在单行。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def meth9 = "hello world"</span></code></pre></td></tr></table></div></figure>


<p>或者定义在大括号包围的代码块中。代码块可以去嵌套。代码块的返回值是最后一个行的运行结果。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def meth3(): String = {"Moof"}
</span><span class='line'>def meth4(): String = {
</span><span class='line'>  val d = new java.util.Date()
</span><span class='line'>  d.toString()
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>参数定义同样可以使用代码块，适合于有少量计算的赋值操作。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val x3: String = {
</span><span class='line'>  val d = new java.util.Date()
</span><span class='line'>  d.toString()
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h4>Call-by-Name</h4>

<p>在java中，所有方法是按call-by-reference或者call-by-value（原始类型）调用。也就是说，在调用栈中的参数的值或者引用（AnyRef）会传递给调用者。</p>

<p>Scala提供另一种传递参数给方法（函数）的方式：call-by-name，可以把方法块传给调用者。 Each time the callee accesses the parameter, the code block is executed and the value is calculated.</p>

<p>Call-by-name容许我们把耗时的操作（但可能不会用到的）当做参数。For example, in a call to the logger you can use call-by-name, and the express to print is only calculated if it’s going to be logged。Call-by-name同样容许我们创建（如while/doWhile）自定义的控制结构。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def nano() ={
</span><span class='line'>  println("Getting nano")
</span><span class='line'>  System.nanoTime
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>def delayed(t: =&gt; Long) = {
</span><span class='line'>  println("In delayed method")
</span><span class='line'>  println("Param: " + t)
</span><span class='line'>  t
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>scala&gt; delayed(nano())
</span><span class='line'>In delayed method
</span><span class='line'>Getting nano
</span><span class='line'>Param: 198642874346225
</span><span class='line'>Getting nano
</span><span class='line'>res1: Long = 198642875202814
</span><span class='line'>
</span><span class='line'>def notDelayed(t: Long) = {
</span><span class='line'>  println("In not delayed method")
</span><span class='line'>  println("Param: " + t)
</span><span class='line'>  t
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>scala&gt; notDelayed(nano)
</span><span class='line'>Getting nano
</span><span class='line'>In not delayed method
</span><span class='line'>Param: 199944029171474
</span><span class='line'>res5: Long = 199944029171474</span></code></pre></td></tr></table></div></figure>


<p>注意println输出的位置和次数。</p>

<h4>方法调用</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>instance.method()
</span><span class='line'>instance.method
</span><span class='line'>
</span><span class='line'>instance.method(param)
</span><span class='line'>instance method param</span></code></pre></td></tr></table></div></figure>


<p>方法没有参数时可以省略括号。当只有可以参数时，可以省去点和括号。</p>

<p>实际运行效果：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; "abc" toUpperCase
</span><span class='line'>warning: there were 1 feature warning(s); re-run with -feature for details
</span><span class='line'>res0: String = ABC
</span><span class='line'>
</span><span class='line'>scala&gt; "abc".toUpperCase
</span><span class='line'>res1: String = ABC
</span><span class='line'>
</span><span class='line'>scala&gt; "abc".charAt 1
</span><span class='line'>&lt;console&gt;:1: error: ';' expected but integer literal found.
</span><span class='line'>       "abc".charAt 1
</span><span class='line'>                    ^
</span><span class='line'>
</span><span class='line'>scala&gt; "abc" charAt 1
</span><span class='line'>res2: Char = b
</span><span class='line'>
</span><span class='line'>scala&gt; "abc" concat "efg"
</span><span class='line'>res3: String = abcefg
</span></code></pre></td></tr></table></div></figure>


<p>Scala允许方法名中包括+/-/*/?， Scala’s dotless method notation creates a syntactically neutral way of invoking methods that are hard-coded operators in Java.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; 2.1.*(4.3)
</span><span class='line'>res4: Double = 9.03
</span><span class='line'>
</span><span class='line'>scala&gt; 2.1 * 4.3
</span><span class='line'>res5: Double = 9.03</span></code></pre></td></tr></table></div></figure>


<p>多参数的方法调用和java一样。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>instance.method(p1, p2)</span></code></pre></td></tr></table></div></figure>


<p>Scala中的泛型方法，编译器可以进行类型推断。当然你也可以显示的指定类型。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>instance.method[TypeParam](p1, p2)</span></code></pre></td></tr></table></div></figure>


<h4>Functions, apply, update, and Compiler Magic</h4>

<p>Scala是一门函数语言，也意味着你可以传递函数，可以把函数作为返回值在函数和方法中返回。</p>

<p>函数是一个带有参数和返回值的代码块。
在JVM中是不容许传递代码块的。Scala中使用特定接口的匿名内部类作为函数内部实现。当传递一个函数时，其实就是传递一个特定接口(trait)的对象。</p>

<p>定义函数的trait使用一个参数和一个返回值:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Function1[A, B]</span></code></pre></td></tr></table></div></figure>


<p>其中A是参数类型，B是返回值类型。</p>

<p>所有的函数接口都有一个apply的方法，用于函数的调用。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Function1.apply(p: A): B</span></code></pre></td></tr></table></div></figure>


<p>Thus, you can define a method that takes a function and invokes the function with the parameter 42:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def answer(f: Function1[Int, String]) = f.apply(42)</span></code></pre></td></tr></table></div></figure>


<p>如果（只要）对象包括apply方法，可以省略apply，直接把参数跟在函数名后面。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def answer(f: Function1[Int, String]) = f(42)</span></code></pre></td></tr></table></div></figure>


<p>Scala提供的语法糖，在编译时f(42)会编译成f.apply(42)。这样使用可以让代码更简洁漂亮，同时看起来更像函数调用的写法。</p>

<p>更多的语法糖：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Function1[Int, String]
</span><span class='line'>Int =&gt; String
</span><span class='line'>
</span><span class='line'>def answer(f: Int =&gt; String) = f(42)</span></code></pre></td></tr></table></div></figure>


<p>这种语法糖适用于所有包括apply方法对象。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; class Ap {
</span><span class='line'>     | def apply(in: Int) = in.toString
</span><span class='line'>     | }
</span><span class='line'>defined class Ap
</span><span class='line'>
</span><span class='line'>scala&gt; new Ap()(44)
</span><span class='line'>res0: String = 44
</span><span class='line'>
</span><span class='line'>scala&gt; new Ap(44)
</span><span class='line'>&lt;console&gt;:9: error: too many arguments for constructor Ap: ()Ap
</span><span class='line'>              new Ap(44)
</span><span class='line'>              ^
</span><span class='line'>
</span><span class='line'>scala&gt; val a = new Ap
</span><span class='line'>a: Ap = Ap@18258b2
</span><span class='line'>
</span><span class='line'>scala&gt; a(44)
</span><span class='line'>res2: String = 44</span></code></pre></td></tr></table></div></figure>


<p>如果类包括update方法，编译解析赋值操作时，会调用两个参数的update方法。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; class Up {
</span><span class='line'>     | def update(k: Int, v: String) = println("Hey: " + k + " " + v)
</span><span class='line'>     | }
</span><span class='line'>defined class Up
</span><span class='line'>
</span><span class='line'>scala&gt; val u = new Up
</span><span class='line'>u: Up = Up@7bfd80
</span><span class='line'>
</span><span class='line'>scala&gt; u(33) = "hello"
</span><span class='line'>Hey: 33 hello
</span><span class='line'>
</span><span class='line'>scala&gt; class Update {
</span><span class='line'>     | def update(what: String) = println("Singler: " + what)
</span><span class='line'>     | def update(a: Int, b: Int, what: String) = println("2d update")
</span><span class='line'>     | }
</span><span class='line'>defined class Update
</span><span class='line'>
</span><span class='line'>scala&gt; val u = new Update
</span><span class='line'>u: Update = Update@4bd4d2
</span><span class='line'>
</span><span class='line'>scala&gt; u() = "Foo"
</span><span class='line'>Singler: Foo
</span><span class='line'>
</span><span class='line'>scala&gt; u(3,4) = "Howdy"
</span><span class='line'>2d update
</span></code></pre></td></tr></table></div></figure>


<p>Scala中Array和HashMap使用update的方式进行设值。使用这种方式我们可以编写和Scala类似特性的库。</p>

<p>Scala的这些特性可以让我们编写更易理解的代码。同时理解Scala的这些语法糖，能更好的与java类库一起协作。</p>

<h4>Case Classes</h4>

<p>Scala has a mechanism for creating classes that have the common stuff filled in. Most of the time, when I define a class, I have to write the toString, hashCode, and equals methods.  These methods are boilerplate. Scala provides the case class mechanism for filling in these blanks as well as support for pattern matching.</p>

<p>A case class provides the same facilities as a normal class, but the compiler generates toString,  hashCode, and  equals methods (which you can override).</p>

<p>Case classes can be instantiated without the use of the  new statement. By default, all the parameters in the case class’s constructor become properties on the case class. Here’s how to create a case class:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scala&gt; case class Stuff(name: String, age: Int)
</span><span class='line'>defined class Stuff
</span><span class='line'>
</span><span class='line'>scala&gt; val s = Stuff("David", 45)
</span><span class='line'>s: Stuff = Stuff(David,45)
</span><span class='line'>
</span><span class='line'>scala&gt; s.toString
</span><span class='line'>res0: String = Stuff(David,45)
</span><span class='line'>
</span><span class='line'>scala&gt; s == Stuff("David", 45) // == 相当于java中的equals
</span><span class='line'>res1: Boolean = true
</span><span class='line'>
</span><span class='line'>scala&gt; s == Stuff("David", 42)
</span><span class='line'>res2: Boolean = false
</span><span class='line'>
</span><span class='line'>scala&gt; s.name
</span><span class='line'>res4: String = David
</span><span class='line'>
</span><span class='line'>scala&gt; s.age
</span><span class='line'>res5: Int = 45</span></code></pre></td></tr></table></div></figure>


<p>手写case class功能的类：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>class Stuff(val name: String, val age: Int) {
</span><span class='line'>  override def toString = "Stuff(" + name + "," + age + ")"
</span><span class='line'>  override def hashCode = name.hashCode + age
</span><span class='line'>  override def equals(other: AnyRef) = other match {
</span><span class='line'>      case s: Stuff =&gt; this.name == s.name && this.age = s.age
</span><span class='line'>      case _ =&gt; false
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>object Stuff {
</span><span class='line'>  def apply(name: String, age: Int) = new Stuff(name, age)
</span><span class='line'>  def unapply(s: Stuff) = Some((s.name, s.age))
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h4>Basic Pattern Matching</h4>

<p>模式匹配（Pattern matching）可以使用很少的代码编写非常复杂的判断。Scala Pattern matching和Java switch语句类似， but you can test against almost anything, and you can even assign pieces of the matched value to variables. Like everything in Scala, pattern matching is an expression, so it result s in a value that may be assigned or returned. The most basic pattern matching is like Java’s switch, except there is no  break in each case as the cases do not fall through to each other.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>44 match {
</span><span class='line'>  case 44 =&gt; true
</span><span class='line'>  case _ =&gt; false
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>可以对String进行match操作，类似于C#。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>"David" match {
</span><span class='line'>  case "David" =&gt; 45
</span><span class='line'>  case "Elwood" =&gt; 77
</span><span class='line'>  case _ =&gt; 0
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>可以多case classes进行模式匹配（pattern match）操作。Case classes提供了非常适合与pattern-matching的语法。下面的例子，用于匹配Stuff的name==David以及age==45的对象。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Stuff("David", 45) match {
</span><span class='line'>  case Stuff("David", 45) =&gt; true
</span><span class='line'>  case _ =&gt; false
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>仅匹配名字：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Stuff("David", 45) match {
</span><span class='line'>  case Stuff("David", _) =&gt; "David"
</span><span class='line'>  case _ =&gt; "Other"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>还可以把值提取出来，如把age的值赋给howOld变量：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Stuff("David", 45) match {
</span><span class='line'>  case Stuff("David", howOld) =&gt; "David, age: " + howOld
</span><span class='line'>  case _ =&gt; "Other"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>还可以在pattern和=>之间添加条件。如年龄小于30的返回young David，其他的结果为old David。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Stuff("David", 45) match {
</span><span class='line'>  case Stuff("David", age) if age &lt; 30 =&gt; "young David"
</span><span class='line'>  case Stuff("David", _) =&gt; "old David"
</span><span class='line'>  case _ =&gt; "Other"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Pattern matching还可以根据类型进行匹配：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>x match {
</span><span class='line'>  case d: java.util.Date =&gt; "The date in milliseconds is " + d.getTime
</span><span class='line'>  case u: java.net.URL =&gt; "The URL path: " + u.getPath
</span><span class='line'>  case s: String =&gt; "String: " + s
</span><span class='line'>  case _ =&gt; "Something else"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>如果使用Java代码的话，需要多很多的转换！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>if(x instanceof Date) return "The date in milliseconds is " + ((Date)x).getTime();
</span><span class='line'>if(x instanceof URL) return "The URL path: " + ((URL)x).getPath();
</span><span class='line'>if(x instanceof String) return "String: " + ((String)x);
</span><span class='line'>return "Something else"</span></code></pre></td></tr></table></div></figure>


<h4>if/else and while</h4>

<p>while在Scala中比较少用。if/else使用频率高一些，比java的三目赋值操作符（?:）使用频率更高。if和while表达式总是返回Unit（相当于Java的Void）。if/else的返回值更具各个部分表单时类型确定。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>if(exp) println("yes")
</span><span class='line'>
</span><span class='line'>// multiline
</span><span class='line'>if(exp) {
</span><span class='line'>  println("Line one")
</span><span class='line'>  println("Line two")
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>val i: Int = if(exp) 1 else 3
</span><span class='line'>
</span><span class='line'>val i: Int = if(exp) 1 
</span><span class='line'>else {
</span><span class='line'>  val j = System.currentTimeMillis
</span><span class='line'>  (j % 100L).toInt
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>while executes its code block as long  as its expression evaluates to  true, just like Java. In practice, using recursion, a method calling itself, provides more readab le code and enforces the concept of transforming input to output rather than changing, mutating, variables. Recursive methods can be as efficient as a while loop.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>while (exp) println("Working...")
</span><span class='line'>while (exp) {
</span><span class='line'>  println("Working...")
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h4>for</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for { i &lt;- 1 to 3} println(i)
</span><span class='line'>
</span><span class='line'>for { i &lt;- 1 to 3
</span><span class='line'>      j &lt;- 1 to 3
</span><span class='line'>  } println(i*j)
</span><span class='line'>
</span><span class='line'>def isOdd(in: Int) = in % 2 == 1
</span><span class='line'>for {i &lt;- 1 to 5 if ifOdd(i)} println(i)
</span><span class='line'>
</span><span class='line'>for {i &lt;- 1 to 5
</span><span class='line'>      j &lt;- 1 to 5 if isOdd(i*j)} println(i*j)
</span><span class='line'>
</span><span class='line'>val lst = (1 to 18 by 3).toList
</span><span class='line'>for {i &lt;- lst if isOdd(i)} yield i
</span><span class='line'>
</span><span class='line'>for {i &lt;- lst; j &lt;- lst if isOdd(i*j)} yield i*j</span></code></pre></td></tr></table></div></figure>


<p>将在第三章-集合中更详细的讲解for使用方法。</p>

<h4>throw, try/catch/finally, and synchronized</h4>

<p>try/finally的写法和java类似：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>throw new Exception("Working...")
</span><span class='line'>
</span><span class='line'>try{
</span><span class='line'>  throw new Exception("Working...")
</span><span class='line'>} finally {
</span><span class='line'>  println("This will always be printed")
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>try/catch的语法不大一样，catch对异常进行了封装，首先它是一个表达式其返回值是一个值；使用case（pattern matched）来匹配异常类型。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>try {
</span><span class='line'>  file.write(stuff)
</span><span class='line'>} catch {
</span><span class='line'>  case e: java.io.IOException =&gt; // handle IO Exception
</span><span class='line'>  case n: NullPointerException =&gt; // handle null Exception
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>try { Integer.parseInt("dog") } catch { case _ =&gt; 0 } //0
</span><span class='line'>try { Integer.parseInt("44") } catch { case _ =&gt; 0 } //44</span></code></pre></td></tr></table></div></figure>


<p>基于对象的同步操作，每个类都自带了synchronized方法。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>obj.synchronized {
</span><span class='line'>  // do something that needs to be serialized
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>不像java有synchronized方法修饰符。在Scala中同步方法定义使用：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def foo(): Int = synchronized {
</span><span class='line'>  42
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h4>Comments</h4>

<p>注释基本上类C的语言都一样，单行<code>//</code>、多上<code>/* ... */</code>。</p>

<p>在Scala中还可以嵌套的注释。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/*
</span><span class='line'>  This is an outer comment
</span><span class='line'>  /* And this comment
</span><span class='line'>     is nested
</span><span class='line'>  */
</span><span class='line'>  Outer comment
</span><span class='line'>*/</span></code></pre></td></tr></table></div></figure>


<h4>Scala vs Java vs Ruby</h4>

<p><strong>类和实例</strong></p>

<p>java有原始类型。Scala中操作都是方法调用，所有东西都是对象，无需为了原始类型而进行额外的判断/处理。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>1.hashCode
</span><span class='line'>2.toString</span></code></pre></td></tr></table></div></figure>


<p>我们可以定义一个方法，传递函数（从一个Int到另一个Int转换操作）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def with42(in: Int =&gt; Int) = in(42)
</span><span class='line'>with42( 33 + )</span></code></pre></td></tr></table></div></figure>


<p>在语言级别，如果所有东西都是统一的，在进行编程设计时就会很方便和简单。同时Scala编译时会针对JVM原始类型进行优化，使得scala的代码在效率上非常接近Java。</p>

<p><strong>Traits, Interfaces, and Mixins</strong></p>

<p>在java中除了Object对象，其他对象都有一个唯一的父类。Java类可以实现一个或者多个接口（定义实现类必须实现方法的约定）。这是依赖注入和测试mocks，以及其他抽象模式的基础。</p>

<p>Scala使用traits， Traits提供了Java接口拥有的所有特性。同时Traits可以包括方法的实现以及参数的定义。方法实现一次，把所有继承traits的方法混入子类中。</p>

<p><strong>Object, Static, and Singletons</strong></p>

<p>在Java中，可以定义类的（静态）方法和属性，提供了访问方法的唯一入口，同时不需要实例化对象。类（静态）属性提供了在JVM中全局共享数据的方式。
Scala提供了类似的机制：Objects。Objects是单例模式的实现。在类加载的时刻实例化该对象。这种方式同样可以共享全局状态。而且，objects也是Scala完全的面向对象的一种体现，objects是一个类的实例，而不是某种类级别的常量（some class-level constant）。可以把objects作为参数来进行传递。</p>

<p><strong>Functions, Anonymous Inner Class, and  Lambdas/Procs</strong></p>

<p>The Java construct to pass units of computation as parameters to methods is anonymous inner class. 匿名内部类在Swing UI库非常的常见。在Swing中，许多UI事件处理的接口定义1-2个方法，在编写程序时，实现事件接口的内部类能访问外部类的私有成员数据。</p>

<p>Scala functions对应的就是匿名内部类。Scala functions实现了统一的接口，调用函数时执行接口的apply方法。和Java匿名内部类相比，Scala创建函数的语法更加简洁和优雅。同时，访问本地参数的规则也更加灵活。在Java匿名内部类只能访问final的参数，而Scala functions能访问和修改vars参数。</p>

<p>Scala和Ruby的面向对象模型和函数式编程很相似。同时Scala在访问类库和静态类型方面和Java很类似。Scala博采众长，把Java和Ruby的优点都囊括了。</p>

<h3>总结</h3>

<p>这一章首相讲了安装和运行Scala程序，然后围绕Scala编程的语法结构来展开。下一章讲解Scala的数据类型，使用很少的代码编写功能健壮的程序，同时编码量的减少也能有效的控制bugs的数量。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Expect-批量实现SSH无密钥登录]]></title>
    <link href="http://winseliu.com/blog/2014/09/07/expect-automate-and-batch-config-ssh/"/>
    <updated>2014-09-07T16:11:18+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/07/expect-automate-and-batch-config-ssh</id>
    <content type="html"><![CDATA[<p>在安装部署Hadoop集群的首要步骤就是配置SSH的无密钥登陆。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
</span><span class='line'>cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys
</span><span class='line'>
</span><span class='line'>ssh-copy-id -i ~/.ssh/id_rsa.pub root@$ip</span></code></pre></td></tr></table></div></figure>


<p>然后，可以通过ssh命令来进行批量的操作。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh root@$ip 'cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys'
</span><span class='line'>scp -o StrictHostKeyChecking=no /etc/hosts root@${ip}:/etc/</span></code></pre></td></tr></table></div></figure>


<p>但是，一些需要密码的dialogue形式的输入时，部署N台datanode就需要输入N遍！同时新建用户也是需要输入用户密码的操作！！</p>

<p>Linux Expect就是用来自动化处理这些需求的。Except能根据提示来实现相应的输入。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop-deploy-0.0.1]$ ssh-copy-id localhost
</span><span class='line'>The authenticity of host 'localhost (::1)' can't be established.
</span><span class='line'>RSA key fingerprint is 4e:fe:7a:0a:98:6e:9a:ab:af:e4:65:51:9b:3d:e0:99.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added 'localhost' (RSA) to the list of known hosts.
</span><span class='line'>hadoop@localhost's password: 
</span><span class='line'>Now try logging into the machine, with "ssh 'localhost'", and check in:
</span><span class='line'>
</span><span class='line'>  .ssh/authorized_keys
</span><span class='line'>
</span><span class='line'>to make sure we haven't added extra keys that you weren't expecting.</span></code></pre></td></tr></table></div></figure>


<p>根据需要<strong>提示信息</strong>，以及需要<strong>输入的信息</strong>，可以编写对应expect脚本来进行自动化。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop-deploy-0.0.1]$ cat bin/ssh-copy-id.expect 
</span><span class='line'>#!/usr/bin/expect  
</span><span class='line'>
</span><span class='line'>## Usage $0 [user@]host password
</span><span class='line'>
</span><span class='line'>set host [lrange $argv 0 0];
</span><span class='line'>set password [lrange $argv 1 1] ;
</span><span class='line'>
</span><span class='line'>set timeout 30;
</span><span class='line'>
</span><span class='line'>spawn ssh-copy-id $host ;
</span><span class='line'>
</span><span class='line'>expect {
</span><span class='line'>  "(yes/no)?" { send yes\n; exp_continue; }
</span><span class='line'>  "password:" { send $password\n; exp_continue; }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>exec sleep 1;</span></code></pre></td></tr></table></div></figure>


<p>同样新建用户初始化密码的操作一样可以使用expect来使用：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop-deploy-0.0.1]$ cat bin/passwd.expect
</span><span class='line'>#!/usr/bin/expect  
</span><span class='line'>
</span><span class='line'>## Usage $0 host username password
</span><span class='line'>
</span><span class='line'>set host [lrange $argv 0 0];
</span><span class='line'>set username [lrange $argv 1 1];
</span><span class='line'>set password [lrange $argv 2 2] ;
</span><span class='line'>
</span><span class='line'>set timeout 30;
</span><span class='line'>
</span><span class='line'>##
</span><span class='line'>
</span><span class='line'>spawn ssh $host useradd $username ;
</span><span class='line'>
</span><span class='line'>exec sleep 1;
</span><span class='line'>
</span><span class='line'>##
</span><span class='line'>
</span><span class='line'>spawn ssh $host passwd $username ;
</span><span class='line'>
</span><span class='line'>## password and repasswd all use this
</span><span class='line'>expect {
</span><span class='line'>  "password:" { send $password\n; exp_continue; }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>exec sleep 1;</span></code></pre></td></tr></table></div></figure>


<p>有了上面的脚本，预定义每台机器的root密码，使用ssh-copy-id.expect建立到各台datanode机器的无密钥登录；然后passwd.expect脚本分发给各台机器，然后使用ssh进行运行脚本建立用户初始化密码。</p>

<p>Expect仅在master机器上安装就可以。安装程序的如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install expect</span></code></pre></td></tr></table></div></figure>


<p>or</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>rpm -ivh tcl-8.5.7-6.el6.x86_64.rpm
</span><span class='line'>rpm -ivh expect-5.44.1.15-5.el6_4.x86_64.rpm</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读码] Hadoop2 Balancer磁盘空间平衡（下）]]></title>
    <link href="http://winseliu.com/blog/2014/09/05/read-hadoop-balancer-source-part3/"/>
    <updated>2014-09-05T16:31:15+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/05/read-hadoop-balancer-source-part3</id>
    <content type="html"><![CDATA[<p>前面讲到了节点的初始化，根据节点使用率与集群dfs使用率比较分为
<code>overUtilizedDatanodes</code>，<code>aboveAvgUtilizedDatanodes</code>，<code>belowAvgUtilizedDatanodes</code>，<code>underUtilizedDatanodes</code>，同时进行了节点数据量从Source到Target的配对。</p>

<p>接下来就是最后的数据移动部分了。</p>

<p>5.3 移动数据</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private ReturnStatus run(int iteration, Formatter formatter,
</span><span class='line'>      Configuration conf) {
</span><span class='line'>      ...
</span><span class='line'>      if (!this.nnc.shouldContinue(dispatchBlockMoves())) {
</span><span class='line'>        return ReturnStatus.NO_MOVE_PROGRESS;
</span><span class='line'>      }
</span><span class='line'>      ...
</span><span class='line'>  }    </span></code></pre></td></tr></table></div></figure>


<p>针对一个namenode如果连续5次没有移动数据，就会退出平衡操作，是在<code>NameNodeConnector#shouldContinue(long)</code>中处理的。</p>

<p>由于这里需要进行大量计算，以及耗时的文件传输等操作，这里使用了executorservice，分别为moverExecutor和dispatcherExecutor，有两个配置<code>dfs.balancer.moverThreads</code>（1000）和<code>dfs.balancer.dispatcherThreads</code>（200）来设置线程池的大小。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  Balancer(NameNodeConnector theblockpool, Parameters p, Configuration conf) {
</span><span class='line'>      ...
</span><span class='line'>    this.moverExecutor = Executors.newFixedThreadPool(
</span><span class='line'>            conf.getInt(DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_KEY,
</span><span class='line'>                        DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_DEFAULT));
</span><span class='line'>    this.dispatcherExecutor = Executors.newFixedThreadPool(
</span><span class='line'>            conf.getInt(DFSConfigKeys.DFS_BALANCER_DISPATCHERTHREADS_KEY,
</span><span class='line'>                        DFSConfigKeys.DFS_BALANCER_DISPATCHERTHREADS_DEFAULT));
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>其中<code>dispatchBlockMoves()</code>包装了数据移动的操作，把source的块移动到target节点中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private long dispatchBlockMoves() throws InterruptedException {
</span><span class='line'>    long bytesLastMoved = bytesMoved.get();
</span><span class='line'>    Future&lt;?&gt;[] futures = new Future&lt;?&gt;[sources.size()];
</span><span class='line'>    int i=0;
</span><span class='line'>    for (Source source : sources) {
</span><span class='line'>       // / 新线程来执行块的分发
</span><span class='line'>      futures[i++] = dispatcherExecutor.submit(source.new BlockMoveDispatcher());
</span><span class='line'>    }
</span><span class='line'>    
</span><span class='line'>    // wait for all dispatcher threads to finish
</span><span class='line'>    // / 等待分发操作完成
</span><span class='line'>    for (Future&lt;?&gt; future : futures) { 
</span><span class='line'>        future.get(); 
</span><span class='line'>    }
</span><span class='line'>    
</span><span class='line'>    // wait for all block moving to be done
</span><span class='line'>    // / 等待块的数据移动完成，相当于等待moverExecutor的Future完成
</span><span class='line'>    waitForMoveCompletion(); 
</span><span class='line'>    
</span><span class='line'>    return bytesMoved.get()-bytesLastMoved;
</span><span class='line'>  }
</span><span class='line'>  private void waitForMoveCompletion() {
</span><span class='line'>    boolean shouldWait;
</span><span class='line'>    do {
</span><span class='line'>      shouldWait = false;
</span><span class='line'>      for (BalancerDatanode target : targets) {
</span><span class='line'>          // / 块从source移动到target完成后,会从Pending的列表中移除 @see PendingBlockMove#dispatch()
</span><span class='line'>        if (!target.isPendingQEmpty()) { 
</span><span class='line'>          shouldWait = true;
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>      if (shouldWait) {
</span><span class='line'>        try {
</span><span class='line'>          Thread.sleep(blockMoveWaitTime);
</span><span class='line'>        } catch (InterruptedException ignored) {
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    } while (shouldWait);
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>上面是分发功能主程序执行的代码，调用分发线程和等待执行结果的代码。主要业务逻辑在线程中调用执行。</p>

<p>分发线程dispatcher先获取Source上指定大小的block块，对应到<code>getBlockList()</code>方法。除了用于<strong>块同步</strong>的globalBlockList变量、以及记录当前Source获取的srcBlockList、最重要的当属用于判断获取的块是否符合条件的方法<code>isGoodBlockCandidate(block)</code>。在移动块的选择也会用到该方法，单独拿出来在后面讲。</p>

<p>然后选择Source下哪些块将移动到Targets目标节点。在<code>chooseNodes</code>步骤中把移动和接收<strong>数据</strong>的流向确定了，相关信息存储在Source的nodeTasks列表对象中。这里<code>PendingBlockMove.chooseBlockAndProxy()</code>把Sources需要移动的<strong>块</strong>确定下来，把从Source获取到的srcBlockList分配给Target。然后交给moverExecutor去执行。</p>

<p>其中通过<code>isGoodBlockCandidate</code>和<code>chooseProxySource</code>（选择从那个目标获取block的真实数据，不一定是Source节点哦！）方法筛选合适的符合条件的块，并加入到movedBlocks对象。</p>

<p><img src="http://file.bmob.cn/M00/0C/FA/wKhkA1QLCqqASDGHAAMJbC1ZgZQ339.png" alt="" /></p>

<p>调用的dispatchBlocks方法第一次循环是不会有数据移动的，此时Source对象中srcBlockList可移动块为空，从Source中获取块后再进行块的移动操作<code>chooseNextBlockToMove()</code>。</p>

<p>先讲下Source类属性blocksToReceive，初始值为2*scheduledSize，有三个地方：dispatchBlocks初始化大小，getBlockList从Source节点获取block的量同时减去获取到的block的字节数，还有就是shouldFetchMoreBlocks用于判断是否还有数据需要获取或者移动dispatchBlocks。这个属性其实也就是<strong>设置一个阀</strong>，不管block是否为最终移动的block，获取到块的信息后就会从blocksToReceive减去相应的字节数。</p>

<p><img src="http://file.bmob.cn/M00/0C/D7/wKhkA1QJjnGAT2T-AACHmpdgZc0077.png" alt="" /></p>

<p>前面获取Source block和分配到Target block都使用了isGoodBlockCandidate方法，这里涉及到怎么去评估<strong>块</strong>获取和分配是否合理的问题。需同时满足下面三个条件：</p>

<ul>
<li>当前选中的移动的块，不在已移动块的名单中<code>movedBlocks.contains</code></li>
<li>移动的块在目的机器上没有备份</li>
<li>移动的块不减少含有该数据的机架数量

<ul>
<li>多机架的情况下<code>cluster.isNodeGroupAware()</code>，移动的块在目的机器的机架上没有备份</li>
<li>YES source和target在同一个机架上。</li>
<li>YES source和target不在同一机架上，且该块没有一个备份在target的机架上</li>
<li>YES source和target不在同一机架上，且该块有另一个备份和source在同一机架上</li>
</ul>
</li>
</ul>


<h2>疑问</h2>

<p>一个Datanode只能同时移动/接收5个Block（即MAX_NUM_CONCURRENT_MOVES值），结合<code>chooseProxySource</code>的代码的addTo调用，看的很是辛苦！如block-A所有块都在A机架上，在选择proxySource时，会把该块的<strong>两个</strong>datanode都加上一个pendingBlock，显然这不大合理！！</p>

<p>如果备用的proxySource节点恰好还是target的话，waitForMoveCompletion方法永远不能结束！！应该把没有找到同机架的源情况移到for循环外面进行处理。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>private boolean chooseProxySource() {
</span><span class='line'>  final DatanodeInfo targetDN = target.getDatanode();
</span><span class='line'>  boolean find = false;
</span><span class='line'>  for (BalancerDatanode loc : block.getLocations()) {
</span><span class='line'>    // check if there is replica which is on the same rack with the target
</span><span class='line'>    if (cluster.isOnSameRack(loc.getDatanode(), targetDN) && addTo(loc)) {
</span><span class='line'>      find = true;
</span><span class='line'>      // if cluster is not nodegroup aware or the proxy is on the same 
</span><span class='line'>      // nodegroup with target, then we already find the nearest proxy
</span><span class='line'>      if (!cluster.isNodeGroupAware() 
</span><span class='line'>          || cluster.isOnSameNodeGroup(loc.getDatanode(), targetDN)) {
</span><span class='line'>        return true;
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>    
</span><span class='line'>    if (!find) {
</span><span class='line'>    // 这里的non-busy指的是，pendingBlock小于5份节点
</span><span class='line'>      // find out a non-busy replica out of rack of target
</span><span class='line'>      find = addTo(loc);
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  return find;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/0D/06/wKhkA1QLuziAKEZdAAA8zGjPMGQ901.png" alt="" /></p>

<p>不过无需庸人自扰，一般都在一个rack上，这种问题就不存在了！同时这个也不是能一步到位，加了很多限制（一次迭代一个datanode最多处理10G，获取一次srcBlockList仅2G还限制就一次迭代就5个block），会执行很多次。</p>

<h2>总结</h2>

<p>总体的代码大致就是这样子了。根据集群使用率和阀值，计算需要进行数据接收和移动的节点（初始化），然后进行配对（选择），再进行块的选取和接收节点进行配对（分发），最后就是数据的移动（理解为socket数据传递就好了，调用了HDFS的协议代码。表示看不明），并等待该轮操作结束。</p>

<h2>举例</h2>

<p>除了指定threshold为5，其他是默认参数。由于仅单namenode和单rack，所以直接分析第五部分的namenode平衡处理。</p>

<p>根据所给的数据，（initNodes）第一步计算使用率，得出需要移动的数据量，把datanodes对号入座到over/above/below/under四个分类中。</p>

<p><img src="http://file.bmob.cn/M00/0D/08/wKhkA1QLwxiAOEV3AAAu5v9zggc374.png" alt="" /></p>

<p>（chooseNodes）第二步进行Source到Target节点的计划移动数据量计算。</p>

<p>在初始化BalancerDatanode的时刻，就计算出了节点的maxSize2Move。从给出的数据，只有一个节点超过阀值，另外两个是都在阀值内，一个高于平均值一个低于平均值。</p>

<p>这里就是把A1超出部分的数据（小于10G）移到A2，计算Source和Target的scheduledSize的大小。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chooseDatanodes(overUtilizedDatanodes, belowAvgUtilizedDatanodes, matcher);
</span><span class='line'>chooseForOneDatanode(datanode, candidates, matcher)
</span><span class='line'>chooseCandidate(dn, i, matcher)
</span><span class='line'>// 把所有A1超出部分全部移到A2，并NodeTask(A2, 8428571.429)存储到Source：A1的nodeTaskList对象中
</span><span class='line'>matchSourceWithTargetToMove((Source)dn, chosen);</span></code></pre></td></tr></table></div></figure>


<p>（dispatchBlockMoves）第三步就是分发进行块的转移。</p>

<p>先设置blocksToReceive（2*scheduledSize=16857142.86）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chooseNextBlockToMove
</span><span class='line'>chooseBlockAndProxy
</span><span class='line'>markMovedIfGoodBlock
</span><span class='line'>isGoodBlockCandidate
</span><span class='line'>chooseProxySource
</span><span class='line'>
</span><span class='line'>scheduleBlockMove
</span><span class='line'>
</span><span class='line'>getBlockList</span></code></pre></td></tr></table></div></figure>


<p>从Source获取块时，可能在A2上已经有了，会通过isGoodBlockCandidate来进行过滤。然后就是把它交给moverExecutor执行数据块的移动，完成后修改处理的数据量byteMoved，把移动的块从target和proxySource的pendingBlockList中删除。</p>

<p>重复进行以上步骤，直到全部所有节点的使用率都在阀值内，顺利结束本次平衡处理。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读码] Hadoop2 Balancer磁盘空间平衡（中）]]></title>
    <link href="http://winseliu.com/blog/2014/09/05/read-hadoop-balancer-source-part2/"/>
    <updated>2014-09-05T14:57:25+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/05/read-hadoop-balancer-source-part2</id>
    <content type="html"><![CDATA[<h2>code</h2>

<p>执行<code>hadoop-2.2.0/bin/hadoop balancer -h</code>查看可以设置的参数（和sbin/start-balancer.sh一样）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage: java Balancer
</span><span class='line'>  [-policy &lt;policy&gt;]    the balancing policy: datanode or blockpool
</span><span class='line'>  [-threshold &lt;threshold&gt;]  Percentage of disk capacity</span></code></pre></td></tr></table></div></figure>


<p>main方法入口，可以接受threshold（大于等于1小于等于100， 默认值10）和policy（可取datanode[dfsused]/blockpool[
blockpoolused]， 默认值datanode），具体的含义可以查看（上）篇中的javadoc的描述。</p>

<h3>获取初始化参数</h3>

<p>然后通过ToolRunner解析参数，并运行Cli工具类来执行HDFS的平衡。</p>

<p>1 设置检查</p>

<p><code>WIN_WIDTH</code>(默认1.5h) 已移动的数据会记录movedBlocks（list）变量中，在移动成功的数据<code>CUR_WIN</code>的值经过该时间后会被移动到<code>OLD_WIN</code>&mdash;现在感觉作用不大，为了减少map的大小？</p>

<p><code>checkReplicationPolicyCompatibility()</code>检查配置<code>dfs.block.replicator.classname</code>是否为BlockPlacementPolicyDefault子类，即是否满足3份备份的策略（1st本地，2nd另一个rack，3rd和第二份拷贝不同rack的节点）？</p>

<p>2 获取nameserviceuris</p>

<p>通过<code>DFSUtil#getNsServiceRpcUris()</code>来获取namenodes，调用<code>getNameServiceUris()</code>来得到一个URI的结果集：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>+ nsId &lt;- dfs.nameservices
</span><span class='line'>  ? ha  &lt;- dfs.namenode.rpc-address + [dfs.nameservices] + [dfs.ha.namenodes]
</span><span class='line'>    Y+ =&gt; hdfs://nsId
</span><span class='line'>    N+ =&gt; hdfs://[dfs.namenode.servicerpc-address.[nsId]] 或 hdfs://[dfs.namenode.rpc-address.[nsId]] 第二个满足条件的加入到nonPreferredUris
</span><span class='line'>+ hdfs://[dfs.namenode.servicerpc-address] 或 hdfs://[dfs.namenode.rpc-address]  第二个满足条件的加入到nonPreferredUris
</span><span class='line'>? [fs.defaultFs] 以hfds开头，且不在nonPreferredUris集合中是加入结果集</span></code></pre></td></tr></table></div></figure>


<p>HA情况下的地址相关配置项可以查看<a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithQJM.html">官网的文档</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>dfs.nameservices
</span><span class='line'>dfs.ha.namenodes.[nameservice ID]
</span><span class='line'>dfs.namenode.rpc-address.[nameservice ID].[name node ID] </span></code></pre></td></tr></table></div></figure>


<p>3 解析threshold和policy参数</p>

<p>默认值: <strong>BalancingPolicy.Node.INSTANCE, 10.0</strong>。运行打印的日志如下，INFO日志中包括了初始化的参数信息。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-09-05 10:55:12,183 INFO Balancer: Using a threshold of 1.0
</span><span class='line'>2014-09-05 10:55:12,186 INFO Balancer: namenodes = [hdfs://umcc97-44:9000]
</span><span class='line'>2014-09-05 10:55:12,186 INFO Balancer: p         = Balancer.Parameters[BalancingPolicy.Node, threshold=1.0]
</span><span class='line'>2014-09-05 10:55:13,744 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>2014-09-05 10:55:18,154 INFO net.NetworkTopology: Adding a new node: /default-rack/10.18.97.142:50010
</span><span class='line'>2014-09-05 10:55:18,249 INFO net.NetworkTopology: Adding a new node: /default-rack/10.18.97.144:50010
</span><span class='line'>2014-09-05 10:55:18,311 INFO net.NetworkTopology: Adding a new node: /default-rack/10.18.97.143:50010
</span><span class='line'>2014-09-05 10:55:18,319 INFO Balancer: 2 over-utilized: [Source[10.18.97.144:50010, utilization=8.288283273062705], Source[10.18.97.143:50010, utilization=8.302032354001554]]
</span><span class='line'>2014-09-05 10:55:18,320 INFO Balancer: 1 underutilized: [BalancerDatanode[10.18.97.142:50010, utilization=4.716543864576553]]
</span><span class='line'>2014-09-05 10:55:33,918 INFO Balancer: Need to move 3.86 GB to make the cluster balanced.
</span><span class='line'>2014-09-05 11:21:16,875 INFO Balancer: Decided to move 2.43 GB bytes from 10.18.97.144:50010 to 10.18.97.142:50010
</span><span class='line'>2014-09-05 11:24:16,712 INFO Balancer: Decided to move 1.84 GB bytes from 10.18.97.143:50010 to 10.18.97.142:50010
</span><span class='line'>2014-09-05 11:25:55,726 INFO Balancer: Will move 4.27 GB in this iteration</span></code></pre></td></tr></table></div></figure>


<h3>执行Balancer</h3>

<p>4 调用Balancer#run执行</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> # 调试命令
</span><span class='line'> export HADOOP_OPTS=" -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8087 "
</span><span class='line'> sbin/start-balancer.sh </span></code></pre></td></tr></table></div></figure>


<p></p>

<p>Balancer的静态方法run，循环处理所有namenodes。在实例化namenode的NameNodeConnector对象时，会把当前运行balancer程序的hostname写入到HDFS的<code>/system/balancer.id</code>文件中，可以用来控制同时只有一个balancer运行。</p>

<p><img src="http://file.bmob.cn/M00/0C/96/wKhkA1QJJNqAXxeaAAAho0g2bEU520.png" alt="" /></p>

<p>在循环处理的时刻使用<code>Collections.shuffle(connectors)</code>打乱了namenodes的顺序。</p>

<p>Balancer的静态方法run中是一个双层循环，实例化Balancer并调用实例方法run来处理每个namenode的平衡。运行后要么<strong>出错</strong>要么就是平衡<strong>顺利完成</strong>才算结束。平衡的返回状态值及其含义可以查看javadoc（上）篇。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  static int run(Collection&lt;URI&gt; namenodes, final Parameters p,
</span><span class='line'>      Configuration conf) throws IOException, InterruptedException {
</span><span class='line'>    ...
</span><span class='line'>      for (URI uri : namenodes) {
</span><span class='line'>        connectors.add(new NameNodeConnector(uri, conf));
</span><span class='line'>      }
</span><span class='line'>    
</span><span class='line'>      boolean done = false;
</span><span class='line'>      for(int iteration = 0; !done; iteration++) {
</span><span class='line'>        done = true;
</span><span class='line'>        Collections.shuffle(connectors);
</span><span class='line'>        for(NameNodeConnector nnc : connectors) {
</span><span class='line'>          final Balancer b = new Balancer(nnc, p, conf);
</span><span class='line'>          final ReturnStatus r = b.run(iteration, formatter, conf);
</span><span class='line'>          // clean all lists
</span><span class='line'>          b.resetData(conf);
</span><span class='line'>          if (r == ReturnStatus.IN_PROGRESS) {
</span><span class='line'>            done = false;
</span><span class='line'>          } else if (r != ReturnStatus.SUCCESS) {
</span><span class='line'>            //must be an error statue, return.
</span><span class='line'>            return r.code;
</span><span class='line'>          }
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>        if (!done) {
</span><span class='line'>          Thread.sleep(sleeptime);
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    ...
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>5 <strong>针对每个namenode的平衡处理</strong></p>

<p>针对每个namenode的每次迭代，又可以分出初始化节点、选择移动节点、移动数据三个部分。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private ReturnStatus run(int iteration, Formatter formatter, Configuration conf) {
</span><span class='line'>      ...
</span><span class='line'>      final long bytesLeftToMove = initNodes(nnc.client.getDatanodeReport(DatanodeReportType.LIVE));
</span><span class='line'>      if (bytesLeftToMove == 0) {
</span><span class='line'>        System.out.println("The cluster is balanced. Exiting...");
</span><span class='line'>        return ReturnStatus.SUCCESS;
</span><span class='line'>      }
</span><span class='line'>
</span><span class='line'>      final long bytesToMove = chooseNodes();
</span><span class='line'>      if (bytesToMove == 0) {
</span><span class='line'>        System.out.println("No block can be moved. Exiting...");
</span><span class='line'>        return ReturnStatus.NO_MOVE_BLOCK;
</span><span class='line'>      }
</span><span class='line'>
</span><span class='line'>      if (!this.nnc.shouldContinue(dispatchBlockMoves())) {
</span><span class='line'>        return ReturnStatus.NO_MOVE_PROGRESS;
</span><span class='line'>      }
</span><span class='line'>
</span><span class='line'>      return ReturnStatus.IN_PROGRESS;
</span><span class='line'>      ...
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>获取集群Live Datanode节点的信息，和通过50070查看的信息差不多，然后调用initNode()方法。</p>

<p><img src="http://file.bmob.cn/M00/0C/8F/wKhkA1QJFgaAAsSNAAD4HDo1RfA678.png" alt="" /></p>

<p>5.1 初始化节点</p>

<p><code>initNodes()</code>中获取每个Datanode的capacity和dfsUsed数据，计算整个集群dfs的平均使用率avgUtilization。
然后根据每个节点的使用率与集群使用率，以及阀值进行比较划分为4种情况：
<code>overUtilizedDatanodes</code>，<code>aboveAvgUtilizedDatanodes</code>，<code>belowAvgUtilizedDatanodes</code>，<code>underUtilizedDatanodes</code>。</p>

<p><img src="http://file.bmob.cn/M00/0C/95/wKhkA1QJH2uAa8UEAABq7RCSLQ0452.png" alt="" /></p>

<p>同时取超出<strong>平均+阀值</strong>和<strong>低于平均-阀值</strong>的字节数最大值，即集群达到平衡需要移动的字节数。</p>

<p>为了测试，如果集群已经平衡，可以搞点数据让集群不平衡，方便查看调试。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/hadoop fs -D dfs.replication=1 -put XXXXX /abc
</span><span class='line'>
</span><span class='line'>sbin/start-balancer.sh -threshold 1</span></code></pre></td></tr></table></div></figure>


<p>5.2 选择节点</p>

<p>初始化节点后，计算出了需要移动的数据量。接下来就是选择移动数据的节点<code>chooseNodes</code>，以及接收对应数据的节点。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private long chooseNodes() {
</span><span class='line'>    // First, match nodes on the same node group if cluster is node group aware
</span><span class='line'>    if (cluster.isNodeGroupAware()) {
</span><span class='line'>      chooseNodes(SAME_NODE_GROUP);
</span><span class='line'>    }
</span><span class='line'>    
</span><span class='line'>    chooseNodes(SAME_RACK);
</span><span class='line'>    chooseNodes(ANY_OTHER);
</span><span class='line'>
</span><span class='line'>    long bytesToMove = 0L;
</span><span class='line'>    for (Source src : sources) {
</span><span class='line'>      bytesToMove += src.scheduledSize;
</span><span class='line'>    }
</span><span class='line'>    return bytesToMove;
</span><span class='line'>  }
</span><span class='line'>  private void chooseNodes(final Matcher matcher) {
</span><span class='line'>    chooseDatanodes(overUtilizedDatanodes, underUtilizedDatanodes, matcher);
</span><span class='line'>    chooseDatanodes(overUtilizedDatanodes, belowAvgUtilizedDatanodes, matcher);
</span><span class='line'>    chooseDatanodes(underUtilizedDatanodes, aboveAvgUtilizedDatanodes, matcher);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  private &lt;D extends BalancerDatanode, C extends BalancerDatanode&gt; void 
</span><span class='line'>      chooseDatanodes(Collection&lt;D&gt; datanodes, Collection&lt;C&gt; candidates,
</span><span class='line'>          Matcher matcher) {
</span><span class='line'>    for (Iterator&lt;D&gt; i = datanodes.iterator(); i.hasNext();) {
</span><span class='line'>      final D datanode = i.next();
</span><span class='line'>      for(; chooseForOneDatanode(datanode, candidates, matcher); );
</span><span class='line'>      if (!datanode.hasSpaceForScheduling()) {
</span><span class='line'>        i.remove(); // “超出”部分全部有去处了
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  private &lt;C extends BalancerDatanode&gt; boolean chooseForOneDatanode(
</span><span class='line'>      BalancerDatanode dn, Collection&lt;C&gt; candidates, Matcher matcher) {
</span><span class='line'>    final Iterator&lt;C&gt; i = candidates.iterator();
</span><span class='line'>    final C chosen = chooseCandidate(dn, i, matcher);
</span><span class='line'>
</span><span class='line'>    if (chosen == null) {
</span><span class='line'>      return false;
</span><span class='line'>    }
</span><span class='line'>    if (dn instanceof Source) {
</span><span class='line'>      matchSourceWithTargetToMove((Source)dn, chosen);
</span><span class='line'>    } else {
</span><span class='line'>      matchSourceWithTargetToMove((Source)chosen, dn);
</span><span class='line'>    }
</span><span class='line'>    if (!chosen.hasSpaceForScheduling()) {
</span><span class='line'>      i.remove(); // 可用的空间已经全部分配出去了
</span><span class='line'>    }
</span><span class='line'>    return true;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  private &lt;D extends BalancerDatanode, C extends BalancerDatanode&gt;
</span><span class='line'>      C chooseCandidate(D dn, Iterator&lt;C&gt; candidates, Matcher matcher) {
</span><span class='line'>    if (dn.hasSpaceForScheduling()) {
</span><span class='line'>      for(; candidates.hasNext(); ) {
</span><span class='line'>        final C c = candidates.next();
</span><span class='line'>        if (!c.hasSpaceForScheduling()) {
</span><span class='line'>          candidates.remove();
</span><span class='line'>        } else if (matcher.match(cluster, dn.getDatanode(), c.getDatanode())) {
</span><span class='line'>          return c;
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>    return null;
</span><span class='line'>  }  </span></code></pre></td></tr></table></div></figure>


<p>选择到<strong>接收节点</strong>后，接下来计算可以移动的数据量（取双方的available的最大值），然后把<strong>接收节点</strong>和<strong>数据量</strong>的信息NodeTask存储到Source的NodeTasks对象中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private void matchSourceWithTargetToMove(
</span><span class='line'>      Source source, BalancerDatanode target) {
</span><span class='line'>    long size = Math.min(source.availableSizeToMove(), target.availableSizeToMove());
</span><span class='line'>    NodeTask nodeTask = new NodeTask(target, size);
</span><span class='line'>    source.addNodeTask(nodeTask);
</span><span class='line'>    target.incScheduledSize(nodeTask.getSize());
</span><span class='line'>    sources.add(source);
</span><span class='line'>    targets.add(target);
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>5.3 移动数据</p>

<p>（待）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[计算出从1到100之间所有奇数的平方之和]]></title>
    <link href="http://winseliu.com/blog/2014/09/04/scala-quadratic-sum-of-odd-num-in-100/"/>
    <updated>2014-09-04T14:15:40+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/04/scala-quadratic-sum-of-odd-num-in-100</id>
    <content type="html"><![CDATA[<p><a href="http://freewind.github.io/posts/scala-group-entry-problem/">计算出从1到100之间所有奇数的平方之和，代码50字符内（QQ群的验证框长度限制为50）</a>。</p>

<p>如题，题目没啥难度，这50字符的条件莫名的增添压迫感。其实java写也不用50个字符就能搞定的 ！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// (1 to 50) foreach {x =&gt; print("0")}
</span><span class='line'>00000000000000000000000000000000000000000000000000
</span><span class='line'>
</span><span class='line'>// java
</span><span class='line'>int sum=0;for(int i=0;i&lt;100;i+=2)sum+=i*i;
</span><span class='line'>
</span><span class='line'>// scala
</span><span class='line'>(1 to 100).map(a=&gt;if(a%2==1)a*a else 0).foldLeft(0)(_+_)
</span><span class='line'>(0 to 100).foldLeft(0)(_+((a:Int)=&gt;if(a%2==1)a*a else 0)(_))
</span><span class='line'>var sum=0;for(i&lt;- 1 to 100)if(i%2==1)sum+=i*i
</span><span class='line'>var sum=0;for(i&lt;- 1 to 100; if i%2==1)sum+=i*i
</span><span class='line'>
</span><span class='line'>(1 to 100 by 2).foldLeft(0)(_+((a:Int)=&gt;a*a)(_))
</span><span class='line'>(1 to 100 by 2).map(a=&gt;a*a).foldLeft(0)(_+_)
</span><span class='line'>var sum=0;for(i&lt;- 1 to 100 by 2)sum+=i*i
</span><span class='line'>(1 to 100 by 2).map(a=&gt;a*a).reduce(_+_)</span></code></pre></td></tr></table></div></figure>


<p><code>(1 to 100 by 2).map(a=&gt;a*a).reduce(_+_)</code>是里面最短的应该也是最好的了，既没有定义变量同时意义清晰一看就懂。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala Shell #! 惊叹号井号]]></title>
    <link href="http://winseliu.com/blog/2014/09/03/linux-shell-shebang-tanjinghao/"/>
    <updated>2014-09-03T12:55:32+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/03/linux-shell-shebang-tanjinghao</id>
    <content type="html"><![CDATA[<p>工作中主要是写java代码，shell也只是用于交互性操作，写脚本的次数比较少。对于<code>#!</code><strong>井号叹号</strong>仅仅是教条式的添加在脚本开头，并且基本上都是<code>#!/bin/sh</code>。</p>

<p>今天在看scala官方的<a href="http://www.scala-lang.org/documentation/getting-started.html">入门教程</a>尽然发现<code>!#</code>的写法，很是困惑，Google查询也不知道怎么描述关键字，一般搜索引擎都把这些操作符过滤掉了的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/sh
</span><span class='line'>exec scala "$0" "$@"
</span><span class='line'>!#
</span><span class='line'>object HelloWorld extends App {
</span><span class='line'>  println("Hello, world!")
</span><span class='line'>}
</span><span class='line'>HelloWorld.main(args)</span></code></pre></td></tr></table></div></figure>


<p>首先了解下<code>#!</code>作用：如果<code>#!</code>在脚本的最开始，脚本程序会把第一行的剩余部分当做解析器指令；使用当前的解析器来执行程序，同时把当前脚本的路径作为参数传递给解析器。</p>

<blockquote><p>In computing, a shebang is the character sequence consisting of the characters number sign and exclamation mark (that is, &ldquo;#!&rdquo;) at the beginning of a script.</p>

<p>Under Unix-like operating systems, when a script with a shebang is run as a program, the program loader parses the rest of the script&rsquo;s initial line as an interpreter directive; the specified interpreter program is run instead, passing to it as an argument the path that was initially used when attempting to run the script.</p></blockquote>

<p>如果把<code>!#</code>去掉，再执行上面的脚本则会报错：<strong>error: script file does not close its header with !# or ::!#</strong>，查寻一番后，这原来是Scala的脚本功能的内部处理。通过SourceFile.scala关键字搜索到了<a href="http://www.cnblogs.com/agateriver/archive/2010/09/07/scala_pound_bang.html">该文</a>列出了具体的位置，还有<a href="http://alvinalexander.com/scala/scala-shell-script-example-exec-syntax">A Scala shell script example</a>和我有同样疑问。</p>

<p><img src="http://file.bmob.cn/M00/0B/B1/wKhkA1QGuE-AP-ihAAA1mwvYd5E865.png" alt="" /></p>

<p>可以在《Programing in Scala &ndash; A comprehensive step-by-step guide》一书的附录A中 Scala scripts on Unix and Windows 查找到相应的描述：把<code>#!</code>和<code>!#</code>之间的内容忽略掉了。</p>

<p>语法糖的疑惑解决了，针对上面的脚本还有个问题：exec执行完了，下面的内容不执行了？在exec命令的前面打上调试语句，也只输出了<strong>sh start</strong>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ cat script.scala
</span><span class='line'>#!/bin/sh
</span><span class='line'>echo 'sh start'
</span><span class='line'>exec scala "$0" "$@"
</span><span class='line'>echo 'sh end'
</span><span class='line'>!#
</span><span class='line'>object HelloWorld extends App {
</span><span class='line'>    print("hello world")
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>HelloWorld.main(args)
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ sh script.scala
</span><span class='line'>sh start
</span><span class='line'>hello world
</span></code></pre></td></tr></table></div></figure>


<blockquote><p>exec 使用 exec 方式运行script时, 它和 source 一样, 也是让 script 在当前process内执行, 但是 process 内的原代码剩下部分将被终止. 同样, process 内的环境随script 改变而改变.</p></blockquote>

<p>所以，整个脚本流程就是：执行shell，调用exec来调用scala的解释器执行整个脚本内容，而解释器会过滤掉<code>#!</code>和<code>!#</code>之间内容，执行完后，exec退出脚本，实现scala脚本执行的功能。这样折中的使用方式，应该是为了处理<strong>参数传递</strong>*的问题！</p>

<h2>参考</h2>

<ul>
<li><a href="http://bbs.chinaunix.net/thread-3583927-1-1.html">井号加叹号的作用是什么</a></li>
<li><a href="http://en.wikipedia.org/wiki/Shebang_%28Unix%29">Shebang (Unix)</a></li>
<li><a href="http://bbs.chinaunix.net/thread-218853-1-1.html">shell 十三問? </a></li>
<li><a href="http://www.cnblogs.com/agateriver/archive/2010/09/07/scala_pound_bang.html">Scala 脚本的 pound bang 魔术</a></li>
<li><a href="http://alvinalexander.com/scala/scala-shell-script-example-exec-syntax">A Scala shell script example (and discussion)</a></li>
<li><a href="http://tldp.org/LDP/abs/html/abs-guide.html">Advanced Bash-Scripting Guide An in-depth exploration of the art of shell scripting</a></li>
<li><a href="http://blog.chinaunix.net/uid-27653755-id-4385938.html">linux中fork, source和exec的区别 </a></li>
<li><a href="http://ss64.com/bash/exec.html">exec</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Mapreduce输入输出压缩]]></title>
    <link href="http://winseliu.com/blog/2014/09/01/hadoop2-mapreduce-compress/"/>
    <updated>2014-09-01T16:05:13+08:00</updated>
    <id>http://winseliu.com/blog/2014/09/01/hadoop2-mapreduce-compress</id>
    <content type="html"><![CDATA[<p>当数据达到一定量时，自然就想到了对数据进行压缩来降低存储压力。在Hadoop的任务中提供了5个参数来控制输入输出的数据的压缩格式。添加map输出数据压缩可以降低集群间的网络传输，最终reduce输出压缩可以减低hdfs的集群存储空间。</p>

<p>如果是使用hive等工具的话，效果会更加明显。因为hive的查询结果是临时存储在hdfs中，然后再通过一个<strong>Fetch Operator</strong>来获取数据，最后清理掉，压缩存储临时的数据可以减少磁盘的读写。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;Should the job outputs be compressed?
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.output.fileoutputformat.compress.type&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;RECORD&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;If the job outputs are to compressed as SequenceFiles, how should
</span><span class='line'>               they be compressed? Should be one of NONE, RECORD or BLOCK.
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;If the job outputs are compressed, how should they be compressed?
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;Should the outputs of the maps be compressed before being
</span><span class='line'>               sent across the network. Uses SequenceFile compression.
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;If the map outputs are compressed, how should they be 
</span><span class='line'>               compressed?
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>上面5个属性弄好，在core-sitem.xml加下<code>io.compression.codecs</code>基本就完成配置了。</p>

<p>这里主要探究下mapreduce（下面全部简称MR）过程中自动解压缩。刚刚接触Hadoop一般都不会去了解什么压缩不压缩的，先把hdfs-api，MR-api弄一遭。配置的TextInputFormat竟然能正确的读取tar.gz文件的内容，觉得不可思议，TextInputFormat不是直接读取txt行记录的输入嘛？难道还能读取压缩文件，先解压再&hellip;？？</p>

<p>先说下OutputFormat，在MR中调用context.write写入数据的方法时，最终使用OutputFormat创建的RecordWriter进行持久化。在TextOutputFormat创建RecordWriter时，如果使用压缩会在结果文件名上<strong>加对应压缩库的后缀</strong>，如gzip压缩对应的后缀gz、snappy压缩对应后缀snappy等。对应下面代码的<code>getDefaultWorkFile</code>。</p>

<p><img src="http://file.bmob.cn/M00/0B/03/wKhkA1QEjD2ASHiZAAExjXuQ25Y062.png" alt="" /></p>

<p>同样对应的TextInputFormat的RecordReader也进行类似的处理：根据<strong>文件的后缀</strong>来判定该文件是否使用压缩，并使用对应的输入流InputStream来解码。</p>

<p><img src="http://file.bmob.cn/M00/0B/03/wKhkA1QEjZaAdBeJAAEvRVMKVWY059.png" alt="" /></p>

<p>此处的关键代码为<code>CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);</code>，根据分块（split）的文件名来判断使用的压缩算法。
初始化Codec实现、以及根据文件名来获取压缩算法的实现还是挺有意思的：通过反转字符串然后最近匹配（headMap）来获取对应的结果。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private void addCodec(CompressionCodec codec) {
</span><span class='line'>    String suffix = codec.getDefaultExtension();
</span><span class='line'>    codecs.put(new StringBuilder(suffix).reverse().toString(), codec);
</span><span class='line'>    codecsByClassName.put(codec.getClass().getCanonicalName(), codec);
</span><span class='line'>
</span><span class='line'>    String codecName = codec.getClass().getSimpleName();
</span><span class='line'>    codecsByName.put(codecName.toLowerCase(), codec);
</span><span class='line'>    if (codecName.endsWith("Codec")) {
</span><span class='line'>      codecName = codecName.substring(0, codecName.length() - "Codec".length());
</span><span class='line'>      codecsByName.put(codecName.toLowerCase(), codec);
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  public CompressionCodec getCodec(Path file) {
</span><span class='line'>    CompressionCodec result = null;
</span><span class='line'>    if (codecs != null) {
</span><span class='line'>      String filename = file.getName();
</span><span class='line'>      String reversedFilename = new StringBuilder(filename).reverse().toString();
</span><span class='line'>      SortedMap&lt;String, CompressionCodec&gt; subMap = 
</span><span class='line'>        codecs.headMap(reversedFilename);
</span><span class='line'>      if (!subMap.isEmpty()) {
</span><span class='line'>        String potentialSuffix = subMap.lastKey();
</span><span class='line'>        if (reversedFilename.startsWith(potentialSuffix)) {
</span><span class='line'>          result = codecs.get(potentialSuffix);
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>    return result;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>了解了这些，MR（TextInputFormat）的输入文件可以比较随意些：各种压缩文件、原始文件都可以，只要文件有对应压缩算法的后缀即可。hive的解压缩功能也很容易了，如果使用hive存储text形式的文件，进行压缩无需进行额外的程序代码修改，仅仅修改MR的配置即可，注意下<strong>文件后缀</strong>！！</p>

<p>如，在MR中生成了snappy压缩的文件，此时<strong>不能</strong>在文件的后面添加东西。否则在hive查询时，根据<strong>后缀</strong>进行解压会导致结果乱码/不正确。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;hive.exec.compress.output&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt; This controls whether the final outputs of a query (to a local/hdfs file or a hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;hive.exec.compress.intermediate&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt; This controls whether intermediate files produced by hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span></code></pre></td></tr></table></div></figure>


<p>hive也弄了两个参数来控制它自己的MR的输出输入压缩控制属性。其他的配置使用mapred-site.xml的配置即可。</p>

<p><img src="http://file.bmob.cn/M00/0B/1D/wKhkA1QFQLmAfyZSAAIOx4UEIbY016.png" alt="" /></p>

<p>网上一些资料有<code>hive.intermediate.compression.codec</code>和<code>hive.intermediate.compression.type</code>两个参数能调整中间过程的压缩算法。其实和mapreduce的参数功能是一样的。</p>

<p><img src="http://file.bmob.cn/M00/0B/1F/wKhkA1QFQWyAUDMLAAGyNqR_X-c417.png" alt="" /></p>

<p>附上解压缩的全部配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$#core-site.xml
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;io.compression.codecs&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;
</span><span class='line'>  org.apache.hadoop.io.compress.GzipCodec,
</span><span class='line'>  org.apache.hadoop.io.compress.DefaultCodec,
</span><span class='line'>  org.apache.hadoop.io.compress.BZip2Codec,
</span><span class='line'>  org.apache.hadoop.io.compress.SnappyCodec
</span><span class='line'>      &lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>$#mapred-site.xml
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; 
</span><span class='line'>      &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapred.output.compression.codec&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>$#hive-site.xml
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;hive.exec.compress.output&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>运行hive后，临时存储在HDFS的结果数据，注意文件的后缀。</p>

<p><img src="http://file.bmob.cn/M00/0B/20/wKhkA1QFRjSACnLfAABVdoK0f1c803.png" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://www.geek521.com/?p=4814">深入学习《Programing Hive》：数据压缩</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
