<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Winse Blog]]></title>
  <link href="http://winse.github.io/atom.xml" rel="self"/>
  <link href="http://winse.github.io/"/>
  <updated>2014-09-04T14:42:55+08:00</updated>
  <id>http://winse.github.io/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[计算出从1到100之间所有奇数的平方之和]]></title>
    <link href="http://winse.github.io/blog/2014/09/04/scala-quadratic-sum-of-odd-num-in-100/"/>
    <updated>2014-09-04T14:15:40+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/04/scala-quadratic-sum-of-odd-num-in-100</id>
    <content type="html"><![CDATA[<p><a href="http://freewind.github.io/posts/scala-group-entry-problem/">计算出从1到100之间所有奇数的平方之和，代码50字符内（QQ群的验证框长度限制为50）</a>。</p>

<p>如题，题目没啥难度，这50字符的条件莫名的增添压迫感。其实java写也不用50个字符就能搞定的 ！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// (1 to 50) foreach {x =&gt; print("0")}
</span><span class='line'>00000000000000000000000000000000000000000000000000
</span><span class='line'>
</span><span class='line'>// java
</span><span class='line'>int sum=0;for(int i=0;i&lt;100;i+=2)sum+=i*i;
</span><span class='line'>
</span><span class='line'>// scala
</span><span class='line'>(1 to 100).map(a=&gt;if(a%2==1)a*a else 0).foldLeft(0)(_+_)
</span><span class='line'>(0 to 100).foldLeft(0)(_+((a:Int)=&gt;if(a%2==1)a*a else 0)(_))
</span><span class='line'>var sum=0;for(i&lt;- 1 to 100)if(i%2==1)sum+=i*i
</span><span class='line'>var sum=0;for(i&lt;- 1 to 100; if i%2==1)sum+=i*i
</span><span class='line'>
</span><span class='line'>(1 to 100 by 2).foldLeft(0)(_+((a:Int)=&gt;a*a)(_))
</span><span class='line'>(1 to 100 by 2).map(a=&gt;a*a).foldLeft(0)(_+_)
</span><span class='line'>var sum=0;for(i&lt;- 1 to 100 by 2)sum+=i*i
</span><span class='line'>(1 to 100 by 2).map(a=&gt;a*a).reduce(_+_)</span></code></pre></td></tr></table></div></figure>


<p><code>(1 to 100 by 2).map(a=&gt;a*a).reduce(_+_)</code>是里面最短的应该也是最好的了，既没有定义变量同时意义清晰一看就懂。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala Shell #! 惊叹号井号]]></title>
    <link href="http://winse.github.io/blog/2014/09/03/linux-shell-shebang-tanjinghao/"/>
    <updated>2014-09-03T12:55:32+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/03/linux-shell-shebang-tanjinghao</id>
    <content type="html"><![CDATA[<p>工作中主要是写java代码，shell也只是用于交互性操作，写脚本的次数比较少。对于<code>#!</code><strong>井号叹号</strong>仅仅是教条式的添加在脚本开头，并且基本上都是<code>#!/bin/sh</code>。</p>

<p>今天在看scala官方的<a href="http://www.scala-lang.org/documentation/getting-started.html">入门教程</a>尽然发现<code>!#</code>的写法，很是困惑，Google查询也不知道怎么描述关键字，一般搜索引擎都把这些操作符过滤掉了的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/sh
</span><span class='line'>exec scala "$0" "$@"
</span><span class='line'>!#
</span><span class='line'>object HelloWorld extends App {
</span><span class='line'>  println("Hello, world!")
</span><span class='line'>}
</span><span class='line'>HelloWorld.main(args)</span></code></pre></td></tr></table></div></figure>


<p>首先了解下<code>#!</code>作用：如果<code>#!</code>在脚本的最开始，脚本程序会把第一行的剩余部分当做解析器指令；使用当前的解析器来执行程序，同时把当前脚本的路径作为参数传递给解析器。</p>

<blockquote><p>In computing, a shebang is the character sequence consisting of the characters number sign and exclamation mark (that is, &ldquo;#!&rdquo;) at the beginning of a script.</p>

<p>Under Unix-like operating systems, when a script with a shebang is run as a program, the program loader parses the rest of the script&rsquo;s initial line as an interpreter directive; the specified interpreter program is run instead, passing to it as an argument the path that was initially used when attempting to run the script.</p></blockquote>

<p>如果把<code>!#</code>去掉，再执行上面的脚本则会报错：<strong>error: script file does not close its header with !# or ::!#</strong>，查寻一番后，这原来是Scala的脚本功能的内部处理。通过SourceFile.scala关键字搜索到了<a href="http://www.cnblogs.com/agateriver/archive/2010/09/07/scala_pound_bang.html">该文</a>列出了具体的位置，还有<a href="http://alvinalexander.com/scala/scala-shell-script-example-exec-syntax">A Scala shell script example</a>和我有同样疑问。</p>

<p><img src="http://file.bmob.cn/M00/0B/B1/wKhkA1QGuE-AP-ihAAA1mwvYd5E865.png" alt="" /></p>

<p>可以在《Programing in Scala &ndash; A comprehensive step-by-step guide》一书的附录A中 Scala scripts on Unix and Windows 查找到相应的描述：把<code>#!</code>和<code>!#</code>之间的内容忽略掉了。</p>

<p>语法糖的疑惑解决了，针对上面的脚本还有个问题：exec执行完了，下面的内容不执行了？在exec命令的前面打上调试语句，也只输出了<strong>sh start</strong>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ cat script.scala
</span><span class='line'>#!/bin/sh
</span><span class='line'>echo 'sh start'
</span><span class='line'>exec scala "$0" "$@"
</span><span class='line'>echo 'sh end'
</span><span class='line'>!#
</span><span class='line'>object HelloWorld extends App {
</span><span class='line'>    print("hello world")
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>HelloWorld.main(args)
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ sh script.scala
</span><span class='line'>sh start
</span><span class='line'>hello world
</span></code></pre></td></tr></table></div></figure>


<blockquote><p>exec 使用 exec 方式运行script时, 它和 source 一样, 也是让 script 在当前process内执行, 但是 process 内的原代码剩下部分将被终止. 同样, process 内的环境随script 改变而改变.</p></blockquote>

<p>所以，整个脚本流程就是：执行shell，调用exec来调用scala的解释器执行整个脚本内容，而解释器会过滤掉<code>#!</code>和<code>!#</code>之间内容，执行完后，exec退出脚本，实现scala脚本执行的功能。这样折中的使用方式，应该是为了处理<strong>参数传递</strong>*的问题！</p>

<h2>参考</h2>

<ul>
<li><a href="http://bbs.chinaunix.net/thread-3583927-1-1.html">井号加叹号的作用是什么</a></li>
<li><a href="http://en.wikipedia.org/wiki/Shebang_%28Unix%29">Shebang (Unix)</a></li>
<li><a href="http://bbs.chinaunix.net/thread-218853-1-1.html">shell 十三問? </a></li>
<li><a href="http://www.cnblogs.com/agateriver/archive/2010/09/07/scala_pound_bang.html">Scala 脚本的 pound bang 魔术</a></li>
<li><a href="http://alvinalexander.com/scala/scala-shell-script-example-exec-syntax">A Scala shell script example (and discussion)</a></li>
<li><a href="http://tldp.org/LDP/abs/html/abs-guide.html">Advanced Bash-Scripting Guide An in-depth exploration of the art of shell scripting</a></li>
<li><a href="http://blog.chinaunix.net/uid-27653755-id-4385938.html">linux中fork, source和exec的区别 </a></li>
<li><a href="http://ss64.com/bash/exec.html">exec</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Mapreduce输入输出压缩]]></title>
    <link href="http://winse.github.io/blog/2014/09/01/hadoop2-mapreduce-compress/"/>
    <updated>2014-09-01T16:05:13+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/01/hadoop2-mapreduce-compress</id>
    <content type="html"><![CDATA[<p>当数据达到一定量时，自然就想到了对数据进行压缩来降低存储压力。在Hadoop的任务中提供了5个参数来控制输入输出的数据的压缩格式。添加map输出数据压缩可以降低集群间的网络传输，最终reduce输出压缩可以减低hdfs的集群存储空间。</p>

<p>如果是使用hive等工具的话，效果会更加明显。因为hive的查询结果是临时存储在hdfs中，然后再通过一个<strong>Fetch Operator</strong>来获取数据，最后清理掉，压缩存储临时的数据可以减少磁盘的读写。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;Should the job outputs be compressed?
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.output.fileoutputformat.compress.type&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;RECORD&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;If the job outputs are to compressed as SequenceFiles, how should
</span><span class='line'>               they be compressed? Should be one of NONE, RECORD or BLOCK.
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;If the job outputs are compressed, how should they be compressed?
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;Should the outputs of the maps be compressed before being
</span><span class='line'>               sent across the network. Uses SequenceFile compression.
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;If the map outputs are compressed, how should they be 
</span><span class='line'>               compressed?
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>上面5个属性弄好，在core-sitem.xml加下<code>io.compression.codecs</code>基本就完成配置了。</p>

<p>这里主要探究下mapreduce（下面全部简称MR）过程中自动解压缩。刚刚接触Hadoop一般都不会去了解什么压缩不压缩的，先把hdfs-api，MR-api弄一遭。配置的TextInputFormat竟然能正确的读取tar.gz文件的内容，觉得不可思议，TextInputFormat不是直接读取txt行记录的输入嘛？难道还能读取压缩文件，先解压再&hellip;？？</p>

<p>先说下OutputFormat，在MR中调用context.write写入数据的方法时，最终使用OutputFormat创建的RecordWriter进行持久化。在TextOutputFormat创建RecordWriter时，如果使用压缩会在结果文件名上<strong>加对应压缩库的后缀</strong>，如gzip压缩对应的后缀gz、snappy压缩对应后缀snappy等。对应下面代码的<code>getDefaultWorkFile</code>。</p>

<p><img src="http://file.bmob.cn/M00/0B/03/wKhkA1QEjD2ASHiZAAExjXuQ25Y062.png" alt="" /></p>

<p>同样对应的TextInputFormat的RecordReader也进行类似的处理：根据<strong>文件的后缀</strong>来判定该文件是否使用压缩，并使用对应的输入流InputStream来解码。</p>

<p><img src="http://file.bmob.cn/M00/0B/03/wKhkA1QEjZaAdBeJAAEvRVMKVWY059.png" alt="" /></p>

<p>此处的关键代码为<code>CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);</code>，根据分块（split）的文件名来判断使用的压缩算法。
初始化Codec实现、以及根据文件名来获取压缩算法的实现还是挺有意思的：通过反转字符串然后最近匹配（headMap）来获取对应的结果。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private void addCodec(CompressionCodec codec) {
</span><span class='line'>    String suffix = codec.getDefaultExtension();
</span><span class='line'>    codecs.put(new StringBuilder(suffix).reverse().toString(), codec);
</span><span class='line'>    codecsByClassName.put(codec.getClass().getCanonicalName(), codec);
</span><span class='line'>
</span><span class='line'>    String codecName = codec.getClass().getSimpleName();
</span><span class='line'>    codecsByName.put(codecName.toLowerCase(), codec);
</span><span class='line'>    if (codecName.endsWith("Codec")) {
</span><span class='line'>      codecName = codecName.substring(0, codecName.length() - "Codec".length());
</span><span class='line'>      codecsByName.put(codecName.toLowerCase(), codec);
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  public CompressionCodec getCodec(Path file) {
</span><span class='line'>    CompressionCodec result = null;
</span><span class='line'>    if (codecs != null) {
</span><span class='line'>      String filename = file.getName();
</span><span class='line'>      String reversedFilename = new StringBuilder(filename).reverse().toString();
</span><span class='line'>      SortedMap&lt;String, CompressionCodec&gt; subMap = 
</span><span class='line'>        codecs.headMap(reversedFilename);
</span><span class='line'>      if (!subMap.isEmpty()) {
</span><span class='line'>        String potentialSuffix = subMap.lastKey();
</span><span class='line'>        if (reversedFilename.startsWith(potentialSuffix)) {
</span><span class='line'>          result = codecs.get(potentialSuffix);
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>    return result;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>了解了这些，MR（TextInputFormat）的输入文件可以比较随意些：各种压缩文件、原始文件都可以，只要文件有对应压缩算法的后缀即可。hive的解压缩功能也很容易了，如果使用hive存储text形式的文件，进行压缩无需进行额外的程序代码修改，仅仅修改MR的配置即可，注意下<strong>文件后缀</strong>！！</p>

<p>如，在MR中生成了snappy压缩的文件，此时<strong>不能</strong>在文件的后面添加东西。否则在hive查询时，根据<strong>后缀</strong>进行解压会导致结果乱码/不正确。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;hive.exec.compress.output&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt; This controls whether the final outputs of a query (to a local/hdfs file or a hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;hive.exec.compress.intermediate&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt; This controls whether intermediate files produced by hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span></code></pre></td></tr></table></div></figure>


<p>hive也弄了两个参数来控制它自己的MR的输出输入压缩控制属性。其他的配置使用mapred-site.xml的配置即可。</p>

<p><img src="http://file.bmob.cn/M00/0B/1D/wKhkA1QFQLmAfyZSAAIOx4UEIbY016.png" alt="" /></p>

<p>网上一些资料有<code>hive.intermediate.compression.codec</code>和<code>hive.intermediate.compression.type</code>两个参数能调整中间过程的压缩算法。其实和mapreduce的参数功能是一样的。</p>

<p><img src="http://file.bmob.cn/M00/0B/1F/wKhkA1QFQWyAUDMLAAGyNqR_X-c417.png" alt="" /></p>

<p>附上解压缩的全部配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$#core-site.xml
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;io.compression.codecs&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;
</span><span class='line'>  org.apache.hadoop.io.compress.GzipCodec,
</span><span class='line'>  org.apache.hadoop.io.compress.DefaultCodec,
</span><span class='line'>  org.apache.hadoop.io.compress.BZip2Codec,
</span><span class='line'>  org.apache.hadoop.io.compress.SnappyCodec
</span><span class='line'>      &lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>$#mapred-site.xml
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; 
</span><span class='line'>      &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapred.output.compression.codec&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>$#hive-site.xml
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;hive.exec.compress.output&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>运行hive后，临时存储在HDFS的结果数据，注意文件的后缀。</p>

<p><img src="http://file.bmob.cn/M00/0B/20/wKhkA1QFRjSACnLfAABVdoK0f1c803.png" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://www.geek521.com/?p=4814">深入学习《Programing Hive》：数据压缩</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用ADT调试Xamarin程序中的Java库]]></title>
    <link href="http://winse.github.io/blog/2014/08/29/xamarin-application-use-adt-eclipse-debug-java-code/"/>
    <updated>2014-08-29T12:08:11+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/29/xamarin-application-use-adt-eclipse-debug-java-code</id>
    <content type="html"><![CDATA[<p>在编写SDK的时刻，有用户需要使用Xamarin来开发应用。我们这边暂时没有这个方面的经验，有点瞎扯扯意味，路是崎岖的前进是痛苦的。</p>

<h2>封装Android-SDK</h2>

<p>Xamarin是使用C#语言来编写代码的，所以需要先把Android的jar库包装成为C#的代码。<a href="http://developer.xamarin.com/guides/android/advanced_topics/java_integration_overview/">可选方式有3种</a>)，这里选用Wrapper的形式，不过多讲解，看文章<a href="http://developer.xamarin.com/guides/android/advanced_topics/java_integration_overview/binding_a_java_library_(.jar">Binding a Java Library - Consuming .JARs from C#</a>/)。</p>

<p>建立Binding项目，把依赖的包加入到Jars目录下。由于Bmob-Android官方的包是进行混淆的，有些代码不会用到的/没有必要Wrapper生成jni代码调用的，可以通过removenote去掉不生成C#的wrapper类。第二点就是java的泛型是会被抹掉的，而C#的是会编入程序中的，遇到Comparable这种类型的方法时，需要进行参数强制转换下。第三点就是接口回调，有多个方法时会导致名称冲突，需要为每个接口的方法都配置一个Args的节点属性。这些都是官网的例子中有说明，有需要可以具体参考上面链接的文章内容。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;metadata&gt;
</span><span class='line'>  &lt;remove-node path="/api/package[@name='cn.bmob.im.db']" /&gt;
</span><span class='line'>
</span><span class='line'>  &lt;attr path="/api/package[@name='cn.bmob.im.inteface']/interface[@name='DownloadListener']/method[@name='onError']" name="argsType"&gt;DownloadListenerErrorEventArgs&lt;/attr&gt;
</span><span class='line'>&lt;/metadata&gt;
</span><span class='line'>
</span><span class='line'>&lt;enum-method-mappings&gt;
</span><span class='line'>  &lt;mapping jni-class="cn/bmob/im/bean/BmobRecent"&gt;
</span><span class='line'>    &lt;method jni-name="compareTo" parameter="p0" clr-enum-type="Java.Lang.Object" /&gt;
</span><span class='line'>  &lt;/mapping&gt;
</span><span class='line'>  &lt;mapping jni-class="cn/bmob/im/BmobDownloadManager"&gt;
</span><span class='line'>    &lt;method jni-name="doInBackground" parameter="p0" clr-enum-type="Java.Lang.Object[]" /&gt;
</span><span class='line'>  &lt;/mapping&gt;
</span><span class='line'>&lt;/enum-method-mappings&gt;</span></code></pre></td></tr></table></div></figure>


<p>还有另一个坑是，混淆后内部类会被扁平化，导致jar2xml执行时获取类的getSimpleName名称会抛异常，我这里直接反编译源码改成getName就好了，仅仅是代码中全路径和仅类名的却别，暂时来看没啥印象。</p>

<p>然后编译，加入到主项目的依赖中就可以使用该库的Java功能了。名称可能并不能全部对应上，与Java中的方法名和常量名大小写、下划线的不同罢了。</p>

<h2>调试</h2>

<p>下面是重点，但是很简短。</p>

<p>作为写SDK的，肯定不仅仅要用特定的工具，还的把中间的过程也扭顺，即既要做一个好点（example），又得实现连接的线（SDK）。</p>

<p>如果Android SDK的代码没有执行，该怎么办？Xamarin中都是C#的代码并不能用于调试java啊！问题自然归结到怎么用两个工具（Xamarin和Eclipse）来同时调试一个Xamarin Android应用的问题？！</p>

<p>先讲讲我遇到的坑，由于是开发者发给我的应用，不知道结构是怎么样的。我直接用Xamarin打开，是没有带可执行属性的，在Run-With菜单中是能看到我的实体机器的，但是就是不能把程序发布上去！提示【执行失败。未将对象引用设置到对象的实例。】然后就没了。最终在stackoverflow中找到了类型问题的解决方法，需要设置运行属性。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QAAt6AVtg6AALZEEiP4tQ304.png" alt="" /></p>

<p>配置如下，在解决方案属性中【构建-配置-ConfigurationMappings】把项目添加为构建项。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QAA4uAT9qmAAKI8-zKAn8494.png" alt="" /></p>

<p>可能还会遇到的问题是版本的问题，报错【java.lang.RuntimeException: Unable to get provider mono.MonoRuntimeProvider: java.lang.RuntimeException: Unable to find application Mono.Android.DebugRuntime or Mono.Android.Platform.ApiLevel_19!】需要在csproj的配置中修改AndroidUseLatestPlatformSdk属性为false。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QAC4KAOV-HAAFU6knDvFQ527.png" alt="" /></p>

<p>下面的步骤才是本文的关键：</p>

<p>首先，在MainActivity的onCreate方法开始出打个断点，便于初始化功能调试，点击左上角的开始运行按钮。这样就能把代码发布到机器，且运行后会停留在onCreate处。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QACtCAHOqDAAELanVrybU938.png" alt="" /></p>

<p>Xamarin调试效果图</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QAC_yAYqbpAAHBQT48VSA253.png" alt="" /></p>

<p>再，打开ecilpse导入<code>obj\Debug\android</code>目录下的项目【Import-Android-Existing Android Code Into Workspace】，错误什么的无所谓。这个项目只是用了ADT能识别而已。然后再java包的代码里面打上断点。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QADTuAd3cyAACtJUn8Pdk976.png" alt="" /></p>

<p>最后，起到定乾坤作用的就是DDMS的Devices试图的小爬虫，选择你要调试的程序，然后点击它就可以了。切换到Xamarin继续运行程序，接下来就会运行停留到eclipse中的java包中的断点程序出。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QADnWAJQyIAABt1LtWjUA456.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QADueALec-AAEQhd4vxMY835.png" alt="" /></p>

<p>OK，接下来就按照eclipse的调试技巧弄就好了。</p>

<p>步骤很简单，查询的路子却是艰辛的。第一次尝试成本总是昂贵的，第二步自然会慢慢顺起来。fighting&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[查找逐步定位Java程序OOM的异常实践]]></title>
    <link href="http://winse.github.io/blog/2014/08/25/step-by-step-found-java-oom-error/"/>
    <updated>2014-08-25T21:12:13+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/25/step-by-step-found-java-oom-error</id>
    <content type="html"><![CDATA[<p>类C语言，继C++之后的最辉煌耀眼的明星都属Java，其中最突出的又数内存管理。JVM对运行在其上的程序进行内存自动化分配和管理，减少开发人员的工作量之外便于统一的维护和管理。JDK提供了各种各样的工具来让开发实施人员了解运行的运行状态。</p>

<ul>
<li>jps -l -v -m</li>
<li>jstat -gcutil 2000 100</li>
<li>jmap</li>
<li>jinfo <a href="http://file.bmob.cn/M00/03/AD/wKhkA1PE2MGAB4-fAAGTqUeu-cE940.png">查看参数例子</a></li>
<li>jstack</li>
<li>jvisualvm/jconsole</li>
<li>mat(MemoryAnalyzer)</li>
<li>btrace</li>
<li>jclasslib（查看局部变量表）</li>
</ul>


<p>前段时间，接手(前面已经有成型的东西)使用Hadoop存储转换的项目，但是生产环境的程序总是隔三差五的OOM，同时使用的hive0.12.0也偶尔出现内存异常。这对于运维来说就是灭顶之灾！搞不好什么时刻程序就挂了！！必须咬咬牙把这个问题处理解决，开始把老古董们请出来，翻来基本不看的后半部分&ndash;Java内存管理。</p>

<ul>
<li>《Java程序性能优化-让你的Java程序更快、更稳定》第5章JVM调优/第6章Java性能调优工具</li>
<li>《深入理解Java虚拟机-JVM高级特性与最佳实践》第二部分自动内存管理机制</li>
</ul>


<p>这里用到的理论知识比较少。主要用Java自带的工具，加上内存堆分析工具（mat/jvisualvm）找出大对象，同时结合源代码定位问题。</p>

<p>下面主要讲讲实践，查找问题的思路。在本地进行测试的话，我们可以打断点，可以通过jvisualvm来查看整个运行过程内存的变化趋势图。但是到了linux服务器，并且还是生产环境的话，想要有本地一样的图形化工具来监控是比较困难的！一般服务器的内存都设置的比较大，而windows设置的内存又有限！所以内存达到1G左右，立马dump一个堆的内存快照然后下载到本地进行来分析（可以通过<code>-J-Xmx</code><a href="http://file.bmob.cn/M00/09/83/wKhkA1P7TV-ABDnOAAB-OnVBQic050.png">调整jvisualvm的内存</a>）。</p>

<ul>
<li>首先，由于报错是在Configuration加载配置文件时抛出OOM，第一反应肯定Configuraiton对象太多导致！同时查看dump的堆内存也佐证了这一点。直接把程序中的Configuration改成单例。</li>
</ul>


<p>程序对象内存占比排行（<code>jmap -histo PID</code>）：</p>

<p><img src="http://file.bmob.cn/M00/09/81/wKhkA1P7S8yARYSkAAiFW9cVN5w526.png" alt="" /></p>

<p>使用mat或者jvisualvm查看堆，确实Configuration对象过多（<code>jmap -dump:format=b,file=/tmp/bug.hprof PID</code>）：</p>

<p><img src="http://file.bmob.cn/M00/09/83/wKhkA1P7TbmAIdDEAAq3ktPBs6Q266.png" alt="" /></p>

<ul>
<li><p>修改后再次运行，但是没多大用！还是OOM！！</p></li>
<li><p>进一步分析，发现在Configuration中的属性/缓冲的都是弱引用是weakhashmap。</p></li>
</ul>


<p><img src="http://file.bmob.cn/M00/09/83/wKhkA1P7TfaAf4nwAAbcdgFiyXs804.png" alt="" /></p>

<p>OOM最终问题不在Configuration对象中的属性，哪谁hold住了Configuration对象呢？？</p>

<ul>
<li>再次从根源开始查找问题。程序中FileSystem对象使用<code>FileSystem.get(URI, Configuration, String)</code>获取，然后调用<code>get(URI,Configuration)</code>方法，其中的<strong>CACHE</strong>很是刺眼啊！</li>
</ul>


<p><img src="http://file.bmob.cn/M00/09/8D/wKhkA1P72pmAAMdnAAEYMjHFUAI853.png" alt="" /></p>

<p>缓冲FileSystem的Cache对象的Key是URI和UserGroupInformation两个属性来判断是否相等的。对于一个程序来说一般就读取一个HDFS的数据即URI前部分是确定的，重点在UserGroupInformation是通过<code>UserGroupInformation.getCurrentUser()</code>来获取的。</p>

<p>即获取在get时<code>UserGroupInformation.getBestUGI</code>得到的对象。而这个对象在UnSecure情况下每次都是调用<code>createRemoteUser</code>创建新的对象！也就是每调用一次<code>FileSystem.get(URI, Configuration, String)</code>就会缓存一个FileSystem对象，以及其hold住的Configuration都会被保留在内存中。
<img src="http://file.bmob.cn/M00/09/82/wKhkA1P7TBSAaYEoAAhzUA5j5MI991.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/09/82/wKhkA1P7TJ2AfwJzAAhEVFjK7Ek367.png" alt="" /></p>

<p>只消耗不释放终究会坐吃山空啊！到最后也就必然OOM了。从mat的UserGroupInformation的个数查询，以及Cache对象的总量可以印证。</p>

<p><img src="http://file.bmob.cn/M00/09/82/wKhkA1P7TNeAB7JAAAdMg-udeR8285.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/09/83/wKhkA1P7TU-ACoaCAApK4n-52hI027.png" alt="" /></p>

<h2>问题处理</h2>

<p>把程序涉及到FileSystem.get调用去掉user参数，使两个参数的方法。由于都使用getCurrentUser获取对象，也就是说程序整个运行过程中就一个FileSystem对象，但是与此同时就不能关闭获取到的FileSystem，如果当前运行的用户与集群所属用户不同，需要设置环境变量指定当前操作的用户！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>System.setProperty("HADOOP_USER_NAME", "hadoop");</span></code></pre></td></tr></table></div></figure>


<p>查找代码中调用了FileSystem#close是一个痛苦的过程，由于FileSystem实现的是Closeable的close方法，用<strong>Open Call Hierarchy</strong>基本是大海捞中啊，根本不知道那个代码是自己的！！这里用btrace神器让咋也高大上一把。。。</p>

<p>当时操作的步骤找不到了，下图是调用Cache#getInternal方法监控代码<a href="https://gist.github.com/winse/161f6fe9120f2ec6b024">GIST</a>：</p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7UD2AFk2cAAXRQWzniL0296.png" alt="" /></p>

<h2>hive0.12内存溢出问题</h2>

<p>hive0.12.0查询程序MR内容溢出</p>

<p><img src="http://file.bmob.cn/M00/09/81/wKhkA1P7StSAOgX1AAoW9v-Fd4s439.png" alt="" /></p>

<p>在hive-0.13前官网文档中有提到内存溢出这一点，可以对应到FileSystem中代码的判断。</p>

<p><img src="http://file.bmob.cn/M00/09/85/wKhkA1P7UP-ACRVdAAJHBKNTq94580.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>String disableCacheName = String.format("fs.%s.impl.disable.cache", scheme);
</span><span class='line'>if (conf.getBoolean(disableCacheName, false)) {
</span><span class='line'>  return createFileSystem(uri, conf);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>hive0.13.1处理</p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7T_CAcxVqAARr7CGiDvY177.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7T6KAKoUiAAvODPwh1po815.png" alt="" /></p>

<p>新版本在每次查询（session）结束后都会把本次涉及到的FileSystem关闭掉。</p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7T9uAQQB3AAWrj_efwZU495.png" alt="" /></p>

<h2>理论知识</h2>

<p>从GC类型开始讲，对自动化内存的垃圾收集有个整体的感知： 新生代/s0（survivor space0、from space）/s1（survivor space1、to space）/永久代。虚拟机参数<code>-Xmx</code>,<code>-Xms</code>,<code>-Xmn</code>（<code>-Xss</code>）来调节各个代的大小和比例。</p>

<ul>
<li><code>-Xss</code> 参数来设置栈的大小。栈的大小直接决定了函数的调用可达深度</li>
<li><code>-XX:PrintGCDetails -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=15 -Xms40m -Xmx40m -Xmn20m</code></li>
<li><code>-XX:NewSize</code>和<code>-XX:MaxNewSize</code></li>
<li><code>-XX:NewRatio</code>和<code>-XX:SurvivorRatio</code></li>
<li><code>-XX:PermSize=2m -XX:MaxPermSize=4m -XX:+PrintGCDetails</code></li>
<li><code>-verbose:gc</code></li>
<li><code>-XX:+PrintGC</code></li>
<li><code>-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/bug.hprof -XX:OnOutOfMemoryError=/reset.sh</code></li>
<li><code>jmap -dump:format=b,file=/tmp/bug.hprof PID</code></li>
<li><code>jmap -histo PID &gt; /tmp/s.txt</code></li>
<li><code>jstack -l PID</code></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[巧用Equals和Hashcode]]></title>
    <link href="http://winse.github.io/blog/2014/08/20/magical-use-java-equals-hashcode/"/>
    <updated>2014-08-20T11:03:54+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/20/magical-use-java-equals-hashcode</id>
    <content type="html"><![CDATA[<p>java中让人疑惑的一点就是等于的判断，有使用<code>==</code>和<code>equals</code>， 有一些专门字符串初始化的资料来考你在是否已经真正的掌握判断两个对象是否一致。</p>

<p>同时，在重写equals时很多资料都强调要重写hashcode。</p>

<p>java中HashMap就是基于equals和hashcode来实现拉链的键值对。Map中存储了entry&lt;K,V>的数组，数组的下标是基于对象的hashcode再对entry长度[并]<code>&amp;</code>的结果。</p>

<p><img src="http://file.bmob.cn/M00/08/55/wKhkA1P0OO-AE9u3AABNzQK0pr8747.png" alt="" /></p>

<p>使用set/map来实现集合，并且对象重写了equals但没有重写hashcode的情况下，得到的结果与你臆想的不同。同时，在特定场景结合hashcode和equals可以实现很酷的效果。</p>

<ul>
<li>第一个例子A 重写了equals但是没有重写hashcode(ERROR)，此时判断元素是否在集合中结果可能并不是你想要的。</li>
<li>第二个B的例子 重写hashcode和equals对应后就正确了。</li>
<li>第三个AA是个很酷的例子 equals的条件更强，可以实现类似<code>map&lt;string, list&lt;string&gt;&gt;</code>的效果。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>static class A {
</span><span class='line'>
</span><span class='line'>  String name;
</span><span class='line'>  int age;
</span><span class='line'>
</span><span class='line'>  public A(String name, int age) {
</span><span class='line'>      this.name = name;
</span><span class='line'>      this.age = age;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public boolean equals(Object obj) {
</span><span class='line'>      return new EqualsBuilder().append(getClass(), obj.getClass()).append(name, ((A) obj).name).isEquals();
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testA() {
</span><span class='line'>  Set&lt;A&gt; set = new HashSet&lt;&gt;();
</span><span class='line'>
</span><span class='line'>  set.add(new A("abc", 12));
</span><span class='line'>  set.add(new A("abc", 14));
</span><span class='line'>
</span><span class='line'>  System.out.println(set.size());
</span><span class='line'>
</span><span class='line'>  System.out.println(set.contains(new A("abc", 0)));
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>static class B extends A {
</span><span class='line'>
</span><span class='line'>  public B(String name, int age) {
</span><span class='line'>      super(name, age);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public int hashCode() {
</span><span class='line'>      return this.name.hashCode();
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testB() {
</span><span class='line'>  /* Set&lt;A&gt; */Set&lt;B&gt; set = new HashSet&lt;&gt;();
</span><span class='line'>
</span><span class='line'>  set.add(new B("abc", 12));
</span><span class='line'>  set.add(new B("abc", 14));
</span><span class='line'>
</span><span class='line'>  System.out.println(set.size());
</span><span class='line'>
</span><span class='line'>  System.out.println(set.contains(new B("abc", 0)));
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>static class AA extends A {
</span><span class='line'>  public AA(String name, int age) {
</span><span class='line'>      super(name, age);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public boolean equals(Object obj) {
</span><span class='line'>      return new EqualsBuilder().append(getClass(), obj.getClass()).append(name, ((A) obj).name)
</span><span class='line'>              .append(age, ((A) obj).age).isEquals();
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public int hashCode() {
</span><span class='line'>      return this.name.hashCode();
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testAA() {
</span><span class='line'>  // 实现Map&lt;String, Set&lt;String&gt;&gt;的效果
</span><span class='line'>  Set&lt;AA&gt; set = new HashSet&lt;&gt;();
</span><span class='line'>
</span><span class='line'>  set.add(new AA("abc", 12));
</span><span class='line'>  set.add(new AA("abc", 14));
</span><span class='line'>
</span><span class='line'>  System.out.println(set.size());
</span><span class='line'>
</span><span class='line'>  System.out.println(set.contains(new AA("abc", 0)));
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://java.chinaitlab.com/base/879319.html">java中HashMap详解</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop的datanode数据节点软/硬件配置应该一致]]></title>
    <link href="http://winse.github.io/blog/2014/08/02/hadoop-datanode-config-should-equals/"/>
    <updated>2014-08-02T22:21:12+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/02/hadoop-datanode-config-should-equals</id>
    <content type="html"><![CDATA[<p>最好的就是集群的所有的datanode的节点的<strong>硬件配置一样</strong>！当然系统时间也的一致，hosts等等这些。机器配置一样时可以使用脚本进行批量处理，给维护带来很大的便利性。</p>

<p>今天收到运维的信息，说集群的一台机器硬盘爆了！上到环境查看<code>df -h</code>发现硬盘配置和其他datanode的不同！但是hadoop hdfs-site.xml的<code>dfs.datanode.data.dir</code>却是一样的！</p>

<p>经验： dir的配置应该是一个系统设备对应一个路径，而不是一个系统目录对应dir的一个路径！</p>

<h2>问题现象以及根源</h2>

<p>问题机器A的磁盘情况：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver8 ~]$ df -h
</span><span class='line'>文件系统              容量  已用  可用 已用%% 挂载点
</span><span class='line'>/dev/sda3             2.7T  2.5T   53G  98% /
</span><span class='line'>tmpfs                  32G  260K   32G   1% /dev/shm
</span><span class='line'>/dev/sda1              97M   32M   61M  35% /boot
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 /]$ ll
</span><span class='line'>总用量 170
</span><span class='line'>dr-xr-xr-x.   2 root   root    4096 2月  12 19:39 bin
</span><span class='line'>dr-xr-xr-x.   5 root   root    1024 2月  13 02:40 boot
</span><span class='line'>drwxr-xr-x.   2 root   root    4096 2月  23 2012 cgroup
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data1
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data10
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data11
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data12
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data13
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data14
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data15
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data2
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data3
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data4
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data5
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data6
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data7
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data8
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data9</span></code></pre></td></tr></table></div></figure>


<p>再看集群其他机器：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver1 ~]$ df -h
</span><span class='line'>文件系统              容量  已用  可用 已用%% 挂载点
</span><span class='line'>/dev/sda3             2.7T   32G  2.5T   2% /
</span><span class='line'>tmpfs                  32G   88K   32G   1% /dev/shm
</span><span class='line'>/dev/sda1              97M   32M   61M  35% /boot
</span><span class='line'>/dev/sdb1             1.8T  495G  1.3T  29% /data1
</span><span class='line'>/dev/sdb2             1.8T  485G  1.3T  28% /data2
</span><span class='line'>/dev/sdb3             1.8T  492G  1.3T  29% /data3
</span><span class='line'>/dev/sdb4             1.8T  488G  1.3T  28% /data4
</span><span class='line'>/dev/sdb5             1.8T  486G  1.3T  28% /data5
</span><span class='line'>/dev/sdb6             1.8T  480G  1.3T  28% /data6
</span><span class='line'>/dev/sdb7             1.8T  479G  1.3T  28% /data7
</span><span class='line'>/dev/sdb8             1.8T  474G  1.3T  28% /data8
</span><span class='line'>/dev/sdb9             1.8T  480G  1.3T  28% /data9
</span><span class='line'>/dev/sdb10            1.8T  478G  1.3T  28% /data10
</span><span class='line'>/dev/sdb11            1.8T  475G  1.3T  28% /data11
</span><span class='line'>/dev/sdb12            1.8T  489G  1.3T  29% /data12
</span><span class='line'>/dev/sdb13            1.8T  475G  1.3T  28% /data13
</span><span class='line'>/dev/sdb14            1.8T  476G  1.3T  28% /data14
</span><span class='line'>/dev/sdb15            1.8T  469G  1.3T  27% /data15</span></code></pre></td></tr></table></div></figure>


<p>出问题机器没有挂存储，仅仅是建立了对应的目录结构，并不是把目录挂载到单独的存储设备上。</p>

<p>同时查看50070的前面的信息，hadoop把每个逗号分隔后的路径默认都做一个磁盘设备来计算！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Node               Address             ..Admin State CC    Used  NU    RU(%) R(%)      Blocks Block  Pool Used Block Pool Used (%)
</span><span class='line'>hadoop-slaver1    192.168.32.21:50010 2   In Service  26.86   7.05    1.37    18.44   26.25       68.66   264844  7.05    26.25   
</span><span class='line'>hadoop-slaver8    192.168.32.28:50010 1   In Service  37.94   2.46    34.71   0.77    6.48        2.03    29637   2.46    6.48    </span></code></pre></td></tr></table></div></figure>


<p>配置容量是所有配置的路径所在盘容量的<strong>累加</strong>。总的剩余空间（余量）也是各个dir配置路径的剩余空间<strong>累加</strong>的！这样很容易出现问题！
最好的就是集群的所有的datanode的节点的<strong>硬件配置一样</strong>！当然系统时间也的一致，hosts等等这些。</p>

<h2>问题处理</h2>

<p>首先得把问题解决啊：</p>

<ul>
<li>把<code>dfs.datanode.data.dir</code>路径个数调整为磁盘个数！</li>
<li>修改该datanode的hdfs-site的配置，添加<code>dfs.datanode.du.reserved</code>，留给系统的空间设置为400多G。</li>
<li>冗余份数也没有必要3份，浪费空间。如果两台机器同时出现问题，还是同一份数据，那只能说是天意！你可以去趟澳门赌一圈了！</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
</span><span class='line'>&lt;value&gt;/data1/hadoop/data&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;
</span><span class='line'>&lt;value&gt;437438953472&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.replication&lt;/name&gt;
</span><span class='line'>&lt;value&gt;2&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>设置了reserved保留空间后，再看LIVE页面slaver8的容量变少了且正好等于(盘的容量2.7T-430G~=2.26T 计算容量的hdfs源码在<code>FsVolumeImpl.getCapacity()</code>)。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop-slaver8   192.168.32.28:50010 1   In Service  2.26    2.23    0.00    0.03    98.66</span></code></pre></td></tr></table></div></figure>


<p>datanode和blockpool的平衡处理，可以参考<a href="http://hadoop-master1:50070/dfsnodelist.jsp?whatNodes=LIVE">Live Datanodes</a>的容量和进行！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 ~]$ hdfs balancer -help
</span><span class='line'>Usage: java Balancer
</span><span class='line'>        [-policy &lt;policy&gt;]      the balancing policy: datanode or blockpool
</span><span class='line'>        [-threshold &lt;threshold&gt;]        Percentage of disk capacity
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 ~]$ hadoop-2.2.0/bin/hdfs getconf -confkey dfs.datanode.du.reserved
</span><span class='line'>137438953472</span></code></pre></td></tr></table></div></figure>


<p>删除一些没用的备份数据。配置好以后，重启当前slaver8节点，并进行数据平衡（如果觉得麻烦，直接丢掉原来的一个目录下的数据也行，可能更快！均衡器运行的太慢！！）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver8 ~]$  ~/hadoop-2.2.0/sbin/hadoop-daemon.sh stop datanode
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 ~]$  for i in 6 7 8 9 10 11 12 13 14 15; do  cd /data$i/hadoop/data/current/BP-1414312971-192.168.32.11-1392479369615/current/finalized;  find . -type f -exec mv {} /data1/hadoop/data/current/BP-1414312971-192.168.32.11-1392479369615/current/finalized/{} \;; done
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 ~]$  ~/hadoop-2.2.0/sbin/hadoop-daemon.sh start datanode
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ hdfs dfsadmin -setBalancerBandwidth 10485760
</span><span class='line'>[hadoop@hadoop-master1 ~]$ hdfs balancer -threshold 60
</span></code></pre></td></tr></table></div></figure>


<p>查看datanode的日志，由于移动数据，有些blk的id一样，会清理一些数据。对于均衡器程序的阀值越小集群越平衡！默认是10（%），会移动很多的数据（准备看下均衡器的源码，了解各个参数以及运行的逻辑）！</p>

<h2>参考</h2>

<ul>
<li><a href="http://blog.csdn.net/lingzihan1215/article/details/8700532">hadoop的datanode多磁盘空间处理</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Snappy Compress]]></title>
    <link href="http://winse.github.io/blog/2014/07/30/hadoop2-snappy-compress/"/>
    <updated>2014-07-30T00:25:39+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/30/hadoop2-snappy-compress</id>
    <content type="html"><![CDATA[<p>网上查了很多资料说的很复杂，要叉叉叉叉叉！其实hadoop2已经集成了hadoop-snappy，只要安装snappy即可。但是也没有一些文章说的只要编译snappy然后放到lib/native路径下即可，还需要重新编译libhadoop的library包。</p>

<p>查找hadoop-snappy的源码的时刻，在C代码里面定义了<code>HADOOP_SNAPPY_LIBRARY</code>，然后理着这个思路去查找，发现在CMakeFile文件中也定义了对应的变量，然后再查找pom.xml的native profile中定义了snappy.prefix的属性。最后就有了下面的步骤。</p>

<p>1) build snappy</p>

<p>编译Snappy，并把lib拷贝/同步到hadoop的native目录下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxf snappy-1.1.1.tar.gz 
</span><span class='line'>cd snappy-1.1.1
</span><span class='line'>./configure --prefix=/home/hadoop/snappy
</span><span class='line'>make
</span><span class='line'>make install
</span><span class='line'>
</span><span class='line'>cd snappy
</span><span class='line'>cd lib/
</span><span class='line'>rysnc -vaz * ~/hadoop-2.2.0/lib/native/</span></code></pre></td></tr></table></div></figure>


<p>2) rebuild hadoop common project</p>

<p>重新编译hadoop的lib，覆盖原来的文件。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop-common]$ mvn package -Dmaven.javadoc.skip=true -DskipTests -Dsnappy.prefix=/home/hadoop/snappy -Drequire.snappy=true -Pnative 
</span><span class='line'>
</span><span class='line'>[hadoop@master1 hadoop-common]$ cd ~/hadoop-2.2.0-src/hadoop-common-project/hadoop-common/
</span><span class='line'>[hadoop@master1 hadoop-common]$ cd target/native/target/usr/local/lib/
</span><span class='line'>[hadoop@master1 lib]$ ll
</span><span class='line'>total 1252
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 820824 Jul 30 00:18 libhadoop.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop     18 Jul 30 00:18 libhadoop.so -&gt; libhadoop.so.1.0.0
</span><span class='line'>-rwxrwxr-x. 1 hadoop hadoop 455542 Jul 30 00:18 libhadoop.so.1.0.0
</span><span class='line'>[hadoop@master1 lib]$ rsync -vaz * ~/hadoop-2.2.0/lib/native/
</span><span class='line'>sending incremental file list
</span><span class='line'>libhadoop.a
</span><span class='line'>libhadoop.so.1.0.0
</span><span class='line'>
</span><span class='line'>sent 409348 bytes  received 53 bytes  818802.00 bytes/sec
</span><span class='line'>total size is 1276384  speedup is 3.12
</span><span class='line'>[hadoop@master1 lib]$ </span></code></pre></td></tr></table></div></figure>


<p>3) check</p>

<p>检查程序snappy是否已经配置成功</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ hadoop checknative -a
</span><span class='line'>14/07/30 00:22:14 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
</span><span class='line'>14/07/30 00:22:14 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
</span><span class='line'>Native library checking:
</span><span class='line'>hadoop: true /home/hadoop/hadoop-2.2.0/lib/native/libhadoop.so.1.0.0
</span><span class='line'>zlib:   true /lib64/libz.so.1
</span><span class='line'>snappy: true /home/hadoop/hadoop-2.2.0/lib/native/libsnappy.so.1
</span><span class='line'>lz4:    true revision:43
</span><span class='line'>bzip2:  false 
</span><span class='line'>14/07/30 00:22:14 INFO util.ExitUtil: Exiting with status 1
</span><span class='line'>[hadoop@master1 ~]$ </span></code></pre></td></tr></table></div></figure>


<p>4) 跑一个压缩程序</p>

<p>先参考网上的，直接用hbase的带的测试类运行（前提：需要在hbase-env.sh中配置HADOOP_HOME，这样hbase才能找到hadoop下的lib本地库）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ hbase-0.98.3-hadoop2/bin/hbase org.apache.hadoop.hbase.util.CompressionTest file:///tmp/abc.snappy snappy
</span><span class='line'>2014-07-30 08:50:42,617 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
</span><span class='line'>SLF4J: Class path contains multiple SLF4J bindings.
</span><span class='line'>SLF4J: Found binding in [jar:file:/home/hadoop/hbase-0.98.3-hadoop2/lib/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
</span><span class='line'>SLF4J: Found binding in [jar:file:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
</span><span class='line'>SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
</span><span class='line'>2014-07-30 08:50:44,515 INFO  [main] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
</span><span class='line'>2014-07-30 08:50:44,522 INFO  [main] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
</span><span class='line'>2014-07-30 08:50:45,388 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
</span><span class='line'>2014-07-30 08:50:45,408 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
</span><span class='line'>2014-07-30 08:50:45,430 ERROR [main] hbase.KeyValue: Unexpected getShortMidpointKey result, fakeKey:testkey, firstKeyInBlock:testkey
</span><span class='line'>2014-07-30 08:50:47,088 INFO  [main] compress.CodecPool: Got brand-new decompressor [.snappy]
</span><span class='line'>SUCCESS
</span><span class='line'>[hadoop@master1 ~]$ </span></code></pre></td></tr></table></div></figure>


<p>看到最后的<strong>SUCCESS</strong>就说明安装配置成功了！</p>

<p>接下来自己写程序测试压缩/解压缩，首先编写java类：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import java.io.FileInputStream;
</span><span class='line'>import java.io.FileNotFoundException;
</span><span class='line'>import java.io.FileOutputStream;
</span><span class='line'>import java.io.IOException;
</span><span class='line'>
</span><span class='line'>import org.apache.commons.lang.StringUtils;
</span><span class='line'>import org.apache.hadoop.conf.Configuration;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionCodec;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionInputStream;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionOutputStream;
</span><span class='line'>import org.apache.hadoop.io.compress.SnappyCodec;
</span><span class='line'>import org.apache.hadoop.util.ReflectionUtils;
</span><span class='line'>import org.apache.zookeeper.common.IOUtils;
</span><span class='line'>
</span><span class='line'>public class SnappyCompressTest {
</span><span class='line'>
</span><span class='line'>        public static void main(String[] args) throws FileNotFoundException, IOException {
</span><span class='line'>                try {
</span><span class='line'>                        execute(args);
</span><span class='line'>                } catch (Exception e) {
</span><span class='line'>                        System.out.println("Usage: $0 read|write file[.snappy]");
</span><span class='line'>                }
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>        private static void execute(String[] args) throws FileNotFoundException, IOException {
</span><span class='line'>                String op = args[0];
</span><span class='line'>                String file = args[1];
</span><span class='line'>                String snappyFile = file + ".snappy";
</span><span class='line'>
</span><span class='line'>                Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
</span><span class='line'>                CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());
</span><span class='line'>
</span><span class='line'>                if (StringUtils.equalsIgnoreCase(op, "read")) {
</span><span class='line'>                        FileInputStream fin = new FileInputStream(snappyFile);
</span><span class='line'>                        CompressionInputStream in = codec.createInputStream(fin);
</span><span class='line'>                        FileOutputStream fout = new FileOutputStream(file);
</span><span class='line'>                        IOUtils.copyBytes(in, fout, 4096, true);
</span><span class='line'>                } else {
</span><span class='line'>                        FileInputStream fin = new FileInputStream(file);
</span><span class='line'>                        CompressionOutputStream out = codec.createOutputStream(new FileOutputStream(snappyFile));
</span><span class='line'>                        IOUtils.copyBytes(fin, out, 4096, true);
</span><span class='line'>                }
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>编译运行，测试读写功能。使用hadoop命令可以简化很多工作，把当前路径加入到<code>HADOOP_CLASSPATH</code>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 test]$ javac -cp `hadoop classpath` SnappyCompressTest.java 
</span><span class='line'>[hadoop@master1 test]$ export HADOOP_CLASSPATH=$PWD
</span><span class='line'>[hadoop@master1 test]$ hadoop SnappyCompressTest 
</span><span class='line'>Usage: $0 read|write file[.snappy]
</span><span class='line'>[hadoop@master1 test]$ hadoop SnappyCompressTest write test.txt 
</span><span class='line'>[hadoop@master1 test]$ ll
</span><span class='line'>total 16
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:23 test.txt
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
</span><span class='line'>[hadoop@master1 test]$ rm test.txt
</span><span class='line'>[hadoop@master1 test]$ hadoop SnappyCompressTest read test.txt 
</span><span class='line'>[hadoop@master1 test]$ ll
</span><span class='line'>total 16
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:28 test.txt
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
</span><span class='line'>[hadoop@master1 test]$ cat test.txt
</span><span class='line'>abc
</span><span class='line'>abc
</span><span class='line'>abc</span></code></pre></td></tr></table></div></figure>


<p>5) hbase中添加压缩</p>

<p>把所有library，以及hbase的配置同步其他所有从节点。对hbase的表使用Snappy压缩。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hbase(main):001:0&gt; create 'st1', 'f1'
</span><span class='line'>hbase(main):005:0&gt; alter 'st1', {NAME=&gt;'f1', COMPRESSION=&gt;'snappy'}
</span><span class='line'>Updating all regions with the new schema...
</span><span class='line'>0/1 regions updated.
</span><span class='line'>1/1 regions updated.
</span><span class='line'>Done.
</span><span class='line'>0 row(s) in 2.7880 seconds
</span><span class='line'>
</span><span class='line'>hbase(main):010:0&gt; create 'sst1','f1'
</span><span class='line'>0 row(s) in 0.5730 seconds
</span><span class='line'>
</span><span class='line'>=&gt; Hbase::Table - sst1
</span><span class='line'>hbase(main):011:0&gt; flush 'sst1'
</span><span class='line'>0 row(s) in 2.5380 seconds
</span><span class='line'>
</span><span class='line'>hbase(main):012:0&gt; flush 'st1'
</span><span class='line'>0 row(s) in 7.5470 seconds</span></code></pre></td></tr></table></div></figure>


<p>对于hbase来说，使用压缩消耗还是挺大的。插入10w数据中间进行compaction时停顿比较久。最后flush写数据的时间也长了很多！
下面是文件写入后的文件大小对比（由于是进行简单的测试，写入的数据重复比较多。具体比例没有参考价值）：</p>

<p><img src="http://file.bmob.cn/M00/05/5A/wKhkA1PYz9CAB-TdAAEWX8LGpUo149.png" alt="" /></p>

<p>6) 正式环境下解压snappy文件</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>import java.io.FileOutputStream;
</span><span class='line'>import java.io.IOException;
</span><span class='line'>import java.io.InputStream;
</span><span class='line'>
</span><span class='line'>import org.apache.hadoop.conf.Configuration;
</span><span class='line'>import org.apache.hadoop.fs.FileSystem;
</span><span class='line'>import org.apache.hadoop.fs.Path;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionCodec;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionInputStream;
</span><span class='line'>import org.apache.hadoop.io.compress.SnappyCodec;
</span><span class='line'>import org.apache.hadoop.util.ReflectionUtils;
</span><span class='line'>import org.apache.zookeeper.common.IOUtils;
</span><span class='line'>
</span><span class='line'>public class DecompressTest {
</span><span class='line'>  public static void main(String[] args) throws IOException {
</span><span class='line'>
</span><span class='line'>      Configuration conf = new Configuration();
</span><span class='line'>      Path path = new Path(args[0]);
</span><span class='line'>      FileSystem fs = path.getFileSystem(conf);
</span><span class='line'>
</span><span class='line'>      Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
</span><span class='line'>      CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());
</span><span class='line'>
</span><span class='line'>      InputStream fin = fs.open(path);
</span><span class='line'>      CompressionInputStream in = codec.createInputStream(fin);
</span><span class='line'>
</span><span class='line'>      IOUtils.copyBytes(in, System.out, 4096, true);
</span><span class='line'>
</span><span class='line'>      fin.close();
</span><span class='line'>
</span><span class='line'>      System.out.println("SUCCESS");
</span><span class='line'>
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>// build & run
</span><span class='line'>
</span><span class='line'>&gt;DecompressTest.java 
</span><span class='line'>vi DecompressTest.java 
</span><span class='line'>javac -cp `hadoop classpath`  DecompressTest.java 
</span><span class='line'>export HADOOP_CLASSPATH=.
</span><span class='line'># snappyfile on hdfs
</span><span class='line'>hadoop DecompressTest /user/hive/t_ods_access_log2/month=201408/day=20140828/hour=2014082808/t_ods_access_log2-2014082808.our.snappy.1409187524328
</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 ShortCircuit Local Reading]]></title>
    <link href="http://winse.github.io/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading/"/>
    <updated>2014-07-29T20:11:58+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading</id>
    <content type="html"><![CDATA[<p>hadoop一直以来认为是本地读写文件的，但是其实也是通过TCP端口去获取数据，只是都在同一台机器。在hivetuning调优hive的文档中看到了ShortCircuit的HDFS配置属性，查看了ShortCircuit的来由，真正的实现了本地读取文件。蒙查查表示看的不是很明白，最终大致就是通过linux的<strong>文件描述符</strong>来实现功能同时保证文件的权限。</p>

<p>由于仅在自己的机器上面配置来查询hbase的数据，性能方面提升感觉不是很明显。等以后整到正式环境再对比对比。</p>

<p>配置如下。</p>

<p>1 修改hdfs-site.xml</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;/home/hadoop/data/sockets/dn_socket&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>注意：socket路径的权限控制的比较严格。dn_socket<strong>所有的父路径</strong>要么仅有当前启动用户的写权限，要么仅root可写。</p>

<p><img src="http://file.bmob.cn/M00/05/52/wKhkA1PXfbKANLOrAADWJQ5taVs391.png" alt="" /></p>

<p>2 修改hbase的配置，并添加HADOOP_HOME（hbase查找hadoop-native）</p>

<p><img src="http://file.bmob.cn/M00/05/52/wKhkA1PXhRKAZDs6AAChrEauBoU738.png" alt="" /></p>

<p>hbase的脚本找到hadoop命令后，会把hadoop的java.library.path的路径加入到hbase的启动脚本中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ tail -15 hbase-0.98.3-hadoop2/conf/hbase-site.xml 
</span><span class='line'>    &lt;name&gt;hbase.tmp.dir&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;/home/hadoop/data/hbase&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;/home/hadoop/data/sockets/dn_socket&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@master1 ~]$ cat hbase-0.98.3-hadoop2/conf/hbase-env.sh
</span><span class='line'>...
</span><span class='line'>export HADOOP_HOME=/home/hadoop/hadoop-2.2.0
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>3 同步到其他节点，然后重启hdfs,hbase</p>

<h2>参考</h2>

<ul>
<li><a href="http://vdisk.weibo.com/s/z_44nz36hNM3Z">hive-tuning</a></li>
<li><a href="http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/">How Improved Short-Circuit Local Reads Bring Better Performance and Security to Hadoop</a></li>
<li><a href="http://hbase.apache.org/book/perf.hdfs.html">HDFS&ndash;Apache HBase Performance Tuning</a></li>
<li><a href="http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">HDFS Short-Circuit Local Reads</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop安全的关闭datanode节点]]></title>
    <link href="http://winse.github.io/blog/2014/07/29/safely-remove-datanode/"/>
    <updated>2014-07-29T15:08:41+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/29/safely-remove-datanode</id>
    <content type="html"><![CDATA[<p>hadoop默认就有冗余（dfs.replication）的机制，所以一般情况下，一台机器挂了也没所谓。集群会自动的进行复制均衡处理。</p>

<p>作为测试，如果dfs.replication设置为1的情况下，怎么安全的把datanode节点服务关闭呢？例如说，刚刚开始搭建环境是把namenode、datanode放在一台机器上，后面增加了机器如何把datanode分离出来呢？</p>

<p>借助于<strong>dfs.hosts.exclude</strong>即可完成顺序的完成此项任务。</p>

<p>修改hdfs-site.xml配置。我操作的时刻仅修改了master1上的hdfs-site.xml。把<strong>master1</strong>值写入到对应的文件中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop]$ cat hdfs-site.xml 
</span><span class='line'>...
</span><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;/home/hadoop/hadoop-2.2.0/etc/hadoop/exclude&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>[hadoop@master1 hadoop]$ cat /home/hadoop/hadoop-2.2.0/etc/hadoop/exclude
</span><span class='line'>master1
</span></code></pre></td></tr></table></div></figure>


<p>修改完成后，刷新节点即可(完全没有必要重启集dfs)。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop dfsadmin -refreshNodes</span></code></pre></td></tr></table></div></figure>


<p>可以通过<code>dfsadmin -report</code>或者网页查看master1已经变成<em>Decommission In Progress</em>了。</p>

<p><img src="http://file.bmob.cn/M00/05/4C/wKhkA1PXUMOAVvvWAAED6CN-3Rg187.png" alt="" /></p>

<p>注：</p>

<p>问题一： 在新建节点是slaver1的防火墙没关闭，由于master1已经被exclude，而slaver1不能提供服务，上传文件时报错：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop]$ hadoop fs -put slaves  /
</span><span class='line'>14/07/29 15:18:21 WARN hdfs.DFSClient: DataStreamer Exception
</span><span class='line'>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /slaves._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1384)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2477)</span></code></pre></td></tr></table></div></figure>


<p>关闭防火墙一样再次上传，还是报同样的错误。此时，也可以通过刷新节点<code>hadoop dfsadmin -refreshNodes</code>来解决。</p>

<p>问题二： 设置备份数量</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop]$ hadoop fs -setrep 3 /slaves 
</span><span class='line'>Replication 3 set: /slaves</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>问题三： 新增节点</p>

<p>拷贝程序到新增节点，然后启动</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ tar zc hadoop-2.2.0 --exclude=logs | ssh slaver2 'cat | tar zx'
</span><span class='line'>
</span><span class='line'>[hadoop@slaver2 ~]$ cd hadoop-2.2.0/
</span><span class='line'>[hadoop@slaver2 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start datanode</span></code></pre></td></tr></table></div></figure>


<p>也可以修改master上的slavers文件再<code>sbin/start-dfs.sh</code>启动。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读读书]Redis入门指南]]></title>
    <link href="http://winse.github.io/blog/2014/07/27/start-redis/"/>
    <updated>2014-07-27T01:20:44+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/27/start-redis</id>
    <content type="html"><![CDATA[<h2>第一章 简介</h2>

<ul>
<li>讲了redis的产生的缘由</li>
<li>Salvtore Sanfilippo/Pieter Noordhuis被招到VMware专门负责redis</li>
<li>redis的源码可以从github下载编译。</li>
</ul>


<p>redis相比keyvalue，提供了更加丰富的值类型：字符串/散列/列表/集合/有序集合，数据提供多种持久化(RDB/AOF)的方式。</p>

<p>在一台普通的笔记本电脑上，Redis可以在一秒内读写超过十万个键值。</p>

<p>功能丰富，提供TTL，可以做(阻塞)队列、缓冲系统、发布/订阅消息模式。redis是单线程模型，相比memcached的多线程，可以启动多个redis实例。</p>

<h2>第二章 准备</h2>

<p>默认的生产环境使用linux，windows操作系统下也有对应的版本但是版本比较旧。
在linux下，下载完成后直接<code>make</code>就可以使用src目录下生成的命令了，<code>make install</code>会把命令拷贝到/usr/local/bin目录下。同时有介绍iOS和Windows下怎么安装redis。</p>

<h3>启动Redis2.8.3</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>src/redis-server # default port 6379
</span><span class='line'>src/redis-server --port 6380</span></code></pre></td></tr></table></div></figure>


<p>初始化脚本启动Redis</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/sh
</span><span class='line'>#
</span><span class='line'># Simple Redis init.d script conceived to work on linux systems
</span><span class='line'># as it does use of the /proc filesystem.
</span><span class='line'>
</span><span class='line'>REDISPORT=6379
</span><span class='line'>EXEC=/usr/local/bin/redis-server
</span><span class='line'>CLIEXEC=/usr/local/bin/redis-cli
</span><span class='line'>
</span><span class='line'>PIDFILE=/var/run/redis_${REDISPORT}.pid
</span><span class='line'>CONF=/etc/redis/${REDISPORT}.conf
</span><span class='line'>
</span><span class='line'>case "$1" in
</span><span class='line'>start)
</span><span class='line'>  if [ -f $PIDFILE ]
</span><span class='line'>  then
</span><span class='line'>      echo "$PIDFILE exists, process is already running or crashed"
</span><span class='line'>  else
</span><span class='line'>      echo "Starting Redis server..."
</span><span class='line'>      $EXEC $CONF
</span><span class='line'>  fi
</span><span class='line'>  ::
</span><span class='line'>stop)
</span><span class='line'>  if [ ! -f $PIDFILE ]
</span><span class='line'>  then
</span><span class='line'>      echo "$PIDFILE does not exists, process is not running"
</span><span class='line'>  else
</span><span class='line'>      PID=$(cat $PIDFILE)
</span><span class='line'>      echo "Stopping..."
</span><span class='line'>      $CLIEXEC -p $REDISPORT shutdown
</span><span class='line'>      while [ -x /proc/$PID ]
</span><span class='line'>      do 
</span><span class='line'>          echo "Waiting for Redis to shutdown..."
</span><span class='line'>          sleep 1
</span><span class='line'>      done
</span><span class='line'>      echo "Redis stopped"
</span><span class='line'>  fi
</span><span class='line'>  ::
</span><span class='line'>*)
</span><span class='line'>  echo "Please use start or stop as first argument"
</span><span class='line'>  ::
</span><span class='line'>esac</span></code></pre></td></tr></table></div></figure>


<h3>停止Redis</h3>

<p>不要直接强制终止程序(<code>kill -9</code>)。使用redis提供的shutdown来停，会等所有操作都flush到磁盘后再关闭。保证数据不会丢失。
当然也可以使用SIGTERM信号来处理，使用<code>kill PID</code>命令，Redis妥善的处理与发送shutdown命令效果一样。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>src/redis-cli shutdown</span></code></pre></td></tr></table></div></figure>


<h3>命令行客户端(cli Command-Line-Interface)</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>redis-cli -h IP -p PORT
</span><span class='line'>
</span><span class='line'>[hadoop@master1 src]$ ./redis-cli PING
</span><span class='line'>PONG
</span><span class='line'>
</span><span class='line'>[hadoop@master1 src]$ ./redis-cli
</span><span class='line'>127.0.0.1:6379&gt; PING
</span><span class='line'>PONG
</span><span class='line'>127.0.0.1:6379&gt; echo hi
</span><span class='line'>"hi"</span></code></pre></td></tr></table></div></figure>


<p>各种返回值</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; errorcommand
</span><span class='line'>(error) ERR unknown command 'errorcommand'
</span><span class='line'>127.0.0.1:6379&gt; incr foo
</span><span class='line'>(integer) 1
</span><span class='line'>127.0.0.1:6379&gt; get foo
</span><span class='line'>"1"
</span><span class='line'>127.0.0.1:6379&gt; get noexists
</span><span class='line'>(nil)
</span><span class='line'>127.0.0.1:6379&gt; keys *
</span><span class='line'>1) "foo"</span></code></pre></td></tr></table></div></figure>


<h3>配置</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>redis-server CONFPATH --loglevel warning</span></code></pre></td></tr></table></div></figure>


<p>也可以通过客户端设置值</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; config set loglevel warning
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379&gt; config get loglevel
</span><span class='line'>1) "loglevel"
</span><span class='line'>2) "warning"</span></code></pre></td></tr></table></div></figure>


<h3>多数据库</h3>

<p>默认启动的程序启用了16个库（0-15，<code>databases 16</code>），客户端与Redis建立连接后，会自动选择0号数据库，不过可以通过SELECT命令更换数据库:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; select 1
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379[1]&gt; get foo
</span><span class='line'>(nil)
</span><span class='line'>127.0.0.1:6379[1]&gt; set foo 1
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379[1]&gt; get foo
</span><span class='line'>"1"</span></code></pre></td></tr></table></div></figure>


<p>redis不支持为每个数据库设置不同的访问密码，一个客户端要么可以访问全部数据库，要么连一个数据库也没有权限访问。最重要的一点是多个数据库并不是完全的隔离，比如flushall命令可以清空Redis实例中所有的数据库中的数据。所以这些数据库更像是一个命名空间，而不是适合存储不同应用的数据。</p>

<p>但是可以使用0号数据库存储A应用的生产数据而使用1号数据库存储A应用的测试数据，不同的应用应该使用不同的Redis实例存储数据。由于Redis非常轻量级，一个空Redis实例占用内存只有1M左右，所以不用担心多个Redis实例会额外占用很多内存。</p>

<h2>第三章 入门</h2>

<h3>热身</h3>

<p>获取符合规则的键名（glob风格 ?/*/\X/[]） : <code>KEYS pattern</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379[1]&gt; KEYS *
</span><span class='line'>1) "foq"
</span><span class='line'>2) "foo"
</span><span class='line'>3) "fop"
</span><span class='line'>127.0.0.1:6379[1]&gt; keys fo[a-p]
</span><span class='line'>1) "foo"
</span><span class='line'>2) "fop"
</span><span class='line'>
</span><span class='line'>127.0.0.1:6379[1]&gt; exists foa
</span><span class='line'>(integer) 0 #不存在
</span><span class='line'>127.0.0.1:6379[1]&gt; exists foo
</span><span class='line'>(integer) 1 #存在
</span><span class='line'>
</span><span class='line'>127.0.0.1:6379[1]&gt; del foo
</span><span class='line'>(integer) 1
</span><span class='line'>127.0.0.1:6379[1]&gt; del foa
</span><span class='line'>(integer) 0
</span><span class='line'>127.0.0.1:6379[1]&gt; keys *
</span><span class='line'>1) "fop"</span></code></pre></td></tr></table></div></figure>


<p>keys会遍历Redis中的所有键，当数量比较多是会影响性能，不建议在生产环境使用。</p>

<p>del可以删除多个键值，返回值为删除的个数。del命令的参数不支持通配符，但可以通过linux的实现批量删除<code>redis-cli DEL $(redis-cli KEYS "user:*")</code>（有长度限制）来达到效果，效果比xargs效果更好。</p>

<p>获取keyvalue值的类型</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; set foo 1
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379&gt; lpush foo 1
</span><span class='line'>(error) WRONGTYPE Operation against a key holding the wrong kind of value
</span><span class='line'>127.0.0.1:6379&gt; lpush foa 1
</span><span class='line'>(integer) 1
</span><span class='line'>127.0.0.1:6379&gt; type foo
</span><span class='line'>string
</span><span class='line'>127.0.0.1:6379&gt; type foa
</span><span class='line'>list</span></code></pre></td></tr></table></div></figure>


<h3>字符串类型</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set key value
</span><span class='line'>get key
</span><span class='line'>
</span><span class='line'>incr key # 对应的值需为数值
</span><span class='line'>
</span><span class='line'>set foo 1
</span><span class='line'>incr foo
</span><span class='line'>set foo b
</span><span class='line'>incr foo
</span><span class='line'># (error) ERR value is not an integer or out of range
</span><span class='line'>
</span><span class='line'># 增加指定的整数
</span><span class='line'>
</span><span class='line'>incrby key increment
</span><span class='line'>decr key 
</span><span class='line'>decr key decrement
</span><span class='line'>increbyfloat key increment
</span><span class='line'>
</span><span class='line'>append key value
</span><span class='line'>strlen key # 字节数，和java字符串的length不同
</span><span class='line'>
</span><span class='line'>mget key [key ...]
</span><span class='line'>mset key value [key value ...]
</span><span class='line'>
</span><span class='line'>getbit key offset
</span><span class='line'>setbit key offset value
</span><span class='line'>bitcount key [start] [end]
</span><span class='line'>bitop operation destkey key [key ...] # AND OR XOR NOT
</span><span class='line'>
</span><span class='line'>set foo1 bar
</span><span class='line'>set foo2 aar
</span><span class='line'>BITOP OR res foo1 foo2 # 位操作命令可以非常紧凑地存储布尔值
</span><span class='line'>GET res</span></code></pre></td></tr></table></div></figure>


<h3>散列值</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hset key field value
</span><span class='line'>hget key field
</span><span class='line'>hmset key field value [field value ...]
</span><span class='line'>hmget key field [field ...]
</span><span class='line'>hgetall key
</span><span class='line'>
</span><span class='line'>hexists key field
</span><span class='line'>hsetnx key field value # 当字段不存在时赋值 if not exists
</span><span class='line'>
</span><span class='line'>hincrby key field increment
</span><span class='line'>
</span><span class='line'>hdel key field [field ...]
</span><span class='line'>
</span><span class='line'>hkeys key # 仅key
</span><span class='line'>hvals key # 仅value
</span><span class='line'>hlen key  # 字段数量</span></code></pre></td></tr></table></div></figure>


<h3>列表</h3>

<p>双端队列型列表</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>lpush key value [value ...]
</span><span class='line'>rpush key value [value ...]
</span><span class='line'>lpop key
</span><span class='line'>rpop key
</span><span class='line'>llen key
</span><span class='line'>lrange key start stop # 可以使用负索引，从0开始，包括最右边的元素
</span><span class='line'>
</span><span class='line'>lrem key count value 
</span><span class='line'># 删除列表中前count个值为value的元素，返回的是实际删除的元素个数。
</span><span class='line'># count为负数是从右边开始删除
</span><span class='line'># count为0时删除所有值为value的元素
</span><span class='line'>
</span><span class='line'># 获得/设置指定索引的元素值
</span><span class='line'>
</span><span class='line'>lindex key index # index为负数是从右边开始
</span><span class='line'>lset key index value
</span><span class='line'>
</span><span class='line'>ltrim key start end # 只保留列表指定的片段
</span><span class='line'>linsert key BEFORE/AFTER pivotvalue value
</span><span class='line'>
</span><span class='line'>poplpush source destination # 将元素从给一个列表转到另一个列表</span></code></pre></td></tr></table></div></figure>


<h3>集合类型</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sadd key member [member ...]
</span><span class='line'>srem key member [member ...]
</span><span class='line'>smembers key # 获取集合中的元素
</span><span class='line'>sismember key member # 判断元素是否在集合中
</span><span class='line'>
</span><span class='line'>sdiff key [key ...] # 差集 A-B
</span><span class='line'>sinter key [key ...] # A ∩ B
</span><span class='line'>sunion key [key ...] # A ∪ B
</span><span class='line'>
</span><span class='line'>scard key # 获取集合中元素个数
</span><span class='line'>
</span><span class='line'>sdiffstore destination key [key ...]
</span><span class='line'>sinterstore destination key [key ...]
</span><span class='line'>sunionstore destination key [key ...]
</span><span class='line'>
</span><span class='line'>srandmember key [count] 
</span><span class='line'># 随机获取集合中的元素，count参数来一次性获取多个元素
</span><span class='line'># count为负数时，会随机从集合里获得|count|个的元素，这里元素有可能相同。
</span><span class='line'>
</span><span class='line'>spop key # 从集合中随机弹出一个元素</span></code></pre></td></tr></table></div></figure>


<h3>有序集合</h3>

<p>列表类型是通过链表实现的，获取靠近两端的数据速度极快，而当元素增多后，访问中间数据的速度会较慢，所以它更加适合实现和“新鲜事”或“日志”这样很少访问中间元素的应用。有序集合类型是使用散列和跳跃表（Skip list）实现的，所以即使读取位于中间的数据也很快（时间复杂度是O(log(N))）。列表中不能简单地调整某个元素的位置，但是有序集合可以（通过更改这个元素的分数）。有序集合要比列表类型更耗费内存。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>zadd key score member [score member ...]
</span><span class='line'># 如果该元素已经存在则会用新的分数替换原有的分数。zadd命令的返回值是新加入到集合中的元素个数（不包含之前已经存在的元素）。
</span><span class='line'># 其中+inf和-inf分别表示正无穷和负无穷
</span><span class='line'>
</span><span class='line'>zscore key member
</span><span class='line'>
</span><span class='line'>zrange key start stop [withscores] # 获取排名在某个范围的元素列表
</span><span class='line'>zrevrange key start stop [withscores] 
</span><span class='line'># 负数代表从后向前查找（-1表示最后一个元素），O(logn+m)
</span><span class='line'>
</span><span class='line'>zrangebyscore key min max [withscores] [limit offset  count]
</span><span class='line'>
</span><span class='line'># 命令按照元素分数从小到大的顺序返回分数的min和max之间（包含min和max）的元素。
</span><span class='line'># 如果希望分数范围不包含端点值，可以在分数前加上"("符号。例如，希望返回80分到100分的数据，可以含80分，但不包含100分。则稍微修改一下上面的命令即可：
</span><span class='line'>zrangebyscore scoreboard 80 (100
</span><span class='line'>zrangebyscore scoreboard (80 +inf
</span><span class='line'># 本命令中LIMIT offset count与SQL中的用法基本相同。获取分数低于或等于100的前3个人
</span><span class='line'>zrevrangebyscore scoreboard 100 0 limit 0 3
</span><span class='line'>
</span><span class='line'>zincrby key increment memeber # 增加某个元素的分数
</span><span class='line'>
</span><span class='line'>zcard key # 获取集合中元素的数量
</span><span class='line'>zcount key min max # 获得指定分数范围内的元素个数
</span><span class='line'>zrem key member [memeber ...] # 删除一个或多个元素，返回成功删除的元素数量
</span><span class='line'>
</span><span class='line'># 按照排名范围删除元素, 并返回删除的元素数量
</span><span class='line'>zremrangebyrank key start stop
</span><span class='line'># 按照分数范围删除元素
</span><span class='line'>zremrangebyscore key min max
</span><span class='line'>
</span><span class='line'>zrank key member
</span><span class='line'>zrevrank key memeber
</span><span class='line'>
</span><span class='line'>zinterstore destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [aggregate sum|min|max]
</span><span class='line'>zunionstore ...
</span></code></pre></td></tr></table></div></figure>


<h2>第四章 进阶</h2>

<h3>事务</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>multi
</span><span class='line'>sadd "user:1:following" 2
</span><span class='line'>sadd "user:2:followers" 1
</span><span class='line'>exec</span></code></pre></td></tr></table></div></figure>


<p>脚本语法有错，命令不能执行。但是当数据类型等逻辑运行错误时，事务里面的命令会被redis接受并执行。</p>

<p>如果事务里的一条命令出现错误，事务里的其他命令依然会继续执行（包括出错到最后的命令）。对应的返回值会返回错误信息。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; multi
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379&gt; set key 1
</span><span class='line'>QUEUED
</span><span class='line'>127.0.0.1:6379&gt; sadd key 2
</span><span class='line'>QUEUED
</span><span class='line'>127.0.0.1:6379&gt; set key 3
</span><span class='line'>QUEUED
</span><span class='line'>127.0.0.1:6379&gt; exec
</span><span class='line'>1) OK
</span><span class='line'>2) (error) WRONGTYPE Operation against a key holding the wrong kind of value
</span><span class='line'>3) OK
</span><span class='line'>127.0.0.1:6379&gt; get key
</span><span class='line'>"3"</span></code></pre></td></tr></table></div></figure>


<p>redis的事务没有回滚的功能，出现错误事务时必须自己负责收拾剩下的摊子（将数据库复原事务执行前的状态等）。不过由于redis不支持回滚功能，也使得redis在事务上可以保持简洁和快速。其中语法错误完全可以再开发时找出并解决。另外如果能够很好的规划数据库（保证键名规范等）的使用，是不会出现命令与数据类型不匹配这样的错误的。</p>

<p><strong>watch命令</strong></p>

<p>在一个事务中只有当所有命令都依次执行完后才能得到每个结果的返回值。可是有些情况下需要先获得一条命令的返回值，然后再根据这个值执行下一条命令。
如increment的操作，在增加1的是时刻没法保证数据还是原来的数据。为了解决这个问题，可以在GET获取值后保证该键值不会被其他客户端修改，知道函数执行完成后才允许其他客户端修改该键值，这样也可以防止竞态条件。watch命令可以监控一个或多个键，一旦其中一个键被修改（或删除），之后的事务就不会被执行。监控一直持续到exec命令。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; watch key
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379&gt; set key 2
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379&gt; multi
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379&gt; set key 3
</span><span class='line'>QUEUED
</span><span class='line'>127.0.0.1:6379&gt; exec
</span><span class='line'>(nil)
</span><span class='line'>127.0.0.1:6379&gt; get key
</span><span class='line'>"2"</span></code></pre></td></tr></table></div></figure>


<p>执行exec命令会取消对所有键的监控，如果不想执行事务中的命令也可以使用unwatch命令来取消监控。</p>

<h3>生存时间TTL</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>expire key seconds
</span><span class='line'>
</span><span class='line'>ttl key
</span><span class='line'>
</span><span class='line'>127.0.0.1:6379&gt; get key
</span><span class='line'>"2"
</span><span class='line'>127.0.0.1:6379&gt; ttl key
</span><span class='line'>(integer) -1
</span><span class='line'>127.0.0.1:6379&gt; expire key 10
</span><span class='line'>(integer) 1
</span><span class='line'>127.0.0.1:6379&gt; ttl key
</span><span class='line'>(integer) 6
</span><span class='line'>127.0.0.1:6379&gt; ttl key
</span><span class='line'>(integer) 1
</span><span class='line'>127.0.0.1:6379&gt; ttl key
</span><span class='line'>(integer) -2
</span><span class='line'>
</span><span class='line'>pexpire milliseconds #时间的单位为毫秒
</span><span class='line'>expireat UTC
</span><span class='line'>pexpireat 毫秒（UTC*1000）</span></code></pre></td></tr></table></div></figure>


<p>除了persist命令之外，使用set和getset命令为键赋值也会同时清除键的生存时间。使用expire命令会重新设置键的生存时间。其他对键值进行操作的命令（如incr、lpush、hset、zrem）均不会影响键的生存时间。</p>

<p>提示： 如果使用watch命令监测一个拥有生存时间的键，该键时间到期自动删除并不会被watch命令认为该键被改变。</p>

<h3>缓冲</h3>

<p>expire + maxmemory maxmemory-policy(LRU)</p>

<h3>排序</h3>

<p>可以使用multi, zintestore, zrange, del, exec来实现，但太麻烦！</p>

<p>sort命令，可用于集合、列表类型和有序集合类型</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sort key [ALPHA] [BY PREFIXKYE:*-&gt;property] [DESC] [LIMIT offset count] 
</span><span class='line'>
</span><span class='line'>127.0.0.1:6379&gt; lpush mylist 7 1 3 9 0
</span><span class='line'>(integer) 5
</span><span class='line'>127.0.0.1:6379&gt; sort mylist
</span><span class='line'>1) "0"
</span><span class='line'>2) "1"
</span><span class='line'>3) "3"
</span><span class='line'>4) "7"
</span><span class='line'>5) "9"</span></code></pre></td></tr></table></div></figure>


<p>针对有序集合排序时会忽略元素的分数，只针对元素自身的值进行排序。
集合类型中所有元素是无序的，但经常被用于存储对象的ID，很多情况下都是整数。所以redis多这种情况进行了特殊的优化，元素的顺序是有序的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; sadd myset 5 2 6 1 8 1 9 0
</span><span class='line'>(integer) 7
</span><span class='line'>127.0.0.1:6379&gt; smembers myset
</span><span class='line'>1) "0"
</span><span class='line'>2) "1"
</span><span class='line'>3) "2"
</span><span class='line'>4) "5"
</span><span class='line'>5) "6"
</span><span class='line'>6) "8"
</span><span class='line'>7) "9"</span></code></pre></td></tr></table></div></figure>


<p>除了直接对元素排序排序外，还可以通过BY操作来获取关联值来进行排序。BY参数的语法为“BY参考键”，其中参考键可以使字符串类型或者是散列类型键的某个字段（表示为键名->字段名）。如果提供了BY参数，sort命令将不再依据元素自身的值进行排序，而是对每个元素使用元素的值替换参考键中的第一个<code>*</code>并获取取值，然后依据该值对元素排序。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sort tag:ruby:posts BY post:*-&gt;time desc
</span><span class='line'>sort sortbylist BY itemsore:* desc</span></code></pre></td></tr></table></div></figure>


<p>当参考键不包括<code>*</code>时（即常量键名，与元素值无关）。SORT命令将不会执行排序操作，因为redis认为这种情况没有意义（因为所有要比较的值都一样）。没有执行排序操作，在不需要排序但需要借组sort命令获得与元素相关联的数据时，常量键名是很有用的！</p>

<p>如果几个元素的参考键值相同，则SORT命令会在比较元素本身的值来决定元素的顺序。
当某个元素的参考键不存在时，会默认参考键的值为0。
参考键虽然支持散列类型，但是<code>*</code>只能在<code>-&gt;</code>符号前面（即键名部分）才有用，在<code>-&gt;</code>后（即字段名部分）会被当成字段名本身名本身而不会作为占位符被元素的值替换，即常量键名。但是实际运行时会发现一个有趣的结果。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sort sortbylist BY somekey-&gt;somefield:* </span></code></pre></td></tr></table></div></figure>


<p>上面提到了当参考键名是常量键名时SORT命令将不会执行排序操作，然而上例中却是进行了排序，而且只是对元素本身进行排序。这是因为Redis判断参考键名是不是常量键名的方式是判断参考键名中是否包含<code>*</code>，而<code>somekey-&gt;somefield:*</code>中包含<code>*</code>所以不是常量键名。所以在排序的时刻Redis对每个元素都会读取键somekey中的<code>somefield:*</code>字段（<code>*</code>不会被替换）。无论能否获得其值，每个元素的参考键值是相同的，所以redis被按照元素本身的大小排序。</p>

<p>GET参考不影响排序，它的作用是使SORT命令的返回结果不在是元素自身的值。而是GET参数中指定的键值。GET参数的规则和BY参数一样，GET参数也支持字符串类型和散列类型的值，并使用<code>*</code>作为占位符。要实现在排序后直接返回ID对应的违章标题，可以这样写：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; lpush tag:ruby:posts 1 2 3
</span><span class='line'>(integer) 3
</span><span class='line'>127.0.0.1:6379&gt; hmset post:1 time 140801 name HelloWorld
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379&gt; hmset post:2 time 140802 name HelloWorld2
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379&gt; hmset post:3 time 140803 name HelloWorld3
</span><span class='line'>OK
</span><span class='line'>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc
</span><span class='line'>1) "3"
</span><span class='line'>2) "2"
</span><span class='line'>3) "1"
</span><span class='line'>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time DESC GET post:*-&gt;name
</span><span class='line'>1) "HelloWorld3"
</span><span class='line'>2) "HelloWorld2"
</span><span class='line'>3) "HelloWorld"</span></code></pre></td></tr></table></div></figure>


<p>一个sort命令中可以有多个GET参数（而BY参数只能有一个），所以还可以这样用：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc GET post:*-&gt;name GET post:*-&gt;time
</span><span class='line'>1) "HelloWorld3"
</span><span class='line'>2) "140803"
</span><span class='line'>3) "HelloWorld2"
</span><span class='line'>4) "140802"
</span><span class='line'>5) "HelloWorld"
</span><span class='line'>6) "140801"</span></code></pre></td></tr></table></div></figure>


<p>如果还需要返回文章ID，可以使用<code>GET #</code>获得，也就是返回元素本身的值。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc GET post:*-&gt;name GET post:*-&gt;time GET #
</span><span class='line'>1) "HelloWorld3"
</span><span class='line'>2) "140803"
</span><span class='line'>3) "3"
</span><span class='line'>4) "HelloWorld2"
</span><span class='line'>5) "140802"
</span><span class='line'>6) "2"
</span><span class='line'>7) "HelloWorld"
</span><span class='line'>8) "140801"
</span><span class='line'>9) "1"</span></code></pre></td></tr></table></div></figure>


<p>默认情况下SORT会直接返回排序结果，如果希望保存排序结果，可以使用STORE参数。保存后的键的类型为列表类型，如果键已经存在则会覆盖它，加上STORE参数后的SORT命令的返回值的结果的个数。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>127.0.0.1:6379&gt; sort tag:ruby:posts BY post:*-&gt;time desc GET post:*-&gt;name GET post:*-&gt;time GET # STORE tag.ruby.posts.sort
</span><span class='line'>(integer) 9
</span><span class='line'>127.0.0.1:6379&gt; lrange tag.ruby.posts.sort 0 -1
</span><span class='line'>1) "HelloWorld3"
</span><span class='line'>2) "140803"
</span><span class='line'>3) "3"
</span><span class='line'>4) "HelloWorld2"
</span><span class='line'>5) "140802"
</span><span class='line'>6) "2"
</span><span class='line'>7) "HelloWorld"
</span><span class='line'>8) "140801"
</span><span class='line'>9) "1"</span></code></pre></td></tr></table></div></figure>


<p>SORT命令的时间复杂度是O(n+mlogm)，其中n表示要排序的列表（集合或有序集合）中的元素个数，m表示要返回的元素个数。当n较大时SORT命令的性能相对较低，并且redis在排序前会建立一个长度为n的容器来存储排序的元素（当键类型为有序集合且参考键为常量键名时容器大小为m而不是n），虽然是一个临时的过程，但如果同时进行较多的大数据量排序操作则会严重影响性能。</p>

<h3>消息通知</h3>

<p>producer/consumer，松耦合，易于扩展，而且可以分布在不同的服务器中！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>BLPOP key [key ...] timeout
</span><span class='line'>BRPOP key [key ...] timeoutseconds
</span><span class='line'># 超时时间设置为0时，表示不限制等待的时间，即如果没有新元素加入列表就会永远阻塞下去。</span></code></pre></td></tr></table></div></figure>


<p>BRPOP可以同时接收多个键，同时检测多个键，如果所有键都没有元素则阻塞，其中有一个键有元素则会从该键中弹出元素。如果存在键都有元素则从左到右的顺序取第一个键中的一个元素。借此特性可以实现优先级的队列任务。</p>

<p>publish/subscribe模式，发布/订阅模式同样可以实现进程间的消息传递。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>PUBLISH channel.1 hi
</span><span class='line'>SUBSCRIBE channel.1</span></code></pre></td></tr></table></div></figure>


<p>执行SUBSCRIBE命令后，客户端会进入订阅状态，处于此状态下客户端不能使用SUBSCRIBE/UNSUBSCRIBE/PSUBSCRIBE（支持glob风格通配符格式）/PUNSUBSCRIBE这4个属于发布/订阅模式的命令之外的命令，否则会报错。</p>

<p>消息类型： subscribe/message/unsubscribe</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>psubscribe channel.?*</span></code></pre></td></tr></table></div></figure>


<h3>管道pipelining</h3>

<p>在执行多个命令时每条命令都需要等待上一条命令执行完才能执行，即使命令不需要上一条命令的执行结果。通过管道可以一次性发送多条命令并在执行完后一次性将结果返回，当一组命令中每条命令都不依赖与之前命令的执行结果就可以将这一组命令一起通过管道发出。管道通过减少客户端与redis的通信次数来实现降低往返时延。（</p>

<h3>节省空间</h3>

<ul>
<li>精简键名和键值 <code>VIP&lt;-very.important.person</code></li>
<li>内部编码优化（存储和效率的取舍）</li>
</ul>


<p>如果想查看一个键的内部编码方式可以使用<code>OBJECT ENCODING foo</code></p>

<h2>第五章 实践</h2>

<ul>
<li>php用户登录，忘记密码邮件发送队列</li>
<li>ruby自动完成</li>
<li>python在线好友</li>
<li>nodejs的IP段地址查询</li>
</ul>


<h2>第六章 脚本</h2>

<p>代码块多次请求，以及事务竞态等问题，需要用到WATCH，多次请求在网络传输上浪费很多时间。redis的脚本类似于数据库的function，在服务端执行。这种方式不仅代码简单、没有竞态条件（redis的命令都是原子的），而且减少了通过网络发送和接收命令的传输开销。</p>

<p>从2.6开始，允许开发者使用Lua语言编写脚本传到redis中执行。在Lua脚本中可以调用大部分的redis命令。减少网络传输时延，原子操作，复用（发送的脚本永久存储在redis中，其他客户端可以复用）。</p>

<p><strong>访问频率</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>localtimes=redis.call('incr', KEYS[1])
</span><span class='line'>if times==1 then
</span><span class='line'>redis.call('expire', KEYS[1], ARGV[1])
</span><span class='line'>end
</span><span class='line'>
</span><span class='line'>if times&gt;tonumber(ARGV[2]) then
</span><span class='line'>return 0
</span><span class='line'>end
</span><span class='line'>
</span><span class='line'>return 1
</span><span class='line'># redis-cli --eval ratelimiting.lua rate.limiting:127.0.0.1 , 10 3 逗号前的是键，后面的是参数</span></code></pre></td></tr></table></div></figure>


<h3>lua语法（和shell脚本有点像，更简洁）</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>本地变量 local x=10
</span><span class='line'>注释 --xxx
</span><span class='line'>多行注释 --[[xxxx]]
</span><span class='line'>赋值 local a,b=1,2 # a=1, b=2
</span><span class='line'>   local a={1,2,3}
</span><span class='line'>   a[1]=5
</span><span class='line'>数字操作符的操作数如果是字符串会自动转成数字
</span><span class='line'>tonumber
</span><span class='line'>tostring
</span><span class='line'>只要操作数不是nil或者false，逻辑操作符就认为操作数为真，否则为假！
</span><span class='line'>用..来实现字符串连接
</span><span class='line'>取长度 print(#"hello") -- 5
</span></code></pre></td></tr></table></div></figure>


<h3>使用脚本</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>EVAL script numkeys key [key ...] arg [arg ...]
</span><span class='line'>
</span><span class='line'>redis&gt; eval "return redis.call('SET', KEYS[1], ARGV[1])" 1 foo bar
</span><span class='line'>
</span><span class='line'>EVALSHA sha1 numkeys key [key ...] arg [arg ...]</span></code></pre></td></tr></table></div></figure>


<p>同时获取多个散列类型键的键值</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>local result={}
</span><span class='line'>for i,v in ipairs(KEYS) do
</span><span class='line'>result[i]=redis.call("HGETALL", v)
</span><span class='line'>end
</span><span class='line'>return result</span></code></pre></td></tr></table></div></figure>


<p>获取并删除有序集合中分数最小的元素</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>local element=redis.call("ZRANGE", KEY[1], 0, 0)[1]
</span><span class='line'>if element the
</span><span class='line'>redis.call('ZREM', KEYS[1], element)
</span><span class='line'>end
</span><span class='line'>return element</span></code></pre></td></tr></table></div></figure>


<p>处理JSON</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>local sum=0
</span><span class='line'>local users=redis.call('mget', unpack(KEYS))
</span><span class='line'>for _,user in ipairs(users) do 
</span><span class='line'>local courses=cjson.decode(user).course
</span><span class='line'>for _,score in pairs(courses) do
</span><span class='line'>sum=sum+score
</span><span class='line'>end
</span><span class='line'>end
</span><span class='line'>return sum</span></code></pre></td></tr></table></div></figure>


<p>redis脚本禁用使用lua标准库中与文件或系统调用相关的函数，在脚本中只允许对redis的数据进行处理。并且redis还通过禁用脚本的全局变量的方式保证每个脚本都是相对隔离的们不会互相干扰。
使用沙盒不仅是为了保证服务器的安全性，而且还确保了脚本的执行结果值和脚本本身和执行时传递的参数有关，不依赖外界条件（如系统时间、系统中某个文件的内存。。）。这是因为在执行复制和AOF持久化操作时记录的是脚本的内容而不是脚本调用的命令，所以必须保证在脚本内容和参数一样的前提下脚本的执行进行特殊的处理。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>script load 'return 1'
</span><span class='line'>script exists sha1
</span><span class='line'>script flush #清空脚本缓冲
</span><span class='line'>
</span><span class='line'>script kill
</span><span class='line'>script nosave</span></code></pre></td></tr></table></div></figure>


<p>为了限制某个脚本执行时间过长导致redis无法提供服务（如死循环），redis提供了lua-time-limit参数限制脚本的最长运行时间，默认5s。</p>

<h2>第七章 管理</h2>

<ul>
<li>持久化 rdb/AOF</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>save 900 1
</span><span class='line'>save 300 10
</span><span class='line'>save 60 10000
</span><span class='line'>SAVE
</span><span class='line'>BGSAVE
</span><span class='line'>appendonly yes
</span><span class='line'>appendfilename appendonly.aof
</span><span class='line'>auto-aof-rewrite-percentage 100
</span><span class='line'>auto-aof-rewrite-min-size 64mb
</span><span class='line'>BGREWRITEAOF
</span><span class='line'>#appendfsync always
</span><span class='line'>appendfsync everysec
</span><span class='line'>#appendfsync no</span></code></pre></td></tr></table></div></figure>


<ul>
<li>复制</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>redis-server --port 6380 --slaveof 127.0.0.1 6379
</span><span class='line'>SLAVEOF 127.0.0.1 6379
</span><span class='line'>SLAVEOF NO ONE</span></code></pre></td></tr></table></div></figure>


<ul>
<li>读写分离</li>
<li>耗时日志查询</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SLOWLOG GET # slowlog-log-slower-than slowlog-max-len
</span><span class='line'>MONITOR</span></code></pre></td></tr></table></div></figure>


<h2>redis3集群安装cluster</h2>

<p>编译安装和2.8一样，configuration/make/makeinstall即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 cluster-test]$ cat cluster.conf 
</span><span class='line'>port .
</span><span class='line'>cluster-enabled yes
</span><span class='line'>cluster-config-file nodes.conf
</span><span class='line'>cluster-node-timeout 5000
</span><span class='line'>appendonly yes</span></code></pre></td></tr></table></div></figure>


<p>比较苦逼的是需要安装ruby，服务器不能上网！其实ruby在能访问的机器上面安装就可以了！初始化集群的脚本其实就是客户端连接服务端，初始化集群而已。
还有就是在调用命令的时刻要加上<code>-c</code>，这样才是使用集群模式，不然仅仅连单机，读写其他集群服务会报错！</p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVx5-Ab4jaAAA9_Lg7l-I862.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVyXSAC5iEAABfrrHCfuI114.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DB/wKhkA1PVzBSAc3KOAADvQfFIPrs908.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCc-AXZ3EAAHoKZnb1nQ426.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCkWAZYuLAAAWM5VoXJI861.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/04/DF/wKhkA1PWCuSAAuXxAABB-LpH1nQ340.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/05/5F/wKhkA1PZBrSAPaMTAAAcSnjmhXE093.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Win编译32位openjdk]]></title>
    <link href="http://winse.github.io/blog/2014/07/21/build-openjdk/"/>
    <updated>2014-07-21T14:23:57+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/21/build-openjdk</id>
    <content type="html"><![CDATA[<p>win编译32位openjdk7u60:</p>

<div><script src='https://gist.github.com/3e6b42012dfb228a5b02.js?file=OpenJDK-Build-README.md'></script>
<noscript><pre><code>## 编译jdk7u60过程中的注意点: 

0. 先看目录下的README-builds.html，不要太认真看个大概就行
1. 下载最新的Microsoft DirectX SDK，安装时路径不要带括号
2. 安装procps代替free.exe(检查可用内存是会用到，没有应该也可以就是多个警告而已)
3. cygwin-make的版本问题，使用3.81 [snapshot](http://farm6.staticflickr.com/5486/14325549816_da7343282b_o.png)
4. VS2010的cl命令显示的信息一直是中文的话，需要修改源码跳过版本号的检查 [snapshot](http://farm3.staticflickr.com/2928/14162188587_7874083086_o.png)
5. 语言问题，导致编译corba失败！而后面有需要用到这个工程！ [snapshot](http://farm3.staticflickr.com/2936/14347140552_0f29391905_o.png)
6. PATH顺序问题，link.exe和find.exe [snapshot](http://farm6.staticflickr.com/5483/14347108132_9abdac5ae8_o.png)
7. 编译时间有点长！ [snapshot](http://farm4.staticflickr.com/3867/14162097680_40d5f69561_o.png) [java-version](http://farm4.staticflickr.com/3881/14162098450_7e86bd5b0b_o.png)
8. 默认的make不带调试信息的，需要用`make fastdebug_build`。

## 步骤：

1、 下载

可以的话，通过cygwin的setup.exe安装mercurial也行。

* hg: &lt;http://tortoisehg.bitbucket.org/download/index.html&gt;
* source: http://hg.openjdk.java.net/jdk7u/jdk7u60/

```
cd E:
cd git/
mkdir openjdk
cd openjdk/

HG_HOME=/cygdrive/c/Program\ Files/TortoiseHg/
PATH=$PATH:$HG_HOME

hg clone http://hg.openjdk.java.net/jdk7u/jdk7u60/
cd jdk7u60/
ls
./get_source.sh
```

2、 安装依赖软件

* jdk1.7.0_02/apache-ant-1.9.0
* Visual Studio2010
* Cygwin
  * 按照README-builds.html#cygwin，能找的必须安装，找不到的随意。
  * 安装procps，包括了free.exe。
  * 安装binutils，包括了ar.exe。
  * 替换[make.exe](http://www.cmake.org/files/cygwin/make.exe), 添加[cygintl-3.dll](http://www.opendll.com/dll/c/__32-cygintl-3.dll.zip)。
* 下载解压[freetype](http://jaist.dl.sourceforge.net/project/gnuwin32/freetype/2.3.5-1/freetype-2.3.5-1-bin.zip) 
  * 把bin目录下的freetype6.dll文件拷贝到../lib/freetype.dll
  * 添加[zlib1.dll](http://75.duote.org/win_dll/zlib1.zip) 也可以下载[安装版本](http://jaist.dl.sourceforge.net/project/gnuwin32/freetype/2.3.5-1/)包括了zlib1.dll
  * [snapshot](http://farm4.staticflickr.com/3922/14162193337_65d281fc73_o.png)
* 下载安装[Microsoft DirectX](http://download.microsoft.com/download/F/1/7/F178BCE4-FA19-428F-BB60-F3DEE1130BFA/DXSDK_Feb10.exe)
  * 安装路径不要带括号 

3、 配置环境

* 切换为英文语言环境，当你重新启动看到的是【Welcome】的时刻说明你修改成功了！重启后记得make clean再进行后面的操作！
 * Windows7安装更新，添加英文语言包
* 环境变量的所有路径最好是/ 而不是\，不能带双引号（否则中间编译的时刻会遇到问题）[snapshot](http://farm4.staticflickr.com/3862/14162030769_8766efa9a5_o.png)
* PATH路径顺序的问题，cygwin/bin放在vs的后面，但需要放在windows的前面。link.exe和find.exe的问题

如果是在X64机器上编译，需要加ARCH_DATA_MODEL的参数。参见【README-builds.html#creating】

## 参考：

* 【膜拜】[openjdk windows 编译](http://blog.csdn.net/instruder/article/details/8834117)
* 【有点老，不过注意事项还是相同的】 [自己动手编译Windows版的OpenJDK 7](http://icyfenix.iteye.com/blog/1097344) 
* 【linux下安装】&lt;http://khotyn.iteye.com/blog/1225348&gt;
* 【cl版本问题，以及make程序问题】&lt;http://www.myexception.cn/program/779678.html&gt;
* 【emitPermissionCheck问题】&lt;http://mail.openjdk.java.net/pipermail/jdk6-dev/2013-November/003104.html&gt; &lt;http://comments.gmane.org/gmane.comp.java.openjdk.jdk6.devel/976&gt;</code></pre></noscript></div>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install Ganglia on Redhat5+]]></title>
    <link href="http://winse.github.io/blog/2014/07/18/install-ganglia-on-redhat/"/>
    <updated>2014-07-18T14:53:44+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/18/install-ganglia-on-redhat</id>
    <content type="html"><![CDATA[<p>对使用C写的复杂的程序安装心里有阴影，还有本来可以上网的话，使用yum安装会省很多的事情。
但是没办法，环境是这样，正式环境没有提供网络环境，搭建的本地yum环境也不知道行不行。</p>

<p>上次在自己电脑的虚拟机上面成功安装过ganglia，但apache、rrdtool依赖使用yum安装的，安装过程比较揪心。把ganglia安装到正式环境就不了了之的。
上个星期生产环境出现了用户查询数据久久不能返回的问题，由于查询程序写的比较差的缘故。但同时也给自己敲了警钟，都不知道集群机器运行情况，终究是大隐患；安装后监测集群同时为以后程序的调优工作带来便利。</p>

<p>本次安装全部使用源码包安装，有部分lib有重复编译。</p>

<p>总结下，原来安装ganglia就仅是按照网络的步骤一步步的弄，同时各个程序的版本又有可能不一致，每一步都胆战心惊！没有重点重心，以至于浪费了很多的事情。
分步骤有条不紊的操作就可以踏实多了，安装ganglia主要涉及三个核心部分(安装程序包<a href="http://yunpan.cn/QCFUiuyAWSyZI">下载</a>（提取码：0ec4）)：</p>

<ul>
<li>rrdtool</li>
<li>gmetad / gmond</li>
<li>apache / web</li>
<li>集群子节点部署</li>
<li>配置hadoop metrics监控hadoop集群</li>
</ul>


<p>按照顺序一个个的安装就可以了。无需为一个个依赖的版本不一致问题而忧心，同时可以更好的参考网络上的实践。</p>

<h2>安装rrdtool</h2>

<p>推荐按照<a href="http://oss.oetiker.ch/rrdtool/doc/rrdbuild.en.html#IBUILDING_DEPENDENCIES">官网教程</a>步骤操作，很&amp;非常的详细。
教程中环境变量必须得设置！这个很重点！</p>

<p>下面是安装rrdtool过程中用到的软件，列出的顺序即为安装的次序：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 rrdbuild]$ ll -tr | grep -v 'tar'
</span><span class='line'>总计 24132
</span><span class='line'>drwxrwxrwx  6   1000          1000    4096 07-17 12:12 pkg-config-0.23
</span><span class='line'>drwxr-xr-x 11 hadoop            80    4096 07-17 12:28 zlib-1.2.3
</span><span class='line'>drwxr-xr-x  7   1004 avahi-autoipd    4096 07-17 12:29 libpng-1.2.18
</span><span class='line'>drwxr-xr-x  8   1000 users            4096 07-17 12:31 freetype-2.3.5
</span><span class='line'>drwxrwxrwx 15  50138 vcsa            12288 07-17 16:37 libxml2-2.6.32
</span><span class='line'>drwxrwxrwx 15   1488 users            4096 07-17 16:53 fontconfig-2.4.2
</span><span class='line'>drwxrwxrwx  4 sjyw   sjyw             4096 07-17 16:56 pixman-0.10.0
</span><span class='line'>drwxrwsrwx  8   1000 ftp              4096 07-17 16:59 cairo-1.6.4
</span><span class='line'>drwxrwxrwx 12 sjyw   sjyw             4096 07-17 17:01 glib-2.15.4
</span><span class='line'>drwxrwxrwx  9 sjyw   sjyw             4096 07-17 17:16 pango-1.21.1
</span><span class='line'>drwxr-xr-x 11   1003          1001    4096 07-17 17:36 rrdtool-1.4.8</span></code></pre></td></tr></table></div></figure>


<p>具体操作的步骤（原来包括操作步骤，发现太累赘了重新调整了一下）：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 下面几个环境变量时基础！
</span><span class='line'>BUILD_DIR=/home/ganglia/rrdbuild
</span><span class='line'>INSTALL_DIR=/opt/rrdtool-1.4.8
</span><span class='line'>
</span><span class='line'>export PKG_CONFIG_PATH=${INSTALL_DIR}/lib/pkgconfig
</span><span class='line'>export PATH=$INSTALL_DIR/bin:$PATH
</span><span class='line'>
</span><span class='line'>export LDFLAGS="-Wl,--rpath -Wl,${INSTALL_DIR}/lib" 
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 rrdbuild]# tar zxvf pkg-config-0.23.tar.gz 
</span><span class='line'>[root@umcc97-44 rrdbuild]# cd pkg-config-0.23
</span><span class='line'>[root@umcc97-44 pkg-config-0.23]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
</span><span class='line'>[root@umcc97-44 pkg-config-0.23]# make && make install
</span><span class='line'>
</span><span class='line'># 这个环境变量也很重要
</span><span class='line'>[root@umcc97-44 pkg-config-0.23]# export PKG_CONFIG=$INSTALL_DIR/bin/pkg-config
</span><span class='line'>[root@umcc97-44 pkg-config-0.23]# cd ..
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 rrdbuild]# tar zxvf zlib-1.2.3.tar.gz 
</span><span class='line'>[root@umcc97-44 rrdbuild]# cd zlib-1.2.3
</span><span class='line'># 修改了下官网的命令; 64位问题 recompile with -fPIC
</span><span class='line'>[root@umcc97-44 zlib-1.2.3]# CFLAGS="-O3 -fPIC" ./configure
</span><span class='line'>[root@umcc97-44 zlib-1.2.3]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 rrdbuild]# tar zxvf libpng-1.2.18.tar.gz 
</span><span class='line'>[root@umcc97-44 rrdbuild]# cd libpng-1.2.18
</span><span class='line'>[root@umcc97-44 zlib-1.2.3]# cd ../libpng-1.2.18
</span><span class='line'>[root@umcc97-44 libpng-1.2.18]# env CFLAGS="-O3 -fPIC" ./configure --prefix=$INSTALL_DIR
</span><span class='line'>[root@umcc97-44 libpng-1.2.18]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 libpng-1.2.18]# cd ..
</span><span class='line'>[root@umcc97-44 rrdbuild]# tar zxvf freetype-2.3.5.tar.gz 
</span><span class='line'>[root@umcc97-44 rrdbuild]# cd freetype-2.3.5
</span><span class='line'>[root@umcc97-44 freetype-2.3.5]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
</span><span class='line'>[root@umcc97-44 freetype-2.3.5]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 rrdbuild]# tar zxvf libxml2-2.6.32.tar.gz 
</span><span class='line'>[root@umcc97-44 rrdbuild]# cd libxml2-2.6.32
</span><span class='line'>[root@umcc97-44 libxml2-2.6.32]#  ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
</span><span class='line'>[root@umcc97-44 libxml2-2.6.32]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 libxml2-2.6.32]# cd ..
</span><span class='line'>[root@umcc97-44 rrdbuild]# tar zxvf fontconfig-2.4.2.tar.gz 
</span><span class='line'>[root@umcc97-44 rrdbuild]# cd fontconfig-2.4.2
</span><span class='line'>[root@umcc97-44 fontconfig-2.4.2]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC" --with-freetype-config=$INSTALL_DIR/bin/freetype-config
</span><span class='line'>[root@umcc97-44 fontconfig-2.4.2]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 fontconfig-2.4.2]# cd ..
</span><span class='line'>[root@umcc97-44 rrdbuild]# cd pixman-0.10.0
</span><span class='line'>[root@umcc97-44 pixman-0.10.0]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
</span><span class='line'>[root@umcc97-44 pixman-0.10.0]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 pixman-0.10.0]# cd ../cairo-1.6.4
</span><span class='line'>[root@umcc97-44 cairo-1.6.4]# ./configure --prefix=$INSTALL_DIR \
</span><span class='line'>&gt;     --enable-xlib=no \
</span><span class='line'>&gt;     --enable-xlib-render=no \
</span><span class='line'>&gt;     --enable-win32=no \
</span><span class='line'>&gt;     CFLAGS="-O3 -fPIC"
</span><span class='line'>[root@umcc97-44 cairo-1.6.4]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 cairo-1.6.4]# cd ..
</span><span class='line'>[root@umcc97-44 rrdbuild]# tar zxvf glib-2.15.4.tar.gz 
</span><span class='line'>[root@umcc97-44 rrdbuild]# cd glib-2.15.4
</span><span class='line'>[root@umcc97-44 glib-2.15.4]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC"
</span><span class='line'>[root@umcc97-44 glib-2.15.4]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 rrdbuild]# bunzip2 -c pango-1.21.1.tar.bz2 | tar xf -
</span><span class='line'>[root@umcc97-44 rrdbuild]# ll
</span><span class='line'>[root@umcc97-44 rrdbuild]# cd pango-1.21.1
</span><span class='line'>[root@umcc97-44 pango-1.21.1]# ./configure --prefix=$INSTALL_DIR CFLAGS="-O3 -fPIC" --without-x
</span><span class='line'>[root@umcc97-44 pango-1.21.1]# export PATH=$INSTALL_DIR/bin:$PATH
</span><span class='line'>[root@umcc97-44 pango-1.21.1]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 rrdbuild]# cd rrdtool-1.4.8/
</span><span class='line'>[root@umcc97-44 rrdtool-1.4.8]#  ./configure --prefix=$INSTALL_DIR --disable-tcl --disable-python
</span><span class='line'>[root@umcc97-44 rrdtool-1.4.8]# make clean
</span><span class='line'>[root@umcc97-44 rrdtool-1.4.8]# make 
</span><span class='line'>[root@umcc97-44 rrdtool-1.4.8]# make install
</span><span class='line'>   
</span><span class='line'>## 安装完后，搞个例子玩玩   
</span><span class='line'>[root@umcc97-44 rrdtool-1.4.8]# cd /opt/rrdtool-1.4.8/share/rrdtool/examples/
</span><span class='line'>[root@umcc97-44 examples]# ll
</span><span class='line'>[root@umcc97-44 examples]# ./4charts.pl 
</span><span class='line'>This script has created 4charts.png in the current directory
</span><span class='line'>This demonstrates the use of the TIME and % RPN operators
</span><span class='line'># 运行完后，会在当前目录生成不同尺寸的png的图片
</span><span class='line'> 
</span><span class='line'>[hadoop@umcc97-44 ~]$ /opt/rrdtool-1.4.8/bin/rrdtool -v
</span><span class='line'>RRDtool 1.4.8  Copyright 1997-2013 by Tobias Oetiker &lt;tobi@oetiker.ch&gt;
</span><span class='line'>               Compiled Jul 17 2014 17:37:58
</span><span class='line'>
</span><span class='line'>Usage: rrdtool [options] command command_options
</span><span class='line'>Valid commands: create, update, updatev, graph, graphv,  dump, restore,
</span><span class='line'>      last, lastupdate, first, info, fetch, tune,
</span><span class='line'>      resize, xport, flushcached
</span><span class='line'>
</span><span class='line'>RRDtool is distributed under the Terms of the GNU General
</span><span class='line'>Public License Version 2. (www.gnu.org/copyleft/gpl.html)
</span><span class='line'>
</span><span class='line'>For more information read the RRD manpages</span></code></pre></td></tr></table></div></figure>


<p>到这里rrd安装好，遇到zlib的CFLAGS变量设置的问题，以及终端断了必须重新设置<strong>环境变量</strong>两个大点的问题！其他如果按照官网的顺序安装基本顺顺利利了。</p>

<p>同时认识到了pkg，其实类似于java的jar嘛，依赖包不一定非要安装在系统的默认位置，自己管理也是一种简单易行的方式。接下来安装gmetad/gmond也使用这样方式，为后面部署gmond带来便利：所有依赖的包都放在一个目录下嘛！
接下来ganglia程序。</p>

<h2>gmetad安装</h2>

<p>需要用到的软件包：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./gangliabuild/ganglia-web-3.5.12
</span><span class='line'>./gangliabuild/apr-1.5.1
</span><span class='line'>./gangliabuild/apr-util-1.5.3
</span><span class='line'>./gangliabuild/confuse-2.7
</span><span class='line'>./gangliabuild/expat-2.0.1
</span><span class='line'>./gangliabuild/ganglia-3.6.0</span></code></pre></td></tr></table></div></figure>


<p>整个安装过程，除了make的时刻rrd的库找不到的问题（通过LD_LIBRARY_PATH解决），其他都安装的很顺。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 把下载来的tar全部解压
</span><span class='line'>[root@umcc97-44 gangliabuild]# find . -name "*.tar.gz" -exec tar zxvf {} \;
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 gangliabuild]# cd expat-2.0.1
</span><span class='line'>[root@umcc97-44 expat-2.0.1]# INSTALL_DIR=/opt/ganglia
</span><span class='line'>[root@umcc97-44 expat-2.0.1]# ./configure --prefix=$INSTALL_DIR 
</span><span class='line'>[root@umcc97-44 expat-2.0.1]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 expat-2.0.1]# cd ../apr-1.5.1
</span><span class='line'>[root@umcc97-44 apr-1.5.1]# ./configure --prefix=$INSTALL_DIR 
</span><span class='line'>[root@umcc97-44 apr-1.5.1]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 apr-1.5.1]# cd ../apr-util-1.5.3
</span><span class='line'>[root@umcc97-44 apr-util-1.5.3]# ./configure --with-apr=/opt/ganglia --with-expat=/opt/ganglia --prefix=$INSTALL_DIR 
</span><span class='line'>[root@umcc97-44 apr-util-1.5.3]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 apr-util-1.5.3]# cd ../confuse-2.7
</span><span class='line'>[root@umcc97-44 confuse-2.7]# ./configure CFLAGS=-fPIC --disable-nls --prefix=$INSTALL_DIR 
</span><span class='line'>[root@umcc97-44 confuse-2.7]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 confuse-2.7]# cd ../ganglia-3.6.0
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# export LDFLAGS="-Wl,--rpath -Wl,${INSTALL_DIR}/lib" 
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# export PKG_CONFIG_PATH=${INSTALL_DIR}/lib/pkgconfig
</span><span class='line'># 注意sysconfdir，运行程序配置所在的目录
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# ./configure --prefix=$INSTALL_DIR --with-librrd=/opt/rrdtool-1.4.8 --with-libexpat=/opt/ganglia --with-libconfuse=/opt/ganglia --with-libpcre=no  --with-gmetad --enable-gexec --enable-status -sysconfdir=/etc/ganglia
</span><span class='line'>...
</span><span class='line'>Welcome to..
</span><span class='line'>     ______                  ___
</span><span class='line'>    / ____/___ _____  ____ _/ (_)___ _
</span><span class='line'>   / / __/ __ `/ __ \/ __ `/ / / __ `/
</span><span class='line'>  / /_/ / /_/ / / / / /_/ / / / /_/ /
</span><span class='line'>  \____/\__,_/_/ /_/\__, /_/_/\__,_/
</span><span class='line'>                   /____/
</span><span class='line'>
</span><span class='line'>Copyright (c) 2005 University of California, Berkeley
</span><span class='line'>
</span><span class='line'>Version: 3.6.0
</span><span class='line'>Library: Release 3.6.0 0:0:0
</span><span class='line'>
</span><span class='line'>Type "make" to compile.
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# 
</span><span class='line'># 设置rrd的LIB路径
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# export LD_LIBRARY_PATH=/opt/rrdtool-1.4.8/lib
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# make
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# make install</span></code></pre></td></tr></table></div></figure>


<p>接下来是配置gmetad</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@umcc97-44 ganglia-3.6.0]#  cd gmetad
</span><span class='line'>[root@umcc97-44 gmetad]# cp gmetad.init /etc/init.d/gmetad
</span><span class='line'>[root@umcc97-44 gmetad]# chkconfig gmetad on
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 gmetad]# chkconfig --list gmetad
</span><span class='line'>gmetad            0:off   1:off   2:on    3:on    4:on    5:on    6:off
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 gmetad]# mkdir -p /var/lib/ganglia/rrds
</span><span class='line'>[root@umcc97-44 gmetad]# chown nobody:nobody /var/lib/ganglia/rrds
</span><span class='line'>[root@umcc97-44 gmetad]# 
</span><span class='line'># 没有启动起来，程序的路径不对
</span><span class='line'>[root@umcc97-44 gmetad]# service gmetad start
</span><span class='line'>Starting GANGLIA gmetad: 
</span><span class='line'>[root@umcc97-44 gmetad]# 
</span><span class='line'>[root@umcc97-44 gmetad]# ln -s /opt/ganglia/sbin/gmetad /usr/sbin/gmetad
</span><span class='line'>[root@umcc97-44 gmetad]# service gmetad start
</span><span class='line'>Starting GANGLIA gmetad: [  OK  ]
</span><span class='line'>
</span><span class='line'># 配置
</span><span class='line'>[root@umcc97-44 gmetad]# cp gmetad.conf /etc/ganglia/gmetad.conf
</span><span class='line'>[root@umcc97-44 gmetad]# vi /etc/ganglia/gmetad.conf 
</span><span class='line'> datasource "hadoop" localhost
</span><span class='line'> rrd_rootdir "/var/lib/ganglia/rrds"
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 gmetad]# service gmetad restart
</span><span class='line'>Shutting down GANGLIA gmetad: [  OK  ]
</span><span class='line'>Starting GANGLIA gmetad: [  OK  ]
</span><span class='line'>
</span><span class='line'># 测试下
</span><span class='line'>[root@umcc97-44 gmetad]# telnet localhost 8651</span></code></pre></td></tr></table></div></figure>


<h2>gmond安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@umcc97-44 gmetad]# pwd
</span><span class='line'>/home/ganglia/gangliabuild/ganglia-3.6.0/gmetad
</span><span class='line'>[root@umcc97-44 gmetad]# cd ..
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# ./configure --prefix=$INSTALL_DIR  --with-libpcre=no
</span><span class='line'>...
</span><span class='line'>Welcome to..
</span><span class='line'>     ______                  ___
</span><span class='line'>    / ____/___ _____  ____ _/ (_)___ _
</span><span class='line'>   / / __/ __ `/ __ \/ __ `/ / / __ `/
</span><span class='line'>  / /_/ / /_/ / / / / /_/ / / / /_/ /
</span><span class='line'>  \____/\__,_/_/ /_/\__, /_/_/\__,_/
</span><span class='line'>                   /____/
</span><span class='line'>
</span><span class='line'>Copyright (c) 2005 University of California, Berkeley
</span><span class='line'>
</span><span class='line'>Version: 3.6.0
</span><span class='line'>Library: Release 3.6.0 0:0:0
</span><span class='line'>
</span><span class='line'>Type "make" to compile.
</span><span class='line'>
</span><span class='line'># 尽管检查通过了，但是make会报错
</span><span class='line'># 需要指定lib包位置
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# ./configure --prefix=$INSTALL_DIR  --with-libpcre=no  --with-libexpat=/opt/ganglia --with-libconfuse=/opt/ganglia -sysconfdir=/etc/ganglia
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# cd gmond/
</span><span class='line'>[root@umcc97-44 gmond]# ./gmond -t &gt; /etc/ganglia/gmond.conf
</span><span class='line'>
</span><span class='line'># 和gmetad一样，需要把路径把程序做个软连接
</span><span class='line'>[root@umcc97-44 gmond]# cat gmond.init
</span><span class='line'>  #!/bin/sh
</span><span class='line'>  #
</span><span class='line'>  # chkconfig: 2345 70 40
</span><span class='line'>  # description: gmond startup script
</span><span class='line'>  #
</span><span class='line'>  GMOND=/usr/sbin/gmond
</span><span class='line'>
</span><span class='line'>...
</span><span class='line'>[root@umcc97-44 gmond]# ln -s /opt/ganglia/sbin/gmond /usr/sbin/gmond
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 gmond]# cp gmond.init /etc/init.d/gmond
</span><span class='line'>[root@umcc97-44 gmond]# chkconfig --add gmond
</span><span class='line'>[root@umcc97-44 gmond]# chkconfig --list gmond
</span><span class='line'>gmond             0:off   1:off   2:on    3:on    4:on    5:on    6:off
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# vi /etc/ganglia/gmond.conf 
</span><span class='line'> cluster-name
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# service gmond start
</span><span class='line'>Starting GANGLIA gmond: [  OK  ]
</span><span class='line'>
</span><span class='line'># 测试下
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# telnet localhost 8649</span></code></pre></td></tr></table></div></figure>


<p>查看运行情况：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@umcc97-44 ganglia-3.6.0]# ldconfig -v
</span><span class='line'>[root@umcc97-44 ganglia-3.6.0]# /opt/ganglia/bin/gstat -a</span></code></pre></td></tr></table></div></figure>


<h2>apache和php环境安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@umcc97-44 webbuild]# tar zxvf httpd-2.4.9.tar.gz 
</span><span class='line'>[root@umcc97-44 webbuild]# cd httpd-2.4.9
</span><span class='line'>[root@umcc97-44 httpd-2.4.9]# ./configure -with-enable-so -sysconfdir=/etc/httpd
</span><span class='line'>...
</span><span class='line'>checking for APR... no
</span><span class='line'>configure: error: APR not found.  Please read the documentation.
</span><span class='line'>
</span><span class='line'># 前面安装ganglia时也安装过APR但是安装的目录指定的，混用不是很好。查看官方安装2.4的安装文档，可以直接把apr放到srclib下，编译时会同时编译这些依赖
</span><span class='line'>[root@umcc97-44 httpd-2.4.9]# cd srclib/
</span><span class='line'>[root@umcc97-44 srclib]# cp -r /home/ganglia/gangliabuild/apr-1.5.1 ./
</span><span class='line'>[root@umcc97-44 srclib]# cp -r /home/ganglia/gangliabuild/apr-util-1.5.3 ./
</span><span class='line'>[root@umcc97-44 srclib]# mv apr-1.5.1 apr
</span><span class='line'>[root@umcc97-44 srclib]# mv apr-util-1.5.3 apr-util
</span><span class='line'>[root@umcc97-44 srclib]# ll
</span><span class='line'>[root@umcc97-44 srclib]# cd ..
</span><span class='line'>[root@umcc97-44 httpd-2.4.9]#  cd ../
</span><span class='line'>[root@umcc97-44 webbuild]# tar zxvf pcre-8.35.tar.gz 
</span><span class='line'># 正则表达式的包，这里安装默认位置
</span><span class='line'>[root@umcc97-44 webbuild]# cd pcre-8.35
</span><span class='line'>[root@umcc97-44 pcre-8.35]# ./configure 
</span><span class='line'>[root@umcc97-44 pcre-8.35]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 pcre-8.35]# cd ../httpd-2.4.9
</span><span class='line'>[root@umcc97-44 httpd-2.4.9]# ./configure --with-included-apr -with-enable-so -sysconfdir=/etc/httpd
</span><span class='line'>[root@umcc97-44 httpd-2.4.9]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 httpd-2.4.9]# cd /usr/local/apache2/
</span><span class='line'>[root@umcc97-44 apache2]# cd /etc/httpd
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 httpd]# cd /home/ganglia/webbuild/
</span><span class='line'>[root@umcc97-44 webbuild]# tar zxvf php-5.5.14\ \(2\).tar.gz 
</span><span class='line'>[root@umcc97-44 webbuild]# cd php-5.5.14
</span><span class='line'># 用了安装rrd时的libxml
</span><span class='line'>[root@umcc97-44 php-5.5.14]# ./configure -with-apxs2=/usr/local/apache2/bin/apxs --with-libxml-dir=/opt/rrdtool-1.4.8/ -sysconfdir=/etc -with-config-file-path=/etc -with-config-file-scan-dir=/usr/etc/php.d -with-zlib
</span><span class='line'>[root@umcc97-44 php-5.5.14]# make && make install
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 php-5.5.14]#  
</span><span class='line'>[root@umcc97-44 php-5.5.14]#  vi /etc/httpd/httpd.conf
</span><span class='line'>
</span><span class='line'>  LoadModule php5_module        modules/libphp5.so #这个安装php后自动加上了
</span><span class='line'>
</span><span class='line'>  DocumentRoot "/var/www/html"
</span><span class='line'>  &lt;Directory "/var/www/html"&gt;
</span><span class='line'>
</span><span class='line'>  AddType application/x-httpd-php .php
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 php-5.5.14]# /usr/local/apache2/bin/apachectl start
</span><span class='line'>AH00526: Syntax error on line 215 of /etc/httpd/httpd.conf:
</span><span class='line'>DocumentRoot must be a directory
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 php-5.5.14]# mkdir -p /var/www/html
</span><span class='line'>[root@umcc97-44 php-5.5.14]# /usr/local/apache2/bin/apachectl start
</span><span class='line'>AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.18.97.44. Set the 'ServerName' directive globally to suppress this message
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 php-5.5.14]#  vi /etc/httpd/httpd.conf
</span><span class='line'>  ServerName
</span><span class='line'>[root@umcc97-44 php-5.5.14]#/usr/local/apache2/bin/apachectl start
</span><span class='line'>httpd (pid 31416) already running
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 php-5.5.14]# cp /usr/local/apache2/bin/apachectl /etc/init.d/httpd
</span><span class='line'>[root@umcc97-44 php-5.5.14]# chkconfig --add httpd
</span><span class='line'>service httpd does not support chkconfig
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 ~]# vi /etc/init.d/httpd 
</span><span class='line'> #chkconfig: 2345 10 90
</span><span class='line'> #description: Activates/Deactivates Apache Web Server
</span><span class='line'> 
</span><span class='line'>[root@umcc97-44 ~]# service httpd start
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 ~]# cd /var/www/html/
</span><span class='line'>[root@umcc97-44 ~]# vi index.php
</span><span class='line'># http://umcc97-44 浏览器查看下结果
</span><span class='line'>
</span><span class='line'># /usr/local/apache2/bin/apachectl -k stop
</span><span class='line'>[root@umcc97-44 ganglia-web]# service httpd -k stop   
</span><span class='line'># 等apache结束
</span><span class='line'>[root@umcc97-44 ganglia-web]# tail -f /usr/local/apache2/logs/error_log </span></code></pre></td></tr></table></div></figure>


<p>部署ganglia-web：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@umcc97-44 ~]# cd /home/ganglia/gangliabuild/ganglia-web-3.5.12
</span><span class='line'>[root@umcc97-44 ganglia-web-3.5.12]# ls
</span><span class='line'>[root@umcc97-44 ganglia-web-3.5.12]# make install
</span><span class='line'>rsync --exclude "rpmbuild" --exclude "*.gz" --exclude "Makefile" --exclude "*debian*" --exclude "ganglia-web-3.5.12" --exclude ".git*" --exclude "*.in" --exclude "*~" --exclude "#*#" --exclude "ganglia-web.spec" --exclude "apache.conf" -a . ganglia-web-3.5.12
</span><span class='line'>mkdir -p //var/lib/ganglia-web/dwoo/compiled && \
</span><span class='line'>  mkdir -p //var/lib/ganglia-web/dwoo/cache && \
</span><span class='line'>  mkdir -p //var/lib/ganglia-web && \
</span><span class='line'>  rsync -a ganglia-web-3.5.12/conf //var/lib/ganglia-web && \
</span><span class='line'>  mkdir -p //usr/share/ganglia-webfrontend && \
</span><span class='line'>  rsync --exclude "conf" -a ganglia-web-3.5.12/* //usr/share/ganglia-webfrontend && \
</span><span class='line'>  chown -R root:root //var/lib/ganglia-web
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 ganglia-web-3.5.12]# mv /usr/share/ganglia-webfrontend /var/www/html/ganglia
</span><span class='line'>[root@umcc97-44 ganglia-web-3.5.12]# cd /var/www/html/ganglia/    
</span><span class='line'>
</span><span class='line'># 修改配置，在安装完gmetad后有新建/var/lib/ganglia/rrds其实和conf中的配置是一致的
</span><span class='line'>[root@umcc97-44 ganglia]# cp conf_default.php conf.php    
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 ganglia]# cd /var/lib/ganglia-web/
</span><span class='line'>[root@umcc97-44 ganglia-web]# cd dwoo/
</span><span class='line'>[root@umcc97-44 dwoo]# ll
</span><span class='line'>total 8
</span><span class='line'>drwxr-xr-x 2 root root 4096 Jul 17 21:34 cache
</span><span class='line'>drwxr-xr-x 2 root root 4096 Jul 17 21:34 compiled
</span><span class='line'>[root@umcc97-44 dwoo]# chmod 777 *    
</span><span class='line'># http://umcc97-44/ganglia</span></code></pre></td></tr></table></div></figure>


<p>部署gmond到其他集群节点</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>[root@umcc97-44 opt]# cat /etc/init.d/gmond 
</span><span class='line'>  #!/bin/sh
</span><span class='line'>  #
</span><span class='line'>  # chkconfig: 2345 70 40
</span><span class='line'>  # description: gmond startup script
</span><span class='line'>  #
</span><span class='line'>  GMOND=/usr/sbin/gmond   
</span><span class='line'>
</span><span class='line'>[root@umcc97-44 opt]# vi /etc/ganglia/gmetad.conf     
</span><span class='line'> data_source
</span><span class='line'> # 重启gmetad
</span><span class='line'>[root@umcc97-44 opt]# ssh-copy-id -i ~/.ssh/id_rsa.pub umcc97-144
</span><span class='line'>[root@umcc97-44 opt]# scp /etc/init.d/gmond umcc97-144:/etc/init.d/
</span><span class='line'>[root@umcc97-44 opt]# ssh umcc97-144 'mkdir /etc/ganglia' 
</span><span class='line'>[root@umcc97-44 opt]# scp /etc/ganglia/gmond.conf  umcc97-144:/etc/ganglia/
</span><span class='line'>[root@umcc97-44 opt]# rsync -vaz ganglia umcc97-144:/opt/
</span><span class='line'>[root@umcc97-44 opt]# ssh umcc97-144
</span><span class='line'>Last login: Tue Jun 10 12:08:47 2014
</span><span class='line'>
</span><span class='line'>[root@umcc97-144 ~]# ln -s /opt/ganglia/sbin/gmond /usr/sbin/gmond
</span><span class='line'>[root@umcc97-144 ~]# chkconfig --add gmond
</span><span class='line'>[root@umcc97-144 ~]# service gmond start
</span><span class='line'>Starting GANGLIA gmond: [  OK  ]
</span><span class='line'>
</span><span class='line'>[root@umcc97-144 ~]# </span></code></pre></td></tr></table></div></figure>


<h2>Hadoop/Hbase Metrics配置</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 ~]$ cat hadoop-2.2.0/etc/hadoop/hadoop-metrics*
</span><span class='line'>#
</span><span class='line'>#   Licensed to the Apache Software Foundation (ASF) under one or more
</span><span class='line'>#   contributor license agreements.  See the NOTICE file distributed with
</span><span class='line'>#   this work for additional information regarding copyright ownership.
</span><span class='line'>#   The ASF licenses this file to You under the Apache License, Version 2.0
</span><span class='line'>#   (the "License"); you may not use this file except in compliance with
</span><span class='line'>#   the License.  You may obtain a copy of the License at
</span><span class='line'>#
</span><span class='line'>#       http://www.apache.org/licenses/LICENSE-2.0
</span><span class='line'>#
</span><span class='line'>#   Unless required by applicable law or agreed to in writing, software
</span><span class='line'>#   distributed under the License is distributed on an "AS IS" BASIS,
</span><span class='line'>#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
</span><span class='line'>#   See the License for the specific language governing permissions and
</span><span class='line'>#   limitations under the License.
</span><span class='line'>#
</span><span class='line'>
</span><span class='line'># syntax: [prefix].[source|sink].[instance].[options]
</span><span class='line'># See javadoc of package-info.java for org.apache.hadoop.metrics2 for details
</span><span class='line'>
</span><span class='line'># @changed
</span><span class='line'>#*.sink.file.class=org.apache.hadoop.metrics2.sink.FileSink
</span><span class='line'># default sampling period, in seconds
</span><span class='line'>#*.period=10
</span><span class='line'>
</span><span class='line'># The namenode-metrics.out will contain metrics from all context
</span><span class='line'>#namenode.sink.file.filename=namenode-metrics.out
</span><span class='line'># Specifying a special sampling period for namenode:
</span><span class='line'>#namenode.sink.*.period=8
</span><span class='line'>
</span><span class='line'>#datanode.sink.file.filename=datanode-metrics.out
</span><span class='line'>
</span><span class='line'># the following example split metrics of different
</span><span class='line'># context to different sinks (in this case files)
</span><span class='line'>#jobtracker.sink.file_jvm.context=jvm
</span><span class='line'>#jobtracker.sink.file_jvm.filename=jobtracker-jvm-metrics.out
</span><span class='line'>#jobtracker.sink.file_mapred.context=mapred
</span><span class='line'>#jobtracker.sink.file_mapred.filename=jobtracker-mapred-metrics.out
</span><span class='line'>
</span><span class='line'>#tasktracker.sink.file.filename=tasktracker-metrics.out
</span><span class='line'>
</span><span class='line'>#maptask.sink.file.filename=maptask-metrics.out
</span><span class='line'>
</span><span class='line'>#reducetask.sink.file.filename=reducetask-metrics.out
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31
</span><span class='line'>*.sink.ganglia.period=10
</span><span class='line'>
</span><span class='line'>*.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both
</span><span class='line'>*.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40
</span><span class='line'>
</span><span class='line'>namenode.sink.ganglia.servers=umcc97-44:8649
</span><span class='line'>resourcemanager.sink.ganglia.servers=umcc97-44:8649
</span><span class='line'>
</span><span class='line'>datanode.sink.ganglia.servers=umcc97-44:8649
</span><span class='line'>nodemanager.sink.ganglia.servers=umcc97-44:8649
</span><span class='line'>
</span><span class='line'>maptask.sink.ganglia.servers=umcc97-44:8649
</span><span class='line'>reducetask.sink.ganglia.servers=umcc97-44:8649
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># Configuration of the "dfs" context for null
</span><span class='line'>dfs.class=org.apache.hadoop.metrics.spi.NullContext
</span><span class='line'>
</span><span class='line'># Configuration of the "dfs" context for file
</span><span class='line'>#dfs.class=org.apache.hadoop.metrics.file.FileContext
</span><span class='line'>#dfs.period=10
</span><span class='line'>#dfs.fileName=/tmp/dfsmetrics.log
</span><span class='line'>
</span><span class='line'># Configuration of the "dfs" context for ganglia
</span><span class='line'># Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
</span><span class='line'># dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext
</span><span class='line'># dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
</span><span class='line'># dfs.period=10
</span><span class='line'># dfs.servers=localhost:8649
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># Configuration of the "mapred" context for null
</span><span class='line'>mapred.class=org.apache.hadoop.metrics.spi.NullContext
</span><span class='line'>
</span><span class='line'># Configuration of the "mapred" context for file
</span><span class='line'>#mapred.class=org.apache.hadoop.metrics.file.FileContext
</span><span class='line'>#mapred.period=10
</span><span class='line'>#mapred.fileName=/tmp/mrmetrics.log
</span><span class='line'>
</span><span class='line'># Configuration of the "mapred" context for ganglia
</span><span class='line'># Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
</span><span class='line'># mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext
</span><span class='line'># mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
</span><span class='line'># mapred.period=10
</span><span class='line'># mapred.servers=localhost:8649
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># Configuration of the "jvm" context for null
</span><span class='line'>#jvm.class=org.apache.hadoop.metrics.spi.NullContext
</span><span class='line'>
</span><span class='line'># Configuration of the "jvm" context for file
</span><span class='line'>#jvm.class=org.apache.hadoop.metrics.file.FileContext
</span><span class='line'>#jvm.period=10
</span><span class='line'>#jvm.fileName=/tmp/jvmmetrics.log
</span><span class='line'>
</span><span class='line'># Configuration of the "jvm" context for ganglia
</span><span class='line'># jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext
</span><span class='line'># jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
</span><span class='line'># jvm.period=10
</span><span class='line'># jvm.servers=localhost:8649
</span><span class='line'>
</span><span class='line'># Configuration of the "rpc" context for null
</span><span class='line'>rpc.class=org.apache.hadoop.metrics.spi.NullContext
</span><span class='line'>
</span><span class='line'># Configuration of the "rpc" context for file
</span><span class='line'>#rpc.class=org.apache.hadoop.metrics.file.FileContext
</span><span class='line'>#rpc.period=10
</span><span class='line'>#rpc.fileName=/tmp/rpcmetrics.log
</span><span class='line'>
</span><span class='line'># Configuration of the "rpc" context for ganglia
</span><span class='line'># rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext
</span><span class='line'># rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
</span><span class='line'># rpc.period=10
</span><span class='line'># rpc.servers=localhost:8649
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># Configuration of the "ugi" context for null
</span><span class='line'>ugi.class=org.apache.hadoop.metrics.spi.NullContext
</span><span class='line'>
</span><span class='line'># Configuration of the "ugi" context for file
</span><span class='line'>#ugi.class=org.apache.hadoop.metrics.file.FileContext
</span><span class='line'>#ugi.period=10
</span><span class='line'>#ugi.fileName=/tmp/ugimetrics.log
</span><span class='line'>
</span><span class='line'># Configuration of the "ugi" context for ganglia
</span><span class='line'># ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext
</span><span class='line'># ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
</span><span class='line'># ugi.period=10
</span><span class='line'># ugi.servers=localhost:8649
</span><span class='line'>
</span><span class='line'>[hadoop@umcc97-44 ~]$ cat hbase-0.98.3-hadoop2/conf/hadoop-metrics2-hbase.properties 
</span><span class='line'># syntax: [prefix].[source|sink].[instance].[options]
</span><span class='line'># See javadoc of package-info.java for org.apache.hadoop.metrics2 for details
</span><span class='line'>
</span><span class='line'>#*.sink.file*.class=org.apache.hadoop.metrics2.sink.FileSink
</span><span class='line'># default sampling period
</span><span class='line'>#*.period=10
</span><span class='line'>
</span><span class='line'># Below are some examples of sinks that could be used
</span><span class='line'># to monitor different hbase daemons.
</span><span class='line'>
</span><span class='line'># hbase.sink.file-all.class=org.apache.hadoop.metrics2.sink.FileSink
</span><span class='line'># hbase.sink.file-all.filename=all.metrics
</span><span class='line'>
</span><span class='line'># hbase.sink.file0.class=org.apache.hadoop.metrics2.sink.FileSink
</span><span class='line'># hbase.sink.file0.context=hmaster
</span><span class='line'># hbase.sink.file0.filename=master.metrics
</span><span class='line'>
</span><span class='line'># hbase.sink.file1.class=org.apache.hadoop.metrics2.sink.FileSink
</span><span class='line'># hbase.sink.file1.context=thrift-one
</span><span class='line'># hbase.sink.file1.filename=thrift-one.metrics
</span><span class='line'>
</span><span class='line'># hbase.sink.file2.class=org.apache.hadoop.metrics2.sink.FileSink
</span><span class='line'># hbase.sink.file2.context=thrift-two
</span><span class='line'># hbase.sink.file2.filename=thrift-one.metrics
</span><span class='line'>
</span><span class='line'># hbase.sink.file3.class=org.apache.hadoop.metrics2.sink.FileSink
</span><span class='line'># hbase.sink.file3.context=rest
</span><span class='line'># hbase.sink.file3.filename=rest.metrics
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31
</span><span class='line'>*.sink.ganglia.period=10
</span><span class='line'>
</span><span class='line'>hbase.sink.ganglia.period=10
</span><span class='line'>hbase.sink.ganglia.servers=umcc97-44:8649
</span></code></pre></td></tr></table></div></figure>


<p>然后properties配置同步到集群的从节点（datanode/regionserver），重启集群。等一会儿就能在ganglia-web界面看到多了很多很多的指标量。</p>

<h2>参考</h2>

<h3>ganglia</h3>

<ul>
<li><a href="http://oss.oetiker.ch/rrdtool/doc/rrdbuild.en.html#IBUILDING_DEPENDENCIES">RRDTool安装</a></li>
<li><a href="http://www.cnblogs.com/qq78292959/archive/2012/05/30/2526761.html">CFLAGS=&ldquo;-O3 -fPIC&#8221;为64位编译参数</a></li>
<li><a href="http://www.codesky.net/article/201107/174186.html">pkgconfig作用处理包依赖</a></li>
<li><a href="http://blog.chinaunix.net/uid-23916356-id-3290237.html">gmetad和gmond安装以及配置</a></li>
<li><a href="http://wenku.baidu.com/link?url=RH4EhSP3U_dp4I7goEVA_DFkb0DrgZ3uWw_mSt2hhaRb6mQJLtWxaa75RrwETwtY5e8BvOCI_p9RNrmXn_qbEexTE-PGlgtf6f5T3cGglKq">gmond节点拷贝安装</a></li>
<li><a href="http://blog.chinaunix.net/uid-11121450-id-3147002.html">http://blog.chinaunix.net/uid-11121450-id-3147002.html</a></li>
<li><a href="http://blog.chinaunix.net/uid-23916356-id-3290237.html">http://blog.chinaunix.net/uid-23916356-id-3290237.html</a></li>
<li><a href="http://wenku.baidu.com/link?url=qY7vCTyodgSCsoIg6c2UiHXWv0nEGkS9nd0DbQERxFGEaTvgvi7FMQTKv5Sn1L9H8CX5_gDgAbJJ5jaQh3KhZED7PoB2Bgr2I6mS-vDc1LS">虚拟机操作从零开始弄, 搭了个本地源, 配置</a></li>
<li><a href="http://www.linuxidc.com/Linux/2014-01/95804p2.htm">Hadoop/Hbase metrics2配置</a></li>
<li><a href="https://github.com/cbuchner1/CudaMiner/issues/23">https://github.com/cbuchner1/CudaMiner/issues/23</a></li>
<li><a href="http://bbs.csdn.net/topics/390546319">http://bbs.csdn.net/topics/390546319</a> LIBRARY_PATH是编译时使用的，LD_LIBRARY_PATH是运行时使用的。</li>
</ul>


<h3>apache web</h3>

<ul>
<li><a href="http://blog.sina.com.cn/s/blog_70121e200100lq0h.html">apache程序安装</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_5d15305b0101ceft.html">apache服务安装配置</a></li>
<li><a href="http://www.cnblogs.com/yuboyue/archive/2011/07/18/2109875.html">apache关闭服务</a></li>
<li><a href="http://www.soadmin.com/zonghe/operating-system/1008085.htm">目录权限处理</a></li>
<li><a href="http://blog.163.com/figo_2007@126/blog/static/2318076520111149413935/">http://blog.163.com/figo_2007@126/blog/static/2318076520111149413935/</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_70121e200100lq0h.html">http://blog.sina.com.cn/s/blog_70121e200100lq0h.html</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_815611fb0101cxnl.html">http://blog.sina.com.cn/s/blog_815611fb0101cxnl.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Upgrade Hive: 0.12.0 to 0.13.1]]></title>
    <link href="http://winse.github.io/blog/2014/06/21/upgrade-hive/"/>
    <updated>2014-06-21T02:34:59+08:00</updated>
    <id>http://winse.github.io/blog/2014/06/21/upgrade-hive</id>
    <content type="html"><![CDATA[<p>由于hive-0.12.0的FileSystem使用不当导致内存溢出问题，最终考虑升级hive。升级的过程没想象中的那么可怕，步骤很简单：对源数据库执行升级脚本，拷贝原hive-0.12.0的配置和jar，然后把添加jar重启hiverserver2即可。</p>

<h2>修改环境变量</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>HIVE_HOME=/home/hadoop/apache-hive-0.13.1-bin
</span><span class='line'>PATH=$JAVA_HOME/bin:$HIVE_HOME/bin:$PATH</span></code></pre></td></tr></table></div></figure>


<h2>升级metadata</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@ismp0 ~]$ cd apache-hive-0.13.1-bin/scripts/metastore/upgrade/mysql/
</span><span class='line'>
</span><span class='line'>[hadoop@ismp0 mysql]$ mysql -uXXX -hXXX -pXXX
</span><span class='line'>mysql&gt; use hive
</span><span class='line'>Reading table information for completion of table and column names
</span><span class='line'>You can turn off this feature to get a quicker startup with -A
</span><span class='line'>
</span><span class='line'>Database changed
</span><span class='line'>mysql&gt; source upgrade-0.12.0-to-0.13.0.mysql.sql
</span><span class='line'>+--------------------------------------------------+
</span><span class='line'>|                                                  |
</span><span class='line'>+--------------------------------------------------+
</span><span class='line'>| Upgrading MetaStore schema from 0.12.0 to 0.13.0 |
</span><span class='line'>+--------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>+-----------------------------------------------------------------------+
</span><span class='line'>|                                                                       |
</span><span class='line'>+-----------------------------------------------------------------------+
</span><span class='line'>| &lt; HIVE-5700 enforce single date format for partition column storage &gt; |
</span><span class='line'>+-----------------------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.22 sec)
</span><span class='line'>Rows matched: 0  Changed: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>+--------------------------------------------+
</span><span class='line'>|                                            |
</span><span class='line'>+--------------------------------------------+
</span><span class='line'>| &lt; HIVE-6386: Add owner filed to database &gt; |
</span><span class='line'>+--------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.33 sec)
</span><span class='line'>Records: 1  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.16 sec)
</span><span class='line'>Records: 1  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>+---------------------------------------------------------------------------------------------+
</span><span class='line'>|                                                                                             |
</span><span class='line'>+---------------------------------------------------------------------------------------------+
</span><span class='line'>| &lt;HIVE-6458 Add schema upgrade scripts for metastore changes related to permanent functions&gt; |
</span><span class='line'>+---------------------------------------------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>+----------------------------------------------------------------------------------+
</span><span class='line'>|                                                                                  |
</span><span class='line'>+----------------------------------------------------------------------------------+
</span><span class='line'>| &lt;HIVE-6757 Remove deprecated parquet classes from outside of org.apache package&gt; |
</span><span class='line'>+----------------------------------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.04 sec)
</span><span class='line'>Rows matched: 0  Changed: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>Rows matched: 0  Changed: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.01 sec)
</span><span class='line'>Rows matched: 0  Changed: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.07 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.12 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.07 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.05 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.15 sec)
</span><span class='line'>Records: 0  Duplicates: 0  Warnings: 0
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.05 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.07 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 0 rows affected (0.06 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.05 sec)
</span><span class='line'>
</span><span class='line'>Query OK, 1 row affected (0.07 sec)
</span><span class='line'>Rows matched: 1  Changed: 1  Warnings: 0
</span><span class='line'>
</span><span class='line'>+-----------------------------------------------------------+
</span><span class='line'>|                                                           |
</span><span class='line'>+-----------------------------------------------------------+
</span><span class='line'>| Finished upgrading MetaStore schema from 0.12.0 to 0.13.0 |
</span><span class='line'>+-----------------------------------------------------------+
</span><span class='line'>1 row in set, 1 warning (0.00 sec)
</span><span class='line'>
</span><span class='line'>mysql&gt; 
</span><span class='line'>mysql&gt; 
</span><span class='line'>mysql&gt; exit
</span><span class='line'>Bye
</span><span class='line'>
</span><span class='line'>[hadoop@ismp0 ~]$ vi .bash_profile
</span><span class='line'>[hadoop@ismp0 ~]$ source .bash_profile
</span><span class='line'>[hadoop@ismp0 ~]$ cd apache-hive-0.13.1-bin
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ cd conf/
</span><span class='line'>[hadoop@ismp0 conf]$ cp ~/hive-0.12.0/conf/hive-site.xml ./
</span><span class='line'>[hadoop@ismp0 conf]$ cd ..
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ cp ~/hive-0.12.0/lib/mysql-connector-java-5.1.21-bin.jar lib/
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ hive
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ hive
</span><span class='line'>
</span><span class='line'>hive&gt;  select count(*) from t_ods_idc_isp_log2 where day=20140624;
</span><span class='line'>Total jobs = 1
</span><span class='line'>Launching Job 1 out of 1
</span><span class='line'>Number of reduce tasks determined at compile time: 1
</span><span class='line'>In order to change the average load for a reducer (in bytes):
</span><span class='line'>  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;
</span><span class='line'>In order to limit the maximum number of reducers:
</span><span class='line'>  set hive.exec.reducers.max=&lt;number&gt;
</span><span class='line'>In order to set a constant number of reducers:
</span><span class='line'>  set mapreduce.job.reduces=&lt;number&gt;
</span><span class='line'>Starting Job = job_1403006477300_3403, Tracking URL = http://umcc97-79:8088/proxy/application_1403006477300_3403/
</span><span class='line'>Kill Command = /home/hadoop/hadoop-2.2.0/bin/hadoop job  -kill job_1403006477300_3403
</span><span class='line'>Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
</span><span class='line'>2014-06-24 17:19:07,618 Stage-1 map = 0%,  reduce = 0%
</span><span class='line'>2014-06-24 17:19:15,283 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.37 sec
</span><span class='line'>2014-06-24 17:19:16,360 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 5.49 sec
</span><span class='line'>2014-06-24 17:19:22,749 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.99 sec
</span><span class='line'>MapReduce Total cumulative CPU time: 7 seconds 990 msec
</span><span class='line'>Ended Job = job_1403006477300_3403
</span><span class='line'>MapReduce Jobs Launched: 
</span><span class='line'>Job 0: Map: 2  Reduce: 1   Cumulative CPU: 7.99 sec   HDFS Read: 19785618 HDFS Write: 6 SUCCESS
</span><span class='line'>Total MapReduce CPU Time Spent: 7 seconds 990 msec
</span><span class='line'>OK
</span><span class='line'>77625
</span><span class='line'>Time taken: 36.387 seconds, Fetched: 1 row(s)
</span><span class='line'>hive&gt; 
</span><span class='line'>
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ nohup bin/hiveserver2 &
</span><span class='line'>
</span><span class='line'>$# 测试hive-jdbc
</span><span class='line'>[hadoop@ismp0 apache-hive-0.13.1-bin]$ bin/beeline 
</span><span class='line'>Beeline version 0.13.1 by Apache Hive
</span><span class='line'>beeline&gt; !connect jdbc:hive2://10.18.97.22:10000/
</span><span class='line'>scan complete in 7ms
</span><span class='line'>Connecting to jdbc:hive2://10.18.97.22:10000/
</span><span class='line'>Enter username for jdbc:hive2://10.18.97.22:10000/: hadoop
</span><span class='line'>Enter password for jdbc:hive2://10.18.97.22:10000/: 
</span><span class='line'>Connected to: Apache Hive (version 0.13.1)
</span><span class='line'>Driver: Hive JDBC (version 0.13.1)
</span><span class='line'>Transaction isolation: TRANSACTION_REPEATABLE_READ
</span><span class='line'>0: jdbc:hive2://10.18.97.22:10000/&gt; show tables;
</span><span class='line'>+-------------------------+
</span><span class='line'>|        tab_name         |
</span><span class='line'>+-------------------------+
</span><span class='line'>...
</span><span class='line'>| test_123                |
</span><span class='line'>+-------------------------+
</span><span class='line'>10 rows selected (2.547 seconds)
</span><span class='line'>0: jdbc:hive2://10.18.97.22:10000/&gt;  select count(*) from t_ods_idc_isp_log2 where day=20140624;
</span><span class='line'>+--------+
</span><span class='line'>|  _c0   |
</span><span class='line'>+--------+
</span><span class='line'>| 77625  |
</span><span class='line'>+--------+
</span><span class='line'>1 row selected (37.463 seconds)
</span><span class='line'>0: jdbc:hive2://10.18.97.22:10000/&gt; 
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>上一篇tez的安装使用中由于hive的缘故进行了回退，现在升级到hive-0.13后，也在hive上试下tez的功能：</p>

<ul>
<li>本地添加tez依赖，设置环境变量</li>
<li>MR添加tez依赖，添加tez-site.xml</li>
<li>切换到tez的engine</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$# 已上传到HDFS
</span><span class='line'>$ cat etc/hadoop/tez-site.xml 
</span><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- Put site-specific property overrides in this file. --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;tez.lib.uris&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;${fs.default.name}/apps/tez-0.4.0-incubating,${fs.default.name}/apps/tez-0.4.0-incubating/lib/&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>
</span><span class='line'>$ export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tez/*:${HADOOP_HOME}/share/hadoop/tez/lib/*:$HADOOP_CLASSPATH
</span><span class='line'>$ apache-hive-0.13.1-bin/bin/hive
</span><span class='line'>hive&gt; set hive.execution.engine=tez;
</span><span class='line'>hive&gt; select count(*) from t_ods_idc_isp_log2 ;
</span><span class='line'>Time taken: 24.926 seconds, Fetched: 1 row(s)
</span><span class='line'>
</span><span class='line'>hive&gt; set hive.execution.engine=mr;                              
</span><span class='line'>hive&gt; select count(*) from t_ods_idc_isp_log2 where day=20140720;
</span><span class='line'>Time taken: 40.585 seconds, Fetched: 1 row(s)</span></code></pre></td></tr></table></div></figure>


<p>简单从时间上看，还是有效果的。</p>

<p><img src="http://file.bmob.cn/M00/04/A2/wKhkA1PSPSeAb1wWAAER_4gjIug339.png" alt="" /></p>

<h2>调试Hive</h2>

<p>也很简单，hive脚本已经默认集成了这个功能，设置下DEBUG环境变量即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ less apache-hive-0.13.1-bin/bin/ext/debug.sh
</span><span class='line'>[hadoop@master1 bin]$ less hive
</span><span class='line'>
</span><span class='line'>$# 脚本最终会把调试的参数` -agentlib:jdwp=transport=dt_socket,server=y,address=8000,suspend=y`加入到HADOOP_CLIENT_OPTS中，最后合并到HADOOP_OPTS传递给java程序。
</span><span class='line'>
</span><span class='line'>[hadoop@master1 bin]$ DEBUG=true hive
</span><span class='line'>Listening for transport dt_socket at address: 8000</span></code></pre></td></tr></table></div></figure>


<p>然后通过eclipse的远程调试即可一步步的查看整个过程。下面断点处为记录解析功能：</p>

<p><img src="http://file.bmob.cn/M00/0A/D4/wKhkA1QEASyAM9VEAAHQS7gZJlo672.png" alt="" /></p>

<h2>编译源码导入eclipse</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git clone https://github.com/apache/hive.git
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC /cygdrive/e/git/hive
</span><span class='line'>$ git checkout branch-0.13
</span><span class='line'>
</span><span class='line'>E:\git\hive&gt;mvn clean package eclipse:eclipse -DskipTests -Dmaven.test.skip=true -Phadoop-2</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tez编译及使用]]></title>
    <link href="http://winse.github.io/blog/2014/06/18/hadoop-tez-firststep/"/>
    <updated>2014-06-18T04:22:58+08:00</updated>
    <id>http://winse.github.io/blog/2014/06/18/hadoop-tez-firststep</id>
    <content type="html"><![CDATA[<h2>初步了解</h2>

<p>hadoop2自带的mapreduce任务中间只能传递一次，也即一个任务只能聚合一次。而tez项目是对原有yarn架构的一个拓展，使用DAG（无环有向图）实现MRR的任务框架。</p>

<p><img src="http://farm6.staticflickr.com/5571/14256993179_4990fc86d5_o.png" alt="" /></p>

<p>上图中，左边的MR任务完成一个步骤后，需要进行<strong>数据存储</strong>后再执行另一个任务来进行第二个“reduce”； 而tez则可以在reduce后继续执行reduce，减少了中间过程的IO以及mapreduce的启动时间。</p>

<h2>环境整合</h2>

<ul>
<li><a href="http://tez.incubator.apache.org/install.html">Install/Deploy</a></li>
<li>hadoop-2.2.0（umcc97-44：hdfs， umcc97-79：yarn）</li>
<li>windows下使用Cygwin编译</li>
</ul>


<h3>下载编译tez</h3>

<p>首先下载<a href="http://apache.fayea.com/apache-mirror/incubator/tez/tez-0.4.0-incubating/">tez-0.4.0-incubating.tar.gz</a>，同时还需要<a href="http://code.google.com/p/protobuf">protoc</a>的程序支持（编译<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html">hadoop源码</a>也需要这个的）。
解压后，使用mvn编译。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big
</span><span class='line'>$ tar zxvf tez-0.4.0-incubating.tar.gz
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big
</span><span class='line'>$ cd tez-0.4.0-incubating/
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating
</span><span class='line'>$ mvn install -DskipTests -Dmaven.javadoc.skip
</span><span class='line'>...
</span><span class='line'>[INFO] Reactor Summary:
</span><span class='line'>[INFO]
</span><span class='line'>[INFO] tez ............................................... SUCCESS [1.518s]
</span><span class='line'>[INFO] tez-api ........................................... SUCCESS [8.890s]
</span><span class='line'>[INFO] tez-common ........................................ SUCCESS [0.725s]
</span><span class='line'>[INFO] tez-runtime-internals ............................. SUCCESS [2.529s]
</span><span class='line'>[INFO] tez-runtime-library ............................... SUCCESS [5.100s]
</span><span class='line'>[INFO] tez-mapreduce ..................................... SUCCESS [3.666s]
</span><span class='line'>[INFO] tez-mapreduce-examples ............................ SUCCESS [2.692s]
</span><span class='line'>[INFO] tez-dag ........................................... SUCCESS [13.943s]
</span><span class='line'>[INFO] tez-tests ......................................... SUCCESS [1.691s]
</span><span class='line'>[INFO] tez-dist .......................................... SUCCESS [14.370s]
</span><span class='line'>[INFO] Tez ............................................... SUCCESS [0.245s]
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] BUILD SUCCESS
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span><span class='line'>[INFO] Total time: 55.791s
</span><span class='line'>[INFO] Finished at: Tue Jun 17 17:33:45 CST 2014
</span><span class='line'>[INFO] Final Memory: 35M/151M
</span><span class='line'>[INFO] ------------------------------------------------------------------------
</span></code></pre></td></tr></table></div></figure>


<h3>上传tez程序的jars到HDFS</h3>

<p>为了简单我直接把tez放到开发环境的集群上面去测试了。放到本地环境应该也类似。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating
</span><span class='line'>$ cd tez-dist/
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist
</span><span class='line'>$ cd target/
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist/target
</span><span class='line'>$ export HADOOP_USER_NAME=hadoop
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/local/libs/big/tez-0.4.0-incubating/tez-dist/target
</span><span class='line'>$  hadoop dfs -put tez-0.4.0-incubating/tez-0.4.0-incubating/ hdfs://umcc97-44:9000/apps/
</span><span class='line'>DEPRECATED: Use of this script to execute hdfs command is deprecated.
</span><span class='line'>Instead use the hdfs command for it.
</span></code></pre></td></tr></table></div></figure>


<h3>配置集群环境</h3>

<p>首先看下原来集群的classpath路径，由于已经包括了etc/hadoop目录，所以这里我直接把<code>tez-site.xml</code>放到该目录下。把所有的tez-lib上传到share目录下，并添加到HADOOP_CLASSPATH。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  [hadoop@umcc97-79 hadoop]$ hadoop classpath
</span><span class='line'>  /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.2.0/contrib/capacity-scheduler/*.jar
</span><span class='line'>  
</span><span class='line'>  # 用于map/reduce
</span><span class='line'>  [hadoop@umcc97-79 hadoop]$ cat tez-site.xml 
</span><span class='line'>  &lt;?xml version="1.0"?&gt;
</span><span class='line'>  &lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;!-- Put site-specific property overrides in this file. --&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;configuration&gt;
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>      &lt;name&gt;tez.lib.uris&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;${fs.default.name}/apps/tez-0.4.0-incubating,${fs.default.name}/apps/tez-0.4.0-incubating/lib/&lt;/value&gt;
</span><span class='line'>    &lt;/property&gt;
</span><span class='line'>  &lt;/configuration&gt;
</span><span class='line'>  [hadoop@umcc97-79 hadoop]$ 
</span><span class='line'>  
</span><span class='line'>  [hadoop@umcc97-79 hadoop]$ cd ~/hadoop-2.2.0/share/hadoop/tez/
</span><span class='line'>  [hadoop@umcc97-79 tez]$ ll
</span><span class='line'>  total 9616
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  303139 Jun 17 17:33 avro-1.7.4.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop   41123 Jun 17 17:33 commons-cli-1.2.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  610259 Jun 17 17:33 commons-collections4-4.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop 1648200 Jun 17 17:33 guava-11.0.2.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  710492 Jun 17 17:33 guice-3.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  656365 Jun 17 17:33 hadoop-mapreduce-client-common-2.2.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop 1455001 Jun 17 17:33 hadoop-mapreduce-client-core-2.2.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop   21537 Jun 17 17:33 hadoop-mapreduce-client-shuffle-2.2.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop   81743 Jun 17 17:33 jettison-1.3.4.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  533455 Jun 17 17:33 protobuf-java-2.5.0.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  995968 Jun 17 17:33 snappy-java-1.0.4.1.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  749917 Jun 17 17:33 tez-api-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop   34049 Jun 17 17:33 tez-common-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  970987 Jun 17 17:33 tez-dag-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  246409 Jun 17 17:33 tez-mapreduce-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  199934 Jun 17 17:33 tez-mapreduce-examples-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  114692 Jun 17 17:33 tez-runtime-internals-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop  352177 Jun 17 17:33 tez-runtime-library-0.4.0-incubating.jar
</span><span class='line'>  -rw-r--r-- 1 hadoop hadoop    6845 Jun 17 17:33 tez-tests-0.4.0-incubating.jar
</span><span class='line'>  [hadoop@umcc97-79 tez]$ 
</span><span class='line'>  
</span><span class='line'>  # 用于client任务提交
</span><span class='line'>  [hadoop@umcc97-79 hadoop]$ grep HADOOP_CLASSPATH hadoop-env.sh
</span><span class='line'>  export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tez/*:${HADOOP_HOME}/share/hadoop/tez/lib/*:$HADOOP_CLASSPATH
</span><span class='line'>  
</span><span class='line'>  [hadoop@umcc97-79 hadoop]$ sed -n 19,23p mapred-site.xml
</span><span class='line'>  &lt;configuration&gt;
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;yarn-tez&lt;/value&gt;
</span><span class='line'>    &lt;/property&gt;
</span></code></pre></td></tr></table></div></figure>


<h3>同步，重启yarn</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for h in `cat hadoop-2.2.0/etc/hadoop/slaves ` ; do rsync -vaz --exclude=logs --exclude=pid --exclude=tmp  hadoop-2.2.0 $h:~/ ; done
</span><span class='line'>
</span><span class='line'>rsync -vaz --exclude=logs --exclude=pid --exclude=tmp  hadoop-2.2.0 umcc97-44:~/</span></code></pre></td></tr></table></div></figure>


<h3>测试效果</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  [hadoop@umcc97-79 ~]$ hadoop classpath
</span><span class='line'>  /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.2.0/share/hadoop/tez/*:/home/hadoop/hadoop-2.2.0/share/hadoop/tez/lib/*:/home/hadoop/hadoop-2.2.0/contrib/capacity-scheduler/*.jar
</span><span class='line'>  [hadoop@umcc97-79 ~]$ cd hadoop-2.2.0/share/hadoop/mapreduce/
</span><span class='line'>  [hadoop@umcc97-79 mapreduce]$ hadoop jar hadoop-mapreduce-client-jobclient-2.2.0-tests.jar sleep -mt 1 -rt 1 -m 1 -r 1
</span><span class='line'>  
</span><span class='line'>    hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount  /hello/in /hello/out
</span><span class='line'>    hadoop fs -put hadoop-2.2.0/logs/yarn-hadoop-resourcemanager-umcc97-79.* /hello/in
</span><span class='line'>    hadoop fs -rmr /hello/out
</span><span class='line'>    hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount  /hello/in /hello/out
</span></code></pre></td></tr></table></div></figure>


<h3>回滚，使用时临时修改环境变量即可</h3>

<p>使用了tez后，使用hive-0.12.0不能运行了。由于其他同事需要用hive，得把配置全部修改回去【<a href="http://winse.github.io/blog/2014/06/21/upgrade-hive/">hive-0.13中使用tez</a>】。</p>

<p>其实在<strong>提交任务</strong>时指定配置参数即可。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-79 ~]$ export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tez/*:${HADOOP_HOME}/share/hadoop/tez/lib/*:$HADOOP_CLASSPATH
</span><span class='line'>[hadoop@umcc97-79 ~]$ hadoop jar hadoop-2.2.0/share/hadoop/tez/tez-mapreduce-examples-0.4.0-incubating.jar orderedwordcount -Dmapreduce.framework.name=yarn-tez  /hello/in /hello/out</span></code></pre></td></tr></table></div></figure>


<p>org.apache.tez.mapreduce.examples.OrderedWordCount不仅计算出了结果，同时按个数大小进行了排序。</p>

<p>问题： tez的任务的history还不知道怎么弄的，启动historyserver没作用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[远程调试hadoop2以及错误处理方法]]></title>
    <link href="http://winse.github.io/blog/2014/04/22/remote-debug-hadoop2/"/>
    <updated>2014-04-22T06:47:48+08:00</updated>
    <id>http://winse.github.io/blog/2014/04/22/remote-debug-hadoop2</id>
    <content type="html"><![CDATA[<p>了解程序运行过程，除了一行行代码的扫射源代码。更快捷的方式是运行调试源码，通过F6/F7来一步步的带领我们熟悉程序。针对特定细节具体数据，打个断点调试则是水到渠成的方式。</p>

<h2>Java远程调试</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> * JDK 1.3 or earlier -Xnoagent -Djava.compiler=NONE -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=6006
</span><span class='line'> * JDK 1.4(linux ok) -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=6006
</span><span class='line'> * newer JDK(win7 & jdk7) -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=6006</span></code></pre></td></tr></table></div></figure>


<h2>同一操作系统任务提交</h2>

<p>windows提交到windows，linux提交到linux，可以直接通过命令行添加参数调试wordcount任务：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\local\dotfile&gt;hdfs dfs -rmr /out # native-lib放在非path路径下，cmd脚本中有对其进行处理
</span><span class='line'>
</span><span class='line'>E:\local\dotfile&gt;hadoop org.apache.hadoop.examples.WordCount  "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090 -Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native -Dmapreduce.reduce.java.opts=-Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native"  /in /out</span></code></pre></td></tr></table></div></figure>


<p><strong>suspend设置为y，会等待客户端连接再运行</strong>。在eclipse中在WordCount$TokenizerMapper#map打个断点，然后再使用<code>Remote Java Application</code>就可以调试程序了。</p>

<h2>Hadoop集群环境下调试任务</h2>

<p>hadoop有很多的程序，同样有对应的环境变量选项来进行设置！</p>

<ul>
<li>主程序-调试Job提交

<ul>
<li><code>set HADOOP_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090"</code></li>
<li>可以在配置文件中进行设置。需要注意可能会覆盖已经设置的该参数的值。</li>
</ul>
</li>
<li>Nodemanager调试

<ul>
<li><code>set HADOOP_NODEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8092"</code></li>
<li>(linux下需要定义在文件中)<code>YARN_NODEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8092"</code></li>
</ul>
</li>
<li>ResourceManager调试

<ul>
<li>HADOOP_RESOURCEMANAGER_OPTS</li>
<li><code>export YARN_RESOURCEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8091"</code></li>
</ul>
</li>
</ul>


<p>Linux上的设置略有不同，通过SSH再调用的进程(如NodeManager)需要把其OPTS写到命令行脚本文件中！！
linux需要远程调试NodeManager的话，需要写到etc/hadoop/yarn-env.sh文件中！不然，nodemanger不生效（通过ssh去执行的）！</p>

<h3>其他调试技巧</h3>

<p>调试测试集群环境，比本地windows开发环境复杂点。毕竟本地windows的就一个主一个从。而把<strong>任务放到分布式集群</strong>上时，例如调试分布式缓存的！
那么就需要一些小技巧来获取任务运行所在的机器！下面的步骤中有具体操作命令。</p>

<h3>任务配置及运行</h3>

<p>eclipse下windows提交job到linux的补丁，查阅<a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655">[MAPREDUCE-5655]</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 配置
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapred.remote.os&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;Linux&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.job.jar&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;dta-analyser-all.jar&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;-Xmx1024m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapred.task.timeout&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;1800000&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'># 代码，map/reduce数都设置为1 
</span><span class='line'>job.setNumReduceTasks(1);
</span><span class='line'>job.getConfiguration().setInt(MRJobConfig.NUM_MAPS, 1);
</span></code></pre></td></tr></table></div></figure>


<p></p>

<ul>
<li>调试的时刻把超时时间设置的久一点，否则：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> Got exception: java.net.SocketTimeoutException: Call From winseliu/127.0.0.1 to winse.com:2850 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch :</span></code></pre></td></tr></table></div></figure>


<ul>
<li>调试main方法参数设置</li>
</ul>


<p>调试main（转瞬即逝的把suspend设置为true！），map的调试选项的语句写在配置文件里面</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export HADOOP_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8073"
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ sh -x bin/hadoop org.apache.hadoop.examples.WordCount /in /out </span></code></pre></td></tr></table></div></figure>


<h3>遍历所有子节点，查找节点运行map程序的信息</h3>

<p>map调试的端口配置为18090，根据这个选项来查找程序运行的机器。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 ~]$ for h in `cat hadoop-2.2.0/etc/hadoop/slaves` ; do ssh $h 'ps aux|grep java | grep 18090'; echo $h;  done
</span><span class='line'>hadoop    8667  0.0  0.0  63888  1268 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
</span><span class='line'>umcc97-142
</span><span class='line'>hadoop   12686  0.0  0.0  63868  1260 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
</span><span class='line'>umcc97-143
</span><span class='line'>hadoop   23516  0.0  0.0  63856  1108 ?        Ss   18:11   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1605/container_1397006359464_1605_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 57576 attempt_1397006359464_1605_m_000000_0 2 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002/stderr 
</span><span class='line'>hadoop   23522  0.0  0.0 605136 15728 ?        Sl   18:11   0:00 /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1605/container_1397006359464_1605_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1605/container_1397006359464_1605_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 57576 attempt_1397006359464_1605_m_000000_0 2
</span><span class='line'>hadoop   23665  0.0  0.0  63856  1264 ?        Ss   18:21   0:00 bash -c ps aux|grep java | grep 18090
</span><span class='line'>umcc97-144</span></code></pre></td></tr></table></div></figure>


<p>仅打印运行map的节点名称</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 ~]$ for h in `cat hadoop-2.2.0/etc/hadoop/slaves` ; do ssh $h 'if ps aux|grep -v grep | grep java | grep 18090 | grep -v bash 2&gt;&1 1&gt;/dev/null ; then echo `hostname`; fi'; done
</span><span class='line'>umcc97-142
</span><span class='line'>[hadoop@umcc97-44 ~]$ </span></code></pre></td></tr></table></div></figure>


<p>后面的操作就和普通的java程序调试步骤一样了。不再赘述。</p>

<h2>任务运行过程中的数据</h2>

<h4>辅助运行的两个bash程序</h4>

<p>运行的第一个程序（000001）为AppMaster，第二程序（000002）才是我们提交job的map任务。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-143 ~]$ cd hadoop-2.2.0/tmp/nm-local-dir/nmPrivate
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ ls -Rl
</span><span class='line'>.:
</span><span class='line'>total 12
</span><span class='line'>drwxrwxr-x 4 hadoop hadoop 4096 Apr 21 18:34 application_1397006359464_1606
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop    6 Apr 21 18:34 container_1397006359464_1606_01_000001.pid
</span><span class='line'>-rw-rw-r-- 1 hadoop hadoop    6 Apr 21 18:34 container_1397006359464_1606_01_000002.pid
</span><span class='line'>
</span><span class='line'>./application_1397006359464_1606:
</span><span class='line'>total 8
</span><span class='line'>drwxrwxr-x 2 hadoop hadoop 4096 Apr 21 18:34 container_1397006359464_1606_01_000001
</span><span class='line'>drwxrwxr-x 2 hadoop hadoop 4096 Apr 21 18:34 container_1397006359464_1606_01_000002
</span><span class='line'>
</span><span class='line'>./application_1397006359464_1606/container_1397006359464_1606_01_000001:
</span><span class='line'>total 8
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop   95 Apr 21 18:34 container_1397006359464_1606_01_000001.tokens
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop 3121 Apr 21 18:34 launch_container.sh
</span><span class='line'>
</span><span class='line'>./application_1397006359464_1606/container_1397006359464_1606_01_000002:
</span><span class='line'>total 8
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop  129 Apr 21 18:34 container_1397006359464_1606_01_000002.tokens
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop 3532 Apr 21 18:34 launch_container.sh
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ 
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ jps
</span><span class='line'>4692 NodeManager
</span><span class='line'>4173 DataNode
</span><span class='line'>13497 YarnChild
</span><span class='line'>7538 HRegionServer
</span><span class='line'>13376 MRAppMaster
</span><span class='line'>13574 Jps
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ cat *.pid
</span><span class='line'>13366
</span><span class='line'>13491
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ ps aux | grep 13366
</span><span class='line'>hadoop   13366  0.0  0.0  63868  1088 ?        Ss   18:34   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000001/stderr 
</span><span class='line'>hadoop   13594  0.0  0.0  61204   760 pts/2    S+   18:36   0:00 grep 13366
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ ps aux | grep 13491
</span><span class='line'>hadoop   13491  0.0  0.0  63868  1100 ?        Ss   18:34   0:00 /bin/bash -c /home/java/jdk1.7.0_45/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx256m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 -Djava.io.tmpdir=/home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/container_1397006359464_1606_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 10.18.97.143 52046 attempt_1397006359464_1606_m_000000_0 2 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1397006359464_1606/container_1397006359464_1606_01_000002/stderr 
</span><span class='line'>hadoop   13599  0.0  0.0  61204   760 pts/2    S+   18:37   0:00 grep 13491
</span><span class='line'>[hadoop@umcc97-143 nmPrivate]$ </span></code></pre></td></tr></table></div></figure>


<h4>程序运行本地缓存数据</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-143 container_1397006359464_1606_01_000002]$ ls -l
</span><span class='line'>total 28
</span><span class='line'>-rw-r--r-- 1 hadoop hadoop  129 Apr 21 18:34 container_tokens
</span><span class='line'>-rwx------ 1 hadoop hadoop  516 Apr 21 18:34 default_container_executor.sh
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop   65 Apr 21 18:34 filter.io -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/filecache/10/filter.io
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop  120 Apr 21 18:34 job.jar -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/filecache/10/job.jar
</span><span class='line'>lrwxrwxrwx 1 hadoop hadoop  120 Apr 21 18:34 job.xml -&gt; /home/hadoop/hadoop-2.2.0/tmp/nm-local-dir/usercache/hadoop/appcache/application_1397006359464_1606/filecache/13/job.xml
</span><span class='line'>-rwx------ 1 hadoop hadoop 3532 Apr 21 18:34 launch_container.sh
</span><span class='line'>drwx--x--- 2 hadoop hadoop 4096 Apr 21 18:34 tmp
</span><span class='line'>[hadoop@umcc97-143 container_1397006359464_1606_01_000002]$ </span></code></pre></td></tr></table></div></figure>


<h2>处理问题方法</h2>

<ul>
<li>打印DEBUG日志：<code>export HADOOP_ROOT_LOGGER=DEBUG,console</code>

<ul>
<li>日志文件放置在nodemanager节点的logs/userlogs目录下。</li>
</ul>
</li>
<li>打印DEBUG日志也搞不定时，可以在源码里面sysout信息然后把<strong>class覆盖</strong>，来进行定位配置的问题。</li>
<li>如果不清楚shell的执行过程，可以通过<code>sh -x [CMD]</code>，或者在脚本文件的操作前加上<code>set -x</code>。相当于windows-batch的<code>echo on</code>功能。</li>
</ul>


<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/975271/remote-debugging-a-java-application">remote debugger opts</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2学习过程/资源]]></title>
    <link href="http://winse.github.io/blog/2014/04/22/hadoop-category/"/>
    <updated>2014-04-22T06:37:20+08:00</updated>
    <id>http://winse.github.io/blog/2014/04/22/hadoop-category</id>
    <content type="html"><![CDATA[<p>接触集群，起始为毕业论文觉得做一个SSH的内容管理系统觉得无趣，选择了Hadoop相关的选题。尽管做的很烂，但是当时做出来一个东西还是挺开心的。中间断了近一年，但是在鼎象做游戏的时刻部署系统/维护查日志，对linux熟悉了不少。在科韵开始做插件开发，神马都的看源码。而后，真正的做了一个hadoop的项目，相比2年前，对编程和学习的方式都有了提升。hadoop，kettle这些都是很牛掰的很火热软件，但是中文资料相对较少，变化也很快。慢慢的觉得自己看代码和英文的资源都过得去。</p>

<p>再碌碌无为的过了一年，而今又接触hadoop，时代变了，但是基本的技术还是相同的。从hadoop1升级到hadoop2，尽管变化很大，熟悉的过程中再次遇到很多的问题。觉得无能无力，原来的日子好似白过了！记录是一种美德，不仅是走过路过的足迹，亦是摘树后人乘凉的盛举。</p>

<p>要弄hadoop，首先得把对english的偏见放下！如果还是baidu查找你遇到的问题，那或许你会多走很多的弯路！</p>

<h2>书籍</h2>

<p>开始还是推荐下中文资料：</p>

<ul>
<li>[Hadoop权威指南/Hadoop The Definitive Guide] 大师写的书，值的膜拜</li>
<li>[hadoop实战/Hadoop in Action] 相对来说是也是一本不错的</li>
</ul>


<h2>网页资源</h2>

<ol>
<li>Linux部署Hadoop

<ul>
<li><a href="http://developer.yahoo.com/hadoop/tutorial/">Hadoop Tutorial - YDN</a></li>
<li><a href="http://shiyanjun.cn/archives/561.html">Hadoop-2.2.0集群安装配置实践</a></li>
<li><a href="http://www.cnblogs.com/i80386/p/3548132.html">hadoop2.2.0 单机伪分布式（含64位hadoop编译）及 eclipse hadoop开发环境搭建</a></li>
</ul>
</li>
<li>Windows部署Hadoop

<ul>
<li><a href="http://yangshangchuan.iteye.com/blog/1839812">配置Cygwin支持无密码SSH登陆</a></li>
<li><a href="http://wiki.apache.org/hadoop/Hadoop2OnWindows">Hadoop2OnWindows - Hadoop Wiki</a></li>
<li><a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655">[MAPREDUCE-5655] job submit from windows to a linux hadoop cluster fails due to wrong classpath</a></li>
<li><a href="http://stackoverflow.com/questions/11821378/what-does-bashno-job-control-in-this-shell-mean">linux - what does &ldquo;bash:no job control in this shell” mean? - Stack Overflow</a></li>
<li><a href="http://stackoverflow.com/questions/18630019/running-apache-hadoop-2-1-0-on-windows">Running Apache Hadoop 2.1.0 on Windows - Stack Overflow</a></li>
<li><a href="http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval">Error &lsquo;LINK : fatal error LNK1123: failure during conversion to COFF: file invalid or corrupt&rsquo;</a></li>
<li><a href="http://blog.csdn.net/xzz_hust/article/details/9450289">VS2010 error: LINK : fatal error LNK1123: failure during conversion to COFF: file invalid or corrupt</a></li>
<li><a href="https://issues.apache.org/jira/browse/HADOOP-10144">[HADOOP-10144] Error on build in windows 7 box</a></li>
<li><a href="http://www.srccodes.com/p/article/38/build-install-configure-run-apache-hadoop-2.2.0-microsoft-windows-os#comment-1150229068">Build, Install, Configure and Run Apache Hadoop 2.2.0 in Microsoft Windows OS</a></li>
<li><a href="http://blog.csdn.net/bamuta/article/details/13506843">hadoop2.2.0遇到NativeLibraries错误的解决过程</a></li>
<li><a href="http://blog.csdn.net/bamuta/article/details/13506893">hadoop2.2.0遇到64位操作系统平台报错，重新编译hadoop</a></li>
<li><a href="http://stackoverflow.com/questions/19943766/hadoop-unable-to-load-native-hadoop-library-for-your-platform-error-on-centos">Hadoop &ldquo;Unable to load native-hadoop library for your platform&rdquo; error on CentOS</a></li>
</ul>
</li>
<li>eclipse直接访问HDFS/提交任务

<ul>
<li><a href="http://zy19982004.iteye.com/blog/2031172">Hadoop学习三十二：Win7下无法提交MapReduce Job到集群环境</a></li>
<li><a href="http://blog.csdn.net/fansy1990/article/details/22896249">Eclipse调用hadoop2运行MR程序</a></li>
</ul>
</li>
<li>cygwin部署hadoop

<ul>
<li><a href="http://blog.csdn.net/needle2/article/details/5416571">cygwin sshd 安装配置</a></li>
<li><a href="http://www.ipv6bbs.cn/thread-209-1-1.html">如何确定自己是否已接入IPv6网络及故障分析（提问必看）</a></li>
<li><a href="http://www.ipv6bbs.cn/thread-151-1-1.html">在IPv4网络下接入IPv6网络的方法（隧道与第三方软件）</a></li>
</ul>
</li>
<li>hdfs脚本文件系统</li>
<li>编译源码

<ul>
<li><a href="http://blog.163.com/universsky@126/blog/static/112760232201362743156735/">maven工程pom添加log4j依赖 Missing artifact javax.jms:jms:jar:1.1:compile 为pom.xml添加dependency，编译报错缺少jar包，但是本地库中有这个包</a></li>
<li><a href="http://www.cnblogs.com/sysuys/p/3492791.html">找不到org.mortbay.component.AbstractLifeCycle的类文件</a></li>
<li><a href="https://issues.apache.org/jira/browse/HADOOP-10110">[HADOOP-10110]hadoop-auth has a build break due to missing dependency</a></li>
<li><a href="http://blog.csdn.net/superye1983/article/details/16884097">Hadoop CDH5 手动安装伪分布式模式 </a></li>
</ul>
</li>
<li>远程调试

<ul>
<li><a href="http://stackoverflow.com/questions/975271/remote-debugging-a-java-application">Remote debugging a Java application</a></li>
<li><a href="http://liugang594.iteye.com/blog/154710">Eclipse调试框架的学习与理解</a></li>
<li><a href="http://duming115.iteye.com/blog/791218">Java的远程debug</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/opensource/os-eclipse-javadebug/index.html">使用 Eclipse 远程调试 Java 应用程序</a></li>
</ul>
</li>
<li>与正式环境有关

<ul>
<li><a href="http://cn.mzcart.com/2012/04/24.html">putty – 使用putty命令行参数</a></li>
<li><a href="http://www.ctohome.com/FuWuQi/0b/301.html">Windows下使用ssh代理来访问国外的youtube和twitter/fackbook等网站</a></li>
<li><a href="http://hejianhuacn.iteye.com/blog/1972033">使用SecureCRT的SSH端口转发，使用PLSQL访问内网数据库</a></li>
<li><a href="http://qn-lf.iteye.com/blog/859662">用ssh端口转发功能访问远程服务器</a></li>
<li><a href="http://blog.csdn.net/linuxoostudy/article/details/7097418">SecureCRT设置SSH端口转发详解</a></li>
</ul>
</li>
<li>开发参考的资源/代码访问集群

<ul>
<li><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/">Hadoop 新 MapReduce 框架 Yarn 详解</a></li>
<li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-2-0-jobhistory-log/">Hadoop 2.0中作业日志收集原理以及配置方法</a></li>
<li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-logs-placement/">Hadoop日志到底存在哪里？</a></li>
<li><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-distributedcache-details/">Hadoop DistributedCache详解</a></li>
<li><a href="http://hpuxtbjvip0.blog.163.com/blog/static/3674131320132794940734/">Hadoop DistributedCache使用及原理</a></li>
</ul>
</li>
</ol>


<h2>工具</h2>

<ul>
<li>lrzsz</li>
<li>SecureCRT</li>
<li>WinSCP</li>
<li>w3m # 最后使用SSH代理访问取代！</li>
</ul>


<h2>思维</h2>

<ul>
<li>化整为零</li>
</ul>


<h2>技巧</h2>

<ul>
<li>查看jar列表：<code>jar tvf JAR</code></li>
<li>ssh-copy-id</li>
<li>rsync</li>
<li>find</li>
<li>ls -Sl</li>
<li>while/for</li>
<li><p><code>cat unknown.txt | cut -b 62- | sort | uniq</code></p></li>
<li><p>持续的更新 -</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Windows下部署/配置/调试hadoop2]]></title>
    <link href="http://winse.github.io/blog/2014/04/21/hadoop2-windows-startguide/"/>
    <updated>2014-04-21T16:27:11+08:00</updated>
    <id>http://winse.github.io/blog/2014/04/21/hadoop2-windows-startguide</id>
    <content type="html"><![CDATA[<p>Windows作为开发屌丝必备，在windows上如何跑集群方便开发调试，以及怎么把eclipse写好的任务mapreduce提交到测试的集群(linux)上面去跑？这些都是需要直面并解决的问题。</p>

<p>本文主要记录在windows上hadoop集群的环境准备，以及eclipse调试功能等。</p>

<ol>
<li>windows伪分布式部署

<ul>
<li>cmd</li>
<li>cygwin shell</li>
</ul>
</li>
<li>windows-eclipse提交任务到linux集群</li>
<li>导入源码到eclipse</li>
</ol>


<p>这篇文章并非按照操作的时间顺序来进行编写。而是，如果再安装第二遍的话，自己应该如何去操作来组织下文。</p>

<h2>一、Windows伪分布式部署</h2>

<p>尽管一直用windows，但是对windows自带的cmd命令很是不屑！想在cygwin下部署，现在想来，最终用的是windows的java！在cygwin下不就是把路径转换后再传给java执行吗！</p>

<p>所以，如果把cygwin环境搭建好了的话，其实已经把windows的环境也搭建好了！同样hadoop的windows环境配置好了，cygwin环境也同样配置好了。但是，在cygwin下面提交mapreduce任务会有各种&#8221;凌乱&#8221;的问题！</p>

<p>先说说在windows环境搭建的步骤，然后再讲cygwin下运行。</p>

<ol>
<li>需要用到的软件环境</li>
<li>编译windows环境变量配置</li>
<li>编译hadoop-common源代码生成本地依赖库</li>
<li>伪分布式配置</li>
<li>windows下运行</li>
<li>cygwin下运行</li>
</ol>


<h3>1.1 需要用到的软件环境</h3>

<ul>
<li>Win7-x86</li>
<li>hadoop-2.2.0.tar.gz</li>
<li>git</li>
<li>cygwin (源码编译时需要执行sh命令)</li>
<li>visual studio 2010（如果与.net framework4有关的问题请查阅： <a href="http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval">[*]</a> <a href="http://stackoverflow.com/questions/12267158/failure-during-conversion-to-coff-file-invalid-or-corrupt">[*]</a> <a href="http://social.msdn.microsoft.com/Forums/vstudio/en-US/eb4a7699-0f3c-4701-9790-199787f1b359/vs-2010-error-lnk1123-failure-during-conversion-to-coff-file-invalid-or-corrupt?forum=vcgeneral">[*]</a>）</li>
<li>protoc(protoc-2.5.0-win32.zip)(<strong>解压，然后把路径加入到PATH</strong>)</li>
</ul>


<p>搭建环境之前，<strong>建议您看看<a href="http://wiki.apache.org/hadoop/Hadoop2OnWindows">wiki-Hadoop2OnWindows</a></strong>。最终有用的步骤都在上面了！不过在自己瞎折腾的过程中也弄了不少东西，记录下来！</p>

<h3>1.2 编译windows环境变量配置</h3>

<table>
<thead>
<tr>
<th style="text-align:left;"> 变量              </th>
<th style="text-align:left;"> windows</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> Platform          </td>
<td style="text-align:left;"> Win32</td>
</tr>
<tr>
<td style="text-align:left;"> ANT_HOME          </td>
<td style="text-align:left;"> D:\local\usr\apache-ant-1.9.0</td>
</tr>
<tr>
<td style="text-align:left;"> MAVEN_HOME        </td>
<td style="text-align:left;"> D:\local\usr\apache-maven-3.0.4</td>
</tr>
<tr>
<td style="text-align:left;"> JAVA_HOME         </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02</td>
</tr>
<tr>
<td style="text-align:left;"> PATH              </td>
<td style="text-align:left;"> C:\cygwin\bin;C:\protoc;D:\local\usr\apache-maven-3.0.4\bin;D:\local\usr\apache-ant-1.9.0/bin;D:\Java\jdk1.7.0_02\bin;%PATH%</td>
</tr>
</tbody>
</table>


<p><del>编译时，在打开的命令行加入cygwin的路径即可。</del>
在maven编译最后需要用到sh的shell命令，需要把<code>c:\cygwin\bin</code>目录加入到path环境变量。
这里先不配置hadoop的环境变量，因为我只需要用到编译后的本地库而已！！</p>

<h3>1.3 编译源代码生成本地依赖库(dll, exe)</h3>

<p>hadoop2.2.0操作本地文件针对平台的进行了处理。也就是只要在windows运行集群，不管怎么样，你都得先把winutils.exe、hadoop.dll编译出来，用来处理对本地文件赋权、软链接等（类似Linux-Shell的功能）。否则会看到下面的错误：</p>

<ul>
<li><p>命令执行出错，少了winutils.exe</p>

<pre><code class="``">  14/04/14 20:07:58 ERROR util.Shell: Failed to locate the winutils binary in the hadoop binary path
  java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
      at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
      at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
      at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:293)

  14/04/17 21:22:32 INFO service.AbstractService: Service org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerServic
  e failed in state INITED; cause: java.lang.NullPointerException
  java.lang.NullPointerException
          at java.lang.ProcessBuilder.start(ProcessBuilder.java:1010)
          at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
          at org.apache.hadoop.util.Shell.run(Shell.java:379)
          at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
          at org.apache.hadoop.util.Shell.execCommand(Shell.java:678)
          at org.apache.hadoop.util.Shell.execCommand(Shell.java:661)
          at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:639)
          at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:435)
</code></pre></li>
<li><p>少了hadoop.dll的本地库文件</p>

<pre><code class="``">  14/04/17 21:30:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-ja
  va classes where applicable
  14/04/17 21:30:29 FATAL datanode.DataNode: Exception in secureMain
  java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
          at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
          at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:435)
          at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)
          at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)
          at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)
          at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:147)
          at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:1698)
</code></pre></li>
</ul>


<h4>下载源码进行编译</h4>

<p>下面需要用到visual studio修改项目配置信息（或者直接修改sln文件也行），然后再使用maven进行编译。</p>

<p>这里仅编译hadoop-common项目，最后把生成winutils.exe/hadoop.dll放到hadoop程序bin目录下。</p>

<p>第一步 下载源码</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/*  https://github.com/apache/hadoop-common.git  */
</span><span class='line'>
</span><span class='line'>Administrator@WINSELIU /e/git/hadoop-common (master)
</span><span class='line'>$ git checkout branch-2.2.0
</span><span class='line'>Checking out files: 100% (5536/5536), done.
</span><span class='line'>Branch branch-2.2.0 set up to track remote branch branch-2.2.0 from origin.
</span><span class='line'>Switched to a new branch 'branch-2.2.0'</span></code></pre></td></tr></table></div></figure>


<p>第二步 应用补丁patch-native-win32</p>

<p>jira: <a href="https://issues.apache.org/jira/browse/HADOOP-9922">https://issues.apache.org/jira/browse/HADOOP-9922</a>   <br/>
patch: <a href="https://issues.apache.org/jira/secure/attachment/12600760/HADOOP-9922.patch">https://issues.apache.org/jira/secure/attachment/12600760/HADOOP-9922.patch</a></p>

<p>native.sln-patch有点问题下面通过vs修改，使用Visual Studio修改native的活动平台</p>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFebmAcNJsAATafRp0jDs763.png" alt="" /></p>

<p>第三步 在<code>Visual Studio 命令提示(2010)</code>命令行进行Maven编译(仅需编译hadoop-common)</p>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFeeGAfZgdAAWIDc_jnSM492.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\git\hadoop-common\hadoop-common-project\hadoop-common&gt;mvn package -Pdist,native-win -DskipTests -Dtar -Dmaven.javadoc.skip=true
</span><span class='line'>
</span><span class='line'>/*  native files  */
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/git/hadoop-common/hadoop-common-project/hadoop-common
</span><span class='line'>$ ls -1 target/bin/
</span><span class='line'>hadoop.dll
</span><span class='line'>hadoop.exp
</span><span class='line'>hadoop.lib
</span><span class='line'>hadoop.pdb
</span><span class='line'>libwinutils.lib
</span><span class='line'>winutils.exe
</span><span class='line'>winutils.pdb
</span><span class='line'>
</span><span class='line'>Administrator@winseliu /cygdrive/e/git/hadoop-common/hadoop-common-project/hadoop-common
</span><span class='line'>$ cp target/bin/* ~/hadoop/bin/</span></code></pre></td></tr></table></div></figure>


<p>windows的本地库的路径就是PATH环境变量。所以<strong>windows下最好还是把dll放到bin目录下，同时把<code>HADOOP_HOME/bin</code>加入到环境变量中！！</strong>
修改PATH环境变量。</p>

<p>可以把dll放到自定义的位置，但是同样最好把该路径加入到PATH环境变量。java默认会到PATH路径下找动态链接库dll。</p>

<h3>1.4 修改hadoop配置，部署伪分布式环境</h3>

<p>可以直接把linux伪分布式的配置cp过来用。然后修改namenode/datanode/yarn文件的存储路径就可以了。
这里有个坑，<code>hdfs-default.xml</code>中的路径前面都加了<code>file://</code>前缀！所以hdfs配置中涉及到路径的，这里都得进行了修改。</p>

<p><del>Notepad++的Ctrl+D是一个好功能啊</del></p>

<table>
<thead>
<tr>
<th style="text-align:left;"> 属性                                    </th>
<th style="text-align:left;"> 值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> <strong>slaves</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> localhost</td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> <strong>core-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> fs.defaultFS                            </td>
<td style="text-align:left;"> hdfs://localhost:9000</td>
</tr>
<tr>
<td style="text-align:left;"> io.file.buffer.size                     </td>
<td style="text-align:left;"> 10240</td>
</tr>
<tr>
<td style="text-align:left;"> hadoop.tmp.dir                          </td>
<td style="text-align:left;"> file:///e:/tmp/hadoop</td>
</tr>
<tr>
<td style="text-align:left;"> <strong>hdfs-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> dfs.replication                         </td>
<td style="text-align:left;"> 1</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.namenode.secondary.http-address     </td>
<td style="text-align:left;"> localhost:9001 #设置为空可以禁用</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.namenode.name.dir                   </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/name</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.datanode.data.dir                   </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/data</td>
</tr>
<tr>
<td style="text-align:left;"> dfs.namenode.checkpoint.dir             </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/namesecondary</td>
</tr>
<tr>
<td style="text-align:left;"> <del>dfs.namenode.shared.edits.dir</del>       </td>
<td style="text-align:left;"> ${hadoop.tmp.dir}/dfs/shared/edits</td>
</tr>
<tr>
<td style="text-align:left;"> <strong>mapred-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> mapreduce.framework.name                </td>
<td style="text-align:left;"> yarn</td>
</tr>
<tr>
<td style="text-align:left;"> mapreduce.jobhistory.address            </td>
<td style="text-align:left;"> localhost:10020</td>
</tr>
<tr>
<td style="text-align:left;"> mapreduce.jobhistory.webapp.address     </td>
<td style="text-align:left;"> localhost:19888</td>
</tr>
<tr>
<td style="text-align:left;"> <strong>yarn-site.xml</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> yarn.nodemanager.aux-services           </td>
<td style="text-align:left;"> mapreduce_shuffle</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.nodemanager.aux-services.mapreduce_shuffle.class  </td>
<td style="text-align:left;"> org.apache.hadoop.mapred.ShuffleHandler</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.address            </td>
<td style="text-align:left;"> localhost:8032</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.scheduler.address  </td>
<td style="text-align:left;"> localhost:8030</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.resource-tracker.address  </td>
<td style="text-align:left;"> localhost:8031</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.admin.address      </td>
<td style="text-align:left;"> localhost:8033</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.resourcemanager.webapp.address     </td>
<td style="text-align:left;"> localhost:8088</td>
</tr>
<tr>
<td style="text-align:left;"> yarn.application.classpath              </td>
<td style="text-align:left;"> %HADOOP_CONF_DIR%, %HADOOP_COMMON_HOME%/share/hadoop/common/<em>, %HADOOP_COMMON_HOME%/share/hadoop/common/lib/</em>, %HADOOP_HDFS_HOME%/share/hadoop/hdfs/<em>, %HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/</em>, %HADOOP_YARN_HOME%/share/hadoop/yarn/<em>, %HADOOP_YARN_HOME%/share/hadoop/yarn/lib/</em></td>
</tr>
</tbody>
</table>


<p>注意点：</p>

<ul>
<li>yarn.application.classpath必须定义！尽管程序中有判断不同平台的默认值不同，但是在yarn-default.xml中已经有值了！

<ul>
<li>yarn.application.classpath对启动程序没影响，但是在运行mapreduce时影响巨大破坏力极强！</li>
</ul>
</li>
<li>自定library的路径是个坑！！

<ul>
<li>在windows下，执行java程序java.library.path默认到PATH路径找。这也是需要定义环境变量HADOOP_HOME，以及把bin加入到PATH的原因吧！</li>
</ul>
</li>
</ul>


<hr />

<h3>1.5 Windows直接运行cmd启动</h3>

<p>如果是用windows的cmd的话，到这里已经基本ok了！<strong>格式化namenode</strong>（<code>hadoop namenode -format</code>），启动就ok了！
<del>发现自己其实很傻×，固执的要用cygwin启动运行！用windows的cmd启动，然后用cygwin的终端查看数据不就行了！两不耽误！</del></p>

<p>cmd命令<strong>默认</strong>是去bin目录下找hadoop.dll的，同时hadoop命令会把bin加入到java.library.path路径下。可以直接把hadoop.dll放到bin路径（强烈推荐）。
设置环境变量，启动文件系统：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/* **设置环境变量** */
</span><span class='line'>HADOOP_HOME=E:\local\libs\big\hadoop-2.2.0 
</span><span class='line'>PATH=%HADOOP_HOME%\bin;%PATH%
</span><span class='line'>
</span><span class='line'>/* 格式化namenode */
</span><span class='line'>hadoop namenode -format
</span><span class='line'>
</span><span class='line'>/* 操作HDFS */
</span><span class='line'>set HADOOP_ROOT_LOGGER=DEBUG,console
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;sbin\start-dfs.cmd
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;hdfs dfs -put README.txt /   # 很弱，fs简化操作都不兼容！
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;hdfs dfs -ls /
</span><span class='line'>Found 1 items
</span><span class='line'>-rw-r--r--   1 Administrator supergroup       1366 2014-04-22 22:20 /README.txt</span></code></pre></td></tr></table></div></figure>


<p>JAVA_HOME的路径中最好不要有空格！</p>

<blockquote><p>instead e.g. c:\Progra~1\Java&#8230; instead of c:\Program Files\Java.&hellip;</p></blockquote>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFehCATjV7AAVxPp-3G94526.png" alt="" /></p>

<p>好处也是明显的，直接是windows执行，可以使用jdk自带的工具查看运行情况。</p>

<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFejOAbQmkAAZxcxSbXv0535.png" alt="" /></p>

<p>?疑问： log日志都写在hadoop.log文件中了？反正我是没看到hadoop.log的文件！</p>

<p>HDFS操作文件OK，如果按照上面步骤或者<a href="http://wiki.apache.org/hadoop/Hadoop2OnWindows">官网的wiki</a>操作，则运行mapreduce也是不会出问题的!!</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;sbin\start-yarn.cmd
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;hadoop org.apache.hadoop.examples.WordCount /README.txt /out
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0&gt;hdfs dfs -ls /out
</span><span class='line'>Found 2 items
</span><span class='line'>-rw-r--r--   1 Administrator supergroup          0 2014-04-22 22:22 /out/_SUCCESS
</span><span class='line'>-rw-r--r--   1 Administrator supergroup       1306 2014-04-22 22:22 /out/part-r-00000</span></code></pre></td></tr></table></div></figure>


<p>如果你使用上面的hadoop命令执行不了命令，请把hadoop.cmd的换行（下载下来后是unix的）转成windows的换行！</p>

<h4>问题原因分析</h4>

<p>如果你运行mapreduce失败，不外乎三种情况：没有定义HADOOP_HOME系统环境变量，hadoop.dll没有放在PATH路径下，以及yarn.application.classpath没有设置。这三个问题导致。如果你不幸碰到了，那我们如何来确认问题呢？</p>

<p>下面一步步的来解读这个处理过程。在运行mapreduce时报错，可以使用远程调试方式来确认发生的具体位置。（如果你还没有弄好本地开发环境，请先看[三、导入源码到eclipse]）</p>

<p>第一步 调试NodeManager，从根源下手</p>

<p>由于windows的hadoop的程序都是<strong>直接</strong>运行的，不像linux还要ssh再登陆然后在启动。所以这里直接设置HADOOP_NODEMANAGER_OPTS就可以了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set HADOOP_NODEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8092"
</span><span class='line'>
</span><span class='line'>E:\local\libs\big\hadoop-2.2.0\sbin\start-yarn.cmd
</span><span class='line'>starting yarn daemons
</span><span class='line'>
</span><span class='line'>E:\&gt;hadoop org.apache.hadoop.examples.WordCount /in /out</span></code></pre></td></tr></table></div></figure>


<p>运行任务之前，在ContainerLaunch#call#171行打个断点（可以查看执行的java命令脚本内容，#254writeLaunchEnv写入cmd文件）。同时可以去到<code>nm-local-dir/nmPrivate</code>目录下查看任务的本地临时文件。application_XXX/containter_XXX/launch_container.cmd文件是MRAppMaster/YarnChild/YarnChild的启动脚本。</p>

<ul>
<li><p>调试。备份生成的脚本文件，开启死循环拷贝模式，把缓存留下来慢慢看</p>

<pre><code class="``">  while true ; do cp -rf nm-local-dir/ backup/ ; sleep 0.5; done
</code></pre>

<p>  <img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFe0aAKlntAAg2A_A0xS0493.png" alt="" /></p></li>
<li><p>查看缓存文件</p>

<ul>
<li>真正启动Mapreduce(yarnchild)的脚本文件launch_container.cmd</li>
<li><p>查看系统日志，确定错误</p>

<p>  <img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFd1GAMcbxAAIU6umDgu4394.png" alt="" /></p></li>
<li><p>classpath路径</p>

<p>  <img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFdsqAVo5pAAlRz9SLM3s104.png" alt="" /></p></li>
<li><p>Job任务类型。第三个参数！</p>

<p>  <img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFd4qAJKJAAAr2lBOh9yU947.png" alt="" /></p></li>
</ul>
</li>
</ul>


<p>这里可以查看脚本，确认HADOOP的相关目录是否正确！以及查看classpath的MANIFEST.MF查看依赖的jar是否完整！也可以通过任务的名称了解相关信息。</p>

<ul>
<li>路径问题，不影响大局（可以不关注/不修改）</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0B/28/wKhkA1QFd8uAR1y0AAcQFk5KVVo655.png" alt="" /></p>

<ul>
<li>调试map/reduce</li>
</ul>


<p>调试程序mapreduce比较好办了，毕竟代码都是自己写的好弄。可以使用mrunit。</p>

<p>map和reduce的进程都是动态的，既不能通过命令行的OPTS参数指定。如果要调试map/reduce需要在opts中传递给它们。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop org.apache.hadoop.examples.WordCount  "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090" /in /out</span></code></pre></td></tr></table></div></figure>


<ul>
<li>library问题</li>
</ul>


<p>如果因为library的问题报access$0的错，提交任务都不成功，可以把自定义的dll路径加入java.library.path尝试一下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop org.apache.hadoop.examples.WordCount "-Dmapreduce.map.java.opts= -Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native" "-Dmapreduce.reduce.java.opts=-Djava.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native"  /in /out</span></code></pre></td></tr></table></div></figure>


<hr />

<h3>1.6 cygwin下运行</h3>

<p>要在cygwin下面把hadoop弄起来，你要把cygwin与java的路径区分，理清楚路径，配置工作就成功一半咯！既然用的还是windows的java程序。配置文件也是最终提供给java执行的，所以配置都不需要修改。</p>

<p>要在cygwin中运行hadoop，仅仅搞定脚本就ok了！在执行java命令之前，把cygwin的路径转换为windows。</p>

<ul>
<li>修改了hadoop-env.sh的内容：</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export JAVA_HOME=/cygdrive/d/Java/jdk1.7.0_02 #本来已经在环境变量中定义了，但是执行后台批处理的时刻不会调用环境变量的配置！
</span><span class='line'>export HADOOP_HEAPSIZE=512
</span><span class='line'>export HADOOP_PID_DIR=${HADOOP_PID_DIR:-${HADOOP_LOG_DIR}}</span></code></pre></td></tr></table></div></figure>


<p>cygwin也就是linux的默认加载native的路径是libs/native！！拷一份过去把！！或者配置JAVA_LIBRARY_PATH，参见下面的修改Shell脚本部分。</p>

<p>cygwin自带的工具有个优势：运行脚本和java命令都不出现乱码。（或许把SecureCRT改成GBK编码也行）</p>

<ul>
<li>修改shell脚本命令</li>
</ul>


<p>由于java在windows和linux在识别文件路径上也有差异。如/data传给java，在windows会加上当前路径的盘符(e.g. E)，那写入数据目录就为<code>e:/data</code>。</p>

<p>同时，不同操作系统的classpath的组织方式也不同。(1)需要对classpath已经文件夹的路径进行转换，才能在cygwin下正常的运行java程序。
所以，只要在执行java命令之前对路径和classpath进行转换即可。(2)还需要对getconf返回值的换行符进行处理。涉及到下列的文件：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>libexec/hadoop-config.sh
</span><span class='line'>bin/hadoop
</span><span class='line'>bin/hdfs
</span><span class='line'>bin/mapred
</span><span class='line'>bin/yarn
</span><span class='line'>sbin/start-dfs.sh
</span><span class='line'>sbin/stop-dfs.sh</span></code></pre></td></tr></table></div></figure>


<p>重点修改两个问题如下：</p>

<ul>
<li>配置</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/* hadoop-config.sh */
</span><span class='line'>
</span><span class='line'># 定义时注意，处理cygwin路径时只处理了以/cygdrive开头的路径！ 
</span><span class='line'>export JAVA_LIBRARY_PATH=/cygdrive/e/local/libs/big/hadoop-2.2.0/bin</span></code></pre></td></tr></table></div></figure>


<p>由于windows配置时，把hadoop.dll的动态链接库放到bin目录下，而linux（cygwin）的sh脚本默认是去lib/native下面，所以需要定义一下链接库的查找路径。</p>

<ul>
<li>脚本</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/* hadoop-config.sh */
</span><span class='line'>
</span><span class='line'>/* 在调用java命令前，调用该方法 */
</span><span class='line'>function Cygwin_Patch_PathConvert() {
</span><span class='line'>
</span><span class='line'>  cygwin=false
</span><span class='line'>  case "`uname`" in
</span><span class='line'>  CYGWIN*) cygwin=true;;
</span><span class='line'>  esac
</span><span class='line'>
</span><span class='line'>  # cygwin path translation
</span><span class='line'>  if $cygwin; then
</span><span class='line'>      CLASSPATH=`cygpath -p -w "$CLASSPATH"`
</span><span class='line'>      # ssh过来执行命令是不从.bash_profile获取参数！
</span><span class='line'>      if [ "X$HADOOP_HOME" != "X" ]; then
</span><span class='line'>          HADOOP_HOME=`cygpath -w "$HADOOP_HOME"`
</span><span class='line'>      fi
</span><span class='line'>      HADOOP_LOG_DIR=`cygpath -w "$HADOOP_LOG_DIR"`
</span><span class='line'>      if [ "X$TOOL_PATH" != "X" ]; then
</span><span class='line'>          TOOL_PATH=`cygpath -p -w "$TOOL_PATH"`
</span><span class='line'>      fi
</span><span class='line'>      
</span><span class='line'>HADOOP_COMMON_HOME=`cygpath -w "$HADOOP_COMMON_HOME"`
</span><span class='line'>JAVA_HOME=`cygpath -w "$JAVA_HOME"`
</span><span class='line'>HADOOP_YARN_HOME=`cygpath -w "$HADOOP_YARN_HOME"`
</span><span class='line'>HADOOP_HDFS_HOME=`cygpath -w "$HADOOP_HDFS_HOME"`
</span><span class='line'>HADOOP_CONF_DIR=`cygpath -w "$HADOOP_CONF_DIR"`
</span><span class='line'>
</span><span class='line'># HOME
</span><span class='line'>      
</span><span class='line'>      # 把带/cygdrive/[abc]形式的路径转换为windows路径
</span><span class='line'>      HADOOP_OPTS=`echo $HADOOP_OPTS | awk -F" " '{for(i=1;i&lt;=NF;i++)print $i}' | awk -F"=" ' {if($2~/^\/cygdrive\/[a|b|c|d|e]/){print $1;system("cygpath -w -p " $2 )}else{ print $0 }; print ""}' | awk 'BEGIN{opt="";last=""}{if($0~/^$/){ opt=opt " "; last="" }else{ if(last!=""){ opt=opt "="} opt=opt $0; last=$0; }; }END{ print opt }' `
</span><span class='line'>      
</span><span class='line'>      YARN_OPTS=`echo $YARN_OPTS | awk -F" " '{for(i=1;i&lt;=NF;i++)print $i}' | awk -F"=" ' {if($2~/^\/cygdrive\/[a|b|c|d|e]/){print $1;system("cygpath -w -p " $2 )}else{ print $0 }; print ""}' | awk 'BEGIN{opt="";last=""}{if($0~/^$/){ opt=opt " "; last="" }else{ if(last!=""){ opt=opt "="} opt=opt $0; last=$0; }; }END{ print opt }' `
</span><span class='line'>
</span><span class='line'>      JAVA_LIBRARY_PATH=`cygpath -p -w "$JAVA_LIBRARY_PATH"`
</span><span class='line'>      
</span><span class='line'>  fi
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>/* 系统的换行符不同，需要转换 */
</span><span class='line'>SECONDARY_NAMENODES=$($HADOOP_PREFIX/bin/hdfs getconf -secondarynamenodes 2&gt;/dev/null | sed 's/^M//g' )</span></code></pre></td></tr></table></div></figure>


<p>在解析OPTS时执行cygpath转换的时刻，也需要加上-p的参数！OPTS中有java.library.path的环境变量！</p>

<ul>
<li>HDFS文件系统测试</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/hadoop namenode -format
</span><span class='line'>sbin/start-dfs.sh
</span><span class='line'>ps</span></code></pre></td></tr></table></div></figure>


<p>jps没有作用了；或者也可以通过任务管理器/<strong>ProcessExplorer</strong>查看java.exe，命令行列还可以查看具体的执行命令，对应的什么服务。</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> 映像名称     </th>
<th style="text-align:left;"> 用户名         </th>
<th style="text-align:left;"> 命令行</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_namenode -Xmx512m &hellip; org.apache.hadoop.hdfs.server.namenode.NameNode</td>
</tr>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_datanode -Xmx512m &hellip; org.apache.hadoop.hdfs.server.datanode.DataNode</td>
</tr>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_secondarynamenode &hellip; org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode</td>
</tr>
</tbody>
</table>


<p>修改了hadoop的脚本，启动环境（cygwin下启动和windows启动都可以），就可以操作HDFS了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~
</span><span class='line'>$ hadoop/bin/hadoop fs -put job.xml /
</span><span class='line'>14/04/22 23:53:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~
</span><span class='line'>$ export JAVA_LIBRARY_PATH=/cygdrive/e/local/libs/big/hadoop-2.2.0/bin
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~
</span><span class='line'>$ hadoop/bin/hadoop fs -ls /
</span><span class='line'>Found 4 items
</span><span class='line'>-rw-r--r--   1 Administrator supergroup       1366 2014-04-22 22:20 /README.txt
</span><span class='line'>-rw-r--r--   1 Administrator supergroup      66539 2014-04-22 23:53 /job.xml
</span><span class='line'>drwxr-xr-x   - Administrator supergroup          0 2014-04-22 23:34 /out
</span><span class='line'>drwx------   - Administrator supergroup          0 2014-04-22 22:21 /tmp</span></code></pre></td></tr></table></div></figure>


<p>如果执行权限问题，可以使用设置HADOOP_USER_NAME的方式处理：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ export HADOOP_USER_NAME=Administrator
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ bin/hadoop fs -rmr /out</span></code></pre></td></tr></table></div></figure>


<h4>MapReduce任务测试</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sbin/start-yarn.sh
</span><span class='line'>ps</span></code></pre></td></tr></table></div></figure>


<p>yarn资源框架启动后，任务管理又会添加两个java的程序：</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> 映像名称     </th>
<th style="text-align:left;"> 用户名         </th>
<th style="text-align:left;"> 命令行</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_resourcemanager &hellip;  org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</td>
</tr>
<tr>
<td style="text-align:left;"> java.exe     </td>
<td style="text-align:left;"> Administrator  </td>
<td style="text-align:left;"> D:\Java\jdk1.7.0_02\bin\java.exe -Dproc_nodemanager &hellip; org.apache.hadoop.yarn.server.nodemanager.NodeManager</td>
</tr>
</tbody>
</table>


<h4>提交任务，执行任务处理</h4>

<p>在cygwin环境下，hdfs和yarn都启动成功了，并且能传文件到HDFS中。但是由于cygwin环境最终还是使用windows的java程序集群执行任务！</p>

<p>（可考虑[2.2 Eclipse提交MapReduce]）</p>

<ul>
<li>已处理问题一： cygwin下启动nodemanager，路径没转换</li>
</ul>


<p>由于在cygwin下面启动，大部分的环境变量都是从cygwin带过来的！解析conf中的变量时会使用nodemanager中对应变量的值，如HADOOP_MAPRED_HOME等。</p>

<p>在cygwin使用start-yarn.sh调用java启动程序之前需要转换路径为windows下的路径。在上面的操作已经进行了处理。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 在临时目录下生成了launch_container.cmd文件，用于执行命令，而里面环境变量的值有些cygwin环境下的！
</span><span class='line'>
</span><span class='line'># 设置端口调试nodemanager
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ grep "8092" etc/hadoop/*
</span><span class='line'>etc/hadoop/yarn-env.sh:export YARN_NODEMANAGER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8092"
</span><span class='line'>
</span><span class='line'>## windows下这个方法大有文章，会把客户端传递的CLASSPATH写入jar的MANIFEST.MF中！
</span><span class='line'>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv()</span></code></pre></td></tr></table></div></figure>


<ul>
<li>已处理问题二：执行mapreduce任务时，缺少环境变量（使用Process Explorer工具查看）</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># 设置远程调试map
</span><span class='line'>hadoop org.apache.hadoop.examples.WordCount  "-Dmapreduce.map.java.opts=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8090" /job.xml /out
</span><span class='line'>
</span><span class='line'># mapred-site.xml设置超时时间
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapred.task.timeout&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;1800000&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'># 结束任务
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ bin/hadoop job -kill job_1398407971082_0003</span></code></pre></td></tr></table></div></figure>


<ul>
<li>取不到HADOOP_HOME环境变量，查找winutils.exe时报错！

<ul>
<li>在hadoop-env.sh中增加定义HADOOP_HOME！</li>
</ul>
</li>
<li>library路径问题，解析动态链接库hadoop.dll失败！

<ul>
<li>增加-D参数吧！</li>
</ul>
</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop org.apache.hadoop.examples.WordCount "-Dmapreduce.map.java.opts= -Djava.library.path=E:\local\libs\big\hadoop-2.2.0\bin" "-Dmapreduce.reduce.java.opts=-Djava.library.path=E:\local\libs\big\hadoop-2.2.0\bin"  /job.xml /out</span></code></pre></td></tr></table></div></figure>


<p>windows泽腾啊。</p>

<ul>
<li>问题二：直接提交任务到linux集群，环境变量不匹配</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ bin/hadoop  fs -ls hdfs://192.168.1.104:9000/
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ export HADOOP_USER_NAME=hadoop
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$  bin/hadoop  org.apache.hadoop.examples.WordCount   -fs hdfs://192.168.1.104:9000 -jt 192.168.1.104 /in /out</span></code></pre></td></tr></table></div></figure>


<p>由于本地是windows的java执行任务提交到集群，所以使用了<code>%JAVA_HOME%</code>，以及windows下的CLASSPATH！执行任务时，同时把nodemanager节点的临时目录备份下来再慢慢查看：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@slave temp]$ while true ; do cp -rf nm-local-dir/ backup/ ; sleep 0.1; done
</span><span class='line'>[hadoop@slave temp]$ find . -name "*.sh"</span></code></pre></td></tr></table></div></figure>


<p>修复该问题，可以参考[2.2 Eclipse提交MapReduce]。</p>

<h3>参考</h3>

<hr />

<h2>二、Windows下使用eclipse连接linux集群</h2>

<h3>2.1 java代码操作HDFS</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>public class HelloHdfs {
</span><span class='line'>
</span><span class='line'>  public static boolean FINISH_CLEAN = true;
</span><span class='line'>
</span><span class='line'>  public static void main(String[] args) throws IOException {
</span><span class='line'>      System.setProperty("HADOOP_USER_NAME", "hadoop"); // 设置用户，否则会有读取权限的问题
</span><span class='line'>      
</span><span class='line'>      FileSystem fs = FileSystem.get(new Configuration());
</span><span class='line'>
</span><span class='line'>      fs.mkdirs(new Path("/java/folder"));
</span><span class='line'>      OutputStream os = fs.create(new Path("/java/folder/hello.txt"));
</span><span class='line'>      Writer w = new BufferedWriter(new OutputStreamWriter(os, "UTF-8"));
</span><span class='line'>      w.write("hello hadoop!");
</span><span class='line'>      w.flush();
</span><span class='line'>      w.close();
</span><span class='line'>      os.close();
</span><span class='line'>
</span><span class='line'>      FSDataInputStream is = fs.open(new Path("/java/folder/hello.txt"));
</span><span class='line'>      BufferedReader br = new BufferedReader(new InputStreamReader(is, "UTF-8"));
</span><span class='line'>      System.out.println(br.readLine());
</span><span class='line'>      br.close();
</span><span class='line'>      is.close();
</span><span class='line'>
</span><span class='line'>      // IOUtils.copyBytes(in, out, 4096, true);
</span><span class='line'>
</span><span class='line'>      if (FINISH_CLEAN)
</span><span class='line'>          fs.delete(new Path("/java"), true);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>对于访问linux集群的hdfs，只要编译通过，对集群HDFS文件系统的CRUD基本没有不会遇到什么问题。写代码过程中遇到过下面两个问题：</p>

<ul>
<li><p>如果你也引入了hive的包，可能会抛不能重写final方法的错误！由于hive中也就了proto的代码（final），调整下顺序先加载proto的包就可以了！</p>

<pre><code class="``">  log4j:WARN Please initialize the log4j system properly.
  Exception in thread "main" java.lang.VerifyError: class org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
      at java.lang.ClassLoader.defineClass1(Native Method)
      at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
</code></pre></li>
<li><p>Permission denied: user=Administrator, access=WRITE, inode=&ldquo;/&rdquo;:hadoop:supergroup:drwxr-xr-x
  这个问题的处理方式有很多。</p>

<ul>
<li>hadoop fs -chmod 777 /</li>
<li>在hdfs的配置文件中，将dfs.permissions修改为False</li>
<li>System.setProperty(&ldquo;user.name&rdquo;, &ldquo;hduser&rdquo;)/System.setProperty(&ldquo;HADOOP_USER_NAME&rdquo;, &ldquo;hduser&rdquo;)/configuration.set(&ldquo;hadoop.job.ugi&rdquo;, &ldquo;hduser&rdquo;);</li>
</ul>
</li>
</ul>


<h3>2.2 Eclipse提交MapReduce</h3>

<ul>
<li><p>需要设置HADOOP_HOME/hadoop.home.dir的环境变量，即在该目录下面有bin\winutils.exe的文件。否则会报错：</p>

<pre><code class="``">  14/04/14 20:07:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  14/04/14 20:07:58 ERROR util.Shell: Failed to locate the winutils binary in the hadoop binary path
  java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
      at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
      at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
      at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:293)
</code></pre></li>
<li><p>任务端(map/reduce)执行命令的classpath变量在客户端Client拼装的！</p>

<p>  浏览官网的<a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655" title="Remote job submit from windows to a linux hadoop cluster fails due to wrong classpath">jira</a>，然后下载并应用<a href="https://issues.apache.org/jira/secure/attachment/12616981/MRApps.patch">MRApps.patch</a>和<a href="https://issues.apache.org/jira/secure/attachment/12616982/YARNRunner.patch">YARNRunner.patch</a>两个补丁。</p>

<p>  其实就是修改Apps#addToEnvironment(Map&lt;String, String>, String, String)来拼装特定操作系统的classpath。以及JAVA_HOME等一些环境变量的值（<code>$JAVA_HOME</code> or <code>%JAVA_HOME%</code>）</p>

<p>  使用<code>patch -p1 &lt; PATCH</code>进行修复。如果patch文件不在项目根路径，可以删除补丁内容前面文件夹路径，直接与源文件放一起然后应用patch就行了。当然你根据修改的内容手动修改也是OK的。</p></li>
</ul>


<p>如果仅仅是作为客户端client提交任务时使用。如仅在eclipse中运行main提交任务，那么就没有必要打包！直接放到需要项目源码中即可。</p>

<pre><code>* 把应用了补丁的YARNRunner和MRApps加入到项目中
* 然后再configuration中加入`config.set("mapred.remote.os", "Linux")`
* 把mapreduce的任务打包为jar，然后`job.setJar("helloyarn.jar")`
* 最后`Run As -&gt; Java Application`运行提交
</code></pre>

<p>如果很多项目使用，可以打包出来，然后把它添加到classpath中，同时添加加入自定义的xml配置。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>lib-ext&gt;jar tvf window-client-mapreduce-patch.jar
</span><span class='line'>  25 Wed Apr 16 11:21:26 CST 2014 META-INF/MANIFEST.MF
</span><span class='line'> 26684 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapred/YARNRunner.class
</span><span class='line'> 24397 Tue Apr 15 10:32:28 CST 2014 org/apache/hadoop/mapred/YARNRunner.java
</span><span class='line'>  1406 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps$1.class
</span><span class='line'>  2450 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps$TaskAttemptStateUI.class
</span><span class='line'> 19887 Wed Apr 16 10:10:16 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps.class
</span><span class='line'> 18879 Tue Apr 15 11:42:42 CST 2014 org/apache/hadoop/mapreduce/v2/util/MRApps.java</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/0B/2A/wKhkA1QFe3eATYhNAAifW_Xk8HI644.png" alt="" /></p>

<h3>参考：</h3>

<ul>
<li><a href="http://zy19982004.iteye.com/blog/2031172">Hadoop学习三十二：Win7下无法提交MapReduce Job到集群环境</a></li>
<li><a href="http://blog.csdn.net/fansy1990/article/details/22896249">Eclipse调用hadoop2运行MR程序</a></li>
<li><a href="https://issues.apache.org/jira/browse/MAPREDUCE-5655">jira-Remote job submit from windows to a linux hadoop cluster fails due to wrong classpath</a></li>
</ul>


<hr />

<h2>三、导入源码到eclipse</h2>

<h3>环境</h3>

<p>参考前面的【window伪分布式部署】</p>

<h3>打开Visual Studio的命令行工具</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>启动\所有程序\Microsoft Visual Studio 2010\Visual Studio Tools\Visual Studio 命令提示(2010)</span></code></pre></td></tr></table></div></figure>


<h3>获取源码，检查2.2.0的分支</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone git@github.com:apache/hadoop-common.git
</span><span class='line'>git checkout branch-2.2.0</span></code></pre></td></tr></table></div></figure>


<p>也可以下载src的源码包，但是如果想修改点东西的话，clone源码应该是最佳的选择了。</p>

<ul>
<li>前面说的win32的patch，如果记得打上哦！参见[1.3 编译源代码生成本地依赖库(dll, exe)]</li>
<li>编译hadoop-auth项目的时刻报错，需要在pom中添加jetty-util的依赖，参考<a href="http://www.cnblogs.com/sysuys/p/3492791.html">找不到org.mortbay.component.AbstractLifeCycle的类文件</a>。</li>
</ul>


<h3>编译生成打包</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set PATH=c:\cygwin\bin;%PATH%
</span><span class='line'>mvn package -Pdist,native-win -DskipTests -Dmaven.javadoc.skip=true</span></code></pre></td></tr></table></div></figure>


<p>最好加上skipTests条件，不然编译等待时间不是一般的长！！</p>

<h3>导入eclipse</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn eclipse:eclipse</span></code></pre></td></tr></table></div></figure>


<p>然后使用eclipse导入已经存在的工程(existing projects into workspace)，导入后存在两个问题：</p>

<ol>
<li>stream工程的conf源码包找不到。修改为在.project文件中引用，然后把conf引用加入到.classpath。</li>
<li>common下的test代码报错。把<code>target/generated-test-sources/java</code>文件夹的也作为源码包即可。</li>
</ol>


<p><img src="http://file.bmob.cn/M00/0B/29/wKhkA1QFeTmAStwwAAuCI-ODX3Q346.png" alt="" /></p>

<p>eclipse的maven插件你得安装了（要用到M2_REPO路径），同时引用正确conf\settings.xml的Maven配置路径。</p>

<p>注意： 不要使用eclipse导入已经存在的maven方式！eclipse的m2e有些属性和插件还不支持，导入后会报很多错！而使用<code>mvn eclipse:eclipse</code>的方式是把依赖的jar加入到<code>.classpath</code>。</p>

<h3>参考</h3>

<ul>
<li><a href="http://www.cnblogs.com/zhengcong/p/3592490.html">使用Maven将Hadoop2.2.0源码编译成Eclipse项目</a></li>
</ul>


<hr />

<h2>四、胡乱噗噗</h2>

<h3>查看Debug日志</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@umcc97-44 ~]$ export HADOOP_ROOT_LOGGER=DEBUG,console
</span><span class='line'>[hadoop@umcc97-44 ~]$ hadoop fs -ls /</span></code></pre></td></tr></table></div></figure>


<h3>java加载动态链接库的环境变量java.library.path</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>D:\local\cygwin\Administrator\test&gt;java LoadLib
</span><span class='line'>
</span><span class='line'>D:\local\cygwin\Administrator\test&gt;java -Djava.library.path=. LoadLib
</span><span class='line'>Exception in thread "main" java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
</span><span class='line'>        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860)
</span><span class='line'>        at java.lang.Runtime.loadLibrary0(Runtime.java:845)
</span><span class='line'>        at java.lang.System.loadLibrary(System.java:1084)
</span><span class='line'>        at LoadLib.main(LoadLib.java:3)
</span><span class='line'>
</span><span class='line'>D:\local\cygwin\Administrator\test&gt;java -Djava.library.path=".;%PATH%" LoadLib
</span></code></pre></td></tr></table></div></figure>


<p>没有定义的时刻，会去PATH路径下找。一旦定义了java.library.path只会在给定的路径下查找！</p>

<h3>hadoop的本地native-library的位置</h3>

<p>文件具体放什么位置，随便运行一个命令，通过debug的日志就可以看到默认Library的路径。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ export HADOOP_ROOT_LOGGER=DEBUG,console
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ bin/hadoop fs -ls /
</span><span class='line'>
</span><span class='line'>14/04/18 09:48:39 DEBUG util.NativeCodeLoader: java.library.path=E:\local\libs\big\hadoop-2.2.0\lib\native
</span><span class='line'>14/04/18 09:48:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span></code></pre></td></tr></table></div></figure>


<h3>cygwin下运行java程序，路径问题</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ ls
</span><span class='line'>ENV.class  ENV.java  w3m-0.5.2
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ java ENV
</span><span class='line'>Windows 7
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ jar cvf test.jar *.class
</span><span class='line'>已添加清单
</span><span class='line'>正在添加: ENV.class(输入 = 475) (输出 = 307)(压缩了 35%)
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ ls -l
</span><span class='line'>总用量 18
</span><span class='line'>-rwxr-xr-x  1 Administrator None 475 四月  4 15:00 ENV.class
</span><span class='line'>-rw-r--r--  1 Administrator None 117 四月  4 15:00 ENV.java
</span><span class='line'>-rwxr-xr-x  1 Administrator None 758 四月 19 12:03 test.jar
</span><span class='line'>drwxr-xr-x+ 1 Administrator None   0 四月 12 20:31 w3m-0.5.2
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ java -cp test.jar ENV
</span><span class='line'>Windows 7
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ java -cp /home/Administrator/test/test.jar ENV
</span><span class='line'>错误: 找不到或无法加载主类 ENV
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ set -x
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~/test
</span><span class='line'>$ java -cp `cygpath -w /home/Administrator/test/test.jar` ENV
</span><span class='line'>++ cygpath -w /home/Administrator/test/test.jar
</span><span class='line'>+ java -cp 'D:\local\cygwin\Administrator\test\test.jar' ENV
</span><span class='line'>Windows 7</span></code></pre></td></tr></table></div></figure>


<h3>[cygwin]ssh单独用户权限问题</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~
</span><span class='line'>$ hadoop/bin/hadoop  fs -put .bash_profile /bash.info
</span><span class='line'>put: Permission denied: user=Administrator, access=WRITE, inode="/":cyg_server:supergroup:drwxr-xr-x</span></code></pre></td></tr></table></div></figure>


<ul>
<li>设置环境变量<code>HADOOP_USER_NAME=hadoop</code></li>
<li>可以使用dfs.permissions属性设置为false。</li>
<li>给位置chown/chmod赋权: <code>hadoop fs -chmod 777 /</code></li>
<li>也可以使用ssh-host-config的<code>Should privilege separation be used? (yes/no) no</code>设置为<strong>no</strong>。使用当前用户进行管理。
  <code>
  Administrator@winseliu /var
  $ chown Administrator:None empty/
  Administrator@winseliu ~
  $ /usr/sbin/sshd.exe # 启动，也可以弄个脚本到启动项，开机启动
  Administrator@winseliu ~/hadoop
  $ ps | grep ssh
       4384       1    4384       4384  ?        500 02:41:21 /usr/sbin/sshd
 </code></li>
</ul>


<h3>Visual Studio处理winutils工程</h3>

<ul>
<li><p>winutils的32位编译
  .net framework4, vs2010, 属性修改设置
  <a href="http://stackoverflow.com/questions/12267158/failure-during-conversion-to-coff-file-invalid-or-corrupt">http://stackoverflow.com/questions/12267158/failure-during-conversion-to-coff-file-invalid-or-corrupt</a>
  <a href="http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval">http://stackoverflow.com/questions/10888391/error-link-fatal-error-lnk1123-failure-during-conversion-to-coff-file-inval</a>
  <a href="http://social.msdn.microsoft.com/Forums/vstudio/en-US/eb4a7699-0f3c-4701-9790-199787f1b359/vs-2010-error-lnk1123-failure-during-conversion-to-coff-file-invalid-or-corrupt?forum=vcgeneral">http://social.msdn.microsoft.com/Forums/vstudio/en-US/eb4a7699-0f3c-4701-9790-199787f1b359/vs-2010-error-lnk1123-failure-during-conversion-to-coff-file-invalid-or-corrupt?forum=vcgeneral</a></p>

<p>  <a href="http://hi.baidu.com/dreamthief/item/aa690d1494e2caca38cb306d">http://hi.baidu.com/dreamthief/item/aa690d1494e2caca38cb306d</a></p>

<p>  在cygwin安装的时刻也看过这篇，用64位环境maven的是可以编译的
  <a href="http://www.srccodes.com/p/article/38/build-install-configure-run-apache-hadoop-2.2.0-microsoft-windows-os">http://www.srccodes.com/p/article/38/build-install-configure-run-apache-hadoop-2.2.0-microsoft-windows-os</a></p>

<p>  <a href="http://stackoverflow.com/questions/18630019/running-apache-hadoop-2-1-0-on-windows">http://stackoverflow.com/questions/18630019/running-apache-hadoop-2-1-0-on-windows</a></p></li>
</ul>


<h3>[cygwin]ipv6的问题，改成ipv4后不能登陆！</h3>

<p>可能是新版本的openssh的bug！！！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu /cygdrive/h/documents
</span><span class='line'>$ ssh -o AddressFamily=inet localhost -v
</span><span class='line'>OpenSSH_6.5, OpenSSL 1.0.1g 7 Apr 2014
</span><span class='line'>debug1: Reading configuration data /etc/ssh_config
</span><span class='line'>debug1: Connecting to localhost [127.0.0.1] port 22.
</span><span class='line'>debug1: Connection established.
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_rsa type 1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_rsa-cert type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_dsa type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_dsa-cert type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_ecdsa type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_ecdsa-cert type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_ed25519 type -1
</span><span class='line'>debug1: identity file /home/Administrator/.ssh/id_ed25519-cert type -1
</span><span class='line'>debug1: Enabling compatibility mode for protocol 2.0
</span><span class='line'>debug1: Local version string SSH-2.0-OpenSSH_6.5
</span><span class='line'>ssh_exchange_identification: read: Connection reset by peer
</span><span class='line'>
</span><span class='line'>Administrator@winseliu ~
</span><span class='line'>$ ping localhost
</span><span class='line'>
</span><span class='line'>正在 Ping winseliu [::1] 具有 32 字节的数据:
</span><span class='line'>来自 ::1 的回复: 时间&lt;1ms
</span><span class='line'>来自 ::1 的回复: 时间&lt;1ms
</span></code></pre></td></tr></table></div></figure>


<p>还不能再hosts文件中加！如，指定localhost为127.0.0.1后，得到结果为：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@winseliu ~/hadoop
</span><span class='line'>$ ssh localhost
</span><span class='line'>ssh_exchange_identification: read: Connection reset by peer</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GIT操作记录手册]]></title>
    <link href="http://winse.github.io/blog/2014/03/30/git-cheatsheet/"/>
    <updated>2014-03-30T22:15:11+08:00</updated>
    <id>http://winse.github.io/blog/2014/03/30/git-cheatsheet</id>
    <content type="html"><![CDATA[<p>Git的每次提交都有一个<strong>唯一的ID</strong>与之对应，所有的TAG/Branch/Master/HEAD等等，都是一个<strong>软链接/别名</strong>而已！这个是理解好Git的基础！</p>

<h2>提交最佳实践</h2>

<ul>
<li>commit 只改一件事情。</li>
<li>如果一个文档有多个变更，使用<code>git add --patch</code>只选择文档中的<strong>部分变更</strong>进入stage。具体怎么使用，键入命令后在输入<code>?</code></li>
<li>写清楚 commit message</li>
</ul>


<h2>配置</h2>

<h3>内建的图形化 git：</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>gitk</span></code></pre></td></tr></table></div></figure>


<h3>git服务器</h3>

<p>搭建git服务器也很方便，有很多web-server的版本，我试用了下<a href="http://www.scm-manager.org/download/">scm-manager</a>使用挺简单的！
如果已经有了SVN的服务器，可以直接使用git-svn检出到本地！！</p>

<h3>配置环境</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git config --global user.email "XXX"
</span><span class='line'>git config --global user.name "XXX"</span></code></pre></td></tr></table></div></figure>


<h3>换行（\r\n）提交检出均不转换</h3>

<p>基本上都在windows操作系统上工作，不需要进行转换！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git config --global core.autocrlf false</span></code></pre></td></tr></table></div></figure>


<ul>
<li>true 提交时转换为LF，检出时转换为CRLF</li>
<li>input 提交时转换为LF，检出时不转换</li>
<li>false 提交检出均不转换</li>
</ul>


<h3>core.safecrlf</h3>

<ul>
<li>true 拒绝提交包含混合换行符的文件</li>
<li>false 允许提交包含混合换行符的文件</li>
<li>warn 提交包含混合换行符的文件时给出警告</li>
</ul>


<h3>默认分支</h3>

<p>.git/config如下的内容：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[branch "master"]
</span><span class='line'>    remote = origin
</span><span class='line'>    merge = refs/heads/master</span></code></pre></td></tr></table></div></figure>


<p>这等于告诉git两件事:
1. 当你处于master branch, 默认的remote就是origin。
2. 当你在master branch上使用git pull时，没有指定remote和branch，那么git就会采用默认的remote（也就是origin）来merge在master branch上所有的改变</p>

<p>如果不想或者不会编辑config文件的话，可以在bush上输入如下命令行：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git config branch.master.remote origin 
</span><span class='line'>$ git config branch.master.merge refs/heads/master </span></code></pre></td></tr></table></div></figure>


<p>之后再重新git pull下。最后git push你的代码，到此步顺利完成时，则可以在Github上看到你新建的仓库以及你提交到仓库中文件了OK。</p>

<h3>修改默认Git编辑器</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git config core.editor vim
</span><span class='line'>
</span><span class='line'>$ git config --global core.editor vi</span></code></pre></td></tr></table></div></figure>


<h2>常用基本操作</h2>

<table>
<thead>
<tr>
<th style="text-align:left;"> 操作                                          </th>
<th style="text-align:left;"> 说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> git init                                      </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git init &ndash;bare                               </td>
<td style="text-align:left;"> 服务端使用bare（空架子，赤裸）的方式</td>
</tr>
<tr>
<td style="text-align:left;"> git status                                    </td>
<td style="text-align:left;"> 使用git打的最多的就是status命令，查看状态的同时会提示下一步的操作！</td>
</tr>
<tr>
<td style="text-align:left;"> git diff                                      </td>
<td style="text-align:left;"> 工作空间和index/stage进行对比</td>
</tr>
<tr>
<td style="text-align:left;"> git diff &ndash;cached                             </td>
<td style="text-align:left;"> index/stage与本地仓库进行对比</td>
</tr>
<tr>
<td style="text-align:left;"><strong>增加到变更(index/stage)</strong>                    </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git add .                                     </td>
<td style="text-align:left;"> 将当前目录添加到git仓库中，常用命令！</td>
</tr>
<tr>
<td style="text-align:left;"> git add -A                                    </td>
<td style="text-align:left;"> 添加所有改动的文档</td>
</tr>
<tr>
<td style="text-align:left;"> git add -u                                    </td>
<td style="text-align:left;"> 只加修改过的文件,新增的文件不加入</td>
</tr>
<tr>
<td style="text-align:left;"> git rm &ndash;cached <file>&hellip;                     </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"><strong>添加到本地库</strong>                               </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git commit                                    </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git commit -m &ldquo;msg&rdquo;                           </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git commit -a                                 </td>
<td style="text-align:left;"> -a是把所有的修改的（tracked）文件都commit</td>
</tr>
<tr>
<td style="text-align:left;"> git commit &ndash;amend -m &ldquo;commit message.&rdquo;       </td>
<td style="text-align:left;"> 未push到远程分支的提交，快捷的回退再提交。修补提交（修补最近一次的提交而不创建新的提交），可结合git add使用！</td>
</tr>
<tr>
<td style="text-align:left;"> git commit -v                                 </td>
<td style="text-align:left;"> -v 可以看到文件哪些内容被修改</td>
</tr>
<tr>
<td style="text-align:left;"> git commit -m &lsquo;v1.2.0-final&rsquo; &ndash;allow-empty    </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git reset                                     </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git reset HEAD^                               </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git reset &ndash;hard HEAD^                        </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git checkout file                             </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git checkout &ndash;orphan <branchname>; git rm &ndash;cached -r . </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git rebase                                    </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git merge                                     </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"><strong>日志</strong>                                       </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log                                       </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;oneline &ndash;decorate &ndash;graph          </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;stat                                </td>
<td style="text-align:left;"> 查看提交信息及更新的文件</td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;stat -p -1 &ndash;format=raw             </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log -3 <file-name>                        </td>
<td style="text-align:left;"> 文件的最近3次提交的历史版本记录</td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;stat -2                             </td>
<td style="text-align:left;"> 查看最近两次的提交描述及修改文件信息</td>
</tr>
<tr>
<td style="text-align:left;"> git log -p -2                                 </td>
<td style="text-align:left;"> 展开显示每次提交的内容差异，类似git show功能</td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;name-status                         </td>
<td style="text-align:left;"> 仅显示文件的D/M/A的状态</td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;summary                             </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;dirstat -5                          </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;pretty=format:&ldquo;%h %s&rdquo; &ndash;graph       </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git log &ndash;pretty=oneline                      </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git reflog                                    </td>
<td style="text-align:left;"> 查看本地操作历史。 ref log</td>
</tr>
<tr>
<td style="text-align:left;"> git show                                      </td>
<td style="text-align:left;"> 查看某版本文件的内容，版本库中最新提交的diff！</td>
</tr>
<tr>
<td style="text-align:left;"> git show master:index.md                      </td>
<td style="text-align:left;"> 查看历史版本的文件内容</td>
</tr>
<tr>
<td style="text-align:left;"> git show &lt;哈希值:文件目录/文件>               </td>
<td style="text-align:left;"> 查看内容</td>
</tr>
<tr>
<td style="text-align:left;"> git cat-file                                  </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"><strong>分支</strong>                                       </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git branch                                    </td>
<td style="text-align:left;"> 查看本地分支</td>
</tr>
<tr>
<td style="text-align:left;"> git branch <branch>                           </td>
<td style="text-align:left;"> 添加新分支，新分支创建后不会自动切换！！</td>
</tr>
<tr>
<td style="text-align:left;"> git branch &ndash;set-upstream branch-name origin/branch-name      </td>
<td style="text-align:left;"> * 建立本地分支和远程分支的关联</td>
</tr>
<tr>
<td style="text-align:left;"> git branch -a                                 </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git branch &ndash;list &ndash;merged                    </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git branch -r                                 </td>
<td style="text-align:left;"> 查看远程分支</td>
</tr>
<tr>
<td style="text-align:left;"> git checkout &ndash;orphan <new-branch>            </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git checkout <branch>                         </td>
<td style="text-align:left;"> 切换分支</td>
</tr>
<tr>
<td style="text-align:left;"> git checkout -b [new_branch_name]             </td>
<td style="text-align:left;"> 创建新分支并立即切换到新分支。git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致</td>
</tr>
<tr>
<td style="text-align:left;"> git branch -d branch_name                     </td>
<td style="text-align:left;"> -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项</td>
</tr>
<tr>
<td style="text-align:left;"> git branch -d -r remote_name/branch_name      </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git merge origin/local-branch                 </td>
<td style="text-align:left;"> 本地分支与主分支合并</td>
</tr>
<tr>
<td style="text-align:left;"><strong>推/拉</strong>                                      </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git pull                                      </td>
<td style="text-align:left;"> * 等价于git fetch &amp;&amp; git merge</td>
</tr>
<tr>
<td style="text-align:left;"> git fetch                                     </td>
<td style="text-align:left;"> 先把git的东西fetch到你本地然后merge后再push</td>
</tr>
<tr>
<td style="text-align:left;"> git push &ndash;rebase                             </td>
<td style="text-align:left;"> *</td>
</tr>
<tr>
<td style="text-align:left;"> git push                                      </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git push &ndash;set-upstream origin <branch>       </td>
<td style="text-align:left;"> To push the current branch and set the remote as upstream</td>
</tr>
<tr>
<td style="text-align:left;"> git push origin branch-name                   </td>
<td style="text-align:left;"> 创建远程分支(本地分支push到远程)，从本地推送分支。如果推送失败，先用git pull抓取远程的新提交</td>
</tr>
<tr>
<td style="text-align:left;"> git push -u origin master                     </td>
<td style="text-align:left;"> 将代码从本地回传到仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git push origin test:master                   </td>
<td style="text-align:left;"> 提交本地test分支作为远程的master分支</td>
</tr>
<tr>
<td style="text-align:left;"> git push -f                                   </td>
<td style="text-align:left;"> * 强推(&ndash;force)，即利用强覆盖方式用你本地的代码替代git仓库内的内容，这种方式不建议使用。</td>
</tr>
<tr>
<td style="text-align:left;"> git pull [remoteName] [localBranchName]       </td>
<td style="text-align:left;"> 获取远程版本库提交与本地提交进行合并</td>
</tr>
<tr>
<td style="text-align:left;"> git push [remoteName] [localBranchName]       </td>
<td style="text-align:left;"> 提交、推送远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git push &ndash;tags                               </td>
<td style="text-align:left;"> 提交时带上标签信息</td>
</tr>
<tr>
<td style="text-align:left;"> git push <git-url> master                     </td>
<td style="text-align:left;"> 把本地仓库提交到远程仓库的master分支中</td>
</tr>
<tr>
<td style="text-align:left;"> git push origin :branch_name                  </td>
<td style="text-align:left;"> 删除远端分支,(如果:左边的分支为空，那么将删除:右边的远程的分支。)远程的test将被删除，但是本地还会保存的，不用担心。</td>
</tr>
<tr>
<td style="text-align:left;"> git push origin :/refs/tags/tagname           </td>
<td style="text-align:left;"> 删除远端标签</td>
</tr>
<tr>
<td style="text-align:left;"> git clone <a href="http://path/to/git.git">http://path/to/git.git</a>              </td>
<td style="text-align:left;"> clone的内容会放在当前目录下的新目录</td>
</tr>
<tr>
<td style="text-align:left;"> git clone &ndash;branch <remote-branch> <git-url>  </td>
<td style="text-align:left;"> 获取指定分支，检出远程版本的分支。 git clone &ndash;branch unity /d/winsegit/hello helloclone</td>
</tr>
<tr>
<td style="text-align:left;"><strong>TAG</strong>                                        </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git tag                                       </td>
<td style="text-align:left;"> 查看标签</td>
</tr>
<tr>
<td style="text-align:left;"> git tag <tag>                                 </td>
<td style="text-align:left;"> 添加标签</td>
</tr>
<tr>
<td style="text-align:left;"> git tag -d <tag>                              </td>
<td style="text-align:left;"> 删除标签</td>
</tr>
<tr>
<td style="text-align:left;"> git tag -r                                    </td>
<td style="text-align:left;"> 查看远程标签</td>
</tr>
<tr>
<td style="text-align:left;"> git show <tag>                                </td>
<td style="text-align:left;"> 查看标签的信息</td>
</tr>
<tr>
<td style="text-align:left;"> git tag -a <tag> <msg>                        </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"><strong>REMOTE</strong>                                     </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git remote [show]                             </td>
<td style="text-align:left;"> 查看远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git remote -v                                 </td>
<td style="text-align:left;"> 查看远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git remote add [name] [url]                   </td>
<td style="text-align:left;"> 添加远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git remote set-url &ndash;push[name][newUrl]       </td>
<td style="text-align:left;"> 修改远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git remote show origin                        </td>
<td style="text-align:left;"> 远程库origin的详细信息。缺省值推送分支，有哪些远端分支还没有同步到本地，哪些已同步到本地的远端分支在远端服务器上已被删除，git pull 时将自动合并哪些分支！</td>
</tr>
<tr>
<td style="text-align:left;"> git remote show <remote-name>                 </td>
<td style="text-align:left;"> 远程版本信息查看</td>
</tr>
<tr>
<td style="text-align:left;"> git remote add origin <git-url>               </td>
<td style="text-align:left;"> 设置仓库</td>
</tr>
<tr>
<td style="text-align:left;"> git remote rm [name]                          </td>
<td style="text-align:left;"> 删除远程仓库</td>
</tr>
<tr>
<td style="text-align:left;"><strong>文件列表</strong>                                   </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;">git ls-tree &ndash;name-only  -rt <SHA-ID></td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"><strong>打包</strong>                                       </td>
<td></td>
</tr>
<tr>
<td style="text-align:left;"> git archive &ndash;format tar &ndash;output <tar> master</td>
<td style="text-align:left;"> 将 master以tar格式打包到指定文件</td>
</tr>
</tbody>
</table>


<h2>按功能点完整的操作步骤</h2>

<h3>查看指定版本文件内容</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@WINSELIU /e/git/hello (master)
</span><span class='line'>$ git ls-tree master
</span><span class='line'>100644 blob 139b30f9054cf77bd2eeabcebaf6ca3f32cd1d50    abc
</span><span class='line'>
</span><span class='line'>Administrator@WINSELIU /e/git/hello (master)
</span><span class='line'>$ git cat-file -p 139b30f9054cf77bd2eeabcebaf6ca3f32cd1d50</span></code></pre></td></tr></table></div></figure>


<h3>查看提交版本的指定文件内容</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git log abc  # 获取文件提交ID
</span><span class='line'>git cat-file -p &lt;commit-id&gt;  # 获取treeID
</span><span class='line'>git cat-file -p &lt;tree-id&gt;  # 获取当前tree的列表
</span><span class='line'>git cat-file -p &lt;file-blob-id&gt;</span></code></pre></td></tr></table></div></figure>


<h3>根据格式输出日志</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git log --pretty=oneline
</span><span class='line'>$ git log --pretty=short
</span><span class='line'>$ git log --pretty=format:'%h was %an, %ar, message: %s'
</span><span class='line'>$ git log --pretty=format:'%h : %s' --graph
</span><span class='line'>$ git log --pretty=format:'%h : %s' --topo-order --graph
</span><span class='line'>$ git log --pretty=format:'%h : %s' --date-order --graph</span></code></pre></td></tr></table></div></figure>


<p>你也可用‘medium&#8217;,&lsquo;full&rsquo;,&lsquo;fuller&rsquo;,&lsquo;email&rsquo; 或‘raw&#8217;. 如果这些格式不完全符合你的相求， 你也可以用‘&ndash;pretty=format&#8217;参数(参见：git log)来创建你自己的&#8221;格式“.</p>

<h3>本地提交后再次修改</h3>

<p><strong>修改注释</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git commit --amend </span></code></pre></td></tr></table></div></figure>


<p><strong>内容修改</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> # edit file
</span><span class='line'>git add file
</span><span class='line'>git commit --amend</span></code></pre></td></tr></table></div></figure>


<p><strong>提交了不该提交的，并撤回</strong></p>

<p>刚刚提交的不完整，想修改一些东西，加到刚才的提交中</p>

<p>commit -> modify -> add -> amend</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git reset HEAD^
</span><span class='line'>git status
</span><span class='line'>cat abc
</span><span class='line'>git diff
</span><span class='line'>git commit -a -m "for test reset"
</span><span class='line'>git log
</span><span class='line'>git diff
</span><span class='line'>
</span><span class='line'>vi abc
</span><span class='line'>git add abc
</span><span class='line'>git commit --amend
</span><span class='line'>
</span><span class='line'>git status
</span><span class='line'>git diff
</span><span class='line'>git show master:abc
</span><span class='line'>git log</span></code></pre></td></tr></table></div></figure>


<h3>没有push到远程库的提交，本地可以做的事情</h3>

<ul>
<li>git reset: 用于回溯，回到原来的提交节点，多次提交合并为一个</li>
<li>git rebase <origin>：在origin分支的基础上，合并当前分支上的提交，形成线性提交历史。 会把当前分支的提交保存为patch，然后切到origin分支应用patch，形成线性的提交，common-origin-current。</li>
</ul>


<p>rebase冲突处理时，使用git add &amp;&amp; git rebase &ndash;continue。如果你使用了git add &amp;&amp; git commit，那么当前冲突使用git rebase &ndash;skip即可。</p>

<h3>处理本地和服务器之间冲突的方式</h3>

<ul>
<li>以本地为主。 git push -f</li>
<li>归并merge。 git pull 或者 git fetch &amp;&amp; git merge</li>
<li></li>
</ul>


<h3>从Github远程服务上拿其他分支：</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@WINSELIU /e/git/to-markdown (master)
</span><span class='line'>$ git branch -r
</span><span class='line'>  origin/HEAD -&gt; origin/master
</span><span class='line'>  origin/gh-pages
</span><span class='line'>  origin/jquery
</span><span class='line'>  origin/master
</span><span class='line'>
</span><span class='line'>$ git checkout -b jquery origin/jquery</span></code></pre></td></tr></table></div></figure>


<h3>把本地的git项目发布到Github</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>touch README.md
</span><span class='line'>git init
</span><span class='line'>git add README.md
</span><span class='line'>git commit -m "first commit"
</span><span class='line'>git remote add origin git@github.com:winse/flickr-uploader.git
</span><span class='line'>git push -u origin master</span></code></pre></td></tr></table></div></figure>


<p>Push an existing repository from the command line：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git remote add origin git@github.com:winse/flickr-uploader.git
</span><span class='line'>git push -u origin master</span></code></pre></td></tr></table></div></figure>


<p>如果已经存在remote origin，使用下面的方式修改远程的地址：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@WINSELIU /d/winsegit/flickr_uploader/chrome (master)
</span><span class='line'>$ git remote set-url --add origin  git@github.com:winse/flickr-uploader.git
</span><span class='line'>
</span><span class='line'>Administrator@WINSELIU /d/winsegit/flickr_uploader/chrome (master)
</span><span class='line'>$ git remote show origin
</span><span class='line'>Warning: Permanently added 'github.com,192.30.252.128' (RSA) to the list of known hosts.
</span><span class='line'>* remote origin
</span><span class='line'>  Fetch URL: git@github.com:winse/flickr-uploader.git
</span><span class='line'>  Push  URL: git@github.com:winse/flickr-uploader.git
</span><span class='line'>  HEAD branch: (unknown)</span></code></pre></td></tr></table></div></figure>


<h3>git查看本地领先远程的提交</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@WINSELIU /d/winsegit/winse.github.com (master)
</span><span class='line'>$ git status
</span><span class='line'># On branch master
</span><span class='line'># Your branch is ahead of 'origin/master' by 2 commits.
</span><span class='line'>#   (use "git push" to publish your local commits)
</span><span class='line'>#
</span><span class='line'># Changes not staged for commit:
</span><span class='line'>#   (use "git add &lt;file&gt;..." to update what will be committed)
</span><span class='line'>#   (use "git checkout -- &lt;file&gt;..." to discard changes in working directory)
</span><span class='line'>#
</span><span class='line'>#       modified:   about.md
</span><span class='line'>#       modified:   blog/_posts/2014-01-21-monitoring-mobile-networks.md
</span><span class='line'>#
</span><span class='line'># Untracked files:
</span><span class='line'>#   (use "git add &lt;file&gt;..." to include in what will be committed)
</span><span class='line'>#
</span><span class='line'>#       default.html
</span><span class='line'>no changes added to commit (use "git add" and/or "git commit -a")
</span><span class='line'>
</span><span class='line'>Administrator@WINSELIU /d/winsegit/winse.github.com (master)
</span><span class='line'>$ git log --oneline --decorate -5
</span><span class='line'>0425ec5 (HEAD, master) 增加日志Github修改历史功能
</span><span class='line'>f3a4a58 把TAG定位到首页，并分页以及按照年分类
</span><span class='line'>d5097e3 (origin/master, origin/HEAD) plugins disabled in github page!
</span><span class='line'>e75d62b test
</span><span class='line'>876dd42 修复根目录下的md不能通过npp-windows请求编辑的BUG
</span><span class='line'>
</span><span class='line'>Administrator@WINSELIU /d/winsegit/winse.github.com (master)
</span><span class='line'>$ git cherry
</span><span class='line'>+ f3a4a58cfead3aa76e4b92de3342bee5970accb7
</span><span class='line'>+ 0425ec548aa4e3dd29cd6fbfa1b656543e85058e</span></code></pre></td></tr></table></div></figure>


<h3>找回游离的提交</h3>

<p><strong>绑定到新分支</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git reflog # 查看本地操作历史
</span><span class='line'>git branch head23 HEAD@{23} # 把分支head23指向/绑定到游离的提交</span></code></pre></td></tr></table></div></figure>


<p>git的版本都是从分支开始查找的，如果没有被分支管理的提交就游离在版本库中！
所以在reset重新修改时，最好建立分支然后再提交！
如果发现类似的提交问题，就需要尽快的修复，不然提交的ID找不到就S了！</p>

<blockquote><p>那些老的提交会被丢弃。 如果运行垃圾收集命令(pruning garbage collection), 这些被丢弃的提交就会删除. （请查看 git gc)</p></blockquote>

<p><strong>重置HEAD</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git reset --hard HEAD@{23}</span></code></pre></td></tr></table></div></figure>


<h3>删除提交</h3>

<p>删除提交E：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git tag F
</span><span class='line'>$ git tag E HEAD^
</span><span class='line'>$ git tag D HEAD^^
</span><span class='line'>$ git checkout D
</span><span class='line'>$ git cherry-pick master # 把master-patch应用到TAG-D
</span><span class='line'># fix conflicts
</span><span class='line'>$ git status # 提交
</span><span class='line'>$ git checkout master # checkout到master分支
</span><span class='line'>$ git reset --hard HEAD@{1} # 重置master到删除E后的提交
</span></code></pre></td></tr></table></div></figure>


<h3>Git浏览特定版本的文件列表</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git ls-tree --name-only  -rt &lt;SHA-ID&gt;</span></code></pre></td></tr></table></div></figure>


<h3>删除没有被git track的文件</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clean -fd # -f force branch switch/ignore unmerged entries， -d if you have new directory
</span><span class='line'>git clean -x -fd
</span><span class='line'>
</span><span class='line'>git reset --hard ( or git reset then back to 1. )
</span><span class='line'>git checkout . ( or specify with file names )
</span><span class='line'>git reset --hard ( or git reset then back to 3. )</span></code></pre></td></tr></table></div></figure>


<h3>检出SVN项目</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@ZGC-20130605LYE /e/git
</span><span class='line'>$ git svn clone http://chrome-hosts-manager.googlecode.com/svn/trunk/</span></code></pre></td></tr></table></div></figure>


<p><a href="http://www.worldhello.net/2010/02/01/339.html">http://www.worldhello.net/2010/02/01/339.html</a>下面提到的有意思：</p>

<blockquote><p>Git-svn 是 Subversion 的最佳伴侣，可以用 Git 来操作 Subversion 版本库。这带来一个非常有意思的副产品——部分检出：
可以用 git-svn 来对 Subversion 代码库的任何目录进行克隆，克隆出来的是一个git版本库
可以在部分克隆的版本库中用 Git 进行本地提交。
部分克隆版本库中的本地提交可以提交到上游 Subversion 版本库的相应目录中</p></blockquote>

<h3>Github添加项目主页github page(gh-pages)</h3>

<p>提交后就可以访问了<a href="http://winse.github.io/flickr-uploader/popup.html">页面</a>了。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@WINSELIU /d/winsegit/flickr_uploader/chrome (master)
</span><span class='line'>$ git branch -a
</span><span class='line'>* master
</span><span class='line'>  remotes/origin/master
</span><span class='line'>
</span><span class='line'>Administrator@WINSELIU /d/winsegit/flickr_uploader/chrome (master)
</span><span class='line'>$ git push origin master:gh-pages
</span><span class='line'>Warning: Permanently added 'github.com,192.30.252.128' (RSA) to the list of known hosts.
</span><span class='line'>Total 0 (delta 0), reused 0 (delta 0)
</span><span class='line'>To git@github.com:winse/flickr-uploader.git
</span><span class='line'> * [new branch]      master -&gt; gh-pages
</span><span class='line'>
</span><span class='line'>Administrator@WINSELIU /d/winsegit/flickr_uploader/chrome (master)
</span><span class='line'>$ git branch -a
</span><span class='line'>* master
</span><span class='line'>  remotes/origin/gh-pages
</span><span class='line'>  remotes/origin/master
</span></code></pre></td></tr></table></div></figure>


<p><strong><a href="https://help.github.com/articles/creating-project-pages-manually">Creating Project Pages manually</a></strong></p>

<blockquote><pre><code>  cd repository

  git checkout --orphan gh-pages
  # Creates our branch, without any parents (it's an orphan!)
  # Switched to a new branch 'gh-pages'

  git rm -rf .
  # Remove all files from the old working tree
  # rm '.gitignore'

  echo "My GitHub Page" &gt; index.html
  git add index.html
  git commit -a -m "First pages commit"
  git push origin gh-pages
</code></pre></blockquote>

<h3>子模块操作</h3>

<p><a href="http://josephjiang.com/entry.php?id=342">git-submodule教程！</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Administrator@WINSELIU /d/winsegit/jae_winse (master)
</span><span class='line'>$ git submodule add git@github.com:winse/flickr-uploader.git src/main/webapp/flickr
</span><span class='line'>
</span><span class='line'>Administrator@WINSELIU /d/winsegit/jae_winse (master)
</span><span class='line'>$ git submodule status
</span><span class='line'> 635090c5a754eebf5ce6566b7f8c65446b764f51 src/main/webapp/flickr (heads/master)
</span><span class='line'>
</span><span class='line'>Administrator@WINSELIU /d/winsegit/jae_winse (master)
</span><span class='line'>$ git commit -m "add submodule"
</span><span class='line'>[master c7dc8c7] add submodule
</span><span class='line'>warning: LF will be replaced by CRLF in .gitmodules.
</span><span class='line'>The file will have its original line endings in your working directory.
</span><span class='line'> 2 files changed, 4 insertions(+)
</span><span class='line'> create mode 100644 .gitmodules
</span><span class='line'> create mode 160000 src/main/webapp/flickr</span></code></pre></td></tr></table></div></figure>


<p>如：$ git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs</p>

<p>初始化子模块：$ git submodule init &mdash;-只在首次检出仓库时运行一次就行</p>

<p>更新子模块：$ git submodule update &mdash;-每次更新或切换分支后都需要运行一下</p>

<p>删除子模块：（分4步走哦）</p>

<ol>
<li>$ git rm &ndash;cached [path]</li>
<li>编辑“.gitmodules”文件，将子模块的相关配置节点删除掉</li>
<li>编辑“.git/config”文件，将子模块的相关配置节点删除掉</li>
<li>手动删除子模块残留的目录</li>
</ol>


<h2>其他偶尔使用命令</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git diff --check # 检查行尾有没有多余的空白
</span><span class='line'>git remote prune &lt;remotename&gt;
</span><span class='line'>git ls-remote --heads origin
</span><span class='line'>git gc --prune=now
</span><span class='line'>git ls-remote --heads &lt;remote-name&gt;
</span><span class='line'>git rm -r --cached *</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://blog.csdn.net/trochiluses/article/details/8996431">回溯 与 合并:git rebase与git reset</a></li>
<li><a href="http://ihower.tw/git/">git教程的一个站点</a></li>
<li><a href="http://ihower.tw/git/basic.html">git基本操作</a></li>
<li><a href="http://ihower.tw/git/vcs.html">版本管理介绍</a></li>
<li><a href="http://blog.csdn.net/ithomer/article/details/7529841">速查表</a></li>
<li><a href="http://git-scm.com/book/zh/Git-%E5%9F%BA%E7%A1%80-%E6%9F%A5%E7%9C%8B%E6%8F%90%E4%BA%A4%E5%8E%86%E5%8F%B2">Git 基础 - 查看提交历史</a></li>
<li><p><a href="http://gitbook.liuhui998.com/3_4.html">查看历史 －Git日志</a></p></li>
<li><p><a href="http://git-scm.com/book/zh/Git-%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2">http://git-scm.com/book/zh/Git-%E5%B7%A5%E5%85%B7-%E9%87%8D%E5%86%99%E5%8E%86%E5%8F%B2</a></p></li>
<li><a href="http://ihower.tw/blog/archives/2622">http://ihower.tw/blog/archives/2622</a></li>
<li><a href="http://git-scm.com/docs/git-rebase">http://git-scm.com/docs/git-rebase</a></li>
<li><a href="http://xiewenbo.iteye.com/blog/1285693">http://xiewenbo.iteye.com/blog/1285693</a></li>
<li><a href="http://gitready.com/">http://gitready.com/</a></li>
<li><a href="http://git-scm.com/book/zh/Git-%E5%9F%BA%E7%A1%80-%E6%9F%A5%E7%9C%8B%E6%8F%90%E4%BA%A4%E5%8E%86%E5%8F%B2">http://git-scm.com/book/zh/Git-%E5%9F%BA%E7%A1%80-%E6%9F%A5%E7%9C%8B%E6%8F%90%E4%BA%A4%E5%8E%86%E5%8F%B2</a></li>
<li><a href="http://josephjiang.com/entry.php?id=342">http://josephjiang.com/entry.php?id=342</a> git-submodule没有更好的教程了</li>
<li><a href="http://www.cnblogs.com/william9/archive/2012/09/01/2666767.html">http://www.cnblogs.com/william9/archive/2012/09/01/2666767.html</a></li>
<li><a href="http://marklodato.github.io/visual-git-guide/index-zh-cn.html">http://marklodato.github.io/visual-git-guide/index-zh-cn.html</a></li>
<li><a href="http://www.16kan.com/question/detail/321093.html">http://www.16kan.com/question/detail/321093.html</a></li>
<li><a href="http://gitbook.liuhui998.com/3_4.html">http://gitbook.liuhui998.com/3_4.html</a></li>
<li><a href="http://www.bootcss.com/p/git-guide/">http://www.bootcss.com/p/git-guide/</a></li>
<li><a href="http://blog.csdn.net/ithomer/article/details/7529022">http://blog.csdn.net/ithomer/article/details/7529022</a></li>
<li><a href="http://git-scm.com/book/zh/Git-%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF">http://git-scm.com/book/zh/Git-%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF</a></li>
<li><a href="http://git-scm.com/book/zh/Git-%E5%9F%BA%E7%A1%80-%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8">http://git-scm.com/book/zh/Git-%E5%9F%BA%E7%A1%80-%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8</a></li>
<li><a href="http://blog.csdn.net/trochiluses/article/details/14517379">http://blog.csdn.net/trochiluses/article/details/14517379</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jekyll按照tag分页面]]></title>
    <link href="http://winse.github.io/blog/2014/03/20/jekyll-tag-per-page/"/>
    <updated>2014-03-20T06:21:36+08:00</updated>
    <id>http://winse.github.io/blog/2014/03/20/jekyll-tag-per-page</id>
    <content type="html"><![CDATA[<h2>单页实现</h2>

<p>从jekll-bootstrap检出的代码中，<code>tags.html</code>实现了标签的显示。但是所有的标签和日志列表都码在一个文件里面，总感觉有点太拥挤。</p>

<pre><code>&lt;div class="page-header"&gt;
    &lt;h1&gt;{{ page.title }} {% if page.tagline %} &lt;small&gt;{{ page.tagline }}&lt;/small&gt;{% endif %}&lt;/h1&gt;
&lt;/div&gt;

&lt;ul class="tag_box inline"&gt;
    {% assign tags_list = site.tags %}  

    {% if tags_list.first[0] == null %}
        {% for tag in tags_list %} 
        &lt;li&gt;&lt;a href="#{{ tag }}-ref"&gt;{{ tag }} &lt;span&gt;{{ site.tags[tag].size }}&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
        {% endfor %}
    {% else %}
        {% for tag in tags_list %} 
        &lt;li&gt;&lt;a href="#{{ tag[0] }}-ref"&gt;{{ tag[0] }} &lt;span&gt;{{ tag[1].size }}&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
        {% endfor %}
    {% endif %}

    {% assign tags_list = nil %}
&lt;/ul&gt;

{% for tag in site.tags %} 
&lt;h2 id="{{ tag[0] }}-ref"&gt;{{ tag[0] }}&lt;/h2&gt;
&lt;ul class="index"&gt;
    {% assign pages_list = tag[1] %}  

    {% if site.JB.pages_list.provider == "custom" %}
        {% include custom/pages_list %}
    {% else %}
        {% for node in pages_list %}
            {% if node.title != null %}
                {% if group == null or group == node.group %}
                    {% if page.url == node.url %}
                    &lt;li class="active"&gt;
                        &lt;a href="{{ BASE_PATH }}{{ node.url }}" class="active"&gt;{{ node.title | xml_escape }}&lt;/a&gt;
                        &lt;span&gt;&lt;time datetime="{{ node.date | date: "%Y-%m-%d" }}"&gt;{{ node.date | date: "%Y/%m/%d" }}&lt;/time&gt;&lt;/span&gt;
                    &lt;/li&gt;
                    {% else %}
                    &lt;li&gt;
                        &lt;a href="{{ BASE_PATH }}{{ node.url }}" class="active"&gt;{{ node.title | xml_escape }}&lt;/a&gt;
                        &lt;span&gt;&lt;time datetime="{{ node.date | date: "%Y-%m-%d" }}"&gt;{{ node.date | date: "%Y/%m/%d" }}&lt;/time&gt;&lt;/span&gt;
                    &lt;/li&gt;
                    {% endif %}
                {% endif %}
            {% endif %}
        {% endfor %}
    {% endif %}

    {% assign pages_list = nil %}
    {% assign group = nil %}   
&lt;/ul&gt;
{% endfor %}
</code></pre>

<h2>插件实现分页面</h2>

<p>找了一些资料，使用plugin的方式可以生成文件，以及页面的自定义标签。在<code>_plugins</code>目录下新增<code>jekyll-tag-page.rb</code> :</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>module Jekyll
</span><span class='line'>
</span><span class='line'>  class TagPage &lt; Page
</span><span class='line'>      def initialize(site, base, dir, tag)
</span><span class='line'>          @site = site
</span><span class='line'>          @base = base
</span><span class='line'>          @dir = dir
</span><span class='line'>          @name = 'index.html'
</span><span class='line'>
</span><span class='line'>          self.process(@name)
</span><span class='line'>          self.read_yaml(File.join(base, '_layouts'), 'tag_index.html')
</span><span class='line'>
</span><span class='line'>          self.data['tags'] = tag
</span><span class='line'>      end
</span><span class='line'>  end
</span><span class='line'>
</span><span class='line'>  class TagPageGenerator &lt; Generator
</span><span class='line'>      safe true
</span><span class='line'>
</span><span class='line'>      def generate(site)
</span><span class='line'>          if site.layouts.key? 'tag_index'
</span><span class='line'>              dir = site.config['tag_dir'] || 'tags'
</span><span class='line'>              site.tags.keys.each do |tag|
</span><span class='line'>                  site.pages &lt;&lt; TagPage.new(site, site.source, File.join(dir, tag), tag)
</span><span class='line'>              end
</span><span class='line'>          end
</span><span class='line'>      end
</span><span class='line'>  end
</span><span class='line'>end
</span></code></pre></td></tr></table></div></figure>


<p>生成插件为每个TAG生成了一个页面，<code>_layout</code>模板设置为tag_index.html，在模板中可以根据当前页面的tags过滤并只显示该tag的日志列表。文件默认保存到<code>tags/TAG</code>目录下。</p>

<pre><code>{% for tag in site.tags %} 
    {% if page.tags == tag[0] %}
    &lt;h2&gt;{{ tag[0] }}&lt;/h2&gt;
    &lt;ul class="index"&gt;
        {% assign pages_list = tag[1] %}  

        {% for node in pages_list %}
            {% if node.title != null %}
            &lt;li&gt;
                &lt;a href="{{ BASE_PATH }}{{ node.url }}"&gt;{{ node.title | xml_escape }}&lt;/a&gt;
                &lt;span&gt;&lt;time datetime="{{ node.date | date: "%Y-%m-%d" }}"&gt;{{ node.date | date: "%Y/%m/%d" }}&lt;/time&gt;&lt;/span&gt;
            &lt;/li&gt;
            {% endif %}
        {% endfor %}

        {% assign pages_list = nil %}
    &lt;/ul&gt;
    {% endif %}
{% endfor %}
</code></pre>

<h2>使用脚本生成目录和md文件来实现</h2>

<p>但是由于github不支持自定义插件功能，也就是说，就算我提交了<code>_plugin</code>的代码也是无效的。<strong>最终最后的实现</strong>，使用Shell脚本在tags目录下生成文件夹和内容：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd $tools/../tags
</span><span class='line'>
</span><span class='line'>find * -type d -exec rm -rf {} +
</span><span class='line'>
</span><span class='line'>for tag in `cat tags`; do 
</span><span class='line'>mkdir $tag
</span><span class='line'>cat &gt; $tag/index.md &lt;&lt;EOF
</span><span class='line'>---
</span><span class='line'>layout: tag
</span><span class='line'>categories: [$tag]
</span><span class='line'>---
</span><span class='line'>
</span><span class='line'>EOF
</span><span class='line'>done;
</span></code></pre></td></tr></table></div></figure>


<p>脚本列表tags文件内容生成目录和index.md文件。</p>

<p>layout模板tag.html页面代码如下：</p>

<pre><code>&lt;h3&gt;Tag: {{ page.categories[-1] }}&lt;/h3&gt;
&lt;ul class="archive-list"&gt;

{% for tag in site.tags %}
{% if page.categories[-1] == tag[0] %}

{% assign pages_list = tag[1] %} 
{% for node in pages_list %}
    {% if node.title != null %}
        &lt;li class="archive"&gt;
            &lt;span&gt;
                &lt;time datetime="{{ node.date | date: "%Y-%m-%d" }}"&gt;
                    {{ node.date | date: "%Y/%m/%d" }}
                &lt;/time&gt;
            &lt;/span&gt;
            &lt;a href="{{ BASE_PATH }}{{ node.url }}" class="archive-link"&gt;{{ node.title | xml_escape }}&lt;/a&gt;
        &lt;/li&gt;
    {% endif %}
{% endfor %}
{% assign pages_list = nil %} 

{% endif %}
{% endfor %}

&lt;/ul&gt;
</code></pre>

<p>&ndash;END</p>
]]></content>
  </entry>
  
</feed>
