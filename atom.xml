<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Winse Blog]]></title>
  <link href="http://winse.github.io/atom.xml" rel="self"/>
  <link href="http://winse.github.io/"/>
  <updated>2014-10-14T11:36:58+08:00</updated>
  <id>http://winse.github.io/</id>
  <author>
    <name><![CDATA[Winse Liu]]></name>
    <email><![CDATA[winseliu@foxmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[读码] Spark1.1.0前篇]]></title>
    <link href="http://winse.github.io/blog/2014/10/12/read-spark1-source-starter/"/>
    <updated>2014-10-12T13:12:57+08:00</updated>
    <id>http://winse.github.io/blog/2014/10/12/read-spark1-source-starter</id>
    <content type="html"><![CDATA[<p>看过亚太研究院的spark在线教学视频，说spark1.0的源码仅有3w+的代码，蠢蠢欲动。先具体看下源码的量，估算估算；然后搭建eclipse读码环境。</p>

<h2>计算源码行数</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ git branch -v
</span><span class='line'>* (detached from v1.1.0) 2f9b2bd [maven-release-plugin] prepare release v1.1.0-rc4
</span><span class='line'>  master                 4d8ae70 [behind 1246] Cleanup on Connection and ConnectionManager
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ find . -name "*.scala" | grep 'src/main' | xargs sed  -e 's:\/\*.*\*\/::' -e  '/\/\*/, /\*\//{
</span><span class='line'>/\/\*/{
</span><span class='line'> s:\/\*.*::p
</span><span class='line'>}
</span><span class='line'>/\*\//{
</span><span class='line'> s:.*\*\/::p
</span><span class='line'>}
</span><span class='line'>d
</span><span class='line'>}' | sed -e '/^\s*$/d' -e '/^\s*\/\//d' | grep -v '^import' | grep -v '^package' | wc -l
</span><span class='line'>72967
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^scala^java
</span><span class='line'>1749
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^src/main^core/src/main
</span><span class='line'>877
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ ^java^scala
</span><span class='line'>38526
</span></code></pre></td></tr></table></div></figure>


<p>全部源码的数量（去掉测试）大概在7W左右，仅计算core下面的代码量在4W。从量上面来说还是比较乐观的，学习scala然后读spark的源码。</p>

<p>spark1.0.0的核心代码量在3w左右。1.1多了大概1w行！！</p>

<h2>Docker</h2>

<p>查看目录结构的时刻，看到spark1下面竟然有docker，不过看Dockerfile的内容只是简单的安装了scala、把本机的spark映射到docker容器、然后运行spark主从集群。</p>

<h2>导入eclipse</h2>

<p>spark使用主要使用scala编写，首先需要下载<a href="http://scala-ide.org/download/sdk.html">scala-ide</a>直接下载2.10的版本（基于eclipse，很多操作都类似）；然后下载<a href="https://github.com/apache/spark.git">spark的源码</a>检出v1.1.0的；然后使用maven生成eclipse工程文件。</p>

<p>使用<a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-Eclipse">sbt生成工程文件</a>。这种方式会缺少一些依赖的jar，处理比较麻烦，还不清楚到底是少了啥！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd sbt/
</span><span class='line'>$ sed -i 's/^M//g' *
</span><span class='line'>$ cd ..
</span><span class='line'>$ sbt/sbt eclipse -mem 512</span></code></pre></td></tr></table></div></figure>


<p>(推荐)使用MVN编译生成，<a href="http://spark.apache.org/docs/latest/building-with-maven.html">使用Maven生成官网文章</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~/git/spark
</span><span class='line'>$ git clean -x -fd #清理非仓库代码
</span><span class='line'>
</span><span class='line'>$ echo $SCALA_HOME #指定scala-home
</span><span class='line'>/cygdrive/d/scala
</span><span class='line'>
</span><span class='line'># 这里我直接修改默认值，理论上加 -Phadoop-2.2 选项应该也是可以的
</span><span class='line'>$ vi pom.xml # hadoop.version 2.2.0
</span><span class='line'>$ mvn eclipse:eclipse
</span><span class='line'>
</span><span class='line'>$ find . -name ".classpath" | xargs sed -i -e 's/including="\*\*\/\*.java"//' -e 's/excluding="\*\*\/\*.java"//'
</span><span class='line'>
</span><span class='line'>#也可以把添加特性的操作/添加scala源码包操作批量处理掉</span></code></pre></td></tr></table></div></figure>


<p>然后导入到eclipse，然后再针对性的处理报错：</p>

<ul>
<li>先把每个工程都<strong>添加scala特性</strong></li>
<li>把含有python源码包的去掉（手动删除.classpath中classpathentry即可）</li>
<li>确认下并加上<code>src/test/scala</code>的源码包。</li>
</ul>


<p>scala源文件比较多，编译的时间会比较长。把Project->Build Automatically去掉，然后一次性把问题处理掉后再手动build！</p>

<ul>
<li>手动使用<code>existing maven projects</code>导入yarn/stable，然后把<strong>yarn/common以链接的形式引入</strong>，并添加到源码包。</li>
</ul>


<p><img src="http://file.bmob.cn/M00/1C/E7/wKhkA1Q7jQ2AMhweAAOC-l-jcz4872.png" alt="" /></p>

<p>还有一个<strong> value q is not a member of StringContext </strong><a href="http://docs.scala-lang.org/overviews/quasiquotes/intro.html">quasiquotes</a>的错误，有些类需要在2.10添加编译组件才能正常编译，修改scala编译首选项。</p>

<p><img src="http://file.bmob.cn/M00/1D/07/wKhkA1Q76GyAFNYPAAEYJfk_ZGw816.png" alt="" /></p>

<h2>Maven编译spark</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</span><span class='line'>$ mvn -Pyarn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean package</span></code></pre></td></tr></table></div></figure>


<p><code>yarn</code>的profile能够可执行的jar文件（包括所有依赖的spark）。</p>

<h2>小结</h2>

<p>断断续续的写了两天，字数统计弄了大半天，主要在于多行注释的处理。时间最主要都消耗在sbt、maven构建eclipse项目文件（生成、fixed）上。编译scala量上去后确实非常非常的慢，不管是maven还是eclipse都慢！</p>

<p>下一篇将使用docker搭建spark环境，并使用远程调试连接到helloworld程序。</p>

<h2>参考</h2>

<ul>
<li><a href="http://stackoverflow.com/questions/24800129/scala-maven-builder-doesnt-understand-quasiquotes">Scala maven builder doesn&rsquo;t understand quasiquotes</a></li>
<li><a href="http://docs.scala-lang.org/overviews/macros/paradise.html">Macro Paradise</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[思考]]></title>
    <link href="http://winse.github.io/blog/2014/10/07/thinking/"/>
    <updated>2014-10-07T19:07:26+08:00</updated>
    <id>http://winse.github.io/blog/2014/10/07/thinking</id>
    <content type="html"><![CDATA[<p>随着年龄的增大，很多原来不曾想的问题慢慢的都开始环绕在自己四周。开始让自己不得不反思，不得不去改变。</p>

<p>本人是一个性格比较极端，又很内向，所以对自己不关心、无自己原来没有直接联系的东西，很少体现积极主动的一面。时时刻刻展现着保守派的作风。自己又在学习能力方面自我感觉良好，对现状总是很不满，对一样事物的持续坚持的耐久力不足（倒不是不能吃苦、吃不了苦的问题）！</p>

<p>从出生到毕业，一直以来都有亲人朋友让我依靠，有很明确值得挑战和超越的目标（总体水平一般，在我前面的人乌压压一片）。出来工作后一直都很迷失，不知道自己能干啥，可以干啥，师范类专业连教师资格证都没有拿到（不是后悔，自己觉得不应该）！！现在想来其实自己太执拗，像极了不撞南墙死不改的蛮牛！！</p>

<p>年龄增加体力不及，开始思考着应该去锻炼锻炼了，但是一直各种借口无疾而终！觉得身体还行，以后再说。。。
工作资历增加直接辅导指导的大哥不再，开始各种瞎折腾，东一锤西一棒，终究是拣了芝麻丢了西瓜！觉得学习能力强以后都敢都来得及，以后再学呗。。。   <br/>
但是在运动场上，一直坚持运动的同学，打个3、4个小时的羽毛球气不喘一下，这时开始懊悔。
当原来一起协作的同事，开始在领域有所斩获，各种嫉妒羡慕的心里开始作祟。</p>

<p>星期一个个的开始了结束，自己却没有得到该有的锤炼和进度，在大势所趋下，自己却总是那么的慢慢吞吞！阅读一个类的千行源码，竟然断断续续花费了仅2个月！本来年前看tomcat原来的计划最终石沉大海！</p>

<p>总是对自己不够狠；狠下来一次后，总是各种理由，最终不能坚持！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[配置ssh登录docker-centos]]></title>
    <link href="http://winse.github.io/blog/2014/09/30/docker-ssh-on-centos/"/>
    <updated>2014-09-30T00:10:02+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/30/docker-ssh-on-centos</id>
    <content type="html"><![CDATA[<p>上一篇写的是docker的入门知识，并没有进行实战。这些记录下使用ssh登录centos容器。</p>

<p>前文中参考的博客介绍了使用ssh登录tutorial容器（ubuntu），然后进行tomcat的安装，以及通过端口映射在客户机进行访问的例子。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull learn/tutorial
</span><span class='line'>docker run -i -t learn/tutorial /bin/bash
</span><span class='line'>  apt-get update
</span><span class='line'>  apt-get install openssh-server
</span><span class='line'>  which sshd
</span><span class='line'>  /usr/sbin/sshd
</span><span class='line'>  mkdir /var/run/sshd
</span><span class='line'>  passwd #输入用户密码，我这里设置为123456，便于SSH客户端登陆使用
</span><span class='line'>  exit #退出
</span><span class='line'>docker ps -l
</span><span class='line'>docker commit 51774a81beb3 learn/tutorial # 提交后，下次启动就可以基于容器更改的系统
</span><span class='line'>docker run -d -p 49154:22 -p 80:8080 learn/tutorial /usr/sbin/sshd -D
</span><span class='line'>ssh root@127.0.0.1 -p 49154
</span><span class='line'>  # 在ubuntu 12.04上安装oracle jdk 7
</span><span class='line'>  apt-get install python-software-properties
</span><span class='line'>  add-apt-repository ppa:webupd8team/java
</span><span class='line'>  apt-get update
</span><span class='line'>  apt-get install -y wget
</span><span class='line'>  apt-get install oracle-java7-installer
</span><span class='line'>  java -version
</span><span class='line'>  # 下载tomcat 7.0.47
</span><span class='line'>  wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-7/v7.0.47/bin/apache-tomcat-7.0.47.tar.gz
</span><span class='line'>  # 解压，运行
</span><span class='line'>  tar xvf apache-tomcat-7.0.47.tar.gz
</span><span class='line'>  cd apache-tomcat-7.0.47
</span><span class='line'>  bin/startup.sh</span></code></pre></td></tr></table></div></figure>


<p>然而在centos上，运行是不成功的。总结操作如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker pull centos:centos6
</span><span class='line'>[root@docker ~]# docker run -i -t  centos:centos6 /bin/bash
</span><span class='line'>  yum install which openssh-server openssh-clients
</span><span class='line'>
</span><span class='line'>  /usr/sbin/sshd # 这里会报错，需要手动生成key
</span><span class='line'>  ssh-keygen -f /etc/ssh/ssh_host_rsa_key
</span><span class='line'>  ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key
</span><span class='line'>
</span><span class='line'>  vi /etc/pam.d/sshd  # 修改pam_loginuid.so为optional
</span><span class='line'>  # /bin/sed -i 's/.*session.*required.*pam_loginuid.so.*/session optional pam_loginuid.so/g' /etc/pam.d/sshd
</span><span class='line'>  
</span><span class='line'>  passwd # 添加密码</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>[root@docker ~]# docker commit 3a7b6994bb2a winse/hadoop # 保存为自己使用的版本
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d winse/hadoop /usr/sbin/sshd
</span><span class='line'>f5cb57f6ec22dd9d257bf610322e2bd547ea0064262fcad63308b932c0490670
</span><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS                     PORTS               NAMES
</span><span class='line'>f5cb57f6ec22        winse/hadoop:latest   /usr/sbin/sshd      2 seconds ago       Exited (0) 2 seconds ago                       sharp_rosalind      
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run -d -p 8888:22 winse/hadoop /usr/sbin/sshd -D
</span><span class='line'>f9814253159373e8a8df3261904200a733b41c63f55708db3cb56a7ebf650cef
</span><span class='line'>[root@docker ~]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS                  NAMES
</span><span class='line'>f98142531593        winse/hadoop:latest   /usr/sbin/sshd -D   5 seconds ago       Up 4 seconds        0.0.0.0:8888-&gt;22/tcp   boring_bell         
</span><span class='line'>[root@docker ~]# ssh localhost -p 8888
</span><span class='line'>The authenticity of host '[localhost]:8888 ([::1]:8888)' can't be established.
</span><span class='line'>RSA key fingerprint is f5:5e:be:ae:ea:b1:ed:e8:49:43:28:9e:80:87:0d:86.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added '[localhost]:8888' (RSA) to the list of known hosts.
</span><span class='line'>root@localhost's password: 
</span><span class='line'>Last login: Mon Sep 29 14:48:23 2014 from localhost
</span><span class='line'>-bash-4.1# </span></code></pre></td></tr></table></div></figure>


<p>参数<code>-D</code>表示sshd运行在前台。这样当前的docker容器就会一直有程序在运行，不至于执行完指定的任务就被关闭掉了。</p>

<p>在centos配置ssh登录需要进行额外参数的设置。这个还是挺折腾人的。关于把<code>/etc/pam.d/sshd</code>中的<code>pam_loginuid.so</code>修改为optional，<a href="(http://stackoverflow.com/questions/21391142/why-is-it-needed-to-set-pam-loginuid-to-its-optional-value-with-docker">stackoverflow</a>)上的回答还是挺中肯的。</p>

<p>连上ssh后，下一步就和你远程操作服务器一样了。其实docker运行一个容器后，就会分配一个ip，你也可以根据这个ip来连接。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -t -i winse/hadoop /bin/bash
</span><span class='line'>bash-4.1# ssh localhost
</span><span class='line'>ssh: connect to host localhost port 22: Connection refused
</span><span class='line'>bash-4.1# service sshd start
</span><span class='line'>Starting sshd:                                             [  OK  ]
</span><span class='line'>bash-4.1# ifconfig
</span><span class='line'>eth0      Link encap:Ethernet  HWaddr 1E:2B:23:16:98:7E  
</span><span class='line'>          inet addr:172.17.0.31  Bcast:0.0.0.0  Mask:255.255.0.0
</span><span class='line'>          inet6 addr: fe80::1c2b:23ff:fe16:987e/64 Scope:Link
</span><span class='line'>
</span><span class='line'># 新开一个终端
</span><span class='line'>[root@docker ~]# ssh 172.17.0.31
</span><span class='line'>The authenticity of host '172.17.0.31 (172.17.0.31)' can't be established.
</span><span class='line'>RSA key fingerprint is f5:5e:be:ae:ea:b1:ed:e8:49:43:28:9e:80:87:0d:86.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added '172.17.0.31' (RSA) to the list of known hosts.
</span><span class='line'>root@172.17.0.31's password: 
</span><span class='line'>Last login: Mon Sep 29 14:48:23 2014 from localhost
</span><span class='line'>-bash-4.1#           </span></code></pre></td></tr></table></div></figure>


<h2>使用Dockerfile脚本安装</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# mkdir hadoop
</span><span class='line'>[root@docker ~]# cd hadoop/
</span><span class='line'>[root@docker hadoop]# touch Dockerfile
</span><span class='line'>[root@docker hadoop]# vi Dockerfile
</span><span class='line'>  # hadoop2 on docker-centos
</span><span class='line'>  FROM centos:centos6
</span><span class='line'>  MAINTAINER Winse &lt;fuqiuliu2006@qq.com&gt;
</span><span class='line'>  RUN yum install -y which openssh-clients openssh-server #-y表示交互都输入yes
</span><span class='line'>
</span><span class='line'>  RUN ssh-keygen -f /etc/ssh/ssh_host_rsa_key
</span><span class='line'>  RUN ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key
</span><span class='line'>
</span><span class='line'>  RUN echo 'root:hadoop' |chpasswd
</span><span class='line'>
</span><span class='line'>  RUN sed -i '/pam_loginuid.so/c session    optional     pam_loginuid.so'  /etc/pam.d/sshd
</span><span class='line'>
</span><span class='line'>  EXPOSE 22
</span><span class='line'>  CMD /usr/sbin/sshd -D
</span><span class='line'>  
</span><span class='line'>[root@docker hadoop]# docker build -t="winse/hadoop" .
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>winse/hadoop        latest              9d7f115ef0ec        5 minutes ago       289.1 MB
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker run -d --name slaver1 winse/hadoop
</span><span class='line'>[root@docker hadoop]# docker run -d --name slaver2 winse/hadoop
</span><span class='line'>[root@docker hadoop]# docker run -d --name master1 -P --link slaver1:slaver1 --link slaver2:slaver2  winse/hadoop
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker restart slaver1 slaver2 master1
</span><span class='line'>slaver1
</span><span class='line'>slaver2
</span><span class='line'>master1
</span><span class='line'>
</span><span class='line'>[root@docker hadoop]# docker port master1 22
</span><span class='line'>0.0.0.0:49159
</span><span class='line'>[root@docker hadoop]# ssh localhost -p 49159
</span><span class='line'>... 
</span><span class='line'>-bash-4.1# cat /etc/hosts
</span><span class='line'>172.17.0.31     7ef63f98e2d1
</span><span class='line'>127.0.0.1       localhost
</span><span class='line'>::1     localhost ip6-localhost ip6-loopback
</span><span class='line'>fe00::0 ip6-localnet
</span><span class='line'>ff00::0 ip6-mcastprefix
</span><span class='line'>ff02::1 ip6-allnodes
</span><span class='line'>ff02::2 ip6-allrouters
</span><span class='line'>172.17.0.29     slaver1
</span><span class='line'>172.17.0.30     slaver2</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.blogjava.net/yongboy/archive/2013/12/12/407498.html">Docker学习笔记之一，搭建一个JAVA Tomcat运行环境</a></li>
<li><a href="http://www.csdn123.com/html/topnews201408/36/1236.htm">Docker之配置Centos_ssh</a></li>
<li><a href="http://linux.die.net/man/8/pam_loginuid">pam_loginuid(8) - Linux man page</a></li>
<li><a href="http://stackoverflow.com/questions/21391142/why-is-it-needed-to-set-pam-loginuid-to-its-optional-value-with-docker">Why is it needed to set <code>pam_loginuid</code> to its <code>optional</code> value with docker?</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker入门]]></title>
    <link href="http://winse.github.io/blog/2014/09/27/docker-start-guide-on-centos/"/>
    <updated>2014-09-27T20:28:24+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/27/docker-start-guide-on-centos</id>
    <content type="html"><![CDATA[<p>docker进一年来火热，发现挺适合用来做运维系统发布的。如果用来捣鼓hadoop的系统部署感觉还是挺不错的。下面一起来学习下docker吧。</p>

<p>docker中提供了<a href="https://docs.docker.com/installation/windows/">windows的安装文档</a>，但是其实很坑爹啊。尽管<a href="https://github.com/boot2docker/windows-installer/releases">提供exe安装</a>，但是最终还是安装visualbox，然后启动带了docker的linux系统（iso）。</p>

<p>如果你已经安装了vmware，但没有安装linux，可以直接<a href="https://github.com/boot2docker/boot2docker/releases">下载iso</a>，然后通过iso来启动。</p>

<h2>安装</h2>

<p>如果你同时安装了vmware，又已经安装了linux，那下面简单列出安装配置docker中使用的命令。docker需要64位的linux操作系统，我这里使用的是centos6，具体的安装步骤看<a href="https://docs.docker.com/installation/centos/">官网的安装教程</a>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# yum install epel-release
</span><span class='line'>
</span><span class='line'>[root@docker ~]# yum install docker-io
</span><span class='line'>[root@docker ~]# service docker start
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker run learn/tutorial /bin/echo hello world
</span><span class='line'>Unable to find image 'learn/tutorial' locally
</span><span class='line'>Pulling repository learn/tutorial
</span><span class='line'>8dbd9e392a96: Pulling fs layer 
</span><span class='line'>8dbd9e392a96: Download complete 
</span><span class='line'>hello world
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker images
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>learn/tutorial      latest              8dbd9e392a96        17 months ago       128 MB
</span><span class='line'>[root@docker ~]# docker images learn/tutorial 
</span><span class='line'>REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
</span><span class='line'>learn/tutorial      latest              8dbd9e392a96        17 months ago       128 MB</span></code></pre></td></tr></table></div></figure>


<p>docker执行run命令时，如果指定的image本地不存在，会从<a href="https://registry.hub.docker.com/">hub服务器</a>获取。也可以先从服务器获取image，然后在执行。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull centos</span></code></pre></td></tr></table></div></figure>


<h2>简单入门</h2>

<p><a href="https://docs.docker.com/userguide/dockerizing/">HelloWorld教程</a></p>

<h4>单次执行</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run learn/tutorial /bin/echo 'hello world'
</span><span class='line'>hello world</span></code></pre></td></tr></table></div></figure>


<p>命令执行完后，容器就会关闭。</p>

<h4>交互式执行方式</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -t -i learn/tutorial /bin/bash
</span><span class='line'>root@274ede23baad:/# uptime
</span><span class='line'> 12:36:02 up  5:59,  0 users,  load average: 0.00, 0.00, 0.00
</span><span class='line'>root@9db219d2e98b:/# cat /etc/issue
</span><span class='line'>Ubuntu 12.04 LTS \n \l
</span><span class='line'>root@274ede23baad:/# pwd
</span><span class='line'>/
</span><span class='line'>root@274ede23baad:/# ls
</span><span class='line'>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  selinux  srv  sys  tmp  usr  var
</span><span class='line'>root@274ede23baad:/# exit
</span><span class='line'>exit</span></code></pre></td></tr></table></div></figure>


<ul>
<li>-t flag assigns a pseudo-tty or terminal inside our new container。</li>
<li>-i flag allows us to make an interactive connection by grabbing the standard in (STDIN) of the container.</li>
</ul>


<h4>后台任务</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker run -d learn/tutorial /bin/sh -c "while true; do echo hello world; sleep 1; done" 
</span><span class='line'>17e28b56e0cc4ddb5522736e2bcfd752d849a5b1d0b598478ee66b255801aa7c
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS               NAMES
</span><span class='line'>17e28b56e0cc        learn/tutorial:latest   /bin/sh -c 'while tr   2 minutes ago       Up 2 minutes                            trusting_wozniak    </span></code></pre></td></tr></table></div></figure>


<ul>
<li>-d flag tells Docker to run the container and put it in the background, to daemonize it.</li>
</ul>


<p>执行返回的是containter id(唯一ID)。通过ps可以查看当前的后台任务列表。ps列表中的containter id对应，可以查看相应的信息，最后的字段是一个随机指定的名字（也可以指定，后面再讲）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker ~]# docker logs trusting_wozniak
</span><span class='line'>hello world
</span><span class='line'>hello world
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>[root@docker ~]# docker stop trusting_wozniak
</span><span class='line'>trusting_wozniak
</span><span class='line'>[root@docker ~]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span></code></pre></td></tr></table></div></figure>


<p>可以通过logs查看容器的标准输出，通过stop来停止容器。</p>

<h2>深入容器</h2>

<p><a href="https://docs.docker.com/userguide/usingdocker/">Working with Containers</a></p>

<p>可以交互式的方式运行container，也可以后台任务的方式运行。</p>

<p>docker的命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Usage:  [sudo] docker [flags] [command] [arguments] ..
</span><span class='line'># Example:
</span><span class='line'>$ sudo docker run -i -t ubuntu /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>每个命令可以指定跟一系列的开关标识(flags)和参数(arguments)。</p>

<h4>各种参数</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker version
</span><span class='line'>
</span><span class='line'>$ docker run -d -P training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps -l
</span><span class='line'>CONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES
</span><span class='line'>bc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155-&gt;5000/tcp  nostalgic_morse
</span><span class='line'>
</span><span class='line'># docker run -d -p 6379 -v /home/hadoop/redis-2.8.13:/opt/redis-2.8.13 learn/tutorial /opt/redis-2.8.13/src/redis-server 
</span><span class='line'>be0b410f3601ea36070b3e519d9cc7cbe259caa2392f468c2dd2baebef42c4a8
</span><span class='line'>
</span><span class='line'># docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                     NAMES
</span><span class='line'>be0b410f3601        learn/tutorial:latest   /opt/redis-2.8.13/sr   10 seconds ago      Up 10 seconds       0.0.0.0:49153-&gt;6379/tcp   sad_colden          
</span><span class='line'>
</span><span class='line'># /home/hadoop/redis-2.8.13/src/redis-cli -p 49153
</span><span class='line'>127.0.0.1:49153&gt; keys *
</span><span class='line'>(empty list or set)
</span><span class='line'>127.0.0.1:49153&gt; </span></code></pre></td></tr></table></div></figure>


<ul>
<li>-P flag is new and tells Docker to map any required network ports inside our container to our host. This lets us view our web application.</li>
<li>-l tells the docker ps command to return the details of the last container started.</li>
<li>-a the docker ps command only shows information about running containers. If you want to see stopped containers too use the -a flag.</li>
<li>-p Network port bindings are very configurable in Docker. In our last example the -P flag is a shortcut for -p 5000 that maps port 5000 inside the container to a high port (from the range 49153 to 65535) on the local Docker host. We can also bind Docker containers to specific ports using the -p flag。</li>
<li>-v flag you can also mount a directory from your own host into a container.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker run -d -p 6379:6379 -v /home/hadoop/redis-2.8.13:/opt/redis-2.8.13 learn/tutorial /opt/redis-2.8.13/src/redis-server 
</span><span class='line'>2c50850c9437698769e54281a9f4154dc4120da2e113802454f1a23c83ab91fe
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker ps
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                    NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   29 seconds ago      Up 28 seconds       0.0.0.0:6379-&gt;6379/tcp   naughty_yonath  
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker port naughty_yonath 6379
</span><span class='line'>0.0.0.0:6379
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker logs -f naughty_yonath
</span><span class='line'>...
</span><span class='line'>[1] 27 Sep 13:48:12.192 * The server is now ready to accept connections on port 6379
</span><span class='line'>[1] 27 Sep 13:50:33.228 * DB saved on disk
</span><span class='line'>[1] 27 Sep 13:50:43.730 * DB saved on disk</span></code></pre></td></tr></table></div></figure>


<ul>
<li>-f This time though we&rsquo;ve added a new flag, -f. This causes the docker logs command to act like the tail -f command and watch the container&rsquo;s standard out. We can see here the logs from Flask showing the application running on port 5000 and the access log entries for it.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker top naughty_yonath
</span><span class='line'>UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD
</span><span class='line'>root                5015                1433                0                   21:48               ?                   00:00:00            /opt/redis-2.8.13/src/redis-server *:6379
</span><span class='line'>[root@docker redis-2.8.13]# docker inspect naughty_yonath
</span><span class='line'>...
</span><span class='line'>    "Volumes": {
</span><span class='line'>        "/opt/redis-2.8.13": "/home/hadoop/redis-2.8.13"
</span><span class='line'>    },
</span><span class='line'>    "VolumesRW": {
</span><span class='line'>        "/opt/redis-2.8.13": true
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>[root@docker redis-2.8.13]# docker inspect -f '' naughty_yonath
</span><span class='line'>map[/opt/redis-2.8.13:/home/hadoop/redis-2.8.13]
</span></code></pre></td></tr></table></div></figure>


<h4>重启</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[root@docker redis-2.8.13]# docker stop naughty_yonath
</span><span class='line'>naughty_yonath
</span><span class='line'>[root@docker redis-2.8.13]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS                     PORTS               NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   8 minutes ago       Exited (0) 5 seconds ago                       naughty_yonath      
</span><span class='line'>[root@docker redis-2.8.13]# docker start naughty_yonath
</span><span class='line'>naughty_yonath
</span><span class='line'>[root@docker redis-2.8.13]# docker ps -l
</span><span class='line'>CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                    NAMES
</span><span class='line'>2c50850c9437        learn/tutorial:latest   /opt/redis-2.8.13/sr   8 minutes ago       Up 1 seconds        0.0.0.0:6379-&gt;6379/tcp   naughty_yonath</span></code></pre></td></tr></table></div></figure>


<h4>删除</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker stop naughty_yonath
</span><span class='line'>docker rm naughty_yonath</span></code></pre></td></tr></table></div></figure>


<h2>Images</h2>

<p><a href="https://docs.docker.com/userguide/dockerimages/">Working with Docker Images</a></p>

<h4>列出本地的images</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker images
</span><span class='line'># REPO[:TAG]
</span><span class='line'>docker run -t -i ubuntu:14.04 /bin/bash
</span><span class='line'>docker run -t -i ubuntu:latest /bin/bash</span></code></pre></td></tr></table></div></figure>


<h4>从Hub获取镜像Image</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker pull centos
</span><span class='line'>docker run -t -i centos /bin/bash
</span><span class='line'>docker search sinatra 
</span><span class='line'>docker pull training/sinatra</span></code></pre></td></tr></table></div></figure>


<h4>创建自己的images</h4>

<p>直接更新image</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -t -i training/sinatra /bin/bash
</span><span class='line'>root@0b2616b0e5a8:/# gem install json
</span><span class='line'>$ sudo docker commit -m="Added json gem" -a="Kate Smith" \
</span><span class='line'>  0b2616b0e5a8 ouruser/sinatra:v2
</span><span class='line'>$ docker images
</span><span class='line'>$ docker run -t -i ouruser/sinatra:v2 /bin/bash
</span><span class='line'>root@78e82f680994:/#</span></code></pre></td></tr></table></div></figure>


<p>通过DockerFile来添加功能，进行更新。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mkdir sinatra
</span><span class='line'>$ cd sinatra
</span><span class='line'>$ touch Dockerfile
</span><span class='line'>  # This is a comment
</span><span class='line'>  FROM ubuntu:14.04
</span><span class='line'>  MAINTAINER Kate Smith &lt;ksmith@example.com&gt;
</span><span class='line'>  RUN apt-get update && apt-get install -y ruby ruby-dev
</span><span class='line'>  RUN gem install sinatra
</span><span class='line'>
</span><span class='line'>$ docker build -t="ouruser/sinatra:v2" .
</span><span class='line'>$ docker run -t -i ouruser/sinatra:v2 /bin/bash</span></code></pre></td></tr></table></div></figure>


<p>具体的DockerFile中各个指令的含义及其使用方法，参考<a href="https://docs.docker.com/userguide/dockerimages/">Building an image from a Dockerfile</a>和<a href="https://docs.docker.com/articles/dockerfile_best-practices/">Best Practices for Writing Dockerfiles</a>，以及<a href="https://docs.docker.com/reference/builder/">Dockerfile Reference</a>。具体例子<a href="https://github.com/perl/docker-perl/blob/r20140922.0/5.020.001-64bit,threaded/Dockerfile">docker-perl</a></p>

<h4>添加新标签Tag</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker tag 5db5f8471261 ouruser/sinatra:devel
</span><span class='line'>$ docker images ouruser/sinatra
</span><span class='line'>REPOSITORY          TAG     IMAGE ID      CREATED        VIRTUAL SIZE
</span><span class='line'>ouruser/sinatra     latest  5db5f8471261  11 hours ago   446.7 MB
</span><span class='line'>ouruser/sinatra     devel   5db5f8471261  11 hours ago   446.7 MB</span></code></pre></td></tr></table></div></figure>


<h4>上传分享到<a href="https://hub.docker.com/">hub</a></h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker push ouruser/sinatra</span></code></pre></td></tr></table></div></figure>


<h4>从本地删除</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker rmi training/sinatra</span></code></pre></td></tr></table></div></figure>


<h2>多container结合使用</h2>

<p><a href="https://docs.docker.com/userguide/dockerlinks/">Linking Containers Together</a></p>

<h4>端口映射</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run -d -P training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker ps nostalgic_morse
</span><span class='line'>CONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES
</span><span class='line'>bc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155-&gt;5000/tcp  nostalgic_morse
</span><span class='line'>
</span><span class='line'>docker run -d -p 5000:5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker run -d -p 127.0.0.1:5000:5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker run -d -p 127.0.0.1::5000 training/webapp python app.py
</span><span class='line'>
</span><span class='line'># The -p flag can be used multiple times to configure multiple ports.
</span><span class='line'>docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py
</span><span class='line'>
</span><span class='line'>docker port nostalgic_morse 5000
</span><span class='line'>127.0.0.1:49155</span></code></pre></td></tr></table></div></figure>


<h4>Container Linking</h4>

<p>docker想的还是很周到的。面临两个container互相访问，一个db，一个web，哪web怎么访问db的数据呢？</p>

<p>指定container的名称：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -d -P --name web training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps -l
</span><span class='line'>CONTAINER ID  IMAGE                  COMMAND        CREATED       STATUS       PORTS                    NAMES
</span><span class='line'>aed84ee21bde  training/webapp:latest python app.py  12 hours ago  Up 2 seconds 0.0.0.0:49154-&gt;5000/tcp  web
</span><span class='line'>
</span><span class='line'>$ docker inspect -f "" aed84ee21bde
</span><span class='line'>/web</span></code></pre></td></tr></table></div></figure>


<p>容器互通：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -d --name db training/postgres
</span><span class='line'>
</span><span class='line'>$ docker rm -f web
</span><span class='line'>$ docker run -d -P --name web --link db:db training/webapp python app.py
</span><span class='line'>
</span><span class='line'>$ docker ps
</span><span class='line'>CONTAINER ID  IMAGE                     COMMAND               CREATED             STATUS             PORTS                    NAMES
</span><span class='line'>349169744e49  training/postgres:latest  su postgres -c '/usr  About a minute ago  Up About a minute  5432/tcp                 db, web/db
</span><span class='line'>aed84ee21bde  training/webapp:latest    python app.py         16 hours ago        Up 2 minutes       0.0.0.0:49154-&gt;5000/tcp  web</span></code></pre></td></tr></table></div></figure>


<p>链接后，在web容器会添加DB的环境变量，同时把db的ip加入到/etc/hosts中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>$ docker run --rm --name web2 --link db:db training/webapp env
</span><span class='line'>    . . .
</span><span class='line'>    DB_NAME=/web2/db
</span><span class='line'>    DB_PORT=tcp://172.17.0.5:5432
</span><span class='line'>    DB_PORT_5432_TCP=tcp://172.17.0.5:5432
</span><span class='line'>    DB_PORT_5432_TCP_PROTO=tcp
</span><span class='line'>    DB_PORT_5432_TCP_PORT=5432
</span><span class='line'>    DB_PORT_5432_TCP_ADDR=172.17.0.5
</span><span class='line'>
</span><span class='line'>$ docker run -t -i --rm --link db:db training/webapp /bin/bash
</span><span class='line'>root@aed84ee21bde:/opt/webapp# cat /etc/hosts
</span><span class='line'>172.17.0.7  aed84ee21bde
</span><span class='line'>. . .
</span><span class='line'>172.17.0.5  db    </span></code></pre></td></tr></table></div></figure>


<p>You can see that Docker has created a series of environment variables with useful information about the source db container. Each variable is prefixed with <code>DB_</code>, which is populated from the alias you specified above. If the alias were db1, the variables would be prefixed with <code>DB1_</code>.</p>

<h2>存储</h2>

<p><a href="https://docs.docker.com/userguide/dockervolumes/">Managing Data in Containers</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Adding a data volume
</span><span class='line'>docker run -d -P --name web -v /webapp training/webapp python app.py
</span><span class='line'>
</span><span class='line'># Mount a Host Directory as a Data Volume
</span><span class='line'>docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py
</span><span class='line'># 只读
</span><span class='line'>docker run -d -P --name web -v /src/webapp:/opt/webapp:ro training/webapp python app.py
</span><span class='line'>
</span><span class='line'># Mount a Host File as a Data Volume
</span><span class='line'>docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash
</span><span class='line'>
</span><span class='line'># Creating and mounting a Data Volume Container
</span><span class='line'>docker run -d -v /dbdata --name dbdata training/postgres echo Data-only container for postgres
</span><span class='line'>docker run -d --volumes-from dbdata --name db1 training/postgres
</span><span class='line'>docker run -d --volumes-from dbdata --name db2 training/postgres
</span><span class='line'>docker run -d --name db3 --volumes-from db1 training/postgres
</span><span class='line'>
</span><span class='line'># Backup, restore, or migrate data volumes
</span><span class='line'>docker run --volumes-from dbdata -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata
</span><span class='line'>docker run -v /dbdata --name dbdata2 ubuntu /bin/bash
</span><span class='line'>docker run --volumes-from dbdata2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar
</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://www.blogjava.net/yongboy/archive/2013/12/12/407498.html">Docker学习笔记之一，搭建一个JAVA Tomcat运行环境</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在windows开发测试mapreduce几种方式]]></title>
    <link href="http://winse.github.io/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature/"/>
    <updated>2014-09-17T12:55:38+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/17/windows-hadoop2-test-your-mapreduce-feature</id>
    <content type="html"><![CDATA[<blockquote><p>备注： 文后面的maven打包、以及执行的shell脚本还是极好的&hellip;</p></blockquote>

<p>hadoop提供的两大组件HDFS、MapReduce。其中HDFS提供了丰富的API，最重要的有类似shell的脚本进行操作。而编写程序，要很方便的调试测试，其实是一件比较麻烦和繁琐的事情。</p>

<p>首先可能针对拆分的功能进行<strong>单独的方法</strong>级别的单元测试，然后到map/reduce的一个<strong>完整的处理过程</strong>的测试，再就是针对<strong>整个MR</strong>的测试，前面说的都是在IDE中完成后，最后需要到<strong>测试环境</strong>对其进行验证。</p>

<ul>
<li>单独的方法这里就不必多讲，直接使用eclipse自带的junit即可完成。</li>
<li>mrunit，针对map/reduce的测试，以至于整个MR流程的测试，但是mrunit的输入是针对小数据量的。</li>
<li>本地模式运行程序，模拟正式的环境来进行测试，数据直接从hdfs获取。</li>
<li>测试环境远程调试，尽管经过前面的步骤可能还会遇到各种问题，此时可结合<code>remote debug</code>来定位问题。</li>
</ul>


<h3>mrunit测试map/reduce</h3>

<p>首先去到<a href="http://mrunit.apache.org/">官网下载</a>，把对应的jar加入到你项目的依赖。懒得去手工下载的话直接使用maven。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;dependency&gt;
</span><span class='line'>  &lt;groupId&gt;org.apache.mrunit&lt;/groupId&gt;
</span><span class='line'>  &lt;artifactId&gt;mrunit&lt;/artifactId&gt;
</span><span class='line'>  &lt;version&gt;1.1.0&lt;/version&gt;
</span><span class='line'>  &lt;classifier&gt;hadoop2&lt;/classifier&gt;
</span><span class='line'>  &lt;scope&gt;test&lt;/scope&gt;
</span><span class='line'>&lt;/dependency&gt;</span></code></pre></td></tr></table></div></figure>


<p>可以对mapreduce的各种情况（map/reduce/map-reduce/map-combine-reduce）进行简单的测试，验证逻辑上是否存在问题。<a href="https://cwiki.apache.org/confluence/display/MRUNIT/MRUnit+Tutorial">官方文档的例子</a>已经很具体详细了。</p>

<p>先新建初始化driver（MapDriver/ReduceDriver/MapReduceDriver)，然后添加配置配置信息（configuration），再指定withInput来进行输入数据，和withOutput对应的输出数据。运行调用runTest方法就会模拟mr的整个运行机制来对单条的记录进行处理。因为都是在一个jvm中执行，调试是很方便的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>private MapReduceDriver&lt;LongWritable, Text, KeyWrapper, ValueWrapper, Text, Text&gt; mrDriver;
</span><span class='line'>
</span><span class='line'>@Before
</span><span class='line'>public void setUp() {
</span><span class='line'>  AccessLogMapper mapper = new AccessLogMapper();
</span><span class='line'>  AccessLogReducer reducer = new AccessLogReducer();
</span><span class='line'>  // AccessLogCombiner combiner = new AccessLogCombiner();
</span><span class='line'>
</span><span class='line'>  mrDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer);
</span><span class='line'>
</span><span class='line'>  // mDriver = MapDriver.newMapDriver(mapper);
</span><span class='line'>  // mcrDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer, combiner);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>private String[] datas;
</span><span class='line'>
</span><span class='line'>@After
</span><span class='line'>public void run() throws IOException {
</span><span class='line'>  if (datas != null) {
</span><span class='line'>      // 配置
</span><span class='line'>      ...
</span><span class='line'>      mrDriver.setConfiguration(config);
</span><span class='line'>      // mrDriver.getConfiguration().addResource("job_1399189058775_0627_conf.xml");
</span><span class='line'>
</span><span class='line'>    // 输入输出
</span><span class='line'>      Text input = new Text();
</span><span class='line'>      int i = 0;
</span><span class='line'>      for (String data : datas) {
</span><span class='line'>          input.set(data);
</span><span class='line'>          mrDriver.withInput(new LongWritable(++i), new Text(data));
</span><span class='line'>      }
</span><span class='line'>      mrDriver.withOutputFormat(MultipleFileOutputFormat.class, TextInputFormat.class);
</span><span class='line'>      mrDriver.runTest();
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>// / datas
</span><span class='line'>
</span><span class='line'>private String[] datas() {
</span><span class='line'>  return ...;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testOne() throws IOException {
</span><span class='line'>  datas = new String[] { datas()[0] };
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>local方式进行本地测试</h2>

<p>mapreduce默认提供了两种任务框架： local和yarn。YARN环境需要把程序发布到nodemanager上去运行，对于开发测试来讲，还是太繁琐了。</p>

<p>使用local的方式，既不用打包同时拥有IDE本地调试的便利，同时数据直接从HDFS中获取，也就是说，除了任务框架不同，其他都一样，程序的输入输出，任务代码的业务逻辑。为全面开发调试/测试提供了极其重要的方式。</p>

<p>只需要指定服务为local的服务框架，再加上输入输出即可。如果本地用户和hdfs的用户不同，设置下环境变量<code>HADOOP_USER_NAME</code>。同样map、reduce通过线程来模拟，都运行的同一个JVM中，断点调试也很方便。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>public class WordCountTest {
</span><span class='line'>  
</span><span class='line'>  static {
</span><span class='line'>      System.setProperty("HADOOP_USER_NAME", "hadoop");
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  private static final String HDFS_SERVER = "hdfs://umcc97-44:9000";
</span><span class='line'>
</span><span class='line'>  @Test
</span><span class='line'>  public void test() throws Exception {
</span><span class='line'>      WordCount.main(new String[]{
</span><span class='line'>              "-Dmapreduce.framework.name=local", 
</span><span class='line'>              "-Dfs.defaultFS=" + HDFS_SERVER, 
</span><span class='line'>              HDFS_SERVER + "/user/hadoop/dta/001.tar.gz", 
</span><span class='line'>              HDFS_SERVER + "/user/hadoop/output/"});
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>}
</span></code></pre></td></tr></table></div></figure>


<h3>测试环境打包测试</h3>

<p>放到测试环境后，appmanager、map、reduce都是运行在不同的jvm；还有就是需要对程序进行打包，挺啰嗦而且麻烦的事情，依赖包多的话，包还挺大，每次job都需要传递这么大一个文件，也挺浪费的。</p>

<p>提供两种打包方式，一种是直接jar运行的，一种是所有的jar压缩包tar.gz方式。可以结合distributecache减少每次执行程序需要传递给nodemanager的数据量，以及结合mapreduce运行时配置参数可以进行远程调试。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>调试appmanager
</span><span class='line'>-Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090" 
</span><span class='line'>调试map
</span><span class='line'>-Dmapreduce.map.java.opts
</span><span class='line'>调试reduce
</span><span class='line'>-Dmapreduce.reduce.java.opts</span></code></pre></td></tr></table></div></figure>


<h3>小结</h3>

<p>通过以上3中方式基本上能处理工作终于到的大部分问题了。大部分的功能使用mrunit测试就可以了，还可以单独的测试map，或者reduce挺不错的。</p>

<h3>附录：maven打包</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;profile&gt;
</span><span class='line'>  &lt;id&gt;jar&lt;/id&gt;
</span><span class='line'>  &lt;build&gt;
</span><span class='line'>      &lt;plugins&gt;
</span><span class='line'>          &lt;plugin&gt;
</span><span class='line'>              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
</span><span class='line'>              &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
</span><span class='line'>              &lt;executions&gt;
</span><span class='line'>                  &lt;execution&gt;
</span><span class='line'>                      &lt;id&gt;make-assembly&lt;/id&gt;
</span><span class='line'>                      &lt;phase&gt;package&lt;/phase&gt;
</span><span class='line'>                      &lt;goals&gt;
</span><span class='line'>                          &lt;goal&gt;single&lt;/goal&gt;
</span><span class='line'>                      &lt;/goals&gt;
</span><span class='line'>                  &lt;/execution&gt;
</span><span class='line'>              &lt;/executions&gt;
</span><span class='line'>              &lt;configuration&gt;
</span><span class='line'>                  &lt;descriptorRefs&gt;
</span><span class='line'>                      &lt;descriptorRef&gt;
</span><span class='line'>                          jar-with-dependencies
</span><span class='line'>                      &lt;/descriptorRef&gt;
</span><span class='line'>                  &lt;/descriptorRefs&gt;
</span><span class='line'>              &lt;/configuration&gt;
</span><span class='line'>          &lt;/plugin&gt;
</span><span class='line'>
</span><span class='line'>      &lt;/plugins&gt;
</span><span class='line'>  &lt;/build&gt;
</span><span class='line'>&lt;/profile&gt;
</span><span class='line'>
</span><span class='line'>&lt;profile&gt;
</span><span class='line'>  &lt;id&gt;tar&lt;/id&gt;
</span><span class='line'>  &lt;build&gt;
</span><span class='line'>      &lt;plugins&gt;
</span><span class='line'>          &lt;plugin&gt;
</span><span class='line'>              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
</span><span class='line'>              &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
</span><span class='line'>              &lt;executions&gt;
</span><span class='line'>                  &lt;execution&gt;
</span><span class='line'>                      &lt;id&gt;make-assembly&lt;/id&gt;
</span><span class='line'>                      &lt;phase&gt;package&lt;/phase&gt;
</span><span class='line'>                      &lt;goals&gt;
</span><span class='line'>                          &lt;goal&gt;single&lt;/goal&gt;
</span><span class='line'>                      &lt;/goals&gt;
</span><span class='line'>                  &lt;/execution&gt;
</span><span class='line'>              &lt;/executions&gt;
</span><span class='line'>              &lt;configuration&gt;
</span><span class='line'>                  &lt;appendAssemblyId&gt;true&lt;/appendAssemblyId&gt;
</span><span class='line'>                  &lt;descriptors&gt;
</span><span class='line'>                      &lt;descriptor&gt;${basedir}/../assemblies/application.xml&lt;/descriptor&gt;
</span><span class='line'>                  &lt;/descriptors&gt;
</span><span class='line'>              &lt;/configuration&gt;
</span><span class='line'>          &lt;/plugin&gt;
</span><span class='line'>      &lt;/plugins&gt;
</span><span class='line'>  &lt;/build&gt;
</span><span class='line'>&lt;/profile&gt;</span></code></pre></td></tr></table></div></figure>


<p>打包成tar.gz的描述文件：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;assembly&gt;
</span><span class='line'>  &lt;id&gt;dist-${env}&lt;/id&gt;
</span><span class='line'>  &lt;formats&gt;
</span><span class='line'>      &lt;format&gt;tar.gz&lt;/format&gt;
</span><span class='line'>  &lt;/formats&gt;
</span><span class='line'>  &lt;includeBaseDirectory&gt;true&lt;/includeBaseDirectory&gt;
</span><span class='line'>  &lt;fileSets&gt;
</span><span class='line'>      &lt;fileSet&gt;
</span><span class='line'>          &lt;directory&gt;${basedir}/src/main/scripts&lt;/directory&gt;
</span><span class='line'>          &lt;outputDirectory&gt;/bin&lt;/outputDirectory&gt;
</span><span class='line'>          &lt;includes&gt;
</span><span class='line'>              &lt;include&gt;*.sh&lt;/include&gt;
</span><span class='line'>          &lt;/includes&gt;
</span><span class='line'>          &lt;fileMode&gt;0755&lt;/fileMode&gt;
</span><span class='line'>          &lt;lineEnding&gt;unix&lt;/lineEnding&gt;
</span><span class='line'>      &lt;/fileSet&gt;
</span><span class='line'>      &lt;fileSet&gt;
</span><span class='line'>          &lt;directory&gt;${basedir}/target/classes&lt;/directory&gt;
</span><span class='line'>          &lt;outputDirectory&gt;/conf&lt;/outputDirectory&gt;
</span><span class='line'>          &lt;includes&gt;
</span><span class='line'>              &lt;include&gt;*.xml&lt;/include&gt;
</span><span class='line'>              &lt;include&gt;*.properties&lt;/include&gt;
</span><span class='line'>          &lt;/includes&gt;
</span><span class='line'>      &lt;/fileSet&gt;
</span><span class='line'>      &lt;fileSet&gt;
</span><span class='line'>          &lt;directory&gt;${basedir}/target&lt;/directory&gt;
</span><span class='line'>          &lt;outputDirectory&gt;/lib/core&lt;/outputDirectory&gt;
</span><span class='line'>          &lt;includes&gt;
</span><span class='line'>              &lt;include&gt;${project.artifactId}-${project.version}.jar
</span><span class='line'>              &lt;/include&gt;
</span><span class='line'>          &lt;/includes&gt;
</span><span class='line'>      &lt;/fileSet&gt;
</span><span class='line'>  &lt;/fileSets&gt;
</span><span class='line'>  &lt;dependencySets&gt;
</span><span class='line'>      &lt;dependencySet&gt;
</span><span class='line'>          &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt;
</span><span class='line'>          &lt;outputDirectory&gt;/lib/common&lt;/outputDirectory&gt;
</span><span class='line'>          &lt;scope&gt;runtime&lt;/scope&gt;
</span><span class='line'>      &lt;/dependencySet&gt;
</span><span class='line'>  &lt;/dependencySets&gt;
</span><span class='line'>&lt;/assembly&gt;</span></code></pre></td></tr></table></div></figure>


<p>运行整个程序的shell脚本</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'>bin=`which $0`
</span><span class='line'>bin=`dirname ${bin}`
</span><span class='line'>bin=`cd "$bin"; pwd`
</span><span class='line'>
</span><span class='line'>export ANAYSER_HOME=`dirname "$bin"`
</span><span class='line'>
</span><span class='line'>export ANAYSER_LOG_DIR=$ANAYSER_HOME/logs
</span><span class='line'>
</span><span class='line'>export ANAYSER_OPTS="-Dproc_dta_analyser -server -Xms1024M -Xmx2048M -Danalyser.log.dir=${ANAYSER_LOG_DIR}"
</span><span class='line'>
</span><span class='line'>export HADOOP_HOME=${HADOOP_HOME:-/home/hadoop/hadoop-2.2.0}
</span><span class='line'>export ANAYSER_CLASSPATH=$ANAYSER_HOME/conf
</span><span class='line'>export ANAYSER_CLASSPATH=$ANAYSER_CLASSPATH:$HADOOP_HOME/etc/hadoop
</span><span class='line'>
</span><span class='line'>for f in $ANAYSER_HOME/lib/core/*.jar ; do
</span><span class='line'>  export ANAYSER_CLASSPATH+=:$f
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>for f in $ANAYSER_HOME/lib/common/*.jar ; do
</span><span class='line'>  export ANAYSER_CLASSPATH+=:$f
</span><span class='line'>done
</span><span class='line'>
</span><span class='line'>if [ ! -d $ANAYSER_LOG_DIR ] ; then
</span><span class='line'>  mkdir -p $ANAYSER_LOG_DIR
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>[ -w "$ANAYSER_PID_DIR" ] ||  mkdir -p "$ANAYSER_PID_DIR"
</span><span class='line'>
</span><span class='line'>nohup ${JAVA_HOME}/bin/java $ANAYSER_OPTS -cp $ANAYSER_CLASSPATH com.analyser.AnalyserStarter &gt;$ANAYSER_LOG_DIR/stdout 2&gt;$ANAYSER_LOG_DIR/stderr &
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala Wordcount on Hadoop2]]></title>
    <link href="http://winse.github.io/blog/2014/09/12/scala-wordcount-on-hadoop/"/>
    <updated>2014-09-12T07:52:01+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/12/scala-wordcount-on-hadoop</id>
    <content type="html"><![CDATA[<p>从了解scala，到spark再次遇见scala，准备好好学学这门语言。函数式编程大势所趋，简洁的语法，更抽象好用的集合操作。土生土长的JVM的语言，以及凭借其与java的互操作性，发展前景一片光明。在云计算以及手机（android）开发都有其大展拳脚的地方。</p>

<p>工作中大部分时间写mapreduce，项目空白期实践了一下把scala搬上hadoop。整体来说用scala写个helloworld是比较简单的，就一些细节的东西比较繁琐。尽管用了几年的eclipse了，但是<a href="http://scala-ide.org/">scala-ide</a>还是需要再适应适应！scala-idea也没有大家说的那么好，和webstorm比差远了。</p>

<div><script src='https://gist.github.com/5df39f77e8bd59348a7a.js'></script>
<noscript><pre><code>package com.github.winse.hadoop

import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.Reducer
import org.apache.hadoop.io.Text
import org.apache.hadoop.io.IntWritable
import org.apache.hadoop.io.LongWritable
import org.apache.hadoop.mapreduce.Mapper
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
import org.apache.hadoop.fs.Path
import scala.Array.canBuildFrom
import org.apache.hadoop.conf.Configured
import org.apache.hadoop.util.Tool
import org.apache.hadoop.util.ToolRunner

class ScalaMapper extends Mapper[LongWritable, Text, Text, IntWritable] {

  val one = new IntWritable(1);

  override def map(key: LongWritable, value: Text, context: Mapper[LongWritable, Text, Text, IntWritable]#Context) {
    value.toString().split(&quot;\\s+&quot;).map(word =&gt; context.write(new Text(word), one))
  }

}

class ScalaReducer extends Reducer[Text, IntWritable, Text, IntWritable] {

  override def reduce(key: Text, values: java.lang.Iterable[IntWritable], context: Reducer[Text, IntWritable, Text, IntWritable]#Context) {
    var sum: Int = 0

    val itr = values.iterator()
    while (itr.hasNext()) {
      sum += itr.next().get()
    }
    context.write(key, new IntWritable(sum))
  }

}

object HelloScalaMapRed extends Configured with Tool {

  override def run(args: Array[String]): Int = {

    val job = Job.getInstance(getConf(), &quot;WordCount Scala.&quot;)
    job.setJarByClass(getClass())

    job.setOutputKeyClass(classOf[Text])
    job.setOutputValueClass(classOf[IntWritable])

    job.setMapperClass(classOf[ScalaMapper])
    job.setCombinerClass(classOf[ScalaReducer])
    job.setReducerClass(classOf[ScalaReducer])

    FileInputFormat.addInputPath(job, new Path(&quot;/scala/in/&quot;));
    FileOutputFormat.setOutputPath(job, new Path(&quot;/scala/out/&quot;));

    job.waitForCompletion(true) match {
      case true =&gt; 0
      case false =&gt; 1
    }

  }

  def main(args: Array[String]) {
    val res: Int = ToolRunner.run(new Configuration(), this, args)
    System.exit(res);
  }

}</code></pre></noscript></div>


<p>使用scala主要原因：</p>

<ul>
<li>写JavaBean更简单方便</li>
<li>多返回值无需定义Result实体类</li>
<li>集合更抽象的方法真的很好用</li>
<li>trait可以更便捷的进行操作层面的聚合，也就是可以把操作分离出来，进行组合就可以实现新的功能。这不就是decorate模式嘛！java的decorate多麻烦的！加点东西太麻烦了！！！</li>
</ul>


<p>上面的scala代码和java的比较类似，主要在集合操作上不同而已，变量定义简单化。</p>

<p>编写好代码后就是运行调试。</p>

<p>前面其他的文章已经说过了，默认<code>mapreduce.framework.name</code>的配置是本地<code>local</code>，所以直接运行就像运行一个普通的本地java程序。这就不多将了。
这里主要讲讲怎么把代码打包放到真实的集群环境运行，相比java的版本要添加那些步骤。</p>

<p>从项目的maven pom中可以发现，其实就是多了scala-lang的新依赖而已，其他都是hadoop自带的公共包。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHV6AAJoCAABANktCWmk664.png" alt="" /></p>

<p>所以运行程序只需要指定把scala-lang.jar添加到运行环境的classpath中即可。使用maven打包后的项目结构如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ cd lib/
</span><span class='line'>[hadoop@master1 lib]$ ls -l
</span><span class='line'>total 8
</span><span class='line'>drwxrwxr-x. 2 hadoop hadoop 4096 Sep 11 23:10 common
</span><span class='line'>drwxrwxr-x. 2 hadoop hadoop 4096 Sep 11 23:56 core
</span><span class='line'>[hadoop@master1 lib]$ ll core/
</span><span class='line'>total 12
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop 11903 Sep 11 23:55 scalamapred-1.0.5.jar
</span><span class='line'>[hadoop@master1 lib]$ ls common/
</span><span class='line'>activation-1.1.jar                commons-lang-2.6.jar            hadoop-hdfs-2.2.0.jar                     jaxb-api-2.2.2.jar                      log4j-1.2.17.jar
</span><span class='line'>aopalliance-1.0.jar               commons-logging-1.1.1.jar       hadoop-mapreduce-client-common-2.2.0.jar  jaxb-impl-2.2.3-1.jar                   management-api-3.0.0-b012.jar
</span><span class='line'>asm-3.1.jar                       commons-math-2.1.jar            hadoop-mapreduce-client-core-2.2.0.jar    jersey-client-1.9.jar                   netty-3.6.2.Final.jar
</span><span class='line'>avro-1.7.4.jar                    commons-net-3.1.jar             hadoop-yarn-api-2.2.0.jar                 jersey-core-1.9.jar                     paranamer-2.3.jar
</span><span class='line'>commons-beanutils-1.7.0.jar       gmbal-api-only-3.0.0-b023.jar   hadoop-yarn-client-2.2.0.jar              jersey-grizzly2-1.9.jar                 protobuf-java-2.5.0.jar
</span><span class='line'>commons-beanutils-core-1.8.0.jar  grizzly-framework-2.1.2.jar     hadoop-yarn-common-2.2.0.jar              jersey-guice-1.9.jar                    scala-library-2.10.4.jar
</span><span class='line'>commons-cli-1.2.jar               grizzly-http-2.1.2.jar          hadoop-yarn-server-common-2.2.0.jar       jersey-json-1.9.jar                     servlet-api-2.5.jar
</span><span class='line'>commons-codec-1.4.jar             grizzly-http-server-2.1.2.jar   jackson-core-asl-1.8.8.jar                jersey-server-1.9.jar                   slf4j-api-1.7.1.jar
</span><span class='line'>commons-collections-3.2.1.jar     grizzly-http-servlet-2.1.2.jar  jackson-jaxrs-1.8.3.jar                   jersey-test-framework-core-1.9.jar      slf4j-log4j12-1.7.1.jar
</span><span class='line'>commons-compress-1.4.1.jar        grizzly-rcm-2.1.2.jar           jackson-mapper-asl-1.8.8.jar              jersey-test-framework-grizzly2-1.9.jar  snappy-java-1.0.4.1.jar
</span><span class='line'>commons-configuration-1.6.jar     guava-17.0.jar                  jackson-xc-1.8.3.jar                      jets3t-0.6.1.jar                        stax-api-1.0.1.jar
</span><span class='line'>commons-daemon-1.0.13.jar         guice-3.0.jar                   jasper-compiler-5.5.23.jar                jettison-1.1.jar                        xmlenc-0.52.jar
</span><span class='line'>commons-digester-1.8.jar          guice-servlet-3.0.jar           jasper-runtime-5.5.23.jar                 jetty-6.1.26.jar                        xz-1.0.jar
</span><span class='line'>commons-el-1.0.jar                hadoop-annotations-2.2.0.jar    javax.inject-1.jar                        jetty-util-6.1.26.jar                   zookeeper-3.4.5.jar
</span><span class='line'>commons-httpclient-3.1.jar        hadoop-auth-2.2.0.jar           javax.servlet-3.1.jar                     jsch-0.1.42.jar
</span><span class='line'>commons-io-2.1.jar                hadoop-common-2.2.0.jar         javax.servlet-api-3.0.1.jar               jsp-api-2.1.jar
</span><span class='line'>[hadoop@master1 lib]$ </span></code></pre></td></tr></table></div></figure>


<p>在lib文件夹下面包括common和core两放置jar的文件夹，common是项目的依赖包，core下面的是项目的源码jar。</p>

<p>接下来运行程序，通过libjar把<strong>scala-library的包加入到mapreduce的运行时classpath</strong>。当然也可以把scala-library加入到<code>mapreduce.application.classpath</code>（默认值为<code>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</code>）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ for j in `find . -name "*.jar"` ; do export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$j ; done
</span><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ 
</span><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ export HADOOP_CLASSPATH=
</span><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ export HADOOP_CLASSPATH=/home/hadoop/scalamapred-1.0.5/lib/core/*:/home/hadoop/scalamapred-1.0.5/lib/common/*
</span><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed -libjars lib/common/scala-library-2.10.4.jar </span></code></pre></td></tr></table></div></figure>


<h2>问题攻略</h2>

<p>上面如果不加libjar的话，会在nodemanager的代码中抛出异常。本来认为不加依赖包也就不能执行mapreduce里面的代码而已。问题的根源在哪里呢？</p>

<p>给代码添加远程调试的配置，然后运行一步步的查找问题（一次找不到就多运行调试几次）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed  -Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090"
</span><span class='line'>
</span><span class='line'>// 我这里slaver就一台，取到机器上查看运行的程序
</span><span class='line'>
</span><span class='line'>[hadoop@slaver1 nmPrivate]$ ps axu|grep java
</span><span class='line'>hadoop    1427  0.6 10.5 1562760 106344 ?      Sl   Sep11   0:45 /opt/jdk1.7.0_60//bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=hadoop-hadoop-datanode-slaver1.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode
</span><span class='line'>hadoop    2874  2.5 11.7 1599312 118980 ?      Sl   00:08   0:57 /opt/jdk1.7.0_60//bin/java -Dproc_nodemanager -Xmx1000m -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.home.dir= -Dyarn.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -Dyarn.policy.file=hadoop-policy.xml -server -Dhadoop.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.2.0/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.log.file=yarn-hadoop-nodemanager-slaver1.log -Dyarn.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.home.dir=/home/hadoop/hadoop-2.2.0 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hadoop-2.2.0/lib/native -classpath /home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/etc/hadoop:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/common/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.2.0/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/*:/home/hadoop/hadoop-2.2.0/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.2.0/etc/hadoop/nm-config/log4j.properties org.apache.hadoop.yarn.server.nodemanager.NodeManager
</span><span class='line'>hadoop    3750  0.0  0.1 106104  1200 ?        Ss   00:43   0:00 /bin/bash -c /opt/jdk1.7.0_60//bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stderr 
</span><span class='line'>hadoop    3759  0.1  1.8 737648 18232 ?        Sl   00:43   0:00 /opt/jdk1.7.0_60//bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster
</span><span class='line'>hadoop    3778  0.0  0.0 103256   832 pts/0    S+   00:45   0:00 grep java
</span><span class='line'>
</span><span class='line'>// 取到对应的目录下查看launcher.sh的脚本
</span><span class='line'>// appmaster launcher
</span><span class='line'>
</span><span class='line'>[hadoop@slaver1 nm-local-dir]$ cd nmPrivate/application_1410453720744_0007/
</span><span class='line'>[hadoop@slaver1 application_1410453720744_0007]$ ll
</span><span class='line'>total 4
</span><span class='line'>drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:43 container_1410453720744_0007_01_000001
</span><span class='line'>[hadoop@slaver1 application_1410453720744_0007]$ less container_1410453720744_0007_01_000001/
</span><span class='line'>container_1410453720744_0007_01_000001.tokens       launch_container.sh                                 
</span><span class='line'>.container_1410453720744_0007_01_000001.tokens.crc  .launch_container.sh.crc                            
</span><span class='line'>[hadoop@slaver1 application_1410453720744_0007]$ less container_1410453720744_0007_01_000001/launch_container.sh 
</span><span class='line'>#!/bin/bash
</span><span class='line'>
</span><span class='line'>export NM_HTTP_PORT="8042"
</span><span class='line'>export LOCAL_DIRS="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007"
</span><span class='line'>export HADOOP_COMMON_HOME="/home/hadoop/hadoop-2.2.0"
</span><span class='line'>export JAVA_HOME="/opt/jdk1.7.0_60/"
</span><span class='line'>export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
</span><span class='line'>"
</span><span class='line'>export HADOOP_YARN_HOME="/home/hadoop/hadoop-2.2.0"
</span><span class='line'>export CLASSPATH="$PWD:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/share/hadoop/common/*:$HADOOP_COMMON_HOME/share/hadoop/common/lib/*:$HADOOP_HDFS_HOME/share/hadoop/hdfs/*:$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*:$HADOOP_YARN_HOME/share/hadoop/yarn/*:$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*:job.jar/job.jar:job.jar/classes/:job.jar/lib/*:$PWD/*"
</span><span class='line'>export HADOOP_TOKEN_FILE_LOCATION="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001/container_tokens"
</span><span class='line'>export NM_HOST="slaver1"
</span><span class='line'>export APPLICATION_WEB_PROXY_BASE="/proxy/application_1410453720744_0007"
</span><span class='line'>export JVM_PID="$$"
</span><span class='line'>export USER="hadoop"
</span><span class='line'>export HADOOP_HDFS_HOME="/home/hadoop/hadoop-2.2.0"
</span><span class='line'>export PWD="/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001"
</span><span class='line'>export CONTAINER_ID="container_1410453720744_0007_01_000001"
</span><span class='line'>export HOME="/home/"
</span><span class='line'>export NM_PORT="40888"
</span><span class='line'>export LOGNAME="hadoop"
</span><span class='line'>export APP_SUBMIT_TIME_ENV="1410455811401"
</span><span class='line'>export MAX_APP_ATTEMPTS="2"
</span><span class='line'>export HADOOP_CONF_DIR="/home/hadoop/hadoop-2.2.0/etc/hadoop"
</span><span class='line'>export MALLOC_ARENA_MAX="4"
</span><span class='line'>export LOG_DIRS="/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001"
</span><span class='line'>ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/10/job.jar" "job.jar"
</span><span class='line'>ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/13/job.xml" "job.xml"
</span><span class='line'>mkdir -p jobSubmitDir
</span><span class='line'>ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/11/job.splitmetainfo" "jobSubmitDir/job.splitmetainfo"
</span><span class='line'>mkdir -p jobSubmitDir
</span><span class='line'>ln -sf "/home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/12/job.split" "jobSubmitDir/job.split"
</span><span class='line'>exec /bin/bash -c "$JAVA_HOME/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA  -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090 org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stdout 2&gt;/home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410453720744_0007/container_1410453720744_0007_01_000001/stderr "
</span><span class='line'>
</span><span class='line'>// 去到TMP对应的目录下，查看整个运行的根目录
</span><span class='line'>
</span><span class='line'>[hadoop@slaver1 ~]$ cd /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/container_1410453720744_0007_01_000001
</span><span class='line'>[hadoop@slaver1 container_1410453720744_0007_01_000001]$ ll
</span><span class='line'>total 28
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop   95 Sep 12 00:43 container_tokens
</span><span class='line'>-rwx------. 1 hadoop hadoop  468 Sep 12 00:43 default_container_executor.sh
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:43 job.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/10/job.jar
</span><span class='line'>drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:43 jobSubmitDir
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:43 job.xml -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0007/filecache/13/job.xml
</span><span class='line'>-rwx------. 1 hadoop hadoop 3005 Sep 12 00:43 launch_container.sh
</span><span class='line'>drwx--x---. 2 hadoop hadoop 4096 Sep 12 00:43 tmp
</span><span class='line'>[hadoop@slaver1 container_1410453720744_0007_01_000001]$ 
</span></code></pre></td></tr></table></div></figure>


<p>为了对应，我这里列出来在添加了libjar的TMP目录的列表：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 scalamapred-1.0.5]$ hadoop com.github.winse.hadoop.HelloScalaMapRed  -Dyarn.app.mapreduce.am.command-opts="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=18090" -libjars lib/common/scala-library-2.10.4.jar 
</span><span class='line'>
</span><span class='line'>[hadoop@slaver1 container_1410453720744_0007_01_000001]$ cd /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/container_1410453720744_0008_01_000001
</span><span class='line'>[hadoop@slaver1 container_1410453720744_0008_01_000001]$ ll
</span><span class='line'>total 32
</span><span class='line'>-rw-r--r--. 1 hadoop hadoop   95 Sep 12 00:49 container_tokens
</span><span class='line'>-rwx------. 1 hadoop hadoop  468 Sep 12 00:49 default_container_executor.sh
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:49 job.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/filecache/10/job.jar
</span><span class='line'>drwxrwxr-x. 2 hadoop hadoop 4096 Sep 12 00:49 jobSubmitDir
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop  108 Sep 12 00:49 job.xml -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/appcache/application_1410453720744_0008/filecache/13/job.xml
</span><span class='line'>-rwx------. 1 hadoop hadoop 3127 Sep 12 00:49 launch_container.sh
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop   85 Sep 12 00:49 scala-library-2.10.4.jar -&gt; /home/hadoop/data/nm-local-dir/usercache/hadoop/filecache/10/scala-library-2.10.4.jar
</span><span class='line'>drwx--x---. 2 hadoop hadoop 4096 Sep 12 00:49 tmp
</span><span class='line'>[hadoop@slaver1 container_1410453720744_0008_01_000001]$ </span></code></pre></td></tr></table></div></figure>


<p>windows本地使用eclipse和进行跟踪调试代码。</p>

<p><img src="http://file.bmob.cn/M00/0E/A1/wKhkA1QUG0aARyPVAAMnUXGDgbY378.png" alt="" /></p>

<p>此时可以通过8088的网页查看状态，当前有一个mrappmaster在执行，如果第一个失败，会尝试执行第二次。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHDGAe0anAAEfiNTmB1k734.png" alt="" /></p>

<p>运行调试多次后，<strong>最终确定问题</strong>所在。在master中会检查是否为链式mr，而加载该class的时刻，同时要加载父类的class，即scala的类，所以在这里会抛出异常。</p>

<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHFOAWJulAAPOawkAbgo349.png" alt="" /></p>

<p>去到查看程序运行的日志，可以看到程序抛出的异常<strong>NoClassDefFoundError</strong>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@slaver1 ~]$ less /home/hadoop/hadoop-2.2.0/logs/userlogs/application_1410448728371_0003/*/syslog
</span><span class='line'>2014-09-11 22:55:12,616 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1410448728371_0003_000001
</span><span class='line'>...
</span><span class='line'>2014-09-11 22:55:18,677 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1410448728371_0003 to jobTokenSecretManager
</span><span class='line'>2014-09-11 22:55:19,119 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster
</span><span class='line'>java.lang.NoClassDefFoundError: scala/Function1
</span><span class='line'>        at java.lang.Class.forName0(Native Method)
</span><span class='line'>        at java.lang.Class.forName(Class.java:190)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1277)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1217)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:135)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1420)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1358)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
</span><span class='line'>        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:972)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:134)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1227)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1035)
</span><span class='line'>        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1445)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at javax.security.auth.Subject.doAs(Subject.java:415)
</span><span class='line'>        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1441)
</span><span class='line'>        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1374)
</span><span class='line'>Caused by: java.lang.ClassNotFoundException: scala.Function1
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
</span><span class='line'>        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
</span><span class='line'>        at java.security.AccessController.doPrivileged(Native Method)
</span><span class='line'>        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
</span><span class='line'>        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
</span><span class='line'>        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</span><span class='line'>        ... 22 more
</span><span class='line'>2014-09-11 22:55:19,130 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster received a signal. Signaling RMCommunicator and JobHistoryEventHandler.</span></code></pre></td></tr></table></div></figure>


<h2>意外收获</h2>

<ul>
<li>推测执行初始化代码</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHHCATFHtAAMeDcCHWzU166.png" alt="" /></p>

<ul>
<li>OutputFormat的获取Committer代码</li>
</ul>


<p><img src="http://file.bmob.cn/M00/0E/A2/wKhkA1QUHImAJAq1AALGEfA-F9k811.png" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://digifesto.com/2013/04/15/hadoop-with-scala-hacking-notes/">Hadoop with Scala: hacking notes</a></li>
<li><a href="https://github.com/derrickcheng/ScalaOnHadoop/blob/master/src/main/scala/WordCount.scala">ScalaOnHadoop WordCount.scala</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Expect-批量实现SSH无密钥登录]]></title>
    <link href="http://winse.github.io/blog/2014/09/07/expect-automate-and-batch-config-ssh/"/>
    <updated>2014-09-07T16:11:18+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/07/expect-automate-and-batch-config-ssh</id>
    <content type="html"><![CDATA[<p>在安装部署Hadoop集群的首要步骤就是配置SSH的无密钥登陆。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
</span><span class='line'>cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys
</span><span class='line'>
</span><span class='line'>ssh-copy-id -i ~/.ssh/id_rsa.pub root@$ip</span></code></pre></td></tr></table></div></figure>


<p>然后，可以通过ssh命令来进行批量的操作。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh root@$ip 'cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys'
</span><span class='line'>scp -o StrictHostKeyChecking=no /etc/hosts root@${ip}:/etc/</span></code></pre></td></tr></table></div></figure>


<p>但是，一些需要密码的dialogue形式的输入时，部署N台datanode就需要输入N遍！同时新建用户也是需要输入用户密码的操作！！</p>

<p>Linux Expect就是用来自动化处理这些需求的。Except能根据提示来实现相应的输入。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop-deploy-0.0.1]$ ssh-copy-id localhost
</span><span class='line'>The authenticity of host 'localhost (::1)' can't be established.
</span><span class='line'>RSA key fingerprint is 4e:fe:7a:0a:98:6e:9a:ab:af:e4:65:51:9b:3d:e0:99.
</span><span class='line'>Are you sure you want to continue connecting (yes/no)? yes
</span><span class='line'>Warning: Permanently added 'localhost' (RSA) to the list of known hosts.
</span><span class='line'>hadoop@localhost's password: 
</span><span class='line'>Now try logging into the machine, with "ssh 'localhost'", and check in:
</span><span class='line'>
</span><span class='line'>  .ssh/authorized_keys
</span><span class='line'>
</span><span class='line'>to make sure we haven't added extra keys that you weren't expecting.</span></code></pre></td></tr></table></div></figure>


<p>根据需要<strong>提示信息</strong>，以及需要<strong>输入的信息</strong>，可以编写对应expect脚本来进行自动化。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop-deploy-0.0.1]$ cat bin/ssh-copy-id.expect 
</span><span class='line'>#!/usr/bin/expect  
</span><span class='line'>
</span><span class='line'>## Usage $0 [user@]host password
</span><span class='line'>
</span><span class='line'>set host [lrange $argv 0 0];
</span><span class='line'>set password [lrange $argv 1 1] ;
</span><span class='line'>
</span><span class='line'>set timeout 30;
</span><span class='line'>
</span><span class='line'>spawn ssh-copy-id $host ;
</span><span class='line'>
</span><span class='line'>expect {
</span><span class='line'>  "(yes/no)?" { send yes\n; exp_continue; }
</span><span class='line'>  "password:" { send $password\n; exp_continue; }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>exec sleep 1;</span></code></pre></td></tr></table></div></figure>


<p>同样新建用户初始化密码的操作一样可以使用expect来使用：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop-deploy-0.0.1]$ cat bin/passwd.expect
</span><span class='line'>#!/usr/bin/expect  
</span><span class='line'>
</span><span class='line'>## Usage $0 host username password
</span><span class='line'>
</span><span class='line'>set host [lrange $argv 0 0];
</span><span class='line'>set username [lrange $argv 1 1];
</span><span class='line'>set password [lrange $argv 2 2] ;
</span><span class='line'>
</span><span class='line'>set timeout 30;
</span><span class='line'>
</span><span class='line'>##
</span><span class='line'>
</span><span class='line'>spawn ssh $host useradd $username ;
</span><span class='line'>
</span><span class='line'>exec sleep 1;
</span><span class='line'>
</span><span class='line'>##
</span><span class='line'>
</span><span class='line'>spawn ssh $host passwd $username ;
</span><span class='line'>
</span><span class='line'>## password and repasswd all use this
</span><span class='line'>expect {
</span><span class='line'>  "password:" { send $password\n; exp_continue; }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>exec sleep 1;</span></code></pre></td></tr></table></div></figure>


<p>有了上面的脚本，预定义每台机器的root密码，使用ssh-copy-id.expect建立到各台datanode机器的无密钥登录；然后passwd.expect脚本分发给各台机器，然后使用ssh进行运行脚本建立用户初始化密码。</p>

<p>Expect仅在master机器上安装就可以。安装程序的如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install expect</span></code></pre></td></tr></table></div></figure>


<p>or</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>rpm -ivh tcl-8.5.7-6.el6.x86_64.rpm
</span><span class='line'>rpm -ivh expect-5.44.1.15-5.el6_4.x86_64.rpm</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读码] Hadoop2 Balancer磁盘空间平衡（下）]]></title>
    <link href="http://winse.github.io/blog/2014/09/05/read-hadoop-balancer-source-part3/"/>
    <updated>2014-09-05T16:31:15+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/05/read-hadoop-balancer-source-part3</id>
    <content type="html"><![CDATA[<p>前面讲到了节点的初始化，根据节点使用率与集群dfs使用率比较分为
<code>overUtilizedDatanodes</code>，<code>aboveAvgUtilizedDatanodes</code>，<code>belowAvgUtilizedDatanodes</code>，<code>underUtilizedDatanodes</code>，同时进行了节点数据量从Source到Target的配对。</p>

<p>接下来就是最后的数据移动部分了。</p>

<p>5.3 移动数据</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private ReturnStatus run(int iteration, Formatter formatter,
</span><span class='line'>      Configuration conf) {
</span><span class='line'>      ...
</span><span class='line'>      if (!this.nnc.shouldContinue(dispatchBlockMoves())) {
</span><span class='line'>        return ReturnStatus.NO_MOVE_PROGRESS;
</span><span class='line'>      }
</span><span class='line'>      ...
</span><span class='line'>  }    </span></code></pre></td></tr></table></div></figure>


<p>针对一个namenode如果连续5次没有移动数据，就会退出平衡操作，是在<code>NameNodeConnector#shouldContinue(long)</code>中处理的。</p>

<p>由于这里需要进行大量计算，以及耗时的文件传输等操作，这里使用了executorservice，分别为moverExecutor和dispatcherExecutor，有两个配置<code>dfs.balancer.moverThreads</code>（1000）和<code>dfs.balancer.dispatcherThreads</code>（200）来设置线程池的大小。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  Balancer(NameNodeConnector theblockpool, Parameters p, Configuration conf) {
</span><span class='line'>      ...
</span><span class='line'>    this.moverExecutor = Executors.newFixedThreadPool(
</span><span class='line'>            conf.getInt(DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_KEY,
</span><span class='line'>                        DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_DEFAULT));
</span><span class='line'>    this.dispatcherExecutor = Executors.newFixedThreadPool(
</span><span class='line'>            conf.getInt(DFSConfigKeys.DFS_BALANCER_DISPATCHERTHREADS_KEY,
</span><span class='line'>                        DFSConfigKeys.DFS_BALANCER_DISPATCHERTHREADS_DEFAULT));
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>其中<code>dispatchBlockMoves()</code>包装了数据移动的操作，把source的块移动到target节点中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private long dispatchBlockMoves() throws InterruptedException {
</span><span class='line'>    long bytesLastMoved = bytesMoved.get();
</span><span class='line'>    Future&lt;?&gt;[] futures = new Future&lt;?&gt;[sources.size()];
</span><span class='line'>    int i=0;
</span><span class='line'>    for (Source source : sources) {
</span><span class='line'>       // / 新线程来执行块的分发
</span><span class='line'>      futures[i++] = dispatcherExecutor.submit(source.new BlockMoveDispatcher());
</span><span class='line'>    }
</span><span class='line'>    
</span><span class='line'>    // wait for all dispatcher threads to finish
</span><span class='line'>    // / 等待分发操作完成
</span><span class='line'>    for (Future&lt;?&gt; future : futures) { 
</span><span class='line'>        future.get(); 
</span><span class='line'>    }
</span><span class='line'>    
</span><span class='line'>    // wait for all block moving to be done
</span><span class='line'>    // / 等待块的数据移动完成，相当于等待moverExecutor的Future完成
</span><span class='line'>    waitForMoveCompletion(); 
</span><span class='line'>    
</span><span class='line'>    return bytesMoved.get()-bytesLastMoved;
</span><span class='line'>  }
</span><span class='line'>  private void waitForMoveCompletion() {
</span><span class='line'>    boolean shouldWait;
</span><span class='line'>    do {
</span><span class='line'>      shouldWait = false;
</span><span class='line'>      for (BalancerDatanode target : targets) {
</span><span class='line'>          // / 块从source移动到target完成后,会从Pending的列表中移除 @see PendingBlockMove#dispatch()
</span><span class='line'>        if (!target.isPendingQEmpty()) { 
</span><span class='line'>          shouldWait = true;
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>      if (shouldWait) {
</span><span class='line'>        try {
</span><span class='line'>          Thread.sleep(blockMoveWaitTime);
</span><span class='line'>        } catch (InterruptedException ignored) {
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    } while (shouldWait);
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>上面是分发功能主程序执行的代码，调用分发线程和等待执行结果的代码。主要业务逻辑在线程中调用执行。</p>

<p>分发线程dispatcher先获取Source上指定大小的block块，对应到<code>getBlockList()</code>方法。除了用于<strong>块同步</strong>的globalBlockList变量、以及记录当前Source获取的srcBlockList、最重要的当属用于判断获取的块是否符合条件的方法<code>isGoodBlockCandidate(block)</code>。在移动块的选择也会用到该方法，单独拿出来在后面讲。</p>

<p>然后选择Source下哪些块将移动到Targets目标节点。在<code>chooseNodes</code>步骤中把移动和接收<strong>数据</strong>的流向确定了，相关信息存储在Source的nodeTasks列表对象中。这里<code>PendingBlockMove.chooseBlockAndProxy()</code>把Sources需要移动的<strong>块</strong>确定下来，把从Source获取到的srcBlockList分配给Target。然后交给moverExecutor去执行。</p>

<p>其中通过<code>isGoodBlockCandidate</code>和<code>chooseProxySource</code>（选择从那个目标获取block的真实数据，不一定是Source节点哦！）方法筛选合适的符合条件的块，并加入到movedBlocks对象。</p>

<p><img src="http://file.bmob.cn/M00/0C/FA/wKhkA1QLCqqASDGHAAMJbC1ZgZQ339.png" alt="" /></p>

<p>调用的dispatchBlocks方法第一次循环是不会有数据移动的，此时Source对象中srcBlockList可移动块为空，从Source中获取块后再进行块的移动操作<code>chooseNextBlockToMove()</code>。</p>

<p>先讲下Source类属性blocksToReceive，初始值为2*scheduledSize，有三个地方：dispatchBlocks初始化大小，getBlockList从Source节点获取block的量同时减去获取到的block的字节数，还有就是shouldFetchMoreBlocks用于判断是否还有数据需要获取或者移动dispatchBlocks。这个属性其实也就是<strong>设置一个阀</strong>，不管block是否为最终移动的block，获取到块的信息后就会从blocksToReceive减去相应的字节数。</p>

<p><img src="http://file.bmob.cn/M00/0C/D7/wKhkA1QJjnGAT2T-AACHmpdgZc0077.png" alt="" /></p>

<p>前面获取Source block和分配到Target block都使用了isGoodBlockCandidate方法，这里涉及到怎么去评估<strong>块</strong>获取和分配是否合理的问题。需同时满足下面三个条件：</p>

<ul>
<li>当前选中的移动的块，不在已移动块的名单中<code>movedBlocks.contains</code></li>
<li>移动的块在目的机器上没有备份</li>
<li>移动的块不减少含有该数据的机架数量

<ul>
<li>多机架的情况下<code>cluster.isNodeGroupAware()</code>，移动的块在目的机器的机架上没有备份</li>
<li>YES source和target在同一个机架上。</li>
<li>YES source和target不在同一机架上，且该块没有一个备份在target的机架上</li>
<li>YES source和target不在同一机架上，且该块有另一个备份和source在同一机架上</li>
</ul>
</li>
</ul>


<h2>疑问</h2>

<p>一个Datanode只能同时移动/接收5个Block（即MAX_NUM_CONCURRENT_MOVES值），结合<code>chooseProxySource</code>的代码的addTo调用，看的很是辛苦！如block-A所有块都在A机架上，在选择proxySource时，会把该块的<strong>两个</strong>datanode都加上一个pendingBlock，显然这不大合理！！</p>

<p>如果备用的proxySource节点恰好还是target的话，waitForMoveCompletion方法永远不能结束！！应该把没有找到同机架的源情况移到for循环外面进行处理。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>private boolean chooseProxySource() {
</span><span class='line'>  final DatanodeInfo targetDN = target.getDatanode();
</span><span class='line'>  boolean find = false;
</span><span class='line'>  for (BalancerDatanode loc : block.getLocations()) {
</span><span class='line'>    // check if there is replica which is on the same rack with the target
</span><span class='line'>    if (cluster.isOnSameRack(loc.getDatanode(), targetDN) && addTo(loc)) {
</span><span class='line'>      find = true;
</span><span class='line'>      // if cluster is not nodegroup aware or the proxy is on the same 
</span><span class='line'>      // nodegroup with target, then we already find the nearest proxy
</span><span class='line'>      if (!cluster.isNodeGroupAware() 
</span><span class='line'>          || cluster.isOnSameNodeGroup(loc.getDatanode(), targetDN)) {
</span><span class='line'>        return true;
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>    
</span><span class='line'>    if (!find) {
</span><span class='line'>    // 这里的non-busy指的是，pendingBlock小于5份节点
</span><span class='line'>      // find out a non-busy replica out of rack of target
</span><span class='line'>      find = addTo(loc);
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  return find;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p><img src="http://file.bmob.cn/M00/0D/06/wKhkA1QLuziAKEZdAAA8zGjPMGQ901.png" alt="" /></p>

<p>不过无需庸人自扰，一般都在一个rack上，这种问题就不存在了！同时这个也不是能一步到位，加了很多限制（一次迭代一个datanode最多处理10G，获取一次srcBlockList仅2G还限制就一次迭代就5个block），会执行很多次。</p>

<h2>总结</h2>

<p>总体的代码大致就是这样子了。根据集群使用率和阀值，计算需要进行数据接收和移动的节点（初始化），然后进行配对（选择），再进行块的选取和接收节点进行配对（分发），最后就是数据的移动（理解为socket数据传递就好了，调用了HDFS的协议代码。表示看不明），并等待该轮操作结束。</p>

<h2>举例</h2>

<p>除了指定threshold为5，其他是默认参数。由于仅单namenode和单rack，所以直接分析第五部分的namenode平衡处理。</p>

<p>根据所给的数据，（initNodes）第一步计算使用率，得出需要移动的数据量，把datanodes对号入座到over/above/below/under四个分类中。</p>

<p><img src="http://file.bmob.cn/M00/0D/08/wKhkA1QLwxiAOEV3AAAu5v9zggc374.png" alt="" /></p>

<p>（chooseNodes）第二步进行Source到Target节点的计划移动数据量计算。</p>

<p>在初始化BalancerDatanode的时刻，就计算出了节点的maxSize2Move。从给出的数据，只有一个节点超过阀值，另外两个是都在阀值内，一个高于平均值一个低于平均值。</p>

<p>这里就是把A1超出部分的数据（小于10G）移到A2，计算Source和Target的scheduledSize的大小。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chooseDatanodes(overUtilizedDatanodes, belowAvgUtilizedDatanodes, matcher);
</span><span class='line'>chooseForOneDatanode(datanode, candidates, matcher)
</span><span class='line'>chooseCandidate(dn, i, matcher)
</span><span class='line'>// 把所有A1超出部分全部移到A2，并NodeTask(A2, 8428571.429)存储到Source：A1的nodeTaskList对象中
</span><span class='line'>matchSourceWithTargetToMove((Source)dn, chosen);</span></code></pre></td></tr></table></div></figure>


<p>（dispatchBlockMoves）第三步就是分发进行块的转移。</p>

<p>先设置blocksToReceive（2*scheduledSize=16857142.86）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chooseNextBlockToMove
</span><span class='line'>chooseBlockAndProxy
</span><span class='line'>markMovedIfGoodBlock
</span><span class='line'>isGoodBlockCandidate
</span><span class='line'>chooseProxySource
</span><span class='line'>
</span><span class='line'>scheduleBlockMove
</span><span class='line'>
</span><span class='line'>getBlockList</span></code></pre></td></tr></table></div></figure>


<p>从Source获取块时，可能在A2上已经有了，会通过isGoodBlockCandidate来进行过滤。然后就是把它交给moverExecutor执行数据块的移动，完成后修改处理的数据量byteMoved，把移动的块从target和proxySource的pendingBlockList中删除。</p>

<p>重复进行以上步骤，直到全部所有节点的使用率都在阀值内，顺利结束本次平衡处理。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读码] Hadoop2 Balancer磁盘空间平衡（中）]]></title>
    <link href="http://winse.github.io/blog/2014/09/05/read-hadoop-balancer-source-part2/"/>
    <updated>2014-09-05T14:57:25+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/05/read-hadoop-balancer-source-part2</id>
    <content type="html"><![CDATA[<h2>code</h2>

<p>执行<code>hadoop-2.2.0/bin/hadoop balancer -h</code>查看可以设置的参数（和sbin/start-balancer.sh一样）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Usage: java Balancer
</span><span class='line'>  [-policy &lt;policy&gt;]    the balancing policy: datanode or blockpool
</span><span class='line'>  [-threshold &lt;threshold&gt;]  Percentage of disk capacity</span></code></pre></td></tr></table></div></figure>


<p>main方法入口，可以接受threshold（大于等于1小于等于100， 默认值10）和policy（可取datanode[dfsused]/blockpool[
blockpoolused]， 默认值datanode），具体的含义可以查看（上）篇中的javadoc的描述。</p>

<h3>获取初始化参数</h3>

<p>然后通过ToolRunner解析参数，并运行Cli工具类来执行HDFS的平衡。</p>

<p>1 设置检查</p>

<p><code>WIN_WIDTH</code>(默认1.5h) 已移动的数据会记录movedBlocks（list）变量中，在移动成功的数据<code>CUR_WIN</code>的值经过该时间后会被移动到<code>OLD_WIN</code>&mdash;现在感觉作用不大，为了减少map的大小？</p>

<p><code>checkReplicationPolicyCompatibility()</code>检查配置<code>dfs.block.replicator.classname</code>是否为BlockPlacementPolicyDefault子类，即是否满足3份备份的策略（1st本地，2nd另一个rack，3rd和第二份拷贝不同rack的节点）？</p>

<p>2 获取nameserviceuris</p>

<p>通过<code>DFSUtil#getNsServiceRpcUris()</code>来获取namenodes，调用<code>getNameServiceUris()</code>来得到一个URI的结果集：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>+ nsId &lt;- dfs.nameservices
</span><span class='line'>  ? ha  &lt;- dfs.namenode.rpc-address + [dfs.nameservices] + [dfs.ha.namenodes]
</span><span class='line'>    Y+ =&gt; hdfs://nsId
</span><span class='line'>    N+ =&gt; hdfs://[dfs.namenode.servicerpc-address.[nsId]] 或 hdfs://[dfs.namenode.rpc-address.[nsId]] 第二个满足条件的加入到nonPreferredUris
</span><span class='line'>+ hdfs://[dfs.namenode.servicerpc-address] 或 hdfs://[dfs.namenode.rpc-address]  第二个满足条件的加入到nonPreferredUris
</span><span class='line'>? [fs.defaultFs] 以hfds开头，且不在nonPreferredUris集合中是加入结果集</span></code></pre></td></tr></table></div></figure>


<p>HA情况下的地址相关配置项可以查看<a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithQJM.html">官网的文档</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>dfs.nameservices
</span><span class='line'>dfs.ha.namenodes.[nameservice ID]
</span><span class='line'>dfs.namenode.rpc-address.[nameservice ID].[name node ID] </span></code></pre></td></tr></table></div></figure>


<p>3 解析threshold和policy参数</p>

<p>默认值: <strong>BalancingPolicy.Node.INSTANCE, 10.0</strong>。运行打印的日志如下，INFO日志中包括了初始化的参数信息。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>2014-09-05 10:55:12,183 INFO Balancer: Using a threshold of 1.0
</span><span class='line'>2014-09-05 10:55:12,186 INFO Balancer: namenodes = [hdfs://umcc97-44:9000]
</span><span class='line'>2014-09-05 10:55:12,186 INFO Balancer: p         = Balancer.Parameters[BalancingPolicy.Node, threshold=1.0]
</span><span class='line'>2014-09-05 10:55:13,744 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
</span><span class='line'>2014-09-05 10:55:18,154 INFO net.NetworkTopology: Adding a new node: /default-rack/10.18.97.142:50010
</span><span class='line'>2014-09-05 10:55:18,249 INFO net.NetworkTopology: Adding a new node: /default-rack/10.18.97.144:50010
</span><span class='line'>2014-09-05 10:55:18,311 INFO net.NetworkTopology: Adding a new node: /default-rack/10.18.97.143:50010
</span><span class='line'>2014-09-05 10:55:18,319 INFO Balancer: 2 over-utilized: [Source[10.18.97.144:50010, utilization=8.288283273062705], Source[10.18.97.143:50010, utilization=8.302032354001554]]
</span><span class='line'>2014-09-05 10:55:18,320 INFO Balancer: 1 underutilized: [BalancerDatanode[10.18.97.142:50010, utilization=4.716543864576553]]
</span><span class='line'>2014-09-05 10:55:33,918 INFO Balancer: Need to move 3.86 GB to make the cluster balanced.
</span><span class='line'>2014-09-05 11:21:16,875 INFO Balancer: Decided to move 2.43 GB bytes from 10.18.97.144:50010 to 10.18.97.142:50010
</span><span class='line'>2014-09-05 11:24:16,712 INFO Balancer: Decided to move 1.84 GB bytes from 10.18.97.143:50010 to 10.18.97.142:50010
</span><span class='line'>2014-09-05 11:25:55,726 INFO Balancer: Will move 4.27 GB in this iteration</span></code></pre></td></tr></table></div></figure>


<h3>执行Balancer</h3>

<p>4 调用Balancer#run执行</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> # 调试命令
</span><span class='line'> export HADOOP_OPTS=" -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8087 "
</span><span class='line'> sbin/start-balancer.sh </span></code></pre></td></tr></table></div></figure>


<p></p>

<p>Balancer的静态方法run，循环处理所有namenodes。在实例化namenode的NameNodeConnector对象时，会把当前运行balancer程序的hostname写入到HDFS的<code>/system/balancer.id</code>文件中，可以用来控制同时只有一个balancer运行。</p>

<p><img src="http://file.bmob.cn/M00/0C/96/wKhkA1QJJNqAXxeaAAAho0g2bEU520.png" alt="" /></p>

<p>在循环处理的时刻使用<code>Collections.shuffle(connectors)</code>打乱了namenodes的顺序。</p>

<p>Balancer的静态方法run中是一个双层循环，实例化Balancer并调用实例方法run来处理每个namenode的平衡。运行后要么<strong>出错</strong>要么就是平衡<strong>顺利完成</strong>才算结束。平衡的返回状态值及其含义可以查看javadoc（上）篇。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  static int run(Collection&lt;URI&gt; namenodes, final Parameters p,
</span><span class='line'>      Configuration conf) throws IOException, InterruptedException {
</span><span class='line'>    ...
</span><span class='line'>      for (URI uri : namenodes) {
</span><span class='line'>        connectors.add(new NameNodeConnector(uri, conf));
</span><span class='line'>      }
</span><span class='line'>    
</span><span class='line'>      boolean done = false;
</span><span class='line'>      for(int iteration = 0; !done; iteration++) {
</span><span class='line'>        done = true;
</span><span class='line'>        Collections.shuffle(connectors);
</span><span class='line'>        for(NameNodeConnector nnc : connectors) {
</span><span class='line'>          final Balancer b = new Balancer(nnc, p, conf);
</span><span class='line'>          final ReturnStatus r = b.run(iteration, formatter, conf);
</span><span class='line'>          // clean all lists
</span><span class='line'>          b.resetData(conf);
</span><span class='line'>          if (r == ReturnStatus.IN_PROGRESS) {
</span><span class='line'>            done = false;
</span><span class='line'>          } else if (r != ReturnStatus.SUCCESS) {
</span><span class='line'>            //must be an error statue, return.
</span><span class='line'>            return r.code;
</span><span class='line'>          }
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>        if (!done) {
</span><span class='line'>          Thread.sleep(sleeptime);
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    ...
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>5 <strong>针对每个namenode的平衡处理</strong></p>

<p>针对每个namenode的每次迭代，又可以分出初始化节点、选择移动节点、移动数据三个部分。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private ReturnStatus run(int iteration, Formatter formatter, Configuration conf) {
</span><span class='line'>      ...
</span><span class='line'>      final long bytesLeftToMove = initNodes(nnc.client.getDatanodeReport(DatanodeReportType.LIVE));
</span><span class='line'>      if (bytesLeftToMove == 0) {
</span><span class='line'>        System.out.println("The cluster is balanced. Exiting...");
</span><span class='line'>        return ReturnStatus.SUCCESS;
</span><span class='line'>      }
</span><span class='line'>
</span><span class='line'>      final long bytesToMove = chooseNodes();
</span><span class='line'>      if (bytesToMove == 0) {
</span><span class='line'>        System.out.println("No block can be moved. Exiting...");
</span><span class='line'>        return ReturnStatus.NO_MOVE_BLOCK;
</span><span class='line'>      }
</span><span class='line'>
</span><span class='line'>      if (!this.nnc.shouldContinue(dispatchBlockMoves())) {
</span><span class='line'>        return ReturnStatus.NO_MOVE_PROGRESS;
</span><span class='line'>      }
</span><span class='line'>
</span><span class='line'>      return ReturnStatus.IN_PROGRESS;
</span><span class='line'>      ...
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>获取集群Live Datanode节点的信息，和通过50070查看的信息差不多，然后调用initNode()方法。</p>

<p><img src="http://file.bmob.cn/M00/0C/8F/wKhkA1QJFgaAAsSNAAD4HDo1RfA678.png" alt="" /></p>

<p>5.1 初始化节点</p>

<p><code>initNodes()</code>中获取每个Datanode的capacity和dfsUsed数据，计算整个集群dfs的平均使用率avgUtilization。
然后根据每个节点的使用率与集群使用率，以及阀值进行比较划分为4种情况：
<code>overUtilizedDatanodes</code>，<code>aboveAvgUtilizedDatanodes</code>，<code>belowAvgUtilizedDatanodes</code>，<code>underUtilizedDatanodes</code>。</p>

<p><img src="http://file.bmob.cn/M00/0C/95/wKhkA1QJH2uAa8UEAABq7RCSLQ0452.png" alt="" /></p>

<p>同时取超出<strong>平均+阀值</strong>和<strong>低于平均-阀值</strong>的字节数最大值，即集群达到平衡需要移动的字节数。</p>

<p>为了测试，如果集群已经平衡，可以搞点数据让集群不平衡，方便查看调试。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>bin/hadoop fs -D dfs.replication=1 -put XXXXX /abc
</span><span class='line'>
</span><span class='line'>sbin/start-balancer.sh -threshold 1</span></code></pre></td></tr></table></div></figure>


<p>5.2 选择节点</p>

<p>初始化节点后，计算出了需要移动的数据量。接下来就是选择移动数据的节点<code>chooseNodes</code>，以及接收对应数据的节点。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private long chooseNodes() {
</span><span class='line'>    // First, match nodes on the same node group if cluster is node group aware
</span><span class='line'>    if (cluster.isNodeGroupAware()) {
</span><span class='line'>      chooseNodes(SAME_NODE_GROUP);
</span><span class='line'>    }
</span><span class='line'>    
</span><span class='line'>    chooseNodes(SAME_RACK);
</span><span class='line'>    chooseNodes(ANY_OTHER);
</span><span class='line'>
</span><span class='line'>    long bytesToMove = 0L;
</span><span class='line'>    for (Source src : sources) {
</span><span class='line'>      bytesToMove += src.scheduledSize;
</span><span class='line'>    }
</span><span class='line'>    return bytesToMove;
</span><span class='line'>  }
</span><span class='line'>  private void chooseNodes(final Matcher matcher) {
</span><span class='line'>    chooseDatanodes(overUtilizedDatanodes, underUtilizedDatanodes, matcher);
</span><span class='line'>    chooseDatanodes(overUtilizedDatanodes, belowAvgUtilizedDatanodes, matcher);
</span><span class='line'>    chooseDatanodes(underUtilizedDatanodes, aboveAvgUtilizedDatanodes, matcher);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  private &lt;D extends BalancerDatanode, C extends BalancerDatanode&gt; void 
</span><span class='line'>      chooseDatanodes(Collection&lt;D&gt; datanodes, Collection&lt;C&gt; candidates,
</span><span class='line'>          Matcher matcher) {
</span><span class='line'>    for (Iterator&lt;D&gt; i = datanodes.iterator(); i.hasNext();) {
</span><span class='line'>      final D datanode = i.next();
</span><span class='line'>      for(; chooseForOneDatanode(datanode, candidates, matcher); );
</span><span class='line'>      if (!datanode.hasSpaceForScheduling()) {
</span><span class='line'>        i.remove(); // “超出”部分全部有去处了
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  private &lt;C extends BalancerDatanode&gt; boolean chooseForOneDatanode(
</span><span class='line'>      BalancerDatanode dn, Collection&lt;C&gt; candidates, Matcher matcher) {
</span><span class='line'>    final Iterator&lt;C&gt; i = candidates.iterator();
</span><span class='line'>    final C chosen = chooseCandidate(dn, i, matcher);
</span><span class='line'>
</span><span class='line'>    if (chosen == null) {
</span><span class='line'>      return false;
</span><span class='line'>    }
</span><span class='line'>    if (dn instanceof Source) {
</span><span class='line'>      matchSourceWithTargetToMove((Source)dn, chosen);
</span><span class='line'>    } else {
</span><span class='line'>      matchSourceWithTargetToMove((Source)chosen, dn);
</span><span class='line'>    }
</span><span class='line'>    if (!chosen.hasSpaceForScheduling()) {
</span><span class='line'>      i.remove(); // 可用的空间已经全部分配出去了
</span><span class='line'>    }
</span><span class='line'>    return true;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  private &lt;D extends BalancerDatanode, C extends BalancerDatanode&gt;
</span><span class='line'>      C chooseCandidate(D dn, Iterator&lt;C&gt; candidates, Matcher matcher) {
</span><span class='line'>    if (dn.hasSpaceForScheduling()) {
</span><span class='line'>      for(; candidates.hasNext(); ) {
</span><span class='line'>        final C c = candidates.next();
</span><span class='line'>        if (!c.hasSpaceForScheduling()) {
</span><span class='line'>          candidates.remove();
</span><span class='line'>        } else if (matcher.match(cluster, dn.getDatanode(), c.getDatanode())) {
</span><span class='line'>          return c;
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>    return null;
</span><span class='line'>  }  </span></code></pre></td></tr></table></div></figure>


<p>选择到<strong>接收节点</strong>后，接下来计算可以移动的数据量（取双方的available的最大值），然后把<strong>接收节点</strong>和<strong>数据量</strong>的信息NodeTask存储到Source的NodeTasks对象中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private void matchSourceWithTargetToMove(
</span><span class='line'>      Source source, BalancerDatanode target) {
</span><span class='line'>    long size = Math.min(source.availableSizeToMove(), target.availableSizeToMove());
</span><span class='line'>    NodeTask nodeTask = new NodeTask(target, size);
</span><span class='line'>    source.addNodeTask(nodeTask);
</span><span class='line'>    target.incScheduledSize(nodeTask.getSize());
</span><span class='line'>    sources.add(source);
</span><span class='line'>    targets.add(target);
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p>5.3 移动数据</p>

<p>（待）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[计算出从1到100之间所有奇数的平方之和]]></title>
    <link href="http://winse.github.io/blog/2014/09/04/scala-quadratic-sum-of-odd-num-in-100/"/>
    <updated>2014-09-04T14:15:40+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/04/scala-quadratic-sum-of-odd-num-in-100</id>
    <content type="html"><![CDATA[<p><a href="http://freewind.github.io/posts/scala-group-entry-problem/">计算出从1到100之间所有奇数的平方之和，代码50字符内（QQ群的验证框长度限制为50）</a>。</p>

<p>如题，题目没啥难度，这50字符的条件莫名的增添压迫感。其实java写也不用50个字符就能搞定的 ！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// (1 to 50) foreach {x =&gt; print("0")}
</span><span class='line'>00000000000000000000000000000000000000000000000000
</span><span class='line'>
</span><span class='line'>// java
</span><span class='line'>int sum=0;for(int i=0;i&lt;100;i+=2)sum+=i*i;
</span><span class='line'>
</span><span class='line'>// scala
</span><span class='line'>(1 to 100).map(a=&gt;if(a%2==1)a*a else 0).foldLeft(0)(_+_)
</span><span class='line'>(0 to 100).foldLeft(0)(_+((a:Int)=&gt;if(a%2==1)a*a else 0)(_))
</span><span class='line'>var sum=0;for(i&lt;- 1 to 100)if(i%2==1)sum+=i*i
</span><span class='line'>var sum=0;for(i&lt;- 1 to 100; if i%2==1)sum+=i*i
</span><span class='line'>
</span><span class='line'>(1 to 100 by 2).foldLeft(0)(_+((a:Int)=&gt;a*a)(_))
</span><span class='line'>(1 to 100 by 2).map(a=&gt;a*a).foldLeft(0)(_+_)
</span><span class='line'>var sum=0;for(i&lt;- 1 to 100 by 2)sum+=i*i
</span><span class='line'>(1 to 100 by 2).map(a=&gt;a*a).reduce(_+_)</span></code></pre></td></tr></table></div></figure>


<p><code>(1 to 100 by 2).map(a=&gt;a*a).reduce(_+_)</code>是里面最短的应该也是最好的了，既没有定义变量同时意义清晰一看就懂。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scala Shell #! 惊叹号井号]]></title>
    <link href="http://winse.github.io/blog/2014/09/03/linux-shell-shebang-tanjinghao/"/>
    <updated>2014-09-03T12:55:32+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/03/linux-shell-shebang-tanjinghao</id>
    <content type="html"><![CDATA[<p>工作中主要是写java代码，shell也只是用于交互性操作，写脚本的次数比较少。对于<code>#!</code><strong>井号叹号</strong>仅仅是教条式的添加在脚本开头，并且基本上都是<code>#!/bin/sh</code>。</p>

<p>今天在看scala官方的<a href="http://www.scala-lang.org/documentation/getting-started.html">入门教程</a>尽然发现<code>!#</code>的写法，很是困惑，Google查询也不知道怎么描述关键字，一般搜索引擎都把这些操作符过滤掉了的。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/sh
</span><span class='line'>exec scala "$0" "$@"
</span><span class='line'>!#
</span><span class='line'>object HelloWorld extends App {
</span><span class='line'>  println("Hello, world!")
</span><span class='line'>}
</span><span class='line'>HelloWorld.main(args)</span></code></pre></td></tr></table></div></figure>


<p>首先了解下<code>#!</code>作用：如果<code>#!</code>在脚本的最开始，脚本程序会把第一行的剩余部分当做解析器指令；使用当前的解析器来执行程序，同时把当前脚本的路径作为参数传递给解析器。</p>

<blockquote><p>In computing, a shebang is the character sequence consisting of the characters number sign and exclamation mark (that is, &ldquo;#!&rdquo;) at the beginning of a script.</p>

<p>Under Unix-like operating systems, when a script with a shebang is run as a program, the program loader parses the rest of the script&rsquo;s initial line as an interpreter directive; the specified interpreter program is run instead, passing to it as an argument the path that was initially used when attempting to run the script.</p></blockquote>

<p>如果把<code>!#</code>去掉，再执行上面的脚本则会报错：<strong>error: script file does not close its header with !# or ::!#</strong>，查寻一番后，这原来是Scala的脚本功能的内部处理。通过SourceFile.scala关键字搜索到了<a href="http://www.cnblogs.com/agateriver/archive/2010/09/07/scala_pound_bang.html">该文</a>列出了具体的位置，还有<a href="http://alvinalexander.com/scala/scala-shell-script-example-exec-syntax">A Scala shell script example</a>和我有同样疑问。</p>

<p><img src="http://file.bmob.cn/M00/0B/B1/wKhkA1QGuE-AP-ihAAA1mwvYd5E865.png" alt="" /></p>

<p>可以在《Programing in Scala &ndash; A comprehensive step-by-step guide》一书的附录A中 Scala scripts on Unix and Windows 查找到相应的描述：把<code>#!</code>和<code>!#</code>之间的内容忽略掉了。</p>

<p>语法糖的疑惑解决了，针对上面的脚本还有个问题：exec执行完了，下面的内容不执行了？在exec命令的前面打上调试语句，也只输出了<strong>sh start</strong>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ cat script.scala
</span><span class='line'>#!/bin/sh
</span><span class='line'>echo 'sh start'
</span><span class='line'>exec scala "$0" "$@"
</span><span class='line'>echo 'sh end'
</span><span class='line'>!#
</span><span class='line'>object HelloWorld extends App {
</span><span class='line'>    print("hello world")
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>HelloWorld.main(args)
</span><span class='line'>
</span><span class='line'>winse@Lenovo-PC ~
</span><span class='line'>$ sh script.scala
</span><span class='line'>sh start
</span><span class='line'>hello world
</span></code></pre></td></tr></table></div></figure>


<blockquote><p>exec 使用 exec 方式运行script时, 它和 source 一样, 也是让 script 在当前process内执行, 但是 process 内的原代码剩下部分将被终止. 同样, process 内的环境随script 改变而改变.</p></blockquote>

<p>所以，整个脚本流程就是：执行shell，调用exec来调用scala的解释器执行整个脚本内容，而解释器会过滤掉<code>#!</code>和<code>!#</code>之间内容，执行完后，exec退出脚本，实现scala脚本执行的功能。这样折中的使用方式，应该是为了处理<strong>参数传递</strong>*的问题！</p>

<h2>参考</h2>

<ul>
<li><a href="http://bbs.chinaunix.net/thread-3583927-1-1.html">井号加叹号的作用是什么</a></li>
<li><a href="http://en.wikipedia.org/wiki/Shebang_%28Unix%29">Shebang (Unix)</a></li>
<li><a href="http://bbs.chinaunix.net/thread-218853-1-1.html">shell 十三問? </a></li>
<li><a href="http://www.cnblogs.com/agateriver/archive/2010/09/07/scala_pound_bang.html">Scala 脚本的 pound bang 魔术</a></li>
<li><a href="http://alvinalexander.com/scala/scala-shell-script-example-exec-syntax">A Scala shell script example (and discussion)</a></li>
<li><a href="http://tldp.org/LDP/abs/html/abs-guide.html">Advanced Bash-Scripting Guide An in-depth exploration of the art of shell scripting</a></li>
<li><a href="http://blog.chinaunix.net/uid-27653755-id-4385938.html">linux中fork, source和exec的区别 </a></li>
<li><a href="http://ss64.com/bash/exec.html">exec</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Mapreduce输入输出压缩]]></title>
    <link href="http://winse.github.io/blog/2014/09/01/hadoop2-mapreduce-compress/"/>
    <updated>2014-09-01T16:05:13+08:00</updated>
    <id>http://winse.github.io/blog/2014/09/01/hadoop2-mapreduce-compress</id>
    <content type="html"><![CDATA[<p>当数据达到一定量时，自然就想到了对数据进行压缩来降低存储压力。在Hadoop的任务中提供了5个参数来控制输入输出的数据的压缩格式。添加map输出数据压缩可以降低集群间的网络传输，最终reduce输出压缩可以减低hdfs的集群存储空间。</p>

<p>如果是使用hive等工具的话，效果会更加明显。因为hive的查询结果是临时存储在hdfs中，然后再通过一个<strong>Fetch Operator</strong>来获取数据，最后清理掉，压缩存储临时的数据可以减少磁盘的读写。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;Should the job outputs be compressed?
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.output.fileoutputformat.compress.type&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;RECORD&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;If the job outputs are to compressed as SequenceFiles, how should
</span><span class='line'>               they be compressed? Should be one of NONE, RECORD or BLOCK.
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;If the job outputs are compressed, how should they be compressed?
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;Should the outputs of the maps be compressed before being
</span><span class='line'>               sent across the network. Uses SequenceFile compression.
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;
</span><span class='line'>  &lt;description&gt;If the map outputs are compressed, how should they be 
</span><span class='line'>               compressed?
</span><span class='line'>  &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>上面5个属性弄好，在core-sitem.xml加下<code>io.compression.codecs</code>基本就完成配置了。</p>

<p>这里主要探究下mapreduce（下面全部简称MR）过程中自动解压缩。刚刚接触Hadoop一般都不会去了解什么压缩不压缩的，先把hdfs-api，MR-api弄一遭。配置的TextInputFormat竟然能正确的读取tar.gz文件的内容，觉得不可思议，TextInputFormat不是直接读取txt行记录的输入嘛？难道还能读取压缩文件，先解压再&hellip;？？</p>

<p>先说下OutputFormat，在MR中调用context.write写入数据的方法时，最终使用OutputFormat创建的RecordWriter进行持久化。在TextOutputFormat创建RecordWriter时，如果使用压缩会在结果文件名上<strong>加对应压缩库的后缀</strong>，如gzip压缩对应的后缀gz、snappy压缩对应后缀snappy等。对应下面代码的<code>getDefaultWorkFile</code>。</p>

<p><img src="http://file.bmob.cn/M00/0B/03/wKhkA1QEjD2ASHiZAAExjXuQ25Y062.png" alt="" /></p>

<p>同样对应的TextInputFormat的RecordReader也进行类似的处理：根据<strong>文件的后缀</strong>来判定该文件是否使用压缩，并使用对应的输入流InputStream来解码。</p>

<p><img src="http://file.bmob.cn/M00/0B/03/wKhkA1QEjZaAdBeJAAEvRVMKVWY059.png" alt="" /></p>

<p>此处的关键代码为<code>CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);</code>，根据分块（split）的文件名来判断使用的压缩算法。
初始化Codec实现、以及根据文件名来获取压缩算法的实现还是挺有意思的：通过反转字符串然后最近匹配（headMap）来获取对应的结果。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  private void addCodec(CompressionCodec codec) {
</span><span class='line'>    String suffix = codec.getDefaultExtension();
</span><span class='line'>    codecs.put(new StringBuilder(suffix).reverse().toString(), codec);
</span><span class='line'>    codecsByClassName.put(codec.getClass().getCanonicalName(), codec);
</span><span class='line'>
</span><span class='line'>    String codecName = codec.getClass().getSimpleName();
</span><span class='line'>    codecsByName.put(codecName.toLowerCase(), codec);
</span><span class='line'>    if (codecName.endsWith("Codec")) {
</span><span class='line'>      codecName = codecName.substring(0, codecName.length() - "Codec".length());
</span><span class='line'>      codecsByName.put(codecName.toLowerCase(), codec);
</span><span class='line'>    }
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  public CompressionCodec getCodec(Path file) {
</span><span class='line'>    CompressionCodec result = null;
</span><span class='line'>    if (codecs != null) {
</span><span class='line'>      String filename = file.getName();
</span><span class='line'>      String reversedFilename = new StringBuilder(filename).reverse().toString();
</span><span class='line'>      SortedMap&lt;String, CompressionCodec&gt; subMap = 
</span><span class='line'>        codecs.headMap(reversedFilename);
</span><span class='line'>      if (!subMap.isEmpty()) {
</span><span class='line'>        String potentialSuffix = subMap.lastKey();
</span><span class='line'>        if (reversedFilename.startsWith(potentialSuffix)) {
</span><span class='line'>          result = codecs.get(potentialSuffix);
</span><span class='line'>        }
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>    return result;
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>了解了这些，MR（TextInputFormat）的输入文件可以比较随意些：各种压缩文件、原始文件都可以，只要文件有对应压缩算法的后缀即可。hive的解压缩功能也很容易了，如果使用hive存储text形式的文件，进行压缩无需进行额外的程序代码修改，仅仅修改MR的配置即可，注意下<strong>文件后缀</strong>！！</p>

<p>如，在MR中生成了snappy压缩的文件，此时<strong>不能</strong>在文件的后面添加东西。否则在hive查询时，根据<strong>后缀</strong>进行解压会导致结果乱码/不正确。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;hive.exec.compress.output&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt; This controls whether the final outputs of a query (to a local/hdfs file or a hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>  &lt;name&gt;hive.exec.compress.intermediate&lt;/name&gt;
</span><span class='line'>  &lt;value&gt;false&lt;/value&gt;
</span><span class='line'>  &lt;description&gt; This controls whether intermediate files produced by hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* &lt;/description&gt;
</span><span class='line'>&lt;/property&gt;
</span></code></pre></td></tr></table></div></figure>


<p>hive也弄了两个参数来控制它自己的MR的输出输入压缩控制属性。其他的配置使用mapred-site.xml的配置即可。</p>

<p><img src="http://file.bmob.cn/M00/0B/1D/wKhkA1QFQLmAfyZSAAIOx4UEIbY016.png" alt="" /></p>

<p>网上一些资料有<code>hive.intermediate.compression.codec</code>和<code>hive.intermediate.compression.type</code>两个参数能调整中间过程的压缩算法。其实和mapreduce的参数功能是一样的。</p>

<p><img src="http://file.bmob.cn/M00/0B/1F/wKhkA1QFQWyAUDMLAAGyNqR_X-c417.png" alt="" /></p>

<p>附上解压缩的全部配置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$#core-site.xml
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;io.compression.codecs&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;
</span><span class='line'>  org.apache.hadoop.io.compress.GzipCodec,
</span><span class='line'>  org.apache.hadoop.io.compress.DefaultCodec,
</span><span class='line'>  org.apache.hadoop.io.compress.BZip2Codec,
</span><span class='line'>  org.apache.hadoop.io.compress.SnappyCodec
</span><span class='line'>      &lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>$#mapred-site.xml
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; 
</span><span class='line'>      &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;mapred.output.compression.codec&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>$#hive-site.xml
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>      &lt;name&gt;hive.exec.compress.output&lt;/name&gt;
</span><span class='line'>      &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>运行hive后，临时存储在HDFS的结果数据，注意文件的后缀。</p>

<p><img src="http://file.bmob.cn/M00/0B/20/wKhkA1QFRjSACnLfAABVdoK0f1c803.png" alt="" /></p>

<h2>参考</h2>

<ul>
<li><a href="http://www.geek521.com/?p=4814">深入学习《Programing Hive》：数据压缩</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用ADT调试Xamarin程序中的Java库]]></title>
    <link href="http://winse.github.io/blog/2014/08/29/xamarin-application-use-adt-eclipse-debug-java-code/"/>
    <updated>2014-08-29T12:08:11+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/29/xamarin-application-use-adt-eclipse-debug-java-code</id>
    <content type="html"><![CDATA[<p>在编写SDK的时刻，有用户需要使用Xamarin来开发应用。我们这边暂时没有这个方面的经验，有点瞎扯扯意味，路是崎岖的前进是痛苦的。</p>

<h2>封装Android-SDK</h2>

<p>Xamarin是使用C#语言来编写代码的，所以需要先把Android的jar库包装成为C#的代码。<a href="http://developer.xamarin.com/guides/android/advanced_topics/java_integration_overview/">可选方式有3种</a>)，这里选用Wrapper的形式，不过多讲解，看文章<a href="http://developer.xamarin.com/guides/android/advanced_topics/java_integration_overview/binding_a_java_library_(.jar">Binding a Java Library - Consuming .JARs from C#</a>/)。</p>

<p>建立Binding项目，把依赖的包加入到Jars目录下。由于Bmob-Android官方的包是进行混淆的，有些代码不会用到的/没有必要Wrapper生成jni代码调用的，可以通过removenote去掉不生成C#的wrapper类。第二点就是java的泛型是会被抹掉的，而C#的是会编入程序中的，遇到Comparable这种类型的方法时，需要进行参数强制转换下。第三点就是接口回调，有多个方法时会导致名称冲突，需要为每个接口的方法都配置一个Args的节点属性。这些都是官网的例子中有说明，有需要可以具体参考上面链接的文章内容。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;metadata&gt;
</span><span class='line'>  &lt;remove-node path="/api/package[@name='cn.bmob.im.db']" /&gt;
</span><span class='line'>
</span><span class='line'>  &lt;attr path="/api/package[@name='cn.bmob.im.inteface']/interface[@name='DownloadListener']/method[@name='onError']" name="argsType"&gt;DownloadListenerErrorEventArgs&lt;/attr&gt;
</span><span class='line'>&lt;/metadata&gt;
</span><span class='line'>
</span><span class='line'>&lt;enum-method-mappings&gt;
</span><span class='line'>  &lt;mapping jni-class="cn/bmob/im/bean/BmobRecent"&gt;
</span><span class='line'>    &lt;method jni-name="compareTo" parameter="p0" clr-enum-type="Java.Lang.Object" /&gt;
</span><span class='line'>  &lt;/mapping&gt;
</span><span class='line'>  &lt;mapping jni-class="cn/bmob/im/BmobDownloadManager"&gt;
</span><span class='line'>    &lt;method jni-name="doInBackground" parameter="p0" clr-enum-type="Java.Lang.Object[]" /&gt;
</span><span class='line'>  &lt;/mapping&gt;
</span><span class='line'>&lt;/enum-method-mappings&gt;</span></code></pre></td></tr></table></div></figure>


<p>还有另一个坑是，混淆后内部类会被扁平化，导致jar2xml执行时获取类的getSimpleName名称会抛异常，我这里直接反编译源码改成getName就好了，仅仅是代码中全路径和仅类名的却别，暂时来看没啥印象。</p>

<p>然后编译，加入到主项目的依赖中就可以使用该库的Java功能了。名称可能并不能全部对应上，与Java中的方法名和常量名大小写、下划线的不同罢了。</p>

<h2>调试</h2>

<p>下面是重点，但是很简短。</p>

<p>作为写SDK的，肯定不仅仅要用特定的工具，还的把中间的过程也扭顺，即既要做一个好点（example），又得实现连接的线（SDK）。</p>

<p>如果Android SDK的代码没有执行，该怎么办？Xamarin中都是C#的代码并不能用于调试java啊！问题自然归结到怎么用两个工具（Xamarin和Eclipse）来同时调试一个Xamarin Android应用的问题？！</p>

<p>先讲讲我遇到的坑，由于是开发者发给我的应用，不知道结构是怎么样的。我直接用Xamarin打开，是没有带可执行属性的，在Run-With菜单中是能看到我的实体机器的，但是就是不能把程序发布上去！提示【执行失败。未将对象引用设置到对象的实例。】然后就没了。最终在stackoverflow中找到了类型问题的解决方法，需要设置运行属性。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QAAt6AVtg6AALZEEiP4tQ304.png" alt="" /></p>

<p>配置如下，在解决方案属性中【构建-配置-ConfigurationMappings】把项目添加为构建项。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QAA4uAT9qmAAKI8-zKAn8494.png" alt="" /></p>

<p>可能还会遇到的问题是版本的问题，报错【java.lang.RuntimeException: Unable to get provider mono.MonoRuntimeProvider: java.lang.RuntimeException: Unable to find application Mono.Android.DebugRuntime or Mono.Android.Platform.ApiLevel_19!】需要在csproj的配置中修改AndroidUseLatestPlatformSdk属性为false。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QAC4KAOV-HAAFU6knDvFQ527.png" alt="" /></p>

<p>下面的步骤才是本文的关键：</p>

<p>首先，在MainActivity的onCreate方法开始出打个断点，便于初始化功能调试，点击左上角的开始运行按钮。这样就能把代码发布到机器，且运行后会停留在onCreate处。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QACtCAHOqDAAELanVrybU938.png" alt="" /></p>

<p>Xamarin调试效果图</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QAC_yAYqbpAAHBQT48VSA253.png" alt="" /></p>

<p>再，打开ecilpse导入<code>obj\Debug\android</code>目录下的项目【Import-Android-Existing Android Code Into Workspace】，错误什么的无所谓。这个项目只是用了ADT能识别而已。然后再java包的代码里面打上断点。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QADTuAd3cyAACtJUn8Pdk976.png" alt="" /></p>

<p>最后，起到定乾坤作用的就是DDMS的Devices试图的小爬虫，选择你要调试的程序，然后点击它就可以了。切换到Xamarin继续运行程序，接下来就会运行停留到eclipse中的java包中的断点程序出。</p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QADnWAJQyIAABt1LtWjUA456.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/0A/3C/wKhkA1QADueALec-AAEQhd4vxMY835.png" alt="" /></p>

<p>OK，接下来就按照eclipse的调试技巧弄就好了。</p>

<p>步骤很简单，查询的路子却是艰辛的。第一次尝试成本总是昂贵的，第二步自然会慢慢顺起来。fighting&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[查找逐步定位Java程序OOM的异常实践]]></title>
    <link href="http://winse.github.io/blog/2014/08/25/step-by-step-found-java-oom-error/"/>
    <updated>2014-08-25T21:12:13+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/25/step-by-step-found-java-oom-error</id>
    <content type="html"><![CDATA[<p>类C语言，继C++之后的最辉煌耀眼的明星都属Java，其中最突出的又数内存管理。JVM对运行在其上的程序进行内存自动化分配和管理，减少开发人员的工作量之外便于统一的维护和管理。JDK提供了各种各样的工具来让开发实施人员了解运行的运行状态。</p>

<ul>
<li>jps -l -v -m</li>
<li>jstat -gcutil 2000 100</li>
<li>jmap</li>
<li>jinfo <a href="http://file.bmob.cn/M00/03/AD/wKhkA1PE2MGAB4-fAAGTqUeu-cE940.png">查看参数例子</a></li>
<li>jstack</li>
<li>jvisualvm/jconsole</li>
<li>mat(MemoryAnalyzer)</li>
<li>btrace</li>
<li>jclasslib（查看局部变量表）</li>
</ul>


<p>前段时间，接手(前面已经有成型的东西)使用Hadoop存储转换的项目，但是生产环境的程序总是隔三差五的OOM，同时使用的hive0.12.0也偶尔出现内存异常。这对于运维来说就是灭顶之灾！搞不好什么时刻程序就挂了！！必须咬咬牙把这个问题处理解决，开始把老古董们请出来，翻来基本不看的后半部分&ndash;Java内存管理。</p>

<ul>
<li>《Java程序性能优化-让你的Java程序更快、更稳定》第5章JVM调优/第6章Java性能调优工具</li>
<li>《深入理解Java虚拟机-JVM高级特性与最佳实践》第二部分自动内存管理机制</li>
</ul>


<p>这里用到的理论知识比较少。主要用Java自带的工具，加上内存堆分析工具（mat/jvisualvm）找出大对象，同时结合源代码定位问题。</p>

<p>下面主要讲讲实践，查找问题的思路。在本地进行测试的话，我们可以打断点，可以通过jvisualvm来查看整个运行过程内存的变化趋势图。但是到了linux服务器，并且还是生产环境的话，想要有本地一样的图形化工具来监控是比较困难的！一般服务器的内存都设置的比较大，而windows设置的内存又有限！所以内存达到1G左右，立马dump一个堆的内存快照然后下载到本地进行来分析（可以通过<code>-J-Xmx</code><a href="http://file.bmob.cn/M00/09/83/wKhkA1P7TV-ABDnOAAB-OnVBQic050.png">调整jvisualvm的内存</a>）。</p>

<ul>
<li>首先，由于报错是在Configuration加载配置文件时抛出OOM，第一反应肯定Configuraiton对象太多导致！同时查看dump的堆内存也佐证了这一点。直接把程序中的Configuration改成单例。</li>
</ul>


<p>程序对象内存占比排行（<code>jmap -histo PID</code>）：</p>

<p><img src="http://file.bmob.cn/M00/09/81/wKhkA1P7S8yARYSkAAiFW9cVN5w526.png" alt="" /></p>

<p>使用mat或者jvisualvm查看堆，确实Configuration对象过多（<code>jmap -dump:format=b,file=/tmp/bug.hprof PID</code>）：</p>

<p><img src="http://file.bmob.cn/M00/09/83/wKhkA1P7TbmAIdDEAAq3ktPBs6Q266.png" alt="" /></p>

<ul>
<li><p>修改后再次运行，但是没多大用！还是OOM！！</p></li>
<li><p>进一步分析，发现在Configuration中的属性/缓冲的都是弱引用是weakhashmap。</p></li>
</ul>


<p><img src="http://file.bmob.cn/M00/09/83/wKhkA1P7TfaAf4nwAAbcdgFiyXs804.png" alt="" /></p>

<p>OOM最终问题不在Configuration对象中的属性，哪谁hold住了Configuration对象呢？？</p>

<ul>
<li>再次从根源开始查找问题。程序中FileSystem对象使用<code>FileSystem.get(URI, Configuration, String)</code>获取，然后调用<code>get(URI,Configuration)</code>方法，其中的<strong>CACHE</strong>很是刺眼啊！</li>
</ul>


<p><img src="http://file.bmob.cn/M00/09/8D/wKhkA1P72pmAAMdnAAEYMjHFUAI853.png" alt="" /></p>

<p>缓冲FileSystem的Cache对象的Key是URI和UserGroupInformation两个属性来判断是否相等的。对于一个程序来说一般就读取一个HDFS的数据即URI前部分是确定的，重点在UserGroupInformation是通过<code>UserGroupInformation.getCurrentUser()</code>来获取的。</p>

<p>即获取在get时<code>UserGroupInformation.getBestUGI</code>得到的对象。而这个对象在UnSecure情况下每次都是调用<code>createRemoteUser</code>创建新的对象！也就是每调用一次<code>FileSystem.get(URI, Configuration, String)</code>就会缓存一个FileSystem对象，以及其hold住的Configuration都会被保留在内存中。
<img src="http://file.bmob.cn/M00/09/82/wKhkA1P7TBSAaYEoAAhzUA5j5MI991.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/09/82/wKhkA1P7TJ2AfwJzAAhEVFjK7Ek367.png" alt="" /></p>

<p>只消耗不释放终究会坐吃山空啊！到最后也就必然OOM了。从mat的UserGroupInformation的个数查询，以及Cache对象的总量可以印证。</p>

<p><img src="http://file.bmob.cn/M00/09/82/wKhkA1P7TNeAB7JAAAdMg-udeR8285.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/09/83/wKhkA1P7TU-ACoaCAApK4n-52hI027.png" alt="" /></p>

<h2>问题处理</h2>

<p>把程序涉及到FileSystem.get调用去掉user参数，使两个参数的方法。由于都使用getCurrentUser获取对象，也就是说程序整个运行过程中就一个FileSystem对象，但是与此同时就不能关闭获取到的FileSystem，如果当前运行的用户与集群所属用户不同，需要设置环境变量指定当前操作的用户！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>System.setProperty("HADOOP_USER_NAME", "hadoop");</span></code></pre></td></tr></table></div></figure>


<p>查找代码中调用了FileSystem#close是一个痛苦的过程，由于FileSystem实现的是Closeable的close方法，用<strong>Open Call Hierarchy</strong>基本是大海捞中啊，根本不知道那个代码是自己的！！这里用btrace神器让咋也高大上一把。。。</p>

<p>当时操作的步骤找不到了，下图是调用Cache#getInternal方法监控代码<a href="https://gist.github.com/winse/161f6fe9120f2ec6b024">GIST</a>：</p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7UD2AFk2cAAXRQWzniL0296.png" alt="" /></p>

<h2>hive0.12内存溢出问题</h2>

<p>hive0.12.0查询程序MR内容溢出</p>

<p><img src="http://file.bmob.cn/M00/09/81/wKhkA1P7StSAOgX1AAoW9v-Fd4s439.png" alt="" /></p>

<p>在hive-0.13前官网文档中有提到内存溢出这一点，可以对应到FileSystem中代码的判断。</p>

<p><img src="http://file.bmob.cn/M00/09/85/wKhkA1P7UP-ACRVdAAJHBKNTq94580.png" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>String disableCacheName = String.format("fs.%s.impl.disable.cache", scheme);
</span><span class='line'>if (conf.getBoolean(disableCacheName, false)) {
</span><span class='line'>  return createFileSystem(uri, conf);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>hive0.13.1处理</p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7T_CAcxVqAARr7CGiDvY177.png" alt="" /></p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7T6KAKoUiAAvODPwh1po815.png" alt="" /></p>

<p>新版本在每次查询（session）结束后都会把本次涉及到的FileSystem关闭掉。</p>

<p><img src="http://file.bmob.cn/M00/09/84/wKhkA1P7T9uAQQB3AAWrj_efwZU495.png" alt="" /></p>

<h2>理论知识</h2>

<p>从GC类型开始讲，对自动化内存的垃圾收集有个整体的感知： 新生代/s0（survivor space0、from space）/s1（survivor space1、to space）/永久代。虚拟机参数<code>-Xmx</code>,<code>-Xms</code>,<code>-Xmn</code>（<code>-Xss</code>）来调节各个代的大小和比例。</p>

<ul>
<li><code>-Xss</code> 参数来设置栈的大小。栈的大小直接决定了函数的调用可达深度</li>
<li><code>-XX:PrintGCDetails -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=15 -Xms40m -Xmx40m -Xmn20m</code></li>
<li><code>-XX:NewSize</code>和<code>-XX:MaxNewSize</code></li>
<li><code>-XX:NewRatio</code>和<code>-XX:SurvivorRatio</code></li>
<li><code>-XX:PermSize=2m -XX:MaxPermSize=4m -XX:+PrintGCDetails</code></li>
<li><code>-verbose:gc</code></li>
<li><code>-XX:+PrintGC</code></li>
<li><code>-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/bug.hprof -XX:OnOutOfMemoryError=/reset.sh</code></li>
<li><code>jmap -dump:format=b,file=/tmp/bug.hprof PID</code></li>
<li><code>jmap -histo PID &gt; /tmp/s.txt</code></li>
<li><code>jstack -l PID</code></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[巧用Equals和Hashcode]]></title>
    <link href="http://winse.github.io/blog/2014/08/20/magical-use-java-equals-hashcode/"/>
    <updated>2014-08-20T11:03:54+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/20/magical-use-java-equals-hashcode</id>
    <content type="html"><![CDATA[<p>java中让人疑惑的一点就是等于的判断，有使用<code>==</code>和<code>equals</code>， 有一些专门字符串初始化的资料来考你在是否已经真正的掌握判断两个对象是否一致。</p>

<p>同时，在重写equals时很多资料都强调要重写hashcode。</p>

<p>java中HashMap就是基于equals和hashcode来实现拉链的键值对。Map中存储了entry&lt;K,V>的数组，数组的下标是基于对象的hashcode再对entry长度[并]<code>&amp;</code>的结果。</p>

<p><img src="http://file.bmob.cn/M00/08/55/wKhkA1P0OO-AE9u3AABNzQK0pr8747.png" alt="" /></p>

<p>使用set/map来实现集合，并且对象重写了equals但没有重写hashcode的情况下，得到的结果与你臆想的不同。同时，在特定场景结合hashcode和equals可以实现很酷的效果。</p>

<ul>
<li>第一个例子A 重写了equals但是没有重写hashcode(ERROR)，此时判断元素是否在集合中结果可能并不是你想要的。</li>
<li>第二个B的例子 重写hashcode和equals对应后就正确了。</li>
<li>第三个AA是个很酷的例子 equals的条件更强，可以实现类似<code>map&lt;string, list&lt;string&gt;&gt;</code>的效果。</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>static class A {
</span><span class='line'>
</span><span class='line'>  String name;
</span><span class='line'>  int age;
</span><span class='line'>
</span><span class='line'>  public A(String name, int age) {
</span><span class='line'>      this.name = name;
</span><span class='line'>      this.age = age;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public boolean equals(Object obj) {
</span><span class='line'>      return new EqualsBuilder().append(getClass(), obj.getClass()).append(name, ((A) obj).name).isEquals();
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testA() {
</span><span class='line'>  Set&lt;A&gt; set = new HashSet&lt;&gt;();
</span><span class='line'>
</span><span class='line'>  set.add(new A("abc", 12));
</span><span class='line'>  set.add(new A("abc", 14));
</span><span class='line'>
</span><span class='line'>  System.out.println(set.size());
</span><span class='line'>
</span><span class='line'>  System.out.println(set.contains(new A("abc", 0)));
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>static class B extends A {
</span><span class='line'>
</span><span class='line'>  public B(String name, int age) {
</span><span class='line'>      super(name, age);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public int hashCode() {
</span><span class='line'>      return this.name.hashCode();
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testB() {
</span><span class='line'>  /* Set&lt;A&gt; */Set&lt;B&gt; set = new HashSet&lt;&gt;();
</span><span class='line'>
</span><span class='line'>  set.add(new B("abc", 12));
</span><span class='line'>  set.add(new B("abc", 14));
</span><span class='line'>
</span><span class='line'>  System.out.println(set.size());
</span><span class='line'>
</span><span class='line'>  System.out.println(set.contains(new B("abc", 0)));
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>static class AA extends A {
</span><span class='line'>  public AA(String name, int age) {
</span><span class='line'>      super(name, age);
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public boolean equals(Object obj) {
</span><span class='line'>      return new EqualsBuilder().append(getClass(), obj.getClass()).append(name, ((A) obj).name)
</span><span class='line'>              .append(age, ((A) obj).age).isEquals();
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  @Override
</span><span class='line'>  public int hashCode() {
</span><span class='line'>      return this.name.hashCode();
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>@Test
</span><span class='line'>public void testAA() {
</span><span class='line'>  // 实现Map&lt;String, Set&lt;String&gt;&gt;的效果
</span><span class='line'>  Set&lt;AA&gt; set = new HashSet&lt;&gt;();
</span><span class='line'>
</span><span class='line'>  set.add(new AA("abc", 12));
</span><span class='line'>  set.add(new AA("abc", 14));
</span><span class='line'>
</span><span class='line'>  System.out.println(set.size());
</span><span class='line'>
</span><span class='line'>  System.out.println(set.contains(new AA("abc", 0)));
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<h2>参考</h2>

<ul>
<li><a href="http://java.chinaitlab.com/base/879319.html">java中HashMap详解</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[读码] Hadoop2 Balancer磁盘空间平衡（上）]]></title>
    <link href="http://winse.github.io/blog/2014/08/06/read-hadoop-balancer-source-part1/"/>
    <updated>2014-08-06T22:14:29+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/06/read-hadoop-balancer-source-part1</id>
    <content type="html"><![CDATA[<h2>javadoc</h2>

<p>在一些节点满了或者加入了新的节点情况下，使用balancer工具可以平衡HDFS集群磁盘空间使用率。该功能作为一个单独的程序，能同时与集群的其他文件操作一起进行。</p>

<p>threshold（阀值）参数是个介于1%~100%的值，默认情况下是10%。指定了集群达到平衡的指标值。当节点的利用率（所在节点的磁盘使用率，只HDFS可利用的部分）与集群利用率（集群HDFS的使用率）之间的差不大于阀值threshold就表示集群已经处于平衡状态。所以，阀值threshold越小，集群各节点数据分布越平衡（集群越平衡），当然这也会耗费更多的时间来达到小的平衡阀值。同时，当数据同时又在进行读写操作时，可能平衡并不能达到非常小的阀值。</p>

<p><img src="http://file.bmob.cn/M00/0C/95/wKhkA1QJH2uAa8UEAABq7RCSLQ0452.png" alt="" /></p>

<p>这个工具依次地把磁盘使用率高的机器的块移动到使用率低的数据节点上。每次迭代移动/接受的数据小于10G或者节点容量的阀值百分比（In each iteration a datanode moves or receives no more than the lesser of 10G bytes or the threshold fraction of its capacity. ）。每次迭代不会大于20分钟。每次迭代完成后，balancer把数据节点信息更新到namenode，重新计算利用率后，再进行下一次迭代直到集群利用率阀值。</p>

<p>配置<code>dfs.balance.bandwidthPerSec</code>控制balancer操作传输的带宽，默认配置是1048576（1M/s）这个属性决定了一个块从一个数据节点移动到另一个节点的最大速率。默认是1M/s。bandwidth越高集群达到平衡越快，但是程序之间的竞争会更激烈。如果通过配置文件来修改这个属性，需要在下次启动HDFS才能生效。可以通过<code>hdfs dfsadmin -setBalancerBandwidth 10485760</code>来动态的设置。</p>

<p>每次迭代会输出开始时间，迭代的次数，上一次迭代移动的数据量，集群达到平衡还需要移动的数据量，该次迭代将移动的数据量。一般情况下，“Bytes Already Moved”将会增加同时“Bytes Left To Move”将会减少（但是如果此时有大数据量写入，那么Bytes Left To Move可能不减反增）。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span></code></pre></td></tr></table></div></figure>


<p>多个balancer程序不能同时运行。</p>

<p>balancer程序会自动退出当存在以下情况：</p>

<ul>
<li>集群已经平衡</li>
<li>没有块将会被移动</li>
<li>连续5次的处理没有块移动</li>
<li>连接namenode时出现IOException</li>
<li>另一个balancer程序在跑</li>
</ul>


<p>根据上面的5中情况，balancer程序退出，同时会打印如下的信息：</p>

<ul>
<li>The cluster is balanced. Exiting</li>
<li>No block can be moved. Exiting&hellip;</li>
<li>No block has been moved for 5 iterations. Exiting&hellip;</li>
<li>Received an IO exception: failure reason. Exiting&hellip;</li>
<li>Another balancer is running. Exiting&hellip;</li>
</ul>


<p>当balancer运行时，管理员可以随时运行stop-balancer.sh来中断balancer程序。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop的datanode数据节点软/硬件配置应该一致]]></title>
    <link href="http://winse.github.io/blog/2014/08/02/hadoop-datanode-config-should-equals/"/>
    <updated>2014-08-02T22:21:12+08:00</updated>
    <id>http://winse.github.io/blog/2014/08/02/hadoop-datanode-config-should-equals</id>
    <content type="html"><![CDATA[<p>最好的就是集群的所有的datanode的节点的<strong>硬件配置一样</strong>！当然系统时间也的一致，hosts等等这些。机器配置一样时可以使用脚本进行批量处理，给维护带来很大的便利性。</p>

<p>今天收到运维的信息，说集群的一台机器硬盘爆了！上到环境查看<code>df -h</code>发现硬盘配置和其他datanode的不同！但是hadoop hdfs-site.xml的<code>dfs.datanode.data.dir</code>却是一样的！</p>

<p>经验： dir的配置应该是一个系统设备对应一个路径，而不是一个系统目录对应dir的一个路径！</p>

<h2>问题现象以及根源</h2>

<p>问题机器A的磁盘情况：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver8 ~]$ df -h
</span><span class='line'>文件系统              容量  已用  可用 已用%% 挂载点
</span><span class='line'>/dev/sda3             2.7T  2.5T   53G  98% /
</span><span class='line'>tmpfs                  32G  260K   32G   1% /dev/shm
</span><span class='line'>/dev/sda1              97M   32M   61M  35% /boot
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 /]$ ll
</span><span class='line'>总用量 170
</span><span class='line'>dr-xr-xr-x.   2 root   root    4096 2月  12 19:39 bin
</span><span class='line'>dr-xr-xr-x.   5 root   root    1024 2月  13 02:40 boot
</span><span class='line'>drwxr-xr-x.   2 root   root    4096 2月  23 2012 cgroup
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data1
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data10
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data11
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data12
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data13
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data14
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data15
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data2
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data3
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data4
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data5
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data6
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data7
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data8
</span><span class='line'>drwxr-xr-x.   3 hadoop hadoop  4096 6月  30 10:36 data9</span></code></pre></td></tr></table></div></figure>


<p>再看集群其他机器：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver1 ~]$ df -h
</span><span class='line'>文件系统              容量  已用  可用 已用%% 挂载点
</span><span class='line'>/dev/sda3             2.7T   32G  2.5T   2% /
</span><span class='line'>tmpfs                  32G   88K   32G   1% /dev/shm
</span><span class='line'>/dev/sda1              97M   32M   61M  35% /boot
</span><span class='line'>/dev/sdb1             1.8T  495G  1.3T  29% /data1
</span><span class='line'>/dev/sdb2             1.8T  485G  1.3T  28% /data2
</span><span class='line'>/dev/sdb3             1.8T  492G  1.3T  29% /data3
</span><span class='line'>/dev/sdb4             1.8T  488G  1.3T  28% /data4
</span><span class='line'>/dev/sdb5             1.8T  486G  1.3T  28% /data5
</span><span class='line'>/dev/sdb6             1.8T  480G  1.3T  28% /data6
</span><span class='line'>/dev/sdb7             1.8T  479G  1.3T  28% /data7
</span><span class='line'>/dev/sdb8             1.8T  474G  1.3T  28% /data8
</span><span class='line'>/dev/sdb9             1.8T  480G  1.3T  28% /data9
</span><span class='line'>/dev/sdb10            1.8T  478G  1.3T  28% /data10
</span><span class='line'>/dev/sdb11            1.8T  475G  1.3T  28% /data11
</span><span class='line'>/dev/sdb12            1.8T  489G  1.3T  29% /data12
</span><span class='line'>/dev/sdb13            1.8T  475G  1.3T  28% /data13
</span><span class='line'>/dev/sdb14            1.8T  476G  1.3T  28% /data14
</span><span class='line'>/dev/sdb15            1.8T  469G  1.3T  27% /data15</span></code></pre></td></tr></table></div></figure>


<p>出问题机器没有挂存储，仅仅是建立了对应的目录结构，并不是把目录挂载到单独的存储设备上。</p>

<p>同时查看50070的前面的信息，hadoop把每个逗号分隔后的路径默认都做一个磁盘设备来计算！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Node               Address             ..Admin State CC    Used  NU    RU(%) R(%)      Blocks Block  Pool Used Block Pool Used (%)
</span><span class='line'>hadoop-slaver1    192.168.32.21:50010 2   In Service  26.86   7.05    1.37    18.44   26.25       68.66   264844  7.05    26.25   
</span><span class='line'>hadoop-slaver8    192.168.32.28:50010 1   In Service  37.94   2.46    34.71   0.77    6.48        2.03    29637   2.46    6.48    </span></code></pre></td></tr></table></div></figure>


<p>配置容量是所有配置的路径所在盘容量的<strong>累加</strong>。总的剩余空间（余量）也是各个dir配置路径的剩余空间<strong>累加</strong>的！这样很容易出现问题！
最好的就是集群的所有的datanode的节点的<strong>硬件配置一样</strong>！当然系统时间也的一致，hosts等等这些。</p>

<h2>问题处理</h2>

<p>首先得把问题解决啊：</p>

<ul>
<li>把<code>dfs.datanode.data.dir</code>路径个数调整为磁盘个数！</li>
<li>修改该datanode的hdfs-site的配置，添加<code>dfs.datanode.du.reserved</code>，留给系统的空间设置为400多G。</li>
<li>冗余份数也没有必要3份，浪费空间。如果两台机器同时出现问题，还是同一份数据，那只能说是天意！你可以去趟澳门赌一圈了！</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
</span><span class='line'>&lt;value&gt;/data1/hadoop/data&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;
</span><span class='line'>&lt;value&gt;437438953472&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>&lt;property&gt;
</span><span class='line'>&lt;name&gt;dfs.replication&lt;/name&gt;
</span><span class='line'>&lt;value&gt;2&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>设置了reserved保留空间后，再看LIVE页面slaver8的容量变少了且正好等于(盘的容量2.7T-430G~=2.26T 计算容量的hdfs源码在<code>FsVolumeImpl.getCapacity()</code>)。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop-slaver8   192.168.32.28:50010 1   In Service  2.26    2.23    0.00    0.03    98.66</span></code></pre></td></tr></table></div></figure>


<p>datanode和blockpool的平衡处理，可以参考<a href="http://hadoop-master1:50070/dfsnodelist.jsp?whatNodes=LIVE">Live Datanodes</a>的容量和进行！</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-master1 ~]$ hdfs balancer -help
</span><span class='line'>Usage: java Balancer
</span><span class='line'>        [-policy &lt;policy&gt;]      the balancing policy: datanode or blockpool
</span><span class='line'>        [-threshold &lt;threshold&gt;]        Percentage of disk capacity
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 ~]$ hadoop-2.2.0/bin/hdfs getconf -confkey dfs.datanode.du.reserved
</span><span class='line'>137438953472</span></code></pre></td></tr></table></div></figure>


<p>删除一些没用的备份数据。配置好以后，重启当前slaver8节点，并进行数据平衡（如果觉得麻烦，直接丢掉原来的一个目录下的数据也行，可能更快！均衡器运行的太慢！！）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@hadoop-slaver8 ~]$  ~/hadoop-2.2.0/sbin/hadoop-daemon.sh stop datanode
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 ~]$  for i in 6 7 8 9 10 11 12 13 14 15; do  cd /data$i/hadoop/data/current/BP-1414312971-192.168.32.11-1392479369615/current/finalized;  find . -type f -exec mv {} /data1/hadoop/data/current/BP-1414312971-192.168.32.11-1392479369615/current/finalized/{} \;; done
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-slaver8 ~]$  ~/hadoop-2.2.0/sbin/hadoop-daemon.sh start datanode
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>[hadoop@hadoop-master1 ~]$ hdfs dfsadmin -setBalancerBandwidth 10485760
</span><span class='line'>[hadoop@hadoop-master1 ~]$ hdfs balancer -threshold 60
</span></code></pre></td></tr></table></div></figure>


<p>查看datanode的日志，由于移动数据，有些blk的id一样，会清理一些数据。对于均衡器程序的阀值越小集群越平衡！默认是10（%），会移动很多的数据（准备看下均衡器的源码，了解各个参数以及运行的逻辑）！</p>

<h2>参考</h2>

<ul>
<li><a href="http://blog.csdn.net/lingzihan1215/article/details/8700532">hadoop的datanode多磁盘空间处理</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 Snappy Compress]]></title>
    <link href="http://winse.github.io/blog/2014/07/30/hadoop2-snappy-compress/"/>
    <updated>2014-07-30T00:25:39+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/30/hadoop2-snappy-compress</id>
    <content type="html"><![CDATA[<p>网上查了很多资料说的很复杂，要叉叉叉叉叉！其实hadoop2已经集成了hadoop-snappy，只要安装snappy即可。但是也没有一些文章说的只要编译snappy然后放到lib/native路径下即可，还需要重新编译libhadoop的library包。</p>

<p>查找hadoop-snappy的源码的时刻，在C代码里面定义了<code>HADOOP_SNAPPY_LIBRARY</code>，然后理着这个思路去查找，发现在CMakeFile文件中也定义了对应的变量，然后再查找pom.xml的native profile中定义了snappy.prefix的属性。最后就有了下面的步骤。</p>

<p>1) build snappy</p>

<p>编译Snappy，并把lib拷贝/同步到hadoop的native目录下。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar zxf snappy-1.1.1.tar.gz 
</span><span class='line'>cd snappy-1.1.1
</span><span class='line'>./configure --prefix=/home/hadoop/snappy
</span><span class='line'>make
</span><span class='line'>make install
</span><span class='line'>
</span><span class='line'>cd snappy
</span><span class='line'>cd lib/
</span><span class='line'>rysnc -vaz * ~/hadoop-2.2.0/lib/native/</span></code></pre></td></tr></table></div></figure>


<p>2) rebuild hadoop common project</p>

<p>重新编译hadoop的lib，覆盖原来的文件。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop-common]$ mvn package -Dmaven.javadoc.skip=true -DskipTests -Dsnappy.prefix=/home/hadoop/snappy -Drequire.snappy=true -Pnative 
</span><span class='line'>
</span><span class='line'>[hadoop@master1 hadoop-common]$ cd ~/hadoop-2.2.0-src/hadoop-common-project/hadoop-common/
</span><span class='line'>[hadoop@master1 hadoop-common]$ cd target/native/target/usr/local/lib/
</span><span class='line'>[hadoop@master1 lib]$ ll
</span><span class='line'>total 1252
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 820824 Jul 30 00:18 libhadoop.a
</span><span class='line'>lrwxrwxrwx. 1 hadoop hadoop     18 Jul 30 00:18 libhadoop.so -&gt; libhadoop.so.1.0.0
</span><span class='line'>-rwxrwxr-x. 1 hadoop hadoop 455542 Jul 30 00:18 libhadoop.so.1.0.0
</span><span class='line'>[hadoop@master1 lib]$ rsync -vaz * ~/hadoop-2.2.0/lib/native/
</span><span class='line'>sending incremental file list
</span><span class='line'>libhadoop.a
</span><span class='line'>libhadoop.so.1.0.0
</span><span class='line'>
</span><span class='line'>sent 409348 bytes  received 53 bytes  818802.00 bytes/sec
</span><span class='line'>total size is 1276384  speedup is 3.12
</span><span class='line'>[hadoop@master1 lib]$ </span></code></pre></td></tr></table></div></figure>


<p>3) check</p>

<p>检查程序snappy是否已经配置成功</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ hadoop checknative -a
</span><span class='line'>14/07/30 00:22:14 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
</span><span class='line'>14/07/30 00:22:14 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
</span><span class='line'>Native library checking:
</span><span class='line'>hadoop: true /home/hadoop/hadoop-2.2.0/lib/native/libhadoop.so.1.0.0
</span><span class='line'>zlib:   true /lib64/libz.so.1
</span><span class='line'>snappy: true /home/hadoop/hadoop-2.2.0/lib/native/libsnappy.so.1
</span><span class='line'>lz4:    true revision:43
</span><span class='line'>bzip2:  false 
</span><span class='line'>14/07/30 00:22:14 INFO util.ExitUtil: Exiting with status 1
</span><span class='line'>[hadoop@master1 ~]$ </span></code></pre></td></tr></table></div></figure>


<p>4) 跑一个压缩程序</p>

<p>先参考网上的，直接用hbase的带的测试类运行（前提：需要在hbase-env.sh中配置HADOOP_HOME，这样hbase才能找到hadoop下的lib本地库）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ hbase-0.98.3-hadoop2/bin/hbase org.apache.hadoop.hbase.util.CompressionTest file:///tmp/abc.snappy snappy
</span><span class='line'>2014-07-30 08:50:42,617 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
</span><span class='line'>SLF4J: Class path contains multiple SLF4J bindings.
</span><span class='line'>SLF4J: Found binding in [jar:file:/home/hadoop/hbase-0.98.3-hadoop2/lib/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
</span><span class='line'>SLF4J: Found binding in [jar:file:/home/hadoop/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
</span><span class='line'>SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
</span><span class='line'>2014-07-30 08:50:44,515 INFO  [main] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
</span><span class='line'>2014-07-30 08:50:44,522 INFO  [main] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
</span><span class='line'>2014-07-30 08:50:45,388 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
</span><span class='line'>2014-07-30 08:50:45,408 INFO  [main] compress.CodecPool: Got brand-new compressor [.snappy]
</span><span class='line'>2014-07-30 08:50:45,430 ERROR [main] hbase.KeyValue: Unexpected getShortMidpointKey result, fakeKey:testkey, firstKeyInBlock:testkey
</span><span class='line'>2014-07-30 08:50:47,088 INFO  [main] compress.CodecPool: Got brand-new decompressor [.snappy]
</span><span class='line'>SUCCESS
</span><span class='line'>[hadoop@master1 ~]$ </span></code></pre></td></tr></table></div></figure>


<p>看到最后的<strong>SUCCESS</strong>就说明安装配置成功了！</p>

<p>接下来自己写程序测试压缩/解压缩，首先编写java类：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import java.io.FileInputStream;
</span><span class='line'>import java.io.FileNotFoundException;
</span><span class='line'>import java.io.FileOutputStream;
</span><span class='line'>import java.io.IOException;
</span><span class='line'>
</span><span class='line'>import org.apache.commons.lang.StringUtils;
</span><span class='line'>import org.apache.hadoop.conf.Configuration;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionCodec;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionInputStream;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionOutputStream;
</span><span class='line'>import org.apache.hadoop.io.compress.SnappyCodec;
</span><span class='line'>import org.apache.hadoop.util.ReflectionUtils;
</span><span class='line'>import org.apache.zookeeper.common.IOUtils;
</span><span class='line'>
</span><span class='line'>public class SnappyCompressTest {
</span><span class='line'>
</span><span class='line'>        public static void main(String[] args) throws FileNotFoundException, IOException {
</span><span class='line'>                try {
</span><span class='line'>                        execute(args);
</span><span class='line'>                } catch (Exception e) {
</span><span class='line'>                        System.out.println("Usage: $0 read|write file[.snappy]");
</span><span class='line'>                }
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>        private static void execute(String[] args) throws FileNotFoundException, IOException {
</span><span class='line'>                String op = args[0];
</span><span class='line'>                String file = args[1];
</span><span class='line'>                String snappyFile = file + ".snappy";
</span><span class='line'>
</span><span class='line'>                Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
</span><span class='line'>                CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());
</span><span class='line'>
</span><span class='line'>                if (StringUtils.equalsIgnoreCase(op, "read")) {
</span><span class='line'>                        FileInputStream fin = new FileInputStream(snappyFile);
</span><span class='line'>                        CompressionInputStream in = codec.createInputStream(fin);
</span><span class='line'>                        FileOutputStream fout = new FileOutputStream(file);
</span><span class='line'>                        IOUtils.copyBytes(in, fout, 4096, true);
</span><span class='line'>                } else {
</span><span class='line'>                        FileInputStream fin = new FileInputStream(file);
</span><span class='line'>                        CompressionOutputStream out = codec.createOutputStream(new FileOutputStream(snappyFile));
</span><span class='line'>                        IOUtils.copyBytes(fin, out, 4096, true);
</span><span class='line'>                }
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>编译运行，测试读写功能。使用hadoop命令可以简化很多工作，把当前路径加入到<code>HADOOP_CLASSPATH</code>。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 test]$ javac -cp `hadoop classpath` SnappyCompressTest.java 
</span><span class='line'>[hadoop@master1 test]$ export HADOOP_CLASSPATH=$PWD
</span><span class='line'>[hadoop@master1 test]$ hadoop SnappyCompressTest 
</span><span class='line'>Usage: $0 read|write file[.snappy]
</span><span class='line'>[hadoop@master1 test]$ hadoop SnappyCompressTest write test.txt 
</span><span class='line'>[hadoop@master1 test]$ ll
</span><span class='line'>total 16
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:23 test.txt
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
</span><span class='line'>[hadoop@master1 test]$ rm test.txt
</span><span class='line'>[hadoop@master1 test]$ hadoop SnappyCompressTest read test.txt 
</span><span class='line'>[hadoop@master1 test]$ ll
</span><span class='line'>total 16
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1991 Jul 30 09:27 SnappyCompressTest.class
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop 1586 Jul 30 09:23 SnappyCompressTest.java
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   12 Jul 30 09:28 test.txt
</span><span class='line'>-rw-rw-r--. 1 hadoop hadoop   22 Jul 30 09:28 test.txt.snappy
</span><span class='line'>[hadoop@master1 test]$ cat test.txt
</span><span class='line'>abc
</span><span class='line'>abc
</span><span class='line'>abc</span></code></pre></td></tr></table></div></figure>


<p>5) hbase中添加压缩</p>

<p>把所有library，以及hbase的配置同步其他所有从节点。对hbase的表使用Snappy压缩。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hbase(main):001:0&gt; create 'st1', 'f1'
</span><span class='line'>hbase(main):005:0&gt; alter 'st1', {NAME=&gt;'f1', COMPRESSION=&gt;'snappy'}
</span><span class='line'>Updating all regions with the new schema...
</span><span class='line'>0/1 regions updated.
</span><span class='line'>1/1 regions updated.
</span><span class='line'>Done.
</span><span class='line'>0 row(s) in 2.7880 seconds
</span><span class='line'>
</span><span class='line'>hbase(main):010:0&gt; create 'sst1','f1'
</span><span class='line'>0 row(s) in 0.5730 seconds
</span><span class='line'>
</span><span class='line'>=&gt; Hbase::Table - sst1
</span><span class='line'>hbase(main):011:0&gt; flush 'sst1'
</span><span class='line'>0 row(s) in 2.5380 seconds
</span><span class='line'>
</span><span class='line'>hbase(main):012:0&gt; flush 'st1'
</span><span class='line'>0 row(s) in 7.5470 seconds</span></code></pre></td></tr></table></div></figure>


<p>对于hbase来说，使用压缩消耗还是挺大的。插入10w数据中间进行compaction时停顿比较久。最后flush写数据的时间也长了很多！
下面是文件写入后的文件大小对比（由于是进行简单的测试，写入的数据重复比较多。具体比例没有参考价值）：</p>

<p><img src="http://file.bmob.cn/M00/05/5A/wKhkA1PYz9CAB-TdAAEWX8LGpUo149.png" alt="" /></p>

<p>6) 正式环境下解压snappy文件</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>import java.io.FileOutputStream;
</span><span class='line'>import java.io.IOException;
</span><span class='line'>import java.io.InputStream;
</span><span class='line'>
</span><span class='line'>import org.apache.hadoop.conf.Configuration;
</span><span class='line'>import org.apache.hadoop.fs.FileSystem;
</span><span class='line'>import org.apache.hadoop.fs.Path;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionCodec;
</span><span class='line'>import org.apache.hadoop.io.compress.CompressionInputStream;
</span><span class='line'>import org.apache.hadoop.io.compress.SnappyCodec;
</span><span class='line'>import org.apache.hadoop.util.ReflectionUtils;
</span><span class='line'>import org.apache.zookeeper.common.IOUtils;
</span><span class='line'>
</span><span class='line'>public class DecompressTest {
</span><span class='line'>  public static void main(String[] args) throws IOException {
</span><span class='line'>
</span><span class='line'>      Configuration conf = new Configuration();
</span><span class='line'>      Path path = new Path(args[0]);
</span><span class='line'>      FileSystem fs = path.getFileSystem(conf);
</span><span class='line'>
</span><span class='line'>      Class&lt;? extends CompressionCodec&gt; clazz = SnappyCodec.class;
</span><span class='line'>      CompressionCodec codec = ReflectionUtils.newInstance(clazz, new Configuration());
</span><span class='line'>
</span><span class='line'>      InputStream fin = fs.open(path);
</span><span class='line'>      CompressionInputStream in = codec.createInputStream(fin);
</span><span class='line'>
</span><span class='line'>      IOUtils.copyBytes(in, System.out, 4096, true);
</span><span class='line'>
</span><span class='line'>      fin.close();
</span><span class='line'>
</span><span class='line'>      System.out.println("SUCCESS");
</span><span class='line'>
</span><span class='line'>  }
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>// build & run
</span><span class='line'>
</span><span class='line'>&gt;DecompressTest.java 
</span><span class='line'>vi DecompressTest.java 
</span><span class='line'>javac -cp `hadoop classpath`  DecompressTest.java 
</span><span class='line'>export HADOOP_CLASSPATH=.
</span><span class='line'># snappyfile on hdfs
</span><span class='line'>hadoop DecompressTest /user/hive/t_ods_access_log2/month=201408/day=20140828/hour=2014082808/t_ods_access_log2-2014082808.our.snappy.1409187524328
</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop2 ShortCircuit Local Reading]]></title>
    <link href="http://winse.github.io/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading/"/>
    <updated>2014-07-29T20:11:58+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/29/hadoop2-use-shortcircuit-local-reading</id>
    <content type="html"><![CDATA[<p>hadoop一直以来认为是本地读写文件的，但是其实也是通过TCP端口去获取数据，只是都在同一台机器。在hivetuning调优hive的文档中看到了ShortCircuit的HDFS配置属性，查看了ShortCircuit的来由，真正的实现了本地读取文件。蒙查查表示看的不是很明白，最终大致就是通过linux的<strong>文件描述符</strong>来实现功能同时保证文件的权限。</p>

<p>由于仅在自己的机器上面配置来查询hbase的数据，性能方面提升感觉不是很明显。等以后整到正式环境再对比对比。</p>

<p>配置如下。</p>

<p>1 修改hdfs-site.xml</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;/home/hadoop/data/sockets/dn_socket&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<p>注意：socket路径的权限控制的比较严格。dn_socket<strong>所有的父路径</strong>要么仅有当前启动用户的写权限，要么仅root可写。</p>

<p><img src="http://file.bmob.cn/M00/05/52/wKhkA1PXfbKANLOrAADWJQ5taVs391.png" alt="" /></p>

<p>2 修改hbase的配置，并添加HADOOP_HOME（hbase查找hadoop-native）</p>

<p><img src="http://file.bmob.cn/M00/05/52/wKhkA1PXhRKAZDs6AAChrEauBoU738.png" alt="" /></p>

<p>hbase的脚本找到hadoop命令后，会把hadoop的java.library.path的路径加入到hbase的启动脚本中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ tail -15 hbase-0.98.3-hadoop2/conf/hbase-site.xml 
</span><span class='line'>    &lt;name&gt;hbase.tmp.dir&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;/home/hadoop/data/hbase&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;true&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;/home/hadoop/data/sockets/dn_socket&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>
</span><span class='line'>[hadoop@master1 ~]$ cat hbase-0.98.3-hadoop2/conf/hbase-env.sh
</span><span class='line'>...
</span><span class='line'>export HADOOP_HOME=/home/hadoop/hadoop-2.2.0
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>3 同步到其他节点，然后重启hdfs,hbase</p>

<h2>参考</h2>

<ul>
<li><a href="http://vdisk.weibo.com/s/z_44nz36hNM3Z">hive-tuning</a></li>
<li><a href="http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/">How Improved Short-Circuit Local Reads Bring Better Performance and Security to Hadoop</a></li>
<li><a href="http://hbase.apache.org/book/perf.hdfs.html">HDFS&ndash;Apache HBase Performance Tuning</a></li>
<li><a href="http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">HDFS Short-Circuit Local Reads</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop安全的关闭datanode节点]]></title>
    <link href="http://winse.github.io/blog/2014/07/29/safely-remove-datanode/"/>
    <updated>2014-07-29T15:08:41+08:00</updated>
    <id>http://winse.github.io/blog/2014/07/29/safely-remove-datanode</id>
    <content type="html"><![CDATA[<p>hadoop默认就有冗余（dfs.replication）的机制，所以一般情况下，一台机器挂了也没所谓。集群会自动的进行复制均衡处理。</p>

<p>作为测试，如果dfs.replication设置为1的情况下，怎么安全的把datanode节点服务关闭呢？例如说，刚刚开始搭建环境是把namenode、datanode放在一台机器上，后面增加了机器如何把datanode分离出来呢？</p>

<p>借助于<strong>dfs.hosts.exclude</strong>即可完成顺序的完成此项任务。</p>

<p>修改hdfs-site.xml配置。我操作的时刻仅修改了master1上的hdfs-site.xml。把<strong>master1</strong>值写入到对应的文件中。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop]$ cat hdfs-site.xml 
</span><span class='line'>...
</span><span class='line'>&lt;property&gt;
</span><span class='line'>        &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;
</span><span class='line'>        &lt;value&gt;/home/hadoop/hadoop-2.2.0/etc/hadoop/exclude&lt;/value&gt;
</span><span class='line'>&lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span><span class='line'>[hadoop@master1 hadoop]$ cat /home/hadoop/hadoop-2.2.0/etc/hadoop/exclude
</span><span class='line'>master1
</span></code></pre></td></tr></table></div></figure>


<p>修改完成后，刷新节点即可(完全没有必要重启集dfs)。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hadoop dfsadmin -refreshNodes</span></code></pre></td></tr></table></div></figure>


<p>可以通过<code>dfsadmin -report</code>或者网页查看master1已经变成<em>Decommission In Progress</em>了。</p>

<p><img src="http://file.bmob.cn/M00/05/4C/wKhkA1PXUMOAVvvWAAED6CN-3Rg187.png" alt="" /></p>

<p>注：</p>

<p>问题一： 在新建节点是slaver1的防火墙没关闭，由于master1已经被exclude，而slaver1不能提供服务，上传文件时报错：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop]$ hadoop fs -put slaves  /
</span><span class='line'>14/07/29 15:18:21 WARN hdfs.DFSClient: DataStreamer Exception
</span><span class='line'>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /slaves._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.
</span><span class='line'>        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1384)
</span><span class='line'>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2477)</span></code></pre></td></tr></table></div></figure>


<p>关闭防火墙一样再次上传，还是报同样的错误。此时，也可以通过刷新节点<code>hadoop dfsadmin -refreshNodes</code>来解决。</p>

<p>问题二： 设置备份数量</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 hadoop]$ hadoop fs -setrep 3 /slaves 
</span><span class='line'>Replication 3 set: /slaves</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>问题三： 新增节点</p>

<p>拷贝程序到新增节点，然后启动</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[hadoop@master1 ~]$ tar zc hadoop-2.2.0 --exclude=logs | ssh slaver2 'cat | tar zx'
</span><span class='line'>
</span><span class='line'>[hadoop@slaver2 ~]$ cd hadoop-2.2.0/
</span><span class='line'>[hadoop@slaver2 hadoop-2.2.0]$ sbin/hadoop-daemon.sh start datanode</span></code></pre></td></tr></table></div></figure>


<p>也可以修改master上的slavers文件再<code>sbin/start-dfs.sh</code>启动。</p>
]]></content>
  </entry>
  
</feed>
